{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fa08bc",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f0726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796a15",
   "metadata": {},
   "source": [
    "### Directories and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ef93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Data directory found at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data\n",
      "Raw manifest output directory set to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/july-8-manifests-raw\n"
     ]
    }
   ],
   "source": [
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "MANIFEST_OUTPUT_DIR = DATA_DIR / \"july-8-manifests-raw\"\n",
    "SAMPLE_MANIFEST_DIR = DATA_DIR / \"july-7-sample-manifests\"\n",
    "TIER_OUTPUT_DIRS = {f\"tier{i}\": MANIFEST_OUTPUT_DIR / f\"tier{i}\" for i in range(1, 6)}\n",
    "TIER_SAMPLE_DIRS = {f\"tier{i}\": SAMPLE_MANIFEST_DIR / f\"tier{i}\" for i in range(1, 6)}\n",
    "\n",
    "# Make the directory for the tier if it doesn't exist\n",
    "for tier_dir in TIER_OUTPUT_DIRS.values():\n",
    "    tier_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tier_dir in TIER_SAMPLE_DIRS.values():\n",
    "    tier_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root found at: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory found at: {DATA_DIR}\")\n",
    "print(f\"Raw manifest output directory set to: {MANIFEST_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f7330",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a395125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GSM8K dataset\n",
    "GSM8K_TRAIN = load_dataset(\"gsm8k\", \"main\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb1210",
   "metadata": {},
   "source": [
    "### Generating tier lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ae8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tier1     : 2767 samples\n",
      "tier2     : 837 samples\n",
      "tier3     : 3113 samples\n",
      "tier4     : 544 samples\n",
      "tier5     : 212 samples\n",
      "Total     : 7473 samples\n"
     ]
    }
   ],
   "source": [
    "def has_computational_division(solution_text: str) -> bool:\n",
    "    \"\"\"Returns True if a '/' is followed by optional whitespace and then a digit.\"\"\"\n",
    "    pattern = re.compile(r'/\\s*\\d')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def has_float(solution_text: str) -> bool:\n",
    "    \"\"\"Returns True if the solution text contains a floating-point number.\"\"\"\n",
    "    pattern = re.compile(r'(?<!\\d)\\.\\d+|\\d+\\.\\d+')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def is_symbolic(solution_text: str) -> bool:\n",
    "    \"\"\"Returns True if the solution contains a symbolic reasoning line (Let @ ...).\"\"\"\n",
    "    pattern = re.compile(r'^Let [a-zA-Z] ', re.MULTILINE)\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def mutually_disjoint_tiers(dataset):\n",
    "    tiers = {}\n",
    "    symbolic_set = set(\n",
    "        idx for idx, sample in enumerate(dataset)\n",
    "        if is_symbolic(sample.get(\"answer\", \"\"))\n",
    "    )\n",
    "    non_symbolic_indices = [\n",
    "        idx for idx in range(len(dataset)) if idx not in symbolic_set\n",
    "    ]\n",
    "\n",
    "    # Tier 1: Only integer arithmetic (no floats, no computational division)\n",
    "    tiers[\"tier1\"] = sorted([\n",
    "        idx for idx in non_symbolic_indices\n",
    "        if not has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))\n",
    "    ])\n",
    "\n",
    "    # Tier 2: Float arithmetic, no computational division\n",
    "    tiers[\"tier2\"] = sorted([\n",
    "        idx for idx in non_symbolic_indices\n",
    "        if has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))\n",
    "    ])\n",
    "\n",
    "    # Tier 3: Computational division, no floats\n",
    "    tiers[\"tier3\"] = sorted([\n",
    "        idx for idx in non_symbolic_indices\n",
    "        if not has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))\n",
    "    ])\n",
    "\n",
    "    # Tier 4: Both floats and computational division\n",
    "    tiers[\"tier4\"] = sorted([\n",
    "        idx for idx in non_symbolic_indices\n",
    "        if has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))\n",
    "    ])\n",
    "\n",
    "    # Tier 5: Symbolic reasoning (Let @ ...)\n",
    "    tiers[\"tier5\"] = sorted(symbolic_set)\n",
    "\n",
    "    return tiers\n",
    "\n",
    "TIER_LISTS = mutually_disjoint_tiers(GSM8K_TRAIN)\n",
    "\n",
    "# Display the number of samples in each tier\n",
    "for tier, indices in TIER_LISTS.items():\n",
    "    print(f\"{tier:<10}: {len(indices)} samples\")\n",
    "print(f\"{'Total':<10}: {len(GSM8K_TRAIN)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf966b",
   "metadata": {},
   "source": [
    "### Load the system prompt and static prefixes for each tier's user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e6ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a data formalization expert who excels in mathematical reasoning and writing python code. You will be presented with a math word problem accompanied by a step-by-step natural language solution. You goal is to carefully and meticulously analyze the given question and solution, and formalize it by converting it into a structured json object that deconstructs the logic of the solution.\n",
    "\n",
    "You MUST follow all rules and formatting instructions provided in the user prompt without deviation. Your entire output MUST be a single JSON object wrapped in ```json ... ```. Do not include any text or explanation before or after the JSON object.\"\"\"\n",
    "\n",
    "STATIC_PREFIXES = {}\n",
    "for tier in TIER_LISTS.keys():\n",
    "    prefix_file = TIER_SAMPLE_DIRS[tier] / f\"{tier}_user_prompt_prefix.txt\"\n",
    "    with open(prefix_file, 'r', encoding='utf-8') as f:\n",
    "        STATIC_PREFIXES[tier] = f.read()\n",
    "\n",
    "# Display the prefix for tier 1\n",
    "# print(\"Static prefix for tier 1:\")\n",
    "# print(\"=\"*50,\"\\n\")\n",
    "# print(STATIC_PREFIXES[\"tier1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754bea1",
   "metadata": {},
   "source": [
    "### A simple function to append a chosen sample to a user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a1deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt for tier 1, index 4 :\n",
      "\n",
      "In the TASK below, you will be given a math problem and its corresponding step-by-step solution. Each step in the solution is numbered (e.g. \"L1\", \"L2\" and so on), and many of the steps include calculator annotations (e.g. \"<<20*0.1=2>>\"). Your goal is to convert this information into a structured JSON object according to the following schema and detailed instructions.\n",
      "\n",
      "# JSON Schema Definition\n",
      "\n",
      "Your output must adhere to the following JSON structure:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"A single string containing a complete, self-contained Python function that constitutes an end-to-end formalization of the solution.\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"The line number from the original solution (e.g., 'L1', 'L2').\",\n",
      "      \"question_inputs\": \"A (possibly empty) list of variable names with values extracted from the question text, used for the first time in this step.\",\n",
      "      \"WK_inputs\": \"A (possibly empty) list of variable names with values coming from 'world knowledge' (e.g., minutes_per_hour), used for the first time in this step.\",\n",
      "      \"output_variable\": \"The name of the variable being assigned as the result of the main computation in this step.\",\n",
      "      \"solution_line_template\": \"The complete original line from the solution, including the calculator annotation, with all computational numbers replaced by `{variable_name}` placeholders.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# Detailed Field Instructions\n",
      "\n",
      "## 1. \"function_code\"\n",
      "\n",
      "This string must contain a Python function with the following characteristics:\n",
      "\n",
      "*   **1.A. No Imports:** You should not have ANY imports. The very first line MUST be the function definition (i.e. `def solve():`).\n",
      "*   **1.B. Function Naming & Docstring:** The function must be named `solve`, and it should not have any args. It must begin with a docstring that has exactly two lines:\n",
      "    *   **1.B.i.** The first line must be: \"Index: [Index].\" using the index from the task header.\n",
      "    *   **1.B.ii.** The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
      "*   **1.C. Line comments:** For each solution line that is used to compute the final answer, include a comment of the form `# L1`, `# L2` and so on, which references the line number.\n",
      "    *   **1.C.i.** Such a comment must immediately be followed by a code block that precisely formalizes the corresponding solution line. More details about code blocks are provided in 1.D below.\n",
      "    *   **1.C.ii.** If a solution line does not contain any computation relevant to the final answer, then omit it completely from the function code and do NOT add a corresponding line comment.\n",
      "*   **1.D. Code blocks:** Each code block constitutes a complete formalization of its corresponding solution line. It must consist of the following:\n",
      "    *   **1.D.i. Input Variables** First, define any NEW variables needed for the computation, i.e. that will be used for the first time in the solution. Each input variable MUST be followed by a comment (`#`) in the same line. These variables fall into two categories:\n",
      "        *   \"question_inputs\": These are variables whose values are stated in or can be extracted from the question text (only the question text, NOT the answer text). The comment for these variables should quote or refer to the phrase in the question text from which it is extracted.\n",
      "        *   \"WK_inputs\": These are variables drawn from common-sense \"World Knowledge\" (e.g. `minutes_per_hour = 60`, `dozen = 12`). The comment for these variables MUST simply say `# WK`.\n",
      "    *   **1.D.ii. Output Variables** Second, there should be EXACTLY ONE line of code which formalizes the computation in the solution line and assigns the resulting value to a new variable (this is the \"output_variable\" field).\n",
      "*   **1.E. The Direct Substitution Rule:** This is the MOST IMPORTANT RULE, which ensures that the \"solution_line_template\" is purely identical to the original solution line except that numerical values in computations have been replaced with variable placeholders: You MUST define variables in such a way that they can be DIRECTLY SUBSTITUTED into the solution line without changing any operators or surrounding text in the line. \n",
      "*   **1.F. Final Answer:** The line that assigns the final result to the `answer` variable must be immediately preceded by a line containing only the comment `# FA`. The last line of the function must always return the `answer` variable.\n",
      "\n",
      "## 2. \"solution_line_template\"\n",
      "\n",
      "*   **2.A.** The template should be EXACTLY identical to the original solution line, with the ONLY CHANGES being that every NUMERICAL value used in a computation is replaced by its corresponding `{variable_name}` placeholder. This applies to the entire content of the solution line, including the inside and outside of the calculator annotations.\n",
      "*   **2.B.** In particular, EVERY SINGLE numerical value appearing inside the calculator annotation (`<<..>>`) MUST be replaced with a `{variable_name}` placeholder.\n",
      "*   **2.C.** Note: some quantities may appear as words in the solution line (e.g. \"twice as many\"). Do NOT attempt to replace these with variable name placeholders.\n",
      "*   **2.D.** The Direct Substitution Rule will ensure that for correctly defined variables, it will be possible to replace the numerical values with variable name placeholders while leaving all surrounding text, symbols, and operators unchanged. Thus, in a correct \"solution_line_template\", the calculator annotation will not contain any numerical values, and moreover, replacing each `{variable_name}` by its value should exactly recover the original solution line, including the original calculator annotation.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Given below are three examples that illustrate what a perfect formalization will look like. For each example, you are given the following:\n",
      "\n",
      "*   Input: consisting of an index, question, and solution mapping. \n",
      "*   Output: complete output, wrapped inside ```json .. ```\n",
      "\n",
      "In all examples, you will observe the following:\n",
      "*   A rigid adherence to the Direct Substitution Rule (1.E), resulting in a solution_line_template that perfectly satisfies properties 2.A - 2.D. For instance, in Example 1 below, \"two dozen\" is not formalized as a stand-alone variable because the computation in \"L1\" and \"L4\" treat \"two\" and \"dozen\" as separate variables.\n",
      "*   The function_code only uses solution lines with a relevant computation (rule 1.C.ii), and omits lines that are irrelevant (e.g. \"L1\" is ommitted in Example 3 below).\n",
      "*   Code blocks that are precisely formatted as in 1.D, consisting of line comments, then input variable declarations, then a single output variable assignment (i.e. the single computation for that solution line).\n",
      "*   A careful distinction between numerical values and word-based quantities in the solution_line_template. Per rule 2.C, only numerical values like 2 are replaced with placeholders, and word-based quantities are not, even though they are formalized as variables in the function_code (For example, in \"L2\" of Example 2, the word \"Twice\" remains in the solution_line_template).\n",
      "*   Precise categorization of input variables as described in rule 1.D.i. Note how `dozen = 12` is a WK_input (common knowledge), while `father_ate = 8` is a question_input (explicitly stated in the problem text).\n",
      "*   Strict adherence to defining only NEW variables in each step's question_inputs and WK_inputs lists. Observe that once a variable like `days_in_week` is defined in one step, it is simply re-used in later calculations without being re-defined or listed as an input again.\n",
      "*   Comments for question_inputs must cite the question text only, NEVER the solution text. For instance, in Example 3, even though \"60 minutes in an hour\" is stated in L1 of the solution, the variable `minutes_per_hour` is correctly labeled # WK because that fact is not present in the question text, and the number of minutes in an hour constitutes common-sense World Knowledge.\n",
      "\n",
      "## Example 1\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "3946\n",
      "\n",
      "**Question:**\n",
      "Mother made 2 dozen brownies and placed them on the kitchen counter to cool.  Father smelled the brownies, came into the kitchen and ate 8 of them. Then, their daughter, Mooney, wandered into the kitchen and ate 4 of the brownies. The next morning, Mother made another two dozen brownies and added them to those remaining from the day before.  After that, how many brownies were on the counter?\n",
      "\n",
      "**Solution mapping:**\n",
      "{'L1': 'Two dozen brownies is 2 * 12 = <<2*12=24>>24 brownies.', 'L2': 'After Father ate his 8, there were 24 - 8 = <<24-8=16>>16 brownies remaining on the counter.', 'L3': 'After Mooney ate her 4, there were 16 - 4 = <<16-4=12>>12 brownies remaining on the counter.', 'L4': 'Mother made a second batch of two-dozen brownies, or 2 * 12 = <<2*12=24>>24 brownies.', 'L5': 'After Mother added the second two-dozen, there were 12 + 24 = <<12+24=36>>36 brownies on the kitchen counter.'}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"def solve():\\n    \\\"\\\"\\\"Index: 3946.\\n    Returns: the number of brownies on the counter at the end.\\n    \\\"\\\"\\\"\\n    # L1\\n    num_dozen_initial = 2 # 2 dozen brownies\\n    dozen = 12 # WK\\n    initial_brownies = num_dozen_initial * dozen\\n\\n    # L2\\n    father_ate = 8 # ate 8 of them\\n    after_father = initial_brownies - father_ate\\n\\n    # L3\\n    mooney_ate = 4 # ate 4 of the brownies\\n    after_mooney = after_father - mooney_ate\\n\\n    # L4\\n    num_dozen_second_batch = 2 # another two dozen brownies\\n    second_batch = num_dozen_second_batch * dozen\\n\\n    # L5\\n    total_brownies = after_mooney + second_batch\\n\\n    # FA\\n    answer = total_brownies\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"num_dozen_initial\"],\n",
      "      \"WK_inputs\": [\"dozen\"],\n",
      "      \"output_variable\": \"initial_brownies\",\n",
      "      \"solution_line_template\": \"Two dozen brownies is {num_dozen_initial} * {dozen} = <<{num_dozen_initial}*{dozen}={initial_brownies}>>{initial_brownies} brownies.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [\"father_ate\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"after_father\",\n",
      "      \"solution_line_template\": \"After Father ate his {father_ate}, there were {initial_brownies} - {father_ate} = <<{initial_brownies}-{father_ate}={after_father}>>{after_father} brownies remaining on the counter.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"mooney_ate\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"after_mooney\",\n",
      "      \"solution_line_template\": \"After Mooney ate her {mooney_ate}, there were {after_father} - {mooney_ate} = <<{after_father}-{mooney_ate}={after_mooney}>>{after_mooney} brownies remaining on the counter.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [\"num_dozen_second_batch\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"second_batch\",\n",
      "      \"solution_line_template\": \"Mother made a second batch of two-dozen brownies, or {num_dozen_second_batch} * {dozen} = <<{num_dozen_second_batch}*{dozen}={second_batch}>>{second_batch} brownies.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L5\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"total_brownies\",\n",
      "      \"solution_line_template\": \"After Mother added the second two-dozen, there were {after_mooney} + {second_batch} = <<{after_mooney}+{second_batch}={total_brownies}>>{total_brownies} brownies on the kitchen counter.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Example 2\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "4258\n",
      "\n",
      "**Question:**\n",
      "Cowboy Mickey and cowgirl Minnie train horses for a living.  On average, Mickey mounts six less than twice as many horses per day as does Minnie,  while Minnie mounts three more horses per day than there are days in a week.  How many horses does Mickey Mount per week?\n",
      "\n",
      "**Solution mapping:**\n",
      "{'L1': 'If Minnie mounts three more horses per day than there are days in a week, then Minnie mounts 7+3=<<3+7=10>>10 horses per day.', 'L2': 'Twice as many horses per day as Minnie mounts is 10*2=<<10*2=20>>20 horses per day.', 'L3': 'If Mickey mounts six less than twice as many horses per day as does Minnie, then Mickey mounts 20-6=<<20-6=14>>14 horses per day.', 'L4': 'Since there are 7 days in a week, then Mickey mounts 7*14=<<7*14=98>>98 horses per week.'}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"def solve():\\n    \\\"\\\"\\\"Index: 4258.\\n    Returns: the number of horses Mickey mounts per week.\\n    \\\"\\\"\\\"\\n    # L1\\n    days_in_week = 7 # WK\\n    minnie_more_than_week = 3 # three more horses per day than there are days in a week\\n    minnie_per_day = days_in_week + minnie_more_than_week\\n\\n    # L2\\n    multiplier_for_twice = 2 # twice as many horses\\n    twice_minnie = minnie_per_day * multiplier_for_twice\\n\\n    # L3\\n    mickey_less_than_twice_minnie = 6 # six less than twice as many horses\\n    mickey_per_day = twice_minnie - mickey_less_than_twice_minnie\\n\\n    # L4\\n    mickey_per_week = mickey_per_day * days_in_week\\n\\n    # FA\\n    answer = mickey_per_week\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"minnie_more_than_week\"],\n",
      "      \"WK_inputs\": [\"days_in_week\"],\n",
      "      \"output_variable\": \"minnie_per_day\",\n",
      "      \"solution_line_template\": \"If Minnie mounts three more horses per day than there are days in a week, then Minnie mounts {days_in_week}+{minnie_more_than_week}=<<{days_in_week}+{minnie_more_than_week}={minnie_per_day}>>{minnie_per_day} horses per day.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [\"multiplier_for_twice\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"twice_minnie\",\n",
      "      \"solution_line_template\": \"Twice as many horses per day as Minnie mounts is {minnie_per_day}*{multiplier_for_twice}=<<{minnie_per_day}*{multiplier_for_twice}={twice_minnie}>>{twice_minnie} horses per day.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"mickey_less_than_twice_minnie\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"mickey_per_day\",\n",
      "      \"solution_line_template\": \"If Mickey mounts six less than twice as many horses per day as does Minnie, then Mickey mounts {twice_minnie}-{mickey_less_than_twice_minnie}=<<{twice_minnie}-{mickey_less_than_twice_minnie}={mickey_per_day}>>{mickey_per_day} horses per day.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"mickey_per_week\",\n",
      "      \"solution_line_template\": \"Since there are 7 days in a week, then Mickey mounts {days_in_week}*{mickey_per_day}=<<{days_in_week}*{mickey_per_day}={mickey_per_week}>>{mickey_per_week} horses per week.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Example 3\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "1372\n",
      "\n",
      "**Question:**\n",
      "Micah can type 20 words per minute and Isaiah can type 40 words per minute. How many more words can Isaiah type than Micah in an hour?\n",
      "\n",
      "**Solution mapping:**\n",
      "{'L1': 'There are 60 minutes in an hour.', 'L2': 'Micah can type 20 x 60 = <<20*60=1200>>1200 words in an hour.', 'L3': 'Isaiah can type 40 x 60 = <<40*60=2400>>2400 words in an hour.', 'L4': 'Isaiah can type 2400 - 1200 = <<2400-1200=1200>>1200 words more than Micah in an hour.'}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"def solve():\\n    \\\"\\\"\\\"Index: 1372.\\n    Returns: how many more words Isaiah can type than Micah in an hour.\\n    \\\"\\\"\\\"\\n    # L2\\n    micah_wpm = 20 # 20 words per minute\\n    minutes_per_hour = 60 # WK\\n    micah_wph = micah_wpm * minutes_per_hour\\n\\n    # L3\\n    isaiah_wpm = 40 # 40 words per minute\\n    isaiah_wph = isaiah_wpm * minutes_per_hour\\n\\n    # L4\\n    difference_in_words = isaiah_wph - micah_wph\\n\\n    # FA\\n    answer = difference_in_words\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [\"micah_wpm\"],\n",
      "      \"WK_inputs\": [\"minutes_per_hour\"],\n",
      "      \"output_variable\": \"micah_wph\",\n",
      "      \"solution_line_template\": \"Micah can type {micah_wpm} x {minutes_per_hour} = <<{micah_wpm}*{minutes_per_hour}={micah_wph}>>{micah_wph} words in an hour.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"isaiah_wpm\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"isaiah_wph\",\n",
      "      \"solution_line_template\": \"Isaiah can type {isaiah_wpm} x {minutes_per_hour} = <<{isaiah_wpm}*{minutes_per_hour}={isaiah_wph}>>{isaiah_wph} words in an hour.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"difference_in_words\",\n",
      "      \"solution_line_template\": \"Isaiah can type {isaiah_wph} - {micah_wph} = <<{isaiah_wph}-{micah_wph}={difference_in_words}>>{difference_in_words} words more than Micah in an hour.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "# TASK\n",
      "\n",
      "## Input\n",
      "\n",
      "**Index:**:\n",
      "4\n",
      "\n",
      "**Question:**:\n",
      "James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
      "\n",
      "**Solution mapping:**:\n",
      "{'L1': 'He writes each friend 3*2=<<3*2=6>>6 pages a week', 'L2': 'So he writes 6*2=<<6*2=12>>12 pages every week', 'L3': 'That means he writes 12*52=<<12*52=624>>624 pages a year'}\n",
      "\n",
      "## Output\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_solution_mapping(\n",
    "        index: int, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "        exclude_FA: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Extracts the natural language solution for a given problem index,\n",
    "    cleans it, and structures it into a line-numbered dictionary.\n",
    "    \"\"\"\n",
    "    solution_mapping = {}\n",
    "    solution_text = dataset[index][\"answer\"]\n",
    "    lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "\n",
    "    # Improved regex to handle commas in the final answer\n",
    "    if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "        solution_mapping[\"FA\"] = lines.pop(-1).strip()\n",
    "\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        solution_mapping[f\"L{i}\"] = line\n",
    "\n",
    "    if exclude_FA and \"FA\" in solution_mapping:\n",
    "        del solution_mapping[\"FA\"]\n",
    "\n",
    "    return solution_mapping\n",
    "\n",
    "def append_sample_to_user_prompt(\n",
    "        tier: str, \n",
    "        index: int, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Appends a chosen sample from the GSM8K dataset to the user prompt for a specific tier. Returns the complete user prompt, ready to be sent to the LLM for manifest generation.\n",
    "    \"\"\"\n",
    "    if tier not in TIER_LISTS:\n",
    "        raise ValueError(f\"Invalid tier: {tier}. Must be one of {list(TIER_LISTS.keys())}.\")\n",
    "\n",
    "    sample = dataset[index]\n",
    "    question = sample['question']\n",
    "    answer = build_solution_mapping(index, dataset)\n",
    "\n",
    "    task_block = f\"\"\"## Input\n",
    "\n",
    "**Index:**:\n",
    "{index}\n",
    "\n",
    "**Question:**:\n",
    "{question}\n",
    "\n",
    "**Solution mapping:**:\n",
    "{answer}\n",
    "\n",
    "## Output\n",
    "\n",
    "\"\"\"\n",
    "    return STATIC_PREFIXES[tier] + task_block\n",
    "\n",
    "# Example usage\n",
    "idx = TIER_LISTS[\"tier1\"][0]  # Get the first index from tier 1\n",
    "user_prompt = append_sample_to_user_prompt(\"tier1\", idx, GSM8K_TRAIN)\n",
    "print(\"User prompt for tier 1, index\", idx, \":\\n\")\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1604a7f",
   "metadata": {},
   "source": [
    "### API Clients, Concurrency Limits, Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c88c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n",
      "API clients initialized successfully.\n",
      "API concurrency limits set to: {'google': 2, 'anthropic': 2, 'openai': 2}\n",
      "Available models: ['openai_gpt-4.1', 'google_gemini-2.5-flash']\n"
     ]
    }
   ],
   "source": [
    "# Imports for API clients and related functionality\n",
    "import os\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# This must be done once per kernel to allow asyncio to run in a Jupyter notebook..\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load API Keys from .env file\n",
    "load_dotenv()\n",
    "print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "# Initialize Asynchronous API Clients\n",
    "try:\n",
    "    openai_client_async = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    anthropic_client_async = AsyncClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    print(\"API clients initialized successfully.\")\n",
    "except TypeError:\n",
    "    print(\"API key not found for one or more services. Please check your .env file.\")\n",
    "    # Assign None to prevent errors in subsequent cells\n",
    "    openai_client_async = None\n",
    "    anthropic_client_async = None\n",
    "\n",
    "# Define API Concurrency Limits to prevent 429 \"Too Many Requests\" errors.\n",
    "API_CONCURRENCY_LIMITS = {\n",
    "    \"google\": 2,    \n",
    "    \"anthropic\": 2, \n",
    "    \"openai\": 2,    \n",
    "}\n",
    "print(f\"API concurrency limits set to: {API_CONCURRENCY_LIMITS}\")\n",
    "\n",
    "MODEL_DICT = {\n",
    "  \"openai\": \"gpt-4.1\",\n",
    "  \"google\": \"gemini-2.5-flash\"\n",
    "}\n",
    "\n",
    "MODELS = [f\"{provider}_{model}\" for provider, model in MODEL_DICT.items()]\n",
    "print(f\"Available models: {MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993c661",
   "metadata": {},
   "source": [
    "### Main code for making API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c3cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "# These two helpers are generic and can be used by any provider's function.\n",
    "\n",
    "_anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": time.monotonic() + 60}\n",
    "\n",
    "async def _anthropic_throttle(tokens_needed: int):\n",
    "    # (Code for this function is unchanged)\n",
    "    global _anthropic_bucket\n",
    "    while True:\n",
    "        now = time.monotonic()\n",
    "        if now >= _anthropic_bucket[\"reset_at\"]:\n",
    "            _anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": now + 60}\n",
    "        if tokens_needed <= _anthropic_bucket[\"tokens\"]:\n",
    "            _anthropic_bucket[\"tokens\"] -= tokens_needed\n",
    "            return\n",
    "        else:\n",
    "            to_sleep = _anthropic_bucket[\"reset_at\"] - now\n",
    "            await asyncio.sleep(max(to_sleep, 0.01))\n",
    "\n",
    "async def with_api_retries(\n",
    "        send_coroutine_factory,\n",
    "        *,\n",
    "        model_info: str,  # For informative logging\n",
    "        max_attempts: int = 10,\n",
    "        base_wait_seconds: int = 10  # Increased from 5\n",
    "    ):\n",
    "    \"\"\"A wrapper to handle API retries with exponential backoff.\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return await send_coroutine_factory()\n",
    "        except (openai.RateLimitError, anthropic.RateLimitError, Exception) as e:\n",
    "            # Check for specific rate limit error types or a 429 status code in the error string\n",
    "            if isinstance(e, (openai.RateLimitError, anthropic.RateLimitError)) or \"429\" in str(e):\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(f\"❌ Final attempt failed for {model_info}. Giving up.\")\n",
    "                    raise\n",
    "                \n",
    "                # Exponential backoff with jitter\n",
    "                wait_time = base_wait_seconds * (2 ** attempt) + random.uniform(0, 1)\n",
    "                \n",
    "                # More informative error message\n",
    "                print(f\"🕒 Rate limit on {model_info}. Retrying in {wait_time:.2f}s... (Attempt {attempt + 1}/{max_attempts})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                # If it's not a rate limit error, re-raise immediately\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "# --- 2. Provider-Specific API Calling Functions ---\n",
    "\n",
    "async def call_openai_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int  # Pass index for logging\n",
    "    ):\n",
    "    \"\"\"Handles an API call to OpenAI.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    model_info = f\"{model} (Index {index})\" # Create info string for logger\n",
    "    \n",
    "    response = await with_api_retries(\n",
    "        lambda: openai_client_async.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            temperature=0, \n",
    "            max_tokens=4000, \n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        ),\n",
    "        model_info=model_info # Pass info to the retry wrapper\n",
    "    )\n",
    "    \n",
    "    text_response = response.choices[0].message.content\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.prompt_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.completion_tokens\n",
    "        if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details:\n",
    "             usage[\"cached_tokens\"] = response.usage.prompt_tokens_details.get(\"cached_tokens\", 0)\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_google_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int  # Pass index for logging\n",
    "    ):\n",
    "    \"\"\"Handles an API call to Google.\"\"\"\n",
    "    gemini = genai.GenerativeModel(model_name=model, system_instruction=system_prompt)\n",
    "    cfg = genai.types.GenerationConfig(temperature=0, max_output_tokens=4000)\n",
    "    model_info = f\"{model} (Index {index})\" # Create info string for logger\n",
    "\n",
    "    response = await with_api_retries(\n",
    "        lambda: gemini.generate_content_async(user_prompt, generation_config=cfg),\n",
    "        model_info=model_info # Pass info to the retry wrapper\n",
    "    )\n",
    "\n",
    "    text_response = response.text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage_metadata:\n",
    "        usage[\"input_tokens\"] = response.usage_metadata.prompt_token_count\n",
    "        usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_anthropic_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int  # Pass index for logging\n",
    "    ):\n",
    "    \"\"\"Handles an API call to Anthropic, including prompt caching.\"\"\"\n",
    "    system_block = {\"type\": \"text\", \"text\": system_prompt, \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "    model_info = f\"{model} (Index {index})\" # Create info string for logger\n",
    "    \n",
    "    est_tokens = math.ceil(1.2 * len(system_prompt.split()))\n",
    "    await _anthropic_throttle(est_tokens)\n",
    "\n",
    "    response = await with_api_retries(\n",
    "        lambda: anthropic_client_async.messages.create(\n",
    "            model=model, max_tokens=4000, temperature=0,\n",
    "            system=[system_block], messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        ),\n",
    "        model_info=model_info # Pass info to the retry wrapper\n",
    "    )\n",
    "\n",
    "    text_response = response.content[0].text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.input_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.output_tokens\n",
    "        usage[\"cached_tokens\"] = response.usage.cache_read_input_tokens if response.usage.cache_read_input_tokens else 0\n",
    "    return text_response, usage\n",
    "\n",
    "\n",
    "# # --- 2. Per-Problem Concurrent Runner ---\n",
    "# # This function runs API calls for ONE problem concurrently across all models.\n",
    "\n",
    "# async def run_one_problem_concurrently(\n",
    "#     index: int,\n",
    "#     tier: str,\n",
    "#     dataset: 'datasets.Dataset',\n",
    "#     system_prompt: str,\n",
    "#     model_dict: Dict[str, str],\n",
    "#     provider_sems: Dict[str, asyncio.Semaphore],\n",
    "#     output_dir: Path\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Generates manifests for a single problem by calling all model APIs concurrently.\n",
    "#     \"\"\"\n",
    "#     user_prompt = append_sample_to_user_prompt(\n",
    "#         tier=tier,\n",
    "#         index=index,\n",
    "#         dataset=dataset\n",
    "#     )\n",
    "    \n",
    "#     tasks = []\n",
    "#     # Create concurrent tasks for each model\n",
    "#     for provider, model in model_dict.items():\n",
    "#         async with provider_sems[provider]: # Acquire semaphore before creating the task\n",
    "#             if provider == \"openai\":\n",
    "#                 coro = call_openai_async(model, system_prompt, user_prompt, index)\n",
    "#             elif provider == \"google\":\n",
    "#                 coro = call_google_async(model, system_prompt, user_prompt, index)\n",
    "#             elif provider == \"anthropic\":\n",
    "#                 coro = call_anthropic_async(model, system_prompt, user_prompt, index)\n",
    "#             else:\n",
    "#                 async def unknown_provider(): raise ValueError(f\"Unknown provider: {provider}\")\n",
    "#                 coro = unknown_provider()\n",
    "        \n",
    "#             task = asyncio.create_task(coro)\n",
    "#             task.meta = {\n",
    "#                 \"provider\": provider, \"model\": model, \"index\": index, \"start_time\": time.time()\n",
    "#             }\n",
    "#             tasks.append(task)\n",
    "            \n",
    "#     # Wait for all models to finish for this single problem\n",
    "#     task_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "#     problem_results = []\n",
    "#     # Process the results\n",
    "#     for task, result in zip(tasks, task_results):\n",
    "#         meta = task.meta\n",
    "#         elapsed = time.time() - meta[\"start_time\"]\n",
    "#         status = \"Failed\"\n",
    "#         usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "        \n",
    "#         output_path = output_dir / str(meta['index']) / f\"{meta['provider']}_{meta['model']}.txt\"\n",
    "#         output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         if isinstance(result, Exception):\n",
    "#             error_message = f\"--- ERROR ---\\nIndex: {meta['index']}, Model: {meta['model']}\\n{type(result).__name__}: {result}\"\n",
    "#             output_path.write_text(error_message, encoding='utf-8')\n",
    "#             # The retry wrapper already logs, this is for the final failure\n",
    "#             print(f\"❌ FINAL Error for Index {meta['index']}, Model {meta['model']}: {type(result).__name__}\")\n",
    "#         else:\n",
    "#             text_response, usage = result\n",
    "#             output_path.write_text(text_response, encoding='utf-8')\n",
    "#             status = \"Success\"\n",
    "#             print(f\"✅ Success for Index {meta['index']}, Model {meta['model']} in {elapsed:.2f}s\")\n",
    "        \n",
    "#         problem_results.append({\n",
    "#             \"provider\": meta[\"provider\"], \"model\": meta[\"model\"], \"index\": meta[\"index\"],\n",
    "#             \"status\": status, \"time_s\": round(elapsed, 2),\n",
    "#             \"input_tokens\": usage[\"input_tokens\"], \"output_tokens\": usage[\"output_tokens\"],\n",
    "#             \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "#             \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "#         })\n",
    "    \n",
    "#     return problem_results\n",
    "\n",
    "\n",
    "# # --- 3. Main Hybrid Batch Generation Function ---\n",
    "# # This function processes each problem in the list SEQUENTIALLY.\n",
    "\n",
    "# async def generate_manifests_hybrid(\n",
    "#     indices_to_generate: List[int],\n",
    "#     tier: str = \"tier1\",\n",
    "#     dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "#     model_dict: Dict[str, str] = MODEL_DICT,\n",
    "#     system_prompt: str = SYSTEM_PROMPT,\n",
    "#     output_dir: Path = TIER_OUTPUT_DIRS[\"tier1\"],\n",
    "#     concurrency_limits: Dict[str, int] = API_CONCURRENCY_LIMITS\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Runs manifest generation by processing problems sequentially, but running\n",
    "#     model API calls for each problem concurrently.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Starting Manifest Generation (Hybrid: Sequential Problems, Concurrent Models) ---\")\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "#     all_results = []\n",
    "    \n",
    "#     # Process each problem sequentially using a simple for loop\n",
    "#     for index in tqdm(indices_to_generate, desc=f\"Generating Manifests for {tier}\"):\n",
    "#         problem_results = await run_one_problem_concurrently(\n",
    "#             index=index,\n",
    "#             tier=tier,\n",
    "#             dataset=dataset,\n",
    "#             system_prompt=system_prompt,\n",
    "#             model_dict=model_dict,\n",
    "#             provider_sems=provider_semaphores,\n",
    "#             output_dir=output_dir\n",
    "#         )\n",
    "#         all_results.extend(problem_results)\n",
    "\n",
    "#     # Create and save the performance DataFrame\n",
    "#     df = pd.DataFrame(all_results)\n",
    "#     run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "#     df.to_csv(csv_path, index=False)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"\\n--- Manifest Generation Complete ---\")\n",
    "#     print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "#     print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# --- 3. Per-Problem Orchestration (Dispatcher Logic) ---\n",
    "\n",
    "async def run_one_problem_async(\n",
    "    index: int, \n",
    "    tier: str,\n",
    "    dataset: 'datasets.Dataset',\n",
    "    system_prompt: str,\n",
    "    model_dict: Dict[str, str],\n",
    "    provider_sems: Dict[str, asyncio.Semaphore], \n",
    "    output_dir: Path,\n",
    "    pbar: tqdm\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generates manifests for a single problem and returns a list of result dictionaries.\n",
    "    \"\"\"\n",
    "    user_prompt = append_sample_to_user_prompt(\n",
    "        tier=tier,\n",
    "        index=index,\n",
    "        dataset=dataset\n",
    "    )\n",
    "    \n",
    "    problem_results = []\n",
    "    \n",
    "    tasks = []\n",
    "    for provider, model in model_dict.items():\n",
    "        async with provider_sems[provider]: # Acquire semaphore before creating the task\n",
    "            if provider == \"openai\":\n",
    "                coro = call_openai_async(model, system_prompt, user_prompt, index)\n",
    "            elif provider == \"google\":\n",
    "                coro = call_google_async(model, system_prompt, user_prompt, index)\n",
    "            elif provider == \"anthropic\":\n",
    "                coro = call_anthropic_async(model, system_prompt, user_prompt, index)\n",
    "            else:\n",
    "                # Create a coroutine that will immediately raise an error\n",
    "                async def unknown_provider(): raise ValueError(f\"Unknown provider: {provider}\")\n",
    "                coro = unknown_provider()\n",
    "        \n",
    "            task = asyncio.create_task(coro)\n",
    "            task.meta = {\n",
    "                \"provider\": provider, \n",
    "                \"model\": model, \n",
    "                \"index\": index, \n",
    "                \"start_time\": time.time()}\n",
    "            tasks.append(task)\n",
    "        \n",
    "    task_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    for task, result in zip(tasks, task_results):\n",
    "        meta = task.meta\n",
    "        elapsed = time.time() - meta[\"start_time\"]\n",
    "        status = \"Failed\"\n",
    "        usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "        \n",
    "        output_path = output_dir / str(meta['index']) / f\"{meta['provider']}_{meta['model']}.txt\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            error_message = f\"--- ERROR ---\\nIndex: {meta['index']}, Model: {meta['model']}\\n{type(result).__name__}: {result}\"\n",
    "            output_path.write_text(error_message, encoding='utf-8')\n",
    "            print(f\"❌ Error for Index {meta['index']}, Model {meta['model']}: {type(result).__name__}\")\n",
    "        else:\n",
    "            text_response, usage = result\n",
    "            output_path.write_text(text_response, encoding='utf-8')\n",
    "            status = \"Success\"\n",
    "        \n",
    "        # Append to the list of results for this problem\n",
    "        problem_results.append({\n",
    "            \"provider\": meta[\"provider\"], \n",
    "            \"model\": meta[\"model\"], \n",
    "            \"index\": meta[\"index\"],\n",
    "            \"status\": status, \n",
    "            \"time_s\": round(elapsed, 2),\n",
    "            \"input_tokens\": usage[\"input_tokens\"], \"output_tokens\": usage[\"output_tokens\"],\n",
    "            \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "            \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    \n",
    "    pbar.update(1)\n",
    "    # Return result for this problem\n",
    "    return problem_results\n",
    "\n",
    "\n",
    "# --- 4. Main Batch Generation Function ---\n",
    "\n",
    "async def generate_manifests_parallel(\n",
    "    indices_to_generate: List[int],\n",
    "    tier: str = \"tier1\",\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "    model_dict: Dict[str, str] = MODEL_DICT,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    concurrency_limits: Dict[str, int] = API_CONCURRENCY_LIMITS\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the manifest generation process and returns a DataFrame with performance stats.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Manifest Generation ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "\n",
    "    output_dir = TIER_OUTPUT_DIRS[tier]\n",
    "    \n",
    "    with tqdm(total=len(indices_to_generate), desc=\"Generating Manifests\") as pbar:\n",
    "        problem_tasks = [\n",
    "            run_one_problem_async(\n",
    "                index=idx, \n",
    "                tier=tier,\n",
    "                dataset=dataset,\n",
    "                system_prompt=system_prompt,\n",
    "                model_dict=model_dict,\n",
    "                provider_sems=provider_semaphores, \n",
    "                output_dir=output_dir,\n",
    "                pbar=pbar\n",
    "            )\n",
    "            for idx in indices_to_generate\n",
    "        ]\n",
    "        # This will now be a list of lists, e.g., [[results_for_p0], [results_for_p1], ...]\n",
    "        all_results = await asyncio.gather(*problem_tasks)\n",
    "\n",
    "    # Flatten the list of lists into a single list of result dictionaries\n",
    "    flat_results = [item for sublist in all_results for item in sublist]\n",
    "\n",
    "    # Create and save the performance DataFrame\n",
    "    df = pd.DataFrame(flat_results)\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Manifest Generation Complete ---\")\n",
    "    print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffd035",
   "metadata": {},
   "source": [
    "### Running the manifest generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba5fc138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Manifest Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcc6e3e335f4fa4a2fe7184ae381da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Manifests:   0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751997005.543962 11806203 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕒 Rate limit on gpt-4.1 (Index 79). Retrying in 10.89s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 431). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 78). Retrying in 10.53s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 21). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 174). Retrying in 10.30s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 53). Retrying in 10.32s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 294). Retrying in 10.64s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 264). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 208). Retrying in 10.49s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 440). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 396). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 138). Retrying in 10.91s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 142). Retrying in 10.48s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 423). Retrying in 10.02s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 317). Retrying in 10.43s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 418). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 240). Retrying in 10.46s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 65). Retrying in 10.64s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 420). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 298). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 223). Retrying in 10.35s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 190). Retrying in 10.15s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 255). Retrying in 10.29s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 412). Retrying in 10.59s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 433). Retrying in 10.06s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 104). Retrying in 10.33s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 139). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 45). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 243). Retrying in 10.87s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 436). Retrying in 10.11s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 214). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 391). Retrying in 10.24s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 421). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 409). Retrying in 10.78s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 216). Retrying in 10.51s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 283). Retrying in 10.77s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 146). Retrying in 10.48s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 496). Retrying in 10.74s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 163). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 225). Retrying in 10.46s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 235). Retrying in 10.34s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 98). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 213). Retrying in 10.70s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 46). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 415). Retrying in 10.91s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 17). Retrying in 10.55s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 43). Retrying in 10.06s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 196). Retrying in 11.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 147). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 206). Retrying in 10.74s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 342). Retrying in 10.32s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 221). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 22). Retrying in 10.19s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 411). Retrying in 10.77s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 183). Retrying in 10.56s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 75). Retrying in 10.92s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 318). Retrying in 10.26s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 93). Retrying in 10.80s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 350). Retrying in 10.67s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 185). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 114). Retrying in 10.36s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 260). Retrying in 10.55s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 357). Retrying in 10.24s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 217). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 430). Retrying in 10.92s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 355). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 435). Retrying in 10.06s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 134). Retrying in 10.56s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 18). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 272). Retrying in 11.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 324). Retrying in 10.46s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 444). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 337). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 233). Retrying in 10.28s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 279). Retrying in 10.47s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 427). Retrying in 10.78s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 97). Retrying in 10.32s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 446). Retrying in 10.69s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 77). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 88). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 31). Retrying in 10.83s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 207). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 131). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 126). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 28). Retrying in 10.30s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 416). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 84). Retrying in 10.15s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 170). Retrying in 10.48s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 191). Retrying in 10.46s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 401). Retrying in 10.82s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 135). Retrying in 10.82s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 82). Retrying in 10.19s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 376). Retrying in 10.29s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 309). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 103). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 118). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 71). Retrying in 10.42s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 76). Retrying in 10.38s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 311). Retrying in 10.64s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 179). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 209). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 484). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 218). Retrying in 10.67s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 101). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 354). Retrying in 10.27s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 111). Retrying in 10.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 110). Retrying in 10.75s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 186). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 200). Retrying in 10.24s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 448). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 292). Retrying in 10.18s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 41). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 6). Retrying in 10.64s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 59). Retrying in 10.91s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 184). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 392). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 158). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 419). Retrying in 10.49s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 293). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 232). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 426). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 69). Retrying in 10.76s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 325). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 333). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 116). Retrying in 10.38s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 24). Retrying in 10.69s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 60). Retrying in 10.43s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 39). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 288). Retrying in 10.68s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 63). Retrying in 10.25s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 364). Retrying in 10.50s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 351). Retrying in 10.82s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 359). Retrying in 10.98s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 363). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 478). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 490). Retrying in 11.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 475). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 353). Retrying in 10.49s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 454). Retrying in 10.50s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 491). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 472). Retrying in 10.94s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 481). Retrying in 10.75s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 489). Retrying in 10.13s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 465). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 493). Retrying in 10.61s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 494). Retrying in 10.10s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 477). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 458). Retrying in 10.41s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 455). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 125). Retrying in 10.91s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 467). Retrying in 10.77s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 365). Retrying in 10.07s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 356). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 476). Retrying in 10.34s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 366). Retrying in 10.06s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 471). Retrying in 10.59s... (Attempt 1/10)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m INDICES_TO_GENERATE = [idx \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m TIER_LISTS[\u001b[33m\"\u001b[39m\u001b[33mtier1\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx <= UPPER_LIMIT]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Run the manifest generation process\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m perf_df = \u001b[38;5;28;01mawait\u001b[39;00m generate_manifests_parallel(indices_to_generate=INDICES_TO_GENERATE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 382\u001b[39m, in \u001b[36mgenerate_manifests_parallel\u001b[39m\u001b[34m(indices_to_generate, tier, dataset, model_dict, system_prompt, concurrency_limits)\u001b[39m\n\u001b[32m    368\u001b[39m     problem_tasks = [\n\u001b[32m    369\u001b[39m         run_one_problem_async(\n\u001b[32m    370\u001b[39m             index=idx, \n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices_to_generate\n\u001b[32m    380\u001b[39m     ]\n\u001b[32m    381\u001b[39m     \u001b[38;5;66;03m# This will now be a list of lists, e.g., [[results_for_p0], [results_for_p1], ...]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     all_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*problem_tasks)\n\u001b[32m    384\u001b[39m \u001b[38;5;66;03m# Flatten the list of lists into a single list of result dictionaries\u001b[39;00m\n\u001b[32m    385\u001b[39m flat_results = [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m all_results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 310\u001b[39m, in \u001b[36mrun_one_problem_async\u001b[39m\u001b[34m(index, tier, dataset, system_prompt, model_dict, provider_sems, output_dir, pbar)\u001b[39m\n\u001b[32m    303\u001b[39m         task.meta = {\n\u001b[32m    304\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprovider\u001b[39m\u001b[33m\"\u001b[39m: provider, \n\u001b[32m    305\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model, \n\u001b[32m    306\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \n\u001b[32m    307\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstart_time\u001b[39m\u001b[33m\"\u001b[39m: time.time()}\n\u001b[32m    308\u001b[39m         tasks.append(task)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m task_results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tasks, task_results):\n\u001b[32m    313\u001b[39m     meta = task.meta\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Choose the list of indices to generate manifests for\n",
    "UPPER_LIMIT = 500\n",
    "INDICES_TO_GENERATE = [idx for idx in TIER_LISTS[\"tier1\"] if idx <= UPPER_LIMIT]\n",
    "\n",
    "# Run the manifest generation process\n",
    "perf_df = await generate_manifests_parallel(indices_to_generate=INDICES_TO_GENERATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5aa898",
   "metadata": {},
   "source": [
    "### Display the performance DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39481d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>status</th>\n",
       "      <th>time_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>cached_tokens</th>\n",
       "      <th>utc_completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>35.76</td>\n",
       "      <td>4548</td>\n",
       "      <td>453</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>35.76</td>\n",
       "      <td>5115</td>\n",
       "      <td>636</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>6</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.90</td>\n",
       "      <td>4585</td>\n",
       "      <td>473</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>6</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.90</td>\n",
       "      <td>5157</td>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>7</td>\n",
       "      <td>Success</td>\n",
       "      <td>9.82</td>\n",
       "      <td>4671</td>\n",
       "      <td>519</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>7</td>\n",
       "      <td>Success</td>\n",
       "      <td>9.82</td>\n",
       "      <td>5229</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>17</td>\n",
       "      <td>Success</td>\n",
       "      <td>16.91</td>\n",
       "      <td>4755</td>\n",
       "      <td>879</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>17</td>\n",
       "      <td>Success</td>\n",
       "      <td>16.91</td>\n",
       "      <td>5340</td>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>18</td>\n",
       "      <td>Success</td>\n",
       "      <td>10.56</td>\n",
       "      <td>4664</td>\n",
       "      <td>748</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>18</td>\n",
       "      <td>Success</td>\n",
       "      <td>10.56</td>\n",
       "      <td>5224</td>\n",
       "      <td>1124</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>20</td>\n",
       "      <td>Success</td>\n",
       "      <td>8.27</td>\n",
       "      <td>4598</td>\n",
       "      <td>486</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>20</td>\n",
       "      <td>Success</td>\n",
       "      <td>8.27</td>\n",
       "      <td>5168</td>\n",
       "      <td>579</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>21</td>\n",
       "      <td>Success</td>\n",
       "      <td>12.21</td>\n",
       "      <td>4624</td>\n",
       "      <td>591</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>21</td>\n",
       "      <td>Success</td>\n",
       "      <td>12.21</td>\n",
       "      <td>5189</td>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>22</td>\n",
       "      <td>Success</td>\n",
       "      <td>36.37</td>\n",
       "      <td>4590</td>\n",
       "      <td>498</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>22</td>\n",
       "      <td>Success</td>\n",
       "      <td>36.37</td>\n",
       "      <td>5159</td>\n",
       "      <td>611</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:56+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>24</td>\n",
       "      <td>Success</td>\n",
       "      <td>6.47</td>\n",
       "      <td>4577</td>\n",
       "      <td>316</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>24</td>\n",
       "      <td>Success</td>\n",
       "      <td>6.47</td>\n",
       "      <td>5152</td>\n",
       "      <td>433</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>28</td>\n",
       "      <td>Success</td>\n",
       "      <td>6.32</td>\n",
       "      <td>4572</td>\n",
       "      <td>395</td>\n",
       "      <td>4352</td>\n",
       "      <td>2025-07-08T16:22:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>28</td>\n",
       "      <td>Success</td>\n",
       "      <td>6.32</td>\n",
       "      <td>5128</td>\n",
       "      <td>483</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-08T16:22:26+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   provider             model  index   status  time_s  input_tokens  \\\n",
       "0    openai           gpt-4.1      4  Success   35.76          4548   \n",
       "1    google  gemini-2.5-flash      4  Success   35.76          5115   \n",
       "2    openai           gpt-4.1      6  Success    7.90          4585   \n",
       "3    google  gemini-2.5-flash      6  Success    7.90          5157   \n",
       "4    openai           gpt-4.1      7  Success    9.82          4671   \n",
       "5    google  gemini-2.5-flash      7  Success    9.82          5229   \n",
       "6    openai           gpt-4.1     17  Success   16.91          4755   \n",
       "7    google  gemini-2.5-flash     17  Success   16.91          5340   \n",
       "8    openai           gpt-4.1     18  Success   10.56          4664   \n",
       "9    google  gemini-2.5-flash     18  Success   10.56          5224   \n",
       "10   openai           gpt-4.1     20  Success    8.27          4598   \n",
       "11   google  gemini-2.5-flash     20  Success    8.27          5168   \n",
       "12   openai           gpt-4.1     21  Success   12.21          4624   \n",
       "13   google  gemini-2.5-flash     21  Success   12.21          5189   \n",
       "14   openai           gpt-4.1     22  Success   36.37          4590   \n",
       "15   google  gemini-2.5-flash     22  Success   36.37          5159   \n",
       "16   openai           gpt-4.1     24  Success    6.47          4577   \n",
       "17   google  gemini-2.5-flash     24  Success    6.47          5152   \n",
       "18   openai           gpt-4.1     28  Success    6.32          4572   \n",
       "19   google  gemini-2.5-flash     28  Success    6.32          5128   \n",
       "\n",
       "    output_tokens  cached_tokens              utc_completed  \n",
       "0             453           4352  2025-07-08T16:22:55+00:00  \n",
       "1             636              0  2025-07-08T16:22:55+00:00  \n",
       "2             473           4352  2025-07-08T16:22:27+00:00  \n",
       "3             695              0  2025-07-08T16:22:27+00:00  \n",
       "4             519           4352  2025-07-08T16:22:29+00:00  \n",
       "5             697              0  2025-07-08T16:22:29+00:00  \n",
       "6             879           4352  2025-07-08T16:22:36+00:00  \n",
       "7             868              0  2025-07-08T16:22:36+00:00  \n",
       "8             748           4352  2025-07-08T16:22:30+00:00  \n",
       "9            1124              0  2025-07-08T16:22:30+00:00  \n",
       "10            486           4352  2025-07-08T16:22:28+00:00  \n",
       "11            579              0  2025-07-08T16:22:28+00:00  \n",
       "12            591              0  2025-07-08T16:22:32+00:00  \n",
       "13            615              0  2025-07-08T16:22:32+00:00  \n",
       "14            498           4352  2025-07-08T16:22:56+00:00  \n",
       "15            611              0  2025-07-08T16:22:56+00:00  \n",
       "16            316              0  2025-07-08T16:22:26+00:00  \n",
       "17            433              0  2025-07-08T16:22:26+00:00  \n",
       "18            395           4352  2025-07-08T16:22:26+00:00  \n",
       "19            483              0  2025-07-08T16:22:26+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5971e",
   "metadata": {},
   "source": [
    "### Helper function to concatenate manifests (useful for sharing in AI chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatenate_complete_manifests(\n",
    "#         indices: List[int], \n",
    "#         models: List[str], \n",
    "#         manifest_dir: Path, \n",
    "#         dataset: List[Dict[str, Any]]\n",
    "#     ) -> str:\n",
    "#     \"\"\"\n",
    "#     For each index, concatenate the wrapper output and manifest outputs for all models.\n",
    "#     Returns the full text as a string.\n",
    "#     \"\"\"\n",
    "#     def wrapper(index: int):\n",
    "#         sample = dataset[index]\n",
    "#         to_return = \"=\" * 50 + \"\\n\\n\"\n",
    "#         to_return += f\"**Index:** {index}\\n\\n\"\n",
    "#         to_return += \"**Question:**\\n\"\n",
    "#         to_return += f\"{sample['question']}\\n\\n\"\n",
    "#         to_return += \"**Solution mapping:**\\n\"\n",
    "#         to_return += f\"{build_solution_mapping(index, dataset)}\\n\\n\"\n",
    "#         return to_return\n",
    "\n",
    "#     all_text = \"\"\n",
    "#     for index in indices:\n",
    "#         all_text += wrapper(index)\n",
    "#         for model in models:\n",
    "#             filepath = manifest_dir / f\"{index}\" / f\"{model}.txt\"\n",
    "#             with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#                 content = f.read().strip()\n",
    "#             all_text += f\"--- {model} Output for Index {index} ---\\n{content}\\n\\n\"\n",
    "#     return all_text.strip()\n",
    "\n",
    "# # Usage and save to file\n",
    "# initial_manifests_text = concatenate_complete_manifests(\n",
    "#     indices=INDICES_TO_GENERATE, \n",
    "#     models=MODELS, \n",
    "#     manifest_dir=MANIFEST_OUTPUT_DIR, \n",
    "#     dataset=GSM8K_TRAIN\n",
    "# )\n",
    "# with open(MANIFEST_OUTPUT_DIR / \"initial_manifests.txt\", 'w', encoding='utf-8') as f:\n",
    "#     f.write(initial_manifests_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ebfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 61: RPM limit approaching. Waiting 60.00 seconds...\n",
      "Task 51: Request successful. Current Estimated RPM: 51. Response: 51 wasn't like other cats. He appeared one Tuesday...\n",
      "Task 38: Request successful. Current Estimated RPM: 38. Response: 38 was not a name for a cat. It was a serial numbe...\n",
      "Task 48: Request successful. Current Estimated RPM: 48. Response: In the chaotic symphony of a busy animal shelter, ...\n",
      "Task 3: Request successful. Current Estimated RPM: 3. Response: Three wasn't just a number; Three was a cat. A sle...\n",
      "Task 58: Request successful. Current Estimated RPM: 58. Response: The first thing you noticed about 58 was his name....\n",
      "Task 37: Request successful. Current Estimated RPM: 37. Response: In a facility of cool, clinical whites and humming...\n",
      "Task 7: Request successful. Current Estimated RPM: 7. Response: The first light of dawn was always 7's favorite. N...\n",
      "Task 13: Request successful. Current Estimated RPM: 13. Response: The name 13 wasn't given out of malice, but out of...\n",
      "Task 56: Request successful. Current Estimated RPM: 56. Response: The cat’s name was 56. Not Fifty-Six, not Lord 56,...\n",
      "Task 17: Request successful. Current Estimated RPM: 17. Response: Seventeen was a name given to him at the shelter, ...\n",
      "Task 39: Request successful. Current Estimated RPM: 39. Response: 39 was not a name given with love. It was a tag, a...\n",
      "Task 32: Request successful. Current Estimated RPM: 32. Response: In the quiet hum of the animal shelter, Cat Unit 7...\n",
      "Task 10: Request successful. Current Estimated RPM: 10. Response: 10 was a cat who carried his name with a quiet dig...\n",
      "Task 23: Request successful. Current Estimated RPM: 23. Response: 23 wasn't a name, not really. It was a designation...\n",
      "Task 12: Request successful. Current Estimated RPM: 12. Response: The cat was named 12. Not 'Twelve,' just '12.' A t...\n",
      "Task 20: Request successful. Current Estimated RPM: 20. Response: In the echoing quiet of the city’s animal shelter,...\n",
      "Task 60: Request successful. Current Estimated RPM: 60. Response: The cat was named 60. Not because she was the 60th...\n",
      "Task 19: Request successful. Current Estimated RPM: 19. Response: 19 wasn't like other cats. For one, his name wasn'...\n",
      "Task 29: Request successful. Current Estimated RPM: 29. Response: His name was 29. Not Whiskers, or Midnight, or eve...\n",
      "Task 55: Request successful. Current Estimated RPM: 55. Response: 55 wasn't just a number. Not to Elara, who had a p...\n",
      "Task 15: Request successful. Current Estimated RPM: 15. Response: Dr. Aris Thorne, a man whose life was as meticulou...\n",
      "Task 14: Request successful. Current Estimated RPM: 14. Response: There once was a cat named 14. Not Fourteen, the m...\n",
      "Task 49: Request successful. Current Estimated RPM: 49. Response: The shelter hummed with the muted chaos of despera...\n",
      "Task 18: Request successful. Current Estimated RPM: 18. Response: 18 was not a name given lightly, nor was it a numb...\n",
      "Task 47: Request successful. Current Estimated RPM: 47. Response: 47 was not a cat named forty-seven. He was, quite ...\n",
      "Task 1: Request successful. Current Estimated RPM: 1. Response: 1 was a cat of precise habits and singular focus. ...\n",
      "Task 46: Request successful. Current Estimated RPM: 46. Response: 46 was not a name given with affection, but with p...\n",
      "Task 8: Request successful. Current Estimated RPM: 8. Response: Eight wasn't just a cat; he was a silent, sleek ob...\n",
      "Task 11: Request successful. Current Estimated RPM: 11. Response: 11 wasn't a name, not really. It was a designation...\n",
      "Task 42: Request successful. Current Estimated RPM: 42. Response: Sarah had named him 42, a nod to a book she loved,...\n",
      "Task 28: Request successful. Current Estimated RPM: 28. Response: He wasn't a \"Whiskers\" or a \"Mittens.\" He was 28. ...\n",
      "Task 27: Request successful. Current Estimated RPM: 27. Response: Twenty-seven was not a name given with affection. ...\n",
      "Task 50: Request successful. Current Estimated RPM: 50. Response: Fifty was not a common name for a cat, nor was he ...\n",
      "Task 54: Request successful. Current Estimated RPM: 54. Response: In the echoing, clinical expanse of the city shelt...\n",
      "Task 2: Request successful. Current Estimated RPM: 2. Response: No one quite knew why Elara had named the sleek bl...\n",
      "Task 26: Request successful. Current Estimated RPM: 26. Response: The fluorescent hum of the shelter was a constant,...\n",
      "Task 36: Request successful. Current Estimated RPM: 36. Response: 36 wasn't a name you whispered into a fur-lined ea...\n",
      "Task 43: Request successful. Current Estimated RPM: 43. Response: In the sterile, echoing halls of the city animal s...\n",
      "Task 31: Request successful. Current Estimated RPM: 31. Response: 31 knew the world in sterile squares of white ligh...\n",
      "Task 44: Request successful. Current Estimated RPM: 44. Response: In the echoing halls of the City Animal Rehabilita...\n",
      "Task 5: Request successful. Current Estimated RPM: 5. Response: Five was a cat of quiet habits and an even quieter...\n",
      "Task 4: Request successful. Current Estimated RPM: 4. Response: The cat’s name was 4. Not ‘Four’ as in the word, b...\n",
      "Task 57: Request successful. Current Estimated RPM: 57. Response: 57 was a number, not a name. He knew this. The hum...\n",
      "Task 59: Request successful. Current Estimated RPM: 59. Response: 59 wasn't a cat. Not in the fluffy, purring, lap-w...\n",
      "Task 53: Request successful. Current Estimated RPM: 53. Response: In the sterile hum of the shelter, he was just 53....\n",
      "Task 22: Request successful. Current Estimated RPM: 22. Response: The cat named 22 wasn’t a sleek, silent shadow. Oh...\n",
      "Task 6: Request successful. Current Estimated RPM: 6. Response: The cat named Six wasn't an ordinary cat, though t...\n",
      "Task 24: Request successful. Current Estimated RPM: 24. Response: 24 wasn't a name chosen for cuteness. It was a des...\n",
      "Task 30: Request successful. Current Estimated RPM: 30. Response: 30 was a cat of quiet dignity, despite his name. H...\n",
      "Task 40: Request successful. Current Estimated RPM: 40. Response: The cat named 40 wasn't named for his age, nor for...\n",
      "Task 21: Request successful. Current Estimated RPM: 21. Response: In the sterile, echoing halls of the local animal ...\n",
      "Task 34: Request successful. Current Estimated RPM: 34. Response: The cat named 34 was not fluffy, nor Mittens, nor ...\n",
      "Task 9: Request successful. Current Estimated RPM: 9. Response: The cat named 9 was not like other cats. For one, ...\n",
      "Task 41: Request successful. Current Estimated RPM: 41. Response: The fluorescent hum of the shelter was a constant,...\n",
      "Task 52: Request successful. Current Estimated RPM: 52. Response: In a world of Fluffies, Mittens, and Patches, ther...\n",
      "Task 33: Request successful. Current Estimated RPM: 33. Response: The cat known as 33 was not sleek or fluffy, nor w...\n",
      "Task 16: Request successful. Current Estimated RPM: 16. Response: 16 wasn't a typical cat. For one, there was his na...\n",
      "Task 25: Request successful. Current Estimated RPM: 25. Response: For a long time, the scruffy tabby with the slight...\n",
      "Task 45: Request successful. Current Estimated RPM: 45. Response: The cat named 45 wasn't named for a caliber, a spe...\n",
      "Task 35: Request successful. Current Estimated RPM: 35. Response: In the humming, sterile corridors of the old Bio-T...\n",
      "Task 62: RPM limit approaching. Waiting 60.00 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     84\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     end_time = time.time()\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll tasks completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from collections import deque\n",
    "import google.generativeai as genai\n",
    "import threading # For thread-safe deque access in a multi-threaded context (optional, if using threads)\n",
    "\n",
    "# Configure your API key (replace with your actual key)\n",
    "# genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# --- Asynchronous Client Setup ---\n",
    "# Initialize the asynchronous model\n",
    "# Note: genai.GenerativeModel can be used asynchronously directly.\n",
    "# The async client handles the underlying aiohttp requests.\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# --- Asynchronous Client-side RPM Tracker ---\n",
    "# Use a deque for timestamps.\n",
    "# In an async context, if multiple coroutines are modifying this, it's inherently single-threaded\n",
    "# within the event loop, but if you were to spawn multiple processes or actual threads,\n",
    "# you'd need synchronization (e.g., threading.Lock). For pure asyncio, it's fine.\n",
    "request_timestamps = deque()\n",
    "MAX_REQUESTS_PER_MINUTE = 60 # Example limit: adjust based on actual API limits\n",
    "\n",
    "# An asyncio Lock to protect the deque if multiple tasks are modifying it concurrently\n",
    "# (though for a single shared deque, careful design can avoid explicit locking).\n",
    "# For simplicity in this pattern, we'll manage the deque in a way that minimizes race conditions.\n",
    "# However, if you were dynamically creating many concurrent tasks, a lock would be safer.\n",
    "deque_lock = asyncio.Lock()\n",
    "\n",
    "\n",
    "async def make_api_request_with_rpm_check_async(prompt: str, task_id: int):\n",
    "    \"\"\"\n",
    "    Makes an asynchronous API request, respecting RPM limits.\n",
    "    \"\"\"\n",
    "    global request_timestamps\n",
    "\n",
    "    while True:\n",
    "        current_time = time.time() # Use time.time() for wall-clock time\n",
    "        # Acquire lock to safely modify shared deque\n",
    "        async with deque_lock:\n",
    "            # Remove timestamps older than 60 seconds\n",
    "            while request_timestamps and request_timestamps[0] < current_time - 60:\n",
    "                request_timestamps.popleft()\n",
    "\n",
    "            # Check if we are about to exceed the limit\n",
    "            if len(request_timestamps) >= MAX_REQUESTS_PER_MINUTE:\n",
    "                # Calculate how long to wait until the oldest request falls out of the window\n",
    "                time_to_wait = (request_timestamps[0] + 60) - current_time\n",
    "                if time_to_wait > 0:\n",
    "                    print(f\"Task {task_id}: RPM limit approaching. Waiting {time_to_wait:.2f} seconds...\")\n",
    "                    # Release lock before awaiting sleep\n",
    "                    await asyncio.sleep(time_to_wait)\n",
    "                    continue # Re-evaluate after waiting\n",
    "\n",
    "            # If we reach here, we're allowed to make a request\n",
    "            request_timestamps.append(current_time)\n",
    "            current_estimated_rpm = len(request_timestamps)\n",
    "            # Release lock before making the actual API call\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        # Make the actual asynchronous API call\n",
    "        response = await model.generate_content_async(prompt)\n",
    "        print(f\"Task {task_id}: Request successful. Current Estimated RPM: {current_estimated_rpm}. Response: {response.text[:50]}...\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Task {task_id}: API Request Failed: {e}\")\n",
    "        # Implement specific error handling (e.g., retries for RESOURCE_EXHAUSTED)\n",
    "        return None\n",
    "\n",
    "async def main():\n",
    "    test_prompts = [f\"Tell me a short story about a cat named {i}.\" for i in range(1, 101)] # More prompts for concurrent testing\n",
    "\n",
    "    # Create a list of tasks\n",
    "    tasks = []\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        task = asyncio.create_task(make_api_request_with_rpm_check_async(prompt, i + 1))\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Await all tasks to complete\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    asyncio.run(main())\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nAll tasks completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99014469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "# These helpers are generic and can be used by any provider's function.\n",
    "\n",
    "_anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": time.monotonic() + 60}\n",
    "\n",
    "# RPM tracking for OpenAI\n",
    "_openai_request_timestamps = deque()\n",
    "_openai_rpm_lock = asyncio.Lock()\n",
    "OPENAI_MAX_RPM = 300  # Stay below your 500 RPM limit\n",
    "\n",
    "# RPM tracking for Google\n",
    "_google_request_timestamps = deque()\n",
    "_google_rpm_lock = asyncio.Lock()\n",
    "GOOGLE_MAX_RPM = 300  # Google's free tier limit is typically 60 RPM, stay below it\n",
    "\n",
    "async def _anthropic_throttle(tokens_needed: int):\n",
    "    global _anthropic_bucket\n",
    "    while True:\n",
    "        now = time.monotonic()\n",
    "        if now >= _anthropic_bucket[\"reset_at\"]:\n",
    "            _anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": now + 60}\n",
    "        if tokens_needed <= _anthropic_bucket[\"tokens\"]:\n",
    "            _anthropic_bucket[\"tokens\"] -= tokens_needed\n",
    "            return\n",
    "        else:\n",
    "            to_sleep = _anthropic_bucket[\"reset_at\"] - now\n",
    "            await asyncio.sleep(max(to_sleep, 0.01))\n",
    "\n",
    "async def _openai_rpm_check():\n",
    "    \"\"\"Ensure we don't exceed OpenAI RPM limits\"\"\"\n",
    "    global _openai_request_timestamps\n",
    "    \n",
    "    async with _openai_rpm_lock:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Remove timestamps older than 60 seconds\n",
    "        while _openai_request_timestamps and _openai_request_timestamps[0] < current_time - 60:\n",
    "            _openai_request_timestamps.popleft()\n",
    "        \n",
    "        # Check if we need to wait\n",
    "        if len(_openai_request_timestamps) >= OPENAI_MAX_RPM:\n",
    "            wait_time = (_openai_request_timestamps[0] + 60) - current_time\n",
    "            if wait_time > 0:\n",
    "                print(f\"🕒 OpenAI RPM limit reached. Waiting {wait_time:.2f} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "                # Re-check after waiting\n",
    "                return await _openai_rpm_check()\n",
    "        \n",
    "        # Record this request\n",
    "        _openai_request_timestamps.append(current_time)\n",
    "\n",
    "async def _google_rpm_check():\n",
    "    \"\"\"Ensure we don't exceed Google RPM limits\"\"\"\n",
    "    global _google_request_timestamps\n",
    "    \n",
    "    async with _google_rpm_lock:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Remove timestamps older than 60 seconds\n",
    "        while _google_request_timestamps and _google_request_timestamps[0] < current_time - 60:\n",
    "            _google_request_timestamps.popleft()\n",
    "        \n",
    "        # Check if we need to wait\n",
    "        if len(_google_request_timestamps) >= GOOGLE_MAX_RPM:\n",
    "            wait_time = (_google_request_timestamps[0] + 60) - current_time\n",
    "            if wait_time > 0:\n",
    "                print(f\"🕒 Google RPM limit reached. Waiting {wait_time:.2f} seconds...\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "                # Re-check after waiting\n",
    "                return await _google_rpm_check()\n",
    "        \n",
    "        # Record this request\n",
    "        _google_request_timestamps.append(current_time)\n",
    "\n",
    "async def with_api_retries(\n",
    "        send_coroutine_factory,\n",
    "        *,\n",
    "        model_info: str,  # For informative logging\n",
    "        max_attempts: int = 10,\n",
    "        base_wait_seconds: int = 10\n",
    "    ):\n",
    "    \"\"\"A wrapper to handle API retries with exponential backoff.\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return await send_coroutine_factory()\n",
    "        except (openai.RateLimitError, anthropic.RateLimitError, Exception) as e:\n",
    "            # Check for specific rate limit error types or a 429 status code in the error string\n",
    "            if isinstance(e, (openai.RateLimitError, anthropic.RateLimitError)) or \"429\" in str(e):\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(f\"❌ Final attempt failed for {model_info}. Giving up.\")\n",
    "                    raise\n",
    "                \n",
    "                # Exponential backoff with jitter\n",
    "                wait_time = base_wait_seconds * (2 ** attempt) + random.uniform(0, 1)\n",
    "                \n",
    "                # More informative error message\n",
    "                print(f\"🕒 Rate limit on {model_info}. Retrying in {wait_time:.2f}s... (Attempt {attempt + 1}/{max_attempts})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                # If it's not a rate limit error, re-raise immediately\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "# --- 2. Provider-Specific API Calling Functions ---\n",
    "\n",
    "async def call_openai_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int\n",
    "    ):\n",
    "    \"\"\"Handles an API call to OpenAI with RPM limiting.\"\"\"\n",
    "    # Check RPM limits before making the request\n",
    "    await _openai_rpm_check()\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    model_info = f\"{model} (Index {index})\"\n",
    "    \n",
    "    response = await with_api_retries(\n",
    "        lambda: openai_client_async.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            temperature=0, \n",
    "            max_tokens=4000, \n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        ),\n",
    "        model_info=model_info\n",
    "    )\n",
    "    \n",
    "    text_response = response.choices[0].message.content\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.prompt_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.completion_tokens\n",
    "        if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details:\n",
    "             usage[\"cached_tokens\"] = response.usage.prompt_tokens_details.get(\"cached_tokens\", 0)\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_google_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int\n",
    "    ):\n",
    "    \"\"\"Handles an API call to Google with RPM limiting.\"\"\"\n",
    "    # Check RPM limits before making the request\n",
    "    await _google_rpm_check()\n",
    "    \n",
    "    gemini = genai.GenerativeModel(model_name=model, system_instruction=system_prompt)\n",
    "    cfg = genai.types.GenerationConfig(temperature=0, max_output_tokens=4000)\n",
    "    model_info = f\"{model} (Index {index})\"\n",
    "\n",
    "    response = await with_api_retries(\n",
    "        lambda: gemini.generate_content_async(user_prompt, generation_config=cfg),\n",
    "        model_info=model_info\n",
    "    )\n",
    "\n",
    "    text_response = response.text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage_metadata:\n",
    "        usage[\"input_tokens\"] = response.usage_metadata.prompt_token_count\n",
    "        usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_anthropic_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int\n",
    "    ):\n",
    "    \"\"\"Handles an API call to Anthropic, including prompt caching.\"\"\"\n",
    "    system_block = {\"type\": \"text\", \"text\": system_prompt, \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "    model_info = f\"{model} (Index {index})\"\n",
    "    \n",
    "    est_tokens = math.ceil(1.2 * len(system_prompt.split()))\n",
    "    await _anthropic_throttle(est_tokens)\n",
    "\n",
    "    response = await with_api_retries(\n",
    "        lambda: anthropic_client_async.messages.create(\n",
    "            model=model, max_tokens=4000, temperature=0,\n",
    "            system=[system_block], messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        ),\n",
    "        model_info=model_info\n",
    "    )\n",
    "\n",
    "    text_response = response.content[0].text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.input_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.output_tokens\n",
    "        usage[\"cached_tokens\"] = response.usage.cache_read_input_tokens if response.usage.cache_read_input_tokens else 0\n",
    "    return text_response, usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5232de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Fixed Single API Call Function ---\n",
    "\n",
    "async def run_single_api_call(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    index: int,\n",
    "    tier: str,\n",
    "    dataset: 'datasets.Dataset',\n",
    "    system_prompt: str,\n",
    "    output_dir: Path,\n",
    "    provider_sem: asyncio.Semaphore\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Runs a single API call for one provider/model/problem combination.\n",
    "    Semaphore is acquired HERE, before any async operations.\n",
    "    \"\"\"\n",
    "    async with provider_sem:  # Acquire semaphore at the start\n",
    "        user_prompt = append_sample_to_user_prompt(\n",
    "            tier=tier,\n",
    "            index=index,\n",
    "            dataset=dataset\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        status = \"Failed\"\n",
    "        usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "        \n",
    "        try:\n",
    "            # Call the appropriate API function\n",
    "            if provider == \"openai\":\n",
    "                text_response, usage = await call_openai_async(model, system_prompt, user_prompt, index)\n",
    "            elif provider == \"google\":\n",
    "                text_response, usage = await call_google_async(model, system_prompt, user_prompt, index)\n",
    "            elif provider == \"anthropic\":\n",
    "                text_response, usage = await call_anthropic_async(model, system_prompt, user_prompt, index)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown provider: {provider}\")\n",
    "            \n",
    "            status = \"Success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            text_response = f\"--- ERROR ---\\nIndex: {index}, Model: {model}\\n{type(e).__name__}: {e}\"\n",
    "            print(f\"❌ Error for Index {index}, Model {model}: {type(e).__name__}\")\n",
    "        \n",
    "        # Save the output\n",
    "        output_path = output_dir / str(index) / f\"{provider}_{model}.txt\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        output_path.write_text(text_response, encoding='utf-8')\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"provider\": provider,\n",
    "            \"model\": model,\n",
    "            \"index\": index,\n",
    "            \"status\": status,\n",
    "            \"time_s\": round(elapsed, 2),\n",
    "            \"input_tokens\": usage[\"input_tokens\"],\n",
    "            \"output_tokens\": usage[\"output_tokens\"],\n",
    "            \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "            \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "        }\n",
    "\n",
    "# --- 4. Batch Processing Function ---\n",
    "\n",
    "async def generate_manifests_batched(\n",
    "    indices_to_generate: List[int],\n",
    "    tier: str = \"tier1\",\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "    model_dict: Dict[str, str] = MODEL_DICT,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    concurrency_limits: Dict[str, int] = API_CONCURRENCY_LIMITS,\n",
    "    batch_size: int = 20  # Process 20 problems at a time\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs manifest generation in batches to avoid overwhelming APIs.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Manifest Generation (Batched) ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "    output_dir = TIER_OUTPUT_DIRS[tier]\n",
    "    all_results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, len(indices_to_generate), batch_size):\n",
    "        batch_indices = indices_to_generate[batch_start:batch_start + batch_size]\n",
    "        batch_num = batch_start // batch_size + 1\n",
    "        total_batches = math.ceil(len(indices_to_generate) / batch_size)\n",
    "        \n",
    "        print(f\"\\n--- Processing Batch {batch_num}/{total_batches} (Indices {batch_indices[0]} to {batch_indices[-1]}) ---\")\n",
    "        \n",
    "        # Create all tasks for this batch\n",
    "        batch_tasks = []\n",
    "        for index in batch_indices:\n",
    "            for provider, model in model_dict.items():\n",
    "                task = asyncio.create_task(\n",
    "                    run_single_api_call(\n",
    "                        provider=provider,\n",
    "                        model=model,\n",
    "                        index=index,\n",
    "                        tier=tier,\n",
    "                        dataset=dataset,\n",
    "                        system_prompt=system_prompt,\n",
    "                        output_dir=output_dir,\n",
    "                        provider_sem=provider_semaphores[provider]\n",
    "                    )\n",
    "                )\n",
    "                batch_tasks.append(task)\n",
    "        \n",
    "        # Wait for all tasks in this batch to complete\n",
    "        with tqdm(total=len(batch_tasks), desc=f\"Batch {batch_num}\", leave=False) as pbar:\n",
    "            batch_results = []\n",
    "            for task in asyncio.as_completed(batch_tasks):\n",
    "                result = await task\n",
    "                batch_results.append(result)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        all_results.extend(batch_results)\n",
    "        print(f\"✅ Batch {batch_num} completed: {len(batch_results)} API calls\")\n",
    "        \n",
    "        # Brief pause between batches to be nice to the APIs\n",
    "        if batch_start + batch_size < len(indices_to_generate):\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    # Create and save the performance DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Manifest Generation Complete ---\")\n",
    "    print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    success_count = len(df[df['status'] == 'Success'])\n",
    "    total_calls = len(df)\n",
    "    print(f\"Success rate: {success_count}/{total_calls} ({100*success_count/total_calls:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 5. Alternative: Fully Parallel with Better Semaphore Management ---\n",
    "\n",
    "async def generate_manifests_parallel_fixed(\n",
    "    indices_to_generate: List[int],\n",
    "    tier: str = \"tier1\",\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "    model_dict: Dict[str, str] = MODEL_DICT,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    concurrency_limits: Dict[str, int] = API_CONCURRENCY_LIMITS\n",
    "):\n",
    "    \"\"\"\n",
    "    Fully parallel version with proper semaphore management.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Manifest Generation (Parallel Fixed) ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "    output_dir = TIER_OUTPUT_DIRS[tier]\n",
    "    \n",
    "    # Create ALL tasks upfront\n",
    "    all_tasks = []\n",
    "    for index in indices_to_generate:\n",
    "        for provider, model in model_dict.items():\n",
    "            task = asyncio.create_task(\n",
    "                run_single_api_call(\n",
    "                    provider=provider,\n",
    "                    model=model,\n",
    "                    index=index,\n",
    "                    tier=tier,\n",
    "                    dataset=dataset,\n",
    "                    system_prompt=system_prompt,\n",
    "                    output_dir=output_dir,\n",
    "                    provider_sem=provider_semaphores[provider]\n",
    "                )\n",
    "            )\n",
    "            all_tasks.append(task)\n",
    "    \n",
    "    print(f\"Created {len(all_tasks)} total API call tasks\")\n",
    "    \n",
    "    # Execute all tasks with progress bar\n",
    "    with tqdm(total=len(all_tasks), desc=\"API Calls\") as pbar:\n",
    "        results = []\n",
    "        for task in asyncio.as_completed(all_tasks):\n",
    "            result = await task\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Create and save the performance DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Manifest Generation Complete ---\")\n",
    "    print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    success_count = len(df[df['status'] == 'Success'])\n",
    "    total_calls = len(df)\n",
    "    print(f\"Success rate: {success_count}/{total_calls} ({100*success_count/total_calls:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e098e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Manifest Generation (Parallel Fixed) ---\n",
      "Created 2276 total API call tasks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024af5e6d7a940fe83eb1c2237ec452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "API Calls:   0%|          | 0/2276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕒 Rate limit on gpt-4.1 (Index 24). Retrying in 10.59s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 32). Retrying in 10.92s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 32). Retrying in 20.50s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 51). Retrying in 10.73s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 85). Retrying in 10.87s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 97). Retrying in 10.67s... (Attempt 1/10)\n",
      "❌ Error for Index 216, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 103). Retrying in 10.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 131). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 131). Retrying in 20.31s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 146). Retrying in 10.94s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 146). Retrying in 20.66s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 146). Retrying in 40.38s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 190). Retrying in 10.79s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 195). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 208). Retrying in 10.49s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 213). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 223). Retrying in 10.53s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 232). Retrying in 10.28s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 243). Retrying in 10.21s... (Attempt 1/10)\n",
      "❌ Error for Index 618, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 282). Retrying in 10.85s... (Attempt 1/10)\n",
      "❌ Error for Index 675, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 293). Retrying in 10.73s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 298). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 314). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 314). Retrying in 20.87s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 333). Retrying in 10.25s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 333). Retrying in 20.96s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 333). Retrying in 40.96s... (Attempt 3/10)\n",
      "❌ Error for Index 803, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 408). Retrying in 10.17s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 412). Retrying in 10.71s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 427). Retrying in 10.43s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 444). Retrying in 10.34s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 444). Retrying in 20.07s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 444). Retrying in 40.07s... (Attempt 3/10)\n",
      "❌ Error for Index 1116, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 475). Retrying in 10.87s... (Attempt 1/10)\n",
      "❌ Error for Index 1170, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 491). Retrying in 10.42s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 521). Retrying in 10.88s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 528). Retrying in 10.36s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 539). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 550). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 558). Retrying in 10.46s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 573). Retrying in 10.45s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 573). Retrying in 20.38s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 599). Retrying in 10.47s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 608). Retrying in 10.63s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 616). Retrying in 10.04s... (Attempt 1/10)\n",
      "❌ Error for Index 1506, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 618). Retrying in 10.10s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 629). Retrying in 10.02s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 639). Retrying in 10.41s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 639). Retrying in 20.27s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 654). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 670). Retrying in 10.61s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 675). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 680). Retrying in 10.37s... (Attempt 1/10)\n",
      "❌ Error for Index 1680, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 693). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 707). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 712). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 715). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 723). Retrying in 10.80s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 728). Retrying in 10.60s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 728). Retrying in 20.51s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 743). Retrying in 10.80s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 743). Retrying in 20.59s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 763). Retrying in 10.43s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 769). Retrying in 10.14s... (Attempt 1/10)\n",
      "❌ Error for Index 1910, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 775). Retrying in 10.27s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 779). Retrying in 10.14s... (Attempt 1/10)\n",
      "❌ Error for Index 1943, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 795). Retrying in 10.92s... (Attempt 1/10)\n",
      "❌ Error for Index 1973, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 816). Retrying in 10.41s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 816). Retrying in 20.26s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 852). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 856). Retrying in 10.50s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 866). Retrying in 10.67s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 866). Retrying in 20.56s... (Attempt 2/10)\n",
      "❌ Error for Index 2164, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 893). Retrying in 10.38s... (Attempt 1/10)\n",
      "❌ Error for Index 2232, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 911). Retrying in 10.60s... (Attempt 1/10)\n",
      "❌ Error for Index 2241, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 911). Retrying in 20.51s... (Attempt 2/10)\n",
      "❌ Error for Index 2249, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 911). Retrying in 40.42s... (Attempt 3/10)\n",
      "❌ Error for Index 2303, Model gemini-2.5-flash: ValueError\n",
      "❌ Error for Index 2309, Model gemini-2.5-flash: ValueError\n",
      "❌ Error for Index 2338, Model gemini-2.5-flash: ValueError\n",
      "❌ Error for Index 2340, Model gemini-2.5-flash: ValueError\n",
      "❌ Error for Index 2345, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 960). Retrying in 10.45s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 972). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 981). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 993). Retrying in 10.49s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 999). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1008). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1020). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1021). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1029). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1037). Retrying in 10.03s... (Attempt 1/10)\n",
      "❌ Error for Index 2582, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 1050). Retrying in 10.30s... (Attempt 1/10)\n",
      "❌ Error for Index 2652, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 1076). Retrying in 10.18s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1082). Retrying in 10.27s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1085). Retrying in 10.23s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1093). Retrying in 10.77s... (Attempt 1/10)\n",
      "❌ Error for Index 2714, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 1093). Retrying in 20.97s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1116). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1116). Retrying in 20.67s... (Attempt 2/10)\n",
      "❌ Error for Index 2769, Model gemini-2.5-flash: ValueError\n",
      "🕒 Rate limit on gpt-4.1 (Index 1138). Retrying in 10.30s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1147). Retrying in 10.69s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1158). Retrying in 10.28s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1164). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1163). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1165). Retrying in 10.51s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1169). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1181). Retrying in 10.73s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1200). Retrying in 10.30s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1207). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1214). Retrying in 11.00s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1221). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1244). Retrying in 10.80s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1251). Retrying in 10.45s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1255). Retrying in 10.77s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1256). Retrying in 10.63s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1258). Retrying in 10.31s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1258). Retrying in 20.55s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1276). Retrying in 10.88s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1281). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1291). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1294). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1298). Retrying in 10.28s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1298). Retrying in 20.25s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1309). Retrying in 10.89s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1320). Retrying in 10.51s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1347). Retrying in 10.94s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1354). Retrying in 10.79s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1357). Retrying in 10.96s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1364). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1393). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1399). Retrying in 10.47s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1399). Retrying in 20.58s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1399). Retrying in 40.63s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1434). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1461). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1472). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1480). Retrying in 10.54s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1491). Retrying in 10.32s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1492). Retrying in 10.18s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1492). Retrying in 20.44s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1514). Retrying in 10.64s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1514). Retrying in 20.54s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1527). Retrying in 10.72s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1527). Retrying in 20.60s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1527). Retrying in 40.84s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1553). Retrying in 10.22s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1564). Retrying in 10.56s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1575). Retrying in 10.75s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1580). Retrying in 10.71s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1584). Retrying in 10.53s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1589). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1602). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1603). Retrying in 10.38s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1615). Retrying in 10.11s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1624). Retrying in 10.65s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1630). Retrying in 10.81s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1632). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1648). Retrying in 10.08s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1658). Retrying in 10.26s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1660). Retrying in 10.09s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1660). Retrying in 20.50s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1683). Retrying in 10.70s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1691). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1694). Retrying in 10.42s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1709). Retrying in 10.65s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1715). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1741). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1758). Retrying in 10.15s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1761). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1768). Retrying in 10.25s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1773). Retrying in 10.27s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1784). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1787). Retrying in 10.79s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1790). Retrying in 10.51s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1806). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1806). Retrying in 21.00s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1829). Retrying in 10.55s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1847). Retrying in 10.75s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1850). Retrying in 10.35s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1868). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1872). Retrying in 10.08s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1876). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1892). Retrying in 10.15s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1916). Retrying in 10.48s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1922). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1937). Retrying in 10.95s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1939). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1939). Retrying in 20.47s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1943). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1946). Retrying in 10.50s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1956). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1964). Retrying in 10.06s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1968). Retrying in 10.27s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1978). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 1990). Retrying in 10.13s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2007). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2027). Retrying in 10.99s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2029). Retrying in 10.86s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2032). Retrying in 10.70s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2034). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2044). Retrying in 10.50s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2047). Retrying in 10.51s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2058). Retrying in 10.60s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2058). Retrying in 20.13s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2065). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2077). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2081). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2084). Retrying in 10.79s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2100). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2104). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2112). Retrying in 10.04s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2125). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2125). Retrying in 20.21s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2146). Retrying in 10.97s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2150). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2170). Retrying in 10.43s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2192). Retrying in 10.66s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2193). Retrying in 10.78s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2193). Retrying in 20.08s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2207). Retrying in 10.36s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2207). Retrying in 20.70s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2232). Retrying in 10.20s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2239). Retrying in 10.78s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2245). Retrying in 10.79s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2247). Retrying in 10.09s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2251). Retrying in 10.95s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2264). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2271). Retrying in 10.37s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2264). Retrying in 20.54s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2264). Retrying in 40.15s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2298). Retrying in 10.99s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2308). Retrying in 10.97s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2313). Retrying in 10.55s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2314). Retrying in 10.35s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2339). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2347). Retrying in 10.15s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2354). Retrying in 10.52s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2354). Retrying in 20.34s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2382). Retrying in 10.16s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2385). Retrying in 10.03s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2385). Retrying in 20.79s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2390). Retrying in 10.88s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2410). Retrying in 10.89s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2424). Retrying in 10.90s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2424). Retrying in 20.53s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2443). Retrying in 10.62s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2443). Retrying in 20.11s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2483). Retrying in 10.60s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2483). Retrying in 20.67s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2499). Retrying in 10.63s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2505). Retrying in 10.89s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2510). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2522). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2522). Retrying in 20.54s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2540). Retrying in 10.10s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2553). Retrying in 10.13s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2568). Retrying in 10.01s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2557). Retrying in 10.29s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2578). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2579). Retrying in 10.33s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2591). Retrying in 10.71s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2591). Retrying in 20.65s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2611). Retrying in 10.63s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2621). Retrying in 10.58s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2611). Retrying in 20.91s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2623). Retrying in 10.92s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2642). Retrying in 10.25s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2650). Retrying in 10.05s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2650). Retrying in 20.02s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2661). Retrying in 10.28s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2650). Retrying in 40.51s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2664). Retrying in 10.29s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2668). Retrying in 10.86s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2692). Retrying in 10.59s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2692). Retrying in 20.94s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2706). Retrying in 10.54s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2706). Retrying in 20.52s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2710). Retrying in 10.97s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2713). Retrying in 10.84s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2720). Retrying in 10.44s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2733). Retrying in 10.68s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2739). Retrying in 10.14s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2740). Retrying in 10.12s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2749). Retrying in 10.71s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2749). Retrying in 20.32s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2756). Retrying in 10.54s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2766). Retrying in 10.73s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2773). Retrying in 10.88s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2773). Retrying in 20.69s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2789). Retrying in 10.10s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2795). Retrying in 10.68s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2820). Retrying in 10.38s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2834). Retrying in 10.09s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2837). Retrying in 10.39s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2843). Retrying in 10.85s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2846). Retrying in 10.21s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2846). Retrying in 20.52s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2871). Retrying in 10.77s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2871). Retrying in 20.11s... (Attempt 2/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2871). Retrying in 40.51s... (Attempt 3/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2930). Retrying in 10.41s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2933). Retrying in 10.24s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2939). Retrying in 10.19s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2941). Retrying in 10.84s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2946). Retrying in 10.65s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2963). Retrying in 10.22s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2974). Retrying in 10.56s... (Attempt 1/10)\n",
      "🕒 Rate limit on gpt-4.1 (Index 2984). Retrying in 10.51s... (Attempt 1/10)\n",
      "\n",
      "--- Manifest Generation Complete ---\n",
      "Processed 1138 indices in 11503.35 seconds.\n",
      "Performance log saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/july-8-manifests-raw/tier1/generation_performance_20250708_211411.csv\n",
      "Success rate: 2252/2276 (98.9%)\n"
     ]
    }
   ],
   "source": [
    "# # Option 1: Batched (recommended for stability)\n",
    "# perf_df = await generate_manifests_batched(indices_to_generate=INDICES_TO_GENERATE)\n",
    "\n",
    "# Choose the list of indices to generate manifests for\n",
    "UPPER_LIMIT = 3000\n",
    "INDICES_TO_GENERATE = [idx for idx in TIER_LISTS[\"tier1\"] if idx <= UPPER_LIMIT]\n",
    "\n",
    "# Option 2: Fully parallel (faster but may still hit some rate limits)\n",
    "perf_df = await generate_manifests_parallel_fixed(indices_to_generate=INDICES_TO_GENERATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39da917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
