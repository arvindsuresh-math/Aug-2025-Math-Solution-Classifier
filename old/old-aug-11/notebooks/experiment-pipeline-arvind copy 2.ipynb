{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq transformers==4.53.2\n",
    "!pip install -Uq peft\n",
    "!pip install -Uq trl\n",
    "!pip install -Uq accelerate\n",
    "!pip install -Uq datasets\n",
    "!pip install -Uq bitsandbytes\n",
    "\n",
    "# Install Flash Attention 2\n",
    "!pip install flash-attn==2.7.4.post1 \\\n",
    "  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "  --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa10bfe",
   "metadata": {},
   "source": [
    "Cell 1: Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: gene_ter_4N_exp_eln_nl_phi4_20250731_152617\n",
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===== EXPERIMENT CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",  # \"discriminative\" or \"generative\"\n",
    "    \"classification_type\": \"ternary\",   # \"binary\" or \"ternary\"\n",
    "    \"dataset_strategy\": \"4N\",          # \"4N\" or \"3N\" (generative only)\n",
    "    \"include_explanation\": True,      # True or False (generative only)\n",
    "    \"include_eln\": True,              # True or False (generative only)\n",
    "    \"solution_format\": \"nl\",        # \"dict\" or \"nl\" (generative only)\n",
    "    \"model_name\": \"microsoft/phi-4-mini-instruct\",  # or \"Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # Prompting configuration\n",
    "    \"system_prompt\": None,  # Will auto-generate if None, or use custom string\n",
    "    \"include_examples\": False,\n",
    "    \"num_examples\": 3,\n",
    "    \"example_strategy\": \"balanced\",  # \"balanced\", \"error_focused\", \"custom\"\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": 1600,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    # Infrastructure\n",
    "    \"use_lora\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Paths and tokens\n",
    "    # \"base_dataset_dir\": \"/content/drive/MyDrive/sft_datasets\",\n",
    "    \"base_dataset_dir\": \"../data/base-datasets-sanitized\",\n",
    "    \"output_base_dir\": \"/content/drive/MyDrive/sft_experiments\",\n",
    "    # \"hf_token\": \"your_huggingface_token_here\",\n",
    "    # \"wandb_project\": \"math_error_classification\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "    \"use_wandb\": False\n",
    "}\n",
    "\n",
    "# Generate experiment ID\n",
    "import datetime\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],  # \"gene\" or \"disc\"\n",
    "    CONFIG[\"classification_type\"][:3],  # \"bin\" or \"ter\"\n",
    "    CONFIG[\"dataset_strategy\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"qwen\" if \"Qwen\" in CONFIG[\"model_name\"] else \"phi4\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "\n",
    "def setup_output_directory(config):\n",
    "    \"\"\"Creates organized output directory structure\"\"\"\n",
    "    \n",
    "    output_dir = Path(config[\"output_base_dir\"]) / config[\"experiment_id\"]\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"baseline\", \"training\", \"final\", \"checkpoints\"]\n",
    "    for subdir in subdirs:\n",
    "        (output_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = output_dir / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = setup_output_directory(CONFIG)\n",
    "CONFIG[\"output_dir\"] = str(output_dir)\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"Dependencies imported and seeds set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ef498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Classify as 'correct', 'conceptual_error', or 'computational_error'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.\n",
      "\n",
      "To customize the system prompt, run:\n",
      "CONFIG[\"system_prompt\"] = \"Your custom prompt here\"\n"
     ]
    }
   ],
   "source": [
    "def generate_system_prompt(config):\n",
    "    \"\"\"Auto-generates appropriate system prompt based on config\"\"\"\n",
    "    \n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        return \"You are a mathematics tutor. Classify the given solution.\"\n",
    "    \n",
    "    # Generative prompts\n",
    "    base_prompt = \"You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format.\"\n",
    "    \n",
    "    # Add classification instructions\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        base_prompt += \" Determine if the solution is 'correct' or 'flawed'.\"\n",
    "    else:\n",
    "        base_prompt += \" Classify as 'correct', 'conceptual_error', or 'computational_error'.\"\n",
    "    \n",
    "    # Add field instructions\n",
    "    fields = []\n",
    "    if config[\"include_eln\"]:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            fields.append(\"identify the erroneous line number (e.g., 'L1', 'FA')\")\n",
    "        else:\n",
    "            fields.append(\"quote the full erroneous line text\")\n",
    "    \n",
    "    if config[\"include_explanation\"]:\n",
    "        fields.append(\"provide a brief explanation of any error\")\n",
    "    \n",
    "    if fields:\n",
    "        base_prompt += f\" Also {', and '.join(fields)}.\"\n",
    "    \n",
    "    base_prompt += \" Respond only with valid JSON.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Auto-generate system prompt if not provided\n",
    "if CONFIG[\"system_prompt\"] is None:\n",
    "    CONFIG[\"system_prompt\"] = generate_system_prompt(CONFIG)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(CONFIG[\"system_prompt\"])\n",
    "print()\n",
    "\n",
    "# Allow manual override\n",
    "print(\"To customize the system prompt, run:\")\n",
    "print('CONFIG[\"system_prompt\"] = \"Your custom prompt here\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41b208",
   "metadata": {},
   "source": [
    "Cell 4: Example Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3522a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ExampleManager class loaded!\n"
     ]
    }
   ],
   "source": [
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset, config):\n",
    "        # Convert DataFrame to list of dicts if needed\n",
    "        if hasattr(base_dataset, 'to_dict'):  # It's a DataFrame\n",
    "            self.samples = base_dataset.to_dict('records')\n",
    "        else:\n",
    "            self.samples = base_dataset  # Already a list of dicts\n",
    "            \n",
    "        self.config = config\n",
    "        self._prepare_examples_by_problem()\n",
    "    \n",
    "    def _prepare_examples_by_problem(self):\n",
    "        \"\"\"Organizes samples by problem index and error type\"\"\"\n",
    "        self.problems_by_type = {\n",
    "            \"correct\": {},\n",
    "            \"conceptual_error\": {},\n",
    "            \"computational_error\": {}\n",
    "        }\n",
    "        \n",
    "        # Group samples by problem index and error type\n",
    "        for sample in self.samples:\n",
    "            problem_index = sample[\"index\"]\n",
    "            error_type = sample[\"error_type\"]\n",
    "            \n",
    "            if problem_index not in self.problems_by_type[error_type]:\n",
    "                self.problems_by_type[error_type][problem_index] = []\n",
    "            self.problems_by_type[error_type][problem_index].append(sample)\n",
    "        \n",
    "        print(f\"Problems by type: correct={len(self.problems_by_type['correct'])}, \"\n",
    "              f\"conceptual={len(self.problems_by_type['conceptual_error'])}, \"\n",
    "              f\"computational={len(self.problems_by_type['computational_error'])}\")\n",
    "    \n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns examples based on dataset strategy\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        \n",
    "        num_examples = self.config[\"num_examples\"]\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        \n",
    "        import random\n",
    "        \n",
    "        if dataset_strategy == \"3N\":\n",
    "            # Choose num_examples distinct problem indices that have all 3 versions\n",
    "            available_problems = set(self.problems_by_type[\"correct\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"conceptual_error\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            \n",
    "            if not available_problems:\n",
    "                print(\"Warning: No problems found with all 3 versions (correct/conceptual/computational)\")\n",
    "                return []\n",
    "            \n",
    "            # Sample problem indices\n",
    "            selected_problems = random.sample(\n",
    "                list(available_problems), \n",
    "                min(num_examples, len(available_problems))\n",
    "            )\n",
    "            \n",
    "            examples = []\n",
    "            for problem_index in selected_problems:\n",
    "                # Add all 3 versions: correct, conceptual_error, computational_error\n",
    "                examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "            \n",
    "        elif dataset_strategy == \"4N\":\n",
    "            import math\n",
    "            \n",
    "            # Get problems that have conceptual errors (with correct versions)\n",
    "            conceptual_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"conceptual_error\"].keys())\n",
    "            )\n",
    "            \n",
    "            # Get problems that have computational errors (with correct versions)\n",
    "            computational_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            )\n",
    "            \n",
    "            # Calculate splits: floor(n/2) conceptual, ceil(n/2) computational\n",
    "            n_conceptual = num_examples // 2  # This is floor(n/2)\n",
    "            n_computational = math.ceil(num_examples / 2)\n",
    "            \n",
    "            examples = []\n",
    "            \n",
    "            # Sample conceptual problems\n",
    "            if conceptual_problems and n_conceptual > 0:\n",
    "                selected_conceptual = random.sample(\n",
    "                    conceptual_problems,\n",
    "                    min(n_conceptual, len(conceptual_problems))\n",
    "                )\n",
    "                \n",
    "                for problem_index in selected_conceptual:\n",
    "                    # Add correct + conceptual_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "            \n",
    "            # Sample computational problems\n",
    "            if computational_problems and n_computational > 0:\n",
    "                selected_computational = random.sample(\n",
    "                    computational_problems,\n",
    "                    min(n_computational, len(computational_problems))\n",
    "                )\n",
    "                \n",
    "                for problem_index in selected_computational:\n",
    "                    # Add correct + computational_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "        \n",
    "        else:\n",
    "            print(f\"Warning: Unknown dataset strategy '{dataset_strategy}'\")\n",
    "            return []\n",
    "\n",
    "print(\"Updated ExampleManager class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246c873",
   "metadata": {},
   "source": [
    "Cell 5: Dataset Loading and Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc8d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated formatting functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_base_dataset(config):\n",
    "    \"\"\"Loads the appropriate base dataset\"\"\"\n",
    "    dataset_strategy = config[\"dataset_strategy\"]\n",
    "    base_dir = Path(config[\"base_dataset_dir\"])\n",
    "    \n",
    "    dataset_file = base_dir / f\"base_{dataset_strategy}_dataset_sanitized.csv\"\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        raise FileNotFoundError(f\"Base dataset not found: {dataset_file}\")\n",
    "\n",
    "    data = pd.read_csv(dataset_file)\n",
    "\n",
    "    print(f\"Loaded base {dataset_strategy} dataset with {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "def format_solution(sample, config):\n",
    "    \"\"\"Formats solution according to config - updated for CSV structure\"\"\"\n",
    "    # Use wrong_answer for the solution (this contains the solution steps)\n",
    "    solution_text = sample.get('wrong_answer', sample.get('correct_answer', ''))\n",
    "    \n",
    "    if config[\"solution_format\"] == \"dict\":\n",
    "        # Split solution into lines and format as dict\n",
    "        lines = solution_text.strip().split('\\n')\n",
    "        solution = {}\n",
    "        for i, line in enumerate(lines[:-1]):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                solution[f\"L{i+1}\"] = line.strip()\n",
    "        if lines and lines[-1].strip():\n",
    "            solution[\"FA\"] = lines[-1].strip()\n",
    "        return json.dumps(solution, indent=2)\n",
    "    else:\n",
    "        return solution_text.strip()\n",
    "\n",
    "def format_expected_output(sample, config):\n",
    "    \"\"\"Creates the expected JSON output for a sample - updated for CSV structure\"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    # Verdict\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        output[\"verdict\"] = \"correct\" if sample[\"error_type\"] == \"correct\" else \"flawed\"\n",
    "    else:\n",
    "        output[\"verdict\"] = sample[\"error_type\"]\n",
    "    \n",
    "    # ELN (Erroneous Line Number)\n",
    "    if config[\"include_eln\"]:\n",
    "        if sample[\"error_type\"] != \"correct\":\n",
    "            if config[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = sample.get(\"erroneous_line_number\", None)\n",
    "            else:\n",
    "                # For natural language format, try to extract the actual erroneous line text\n",
    "                eln = sample.get(\"erroneous_line_number\")\n",
    "                if eln and pd.notna(eln):\n",
    "                    # Extract line number (e.g., \"L3\" -> 3)\n",
    "                    try:\n",
    "                        if eln.startswith('L'):\n",
    "                            line_num = int(eln[1:]) - 1\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            if 0 <= line_num < len(solution_lines):\n",
    "                                output[\"erroneous_line\"] = solution_lines[line_num].strip()\n",
    "                            else:\n",
    "                                output[\"erroneous_line\"] = eln  # Fallback to the ELN itself\n",
    "                        elif eln == 'FA':\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            output[\"erroneous_line\"] = solution_lines[-1].strip() if solution_lines else None\n",
    "                        else:\n",
    "                            output[\"erroneous_line\"] = eln\n",
    "                    except:\n",
    "                        output[\"erroneous_line\"] = eln\n",
    "                else:\n",
    "                    output[\"erroneous_line\"] = None\n",
    "        else:\n",
    "            key = \"erroneous_line_number\" if config[\"solution_format\"] == \"dict\" else \"erroneous_line\"\n",
    "            output[key] = None\n",
    "    \n",
    "    # Explanation\n",
    "    if config[\"include_explanation\"]:\n",
    "        explanation = sample.get(\"explanation\")\n",
    "        output[\"explanation\"] = explanation if pd.notna(explanation) and sample[\"error_type\"] != \"correct\" else None\n",
    "    \n",
    "    return json.dumps(output)\n",
    "\n",
    "def format_user_message(sample, config):\n",
    "    \"\"\"Format a sample into a user message.\"\"\"\n",
    "    return f\"### Question:\\n{sample['question']}\\n\\n### Answer:\\n{format_solution(sample, config)}\"\n",
    "\n",
    "def create_sample_messages(sample, examples, config):\n",
    "    \"\"\"Create complete message list for a sample.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # System message\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": config[\"system_prompt\"]\n",
    "    })\n",
    "    \n",
    "    # Few-shot examples\n",
    "    if config[\"include_examples\"]:\n",
    "        for example in examples:\n",
    "            user_content = format_user_message(example, config)\n",
    "            assistant_content = format_expected_output(example, config)\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "    \n",
    "    # Actual sample\n",
    "    user_content = format_user_message(sample, config)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "print(\"Updated formatting functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455adbd",
   "metadata": {},
   "source": [
    "Cell 6: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "        messages, \n",
    "        tokenizer, \n",
    "        add_generation_prompt=False, \n",
    "        tokenize=True, \n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Applies chat template to messages with consistent interface\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' keys\n",
    "        tokenizer: The tokenizer to use for formatting\n",
    "        add_generation_prompt: Whether to add generation prompt (for inference)\n",
    "        tokenize: Whether to return tokens (True) or text (False)\n",
    "        **kwargs: Additional arguments for tokenizer (like return_tensors, max_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        If tokenize=True: tokenizer output dict with input_ids, attention_mask, etc.\n",
    "        If tokenize=False: formatted text string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if this is a Qwen3 model and disable thinking if so\n",
    "    template_kwargs = {}\n",
    "    if CONFIG[\"model_name\"].startswith(\"Qwen\"):\n",
    "        template_kwargs['enable_thinking'] = False\n",
    "    \n",
    "    # Apply chat template to get formatted text\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        **template_kwargs\n",
    "    )\n",
    "    \n",
    "    # Return text if not tokenizing\n",
    "    if not tokenize:\n",
    "        return formatted_text\n",
    "    \n",
    "    # Tokenize and return tensor format\n",
    "    return tokenizer(formatted_text, **kwargs)\n",
    "\n",
    "# In Cell 6\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "def step1_create_conversations(sample, examples, system_prompt, config):\n",
    "    \"\"\"[STEP 1] Creates the list of message dictionaries for a sample.\"\"\"\n",
    "    # Create the user message and expected output for the main sample\n",
    "    messages = create_sample_messages(sample, examples, system_prompt, config) # Pass it down\n",
    "    expected_output = format_expected_output(sample, config)\n",
    "    \n",
    "    # Add the final assistant response to complete the conversation\n",
    "    messages.append({\"role\": \"assistant\", \"content\": expected_output})\n",
    "    \n",
    "    # Add a check to find the error\n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg['content'] is None:\n",
    "            raise TypeError(f\"Message content is None at index {i} for sample ID {sample.get('id', 'N/A')}. Message: {msg}\")\n",
    "            \n",
    "    return {\"conversation\": messages}\n",
    "\n",
    "def step2_apply_chat_template(sample, tokenizer):\n",
    "    \"\"\"[STEP 2] Applies the tokenizer's chat template to a conversation.\"\"\"\n",
    "    formatted_text = apply_chat_template(\n",
    "        sample[\"conversation\"],\n",
    "        tokenizer,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "def step3_tokenize_text(sample, tokenizer, config):\n",
    "    \"\"\"[STEP 3] Tokenizes the formatted text.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=config[\"max_length\"],\n",
    "        padding=False\n",
    "    )\n",
    "    # The DataCollatorForLanguageModeling will create the labels automatically\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# New Cell after the modular functions\n",
    "\n",
    "def prepare_dataset(config, tokenizer):\n",
    "    \"\"\"Orchestrates the modular data preparation pipeline.\"\"\"\n",
    "    \n",
    "    # Load base data\n",
    "    base_df = load_base_dataset(config)\n",
    "    raw_dataset = Dataset.from_pandas(base_df)\n",
    "    \n",
    "    # Get few-shot examples\n",
    "    example_manager = ExampleManager(base_df, config)\n",
    "    examples = example_manager.get_examples()\n",
    "    \n",
    "    # Get the system prompt ONCE before the map call\n",
    "    system_prompt = config[\"system_prompt\"]\n",
    "    if system_prompt is None:\n",
    "        raise ValueError(\"System prompt is None! Check cell 3.\")\n",
    "\n",
    "    # --- Modular Pipeline ---\n",
    "    print(\"Executing Step 1: Creating conversations...\")\n",
    "    ds_step1 = raw_dataset.map(\n",
    "        lambda x: step1_create_conversations(x, examples, system_prompt, config) # Pass as argument\n",
    "    )\n",
    "    print(\"✅ Step 1 complete.\")\n",
    "    \n",
    "    print(\"\\nExecuting Step 2: Applying chat template...\")\n",
    "    ds_step2 = ds_step1.map(\n",
    "        lambda x: step2_apply_chat_template(x, tokenizer)\n",
    "    )\n",
    "    print(\"✅ Step 2 complete.\")\n",
    "    \n",
    "    print(\"\\nExecuting Step 3: Tokenizing text...\")\n",
    "    processed_dataset = ds_step2.map(\n",
    "        lambda x: step3_tokenize_text(x, tokenizer, config),\n",
    "        remove_columns=ds_step2.column_names\n",
    "    )\n",
    "    print(\"✅ Step 3 complete.\")\n",
    "    \n",
    "    # Split the final processed dataset\n",
    "    split_dataset = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    print(f\"\\nDataset prepared: {len(split_dataset['train'])} training, {len(split_dataset['test'])} evaluation samples\")\n",
    "    \n",
    "    return split_dataset['train'], split_dataset['test'], examples\n",
    "\n",
    "# Load and process datasets\n",
    "train_dataset, eval_dataset, examples = prepare_dataset(CONFIG, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3af4e",
   "metadata": {},
   "source": [
    "Cell 7: Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4817613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer with proper configuration\"\"\"\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\"  # Ensure left padding for causal models\n",
    "    \n",
    "    print(f\"✓ Tokenizer loaded successfully!\")\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(model_name, config):\n",
    "    \"\"\"Loads model with appropriate configuration\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Configure quantization if using LoRA\n",
    "    bnb_config = None\n",
    "    if config[\"use_lora\"]:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "    \n",
    "    # Load model based on experiment type\n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        # For discriminative, we need a classification model\n",
    "        num_labels = 2 if config[\"classification_type\"] == \"binary\" else 3\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        # For generative, use causal LM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    \n",
    "    # Apply LoRA if configured\n",
    "    if config[\"use_lora\"]:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Configure LoRA based on experiment type\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            task_type = TaskType.SEQ_CLS\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        else:\n",
    "            task_type = TaskType.CAUSAL_LM\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            r=config[\"lora_rank\"],\n",
    "            lora_alpha=config[\"lora_alpha\"],\n",
    "            lora_dropout=config[\"lora_dropout\"],\n",
    "            target_modules=target_modules,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"\n",
    "    Convenience function that loads both model and tokenizer\n",
    "    Uses the modular functions above\n",
    "    \"\"\"\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    # Load components separately\n",
    "    tokenizer = load_tokenizer(model_name)\n",
    "    model = load_model(model_name, config)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d9eb1",
   "metadata": {},
   "source": [
    "Cell 8: Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a089c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458b884d",
   "metadata": {},
   "source": [
    "Cell 10: Compute metrics function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69987512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simplified metrics functions loaded (with text normalization)!\n",
      "\n",
      "To evaluate your baseline results, run:\n",
      "baseline_metrics = evaluate_results(baseline_results, CONFIG, 'Baseline')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for flexible comparison (removes spaces, converts to lowercase).\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', '', str(text).lower().strip())\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"Extract JSON from model response, handling various formatting issues.\"\"\"\n",
    "    if not response: return {}\n",
    "    response = response.strip()\n",
    "    patterns = [r'\\{.*\\}', r'```json\\s*(\\{.*\\})\\s*```', r'```\\s*(\\{.*\\})\\s*```']\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try: return json.loads(match)\n",
    "            except json.JSONDecodeError: continue\n",
    "    try: return json.loads(response)\n",
    "    except json.JSONDecodeError: return {}\n",
    "\n",
    "def compute_metrics_for_trainer(eval_pred: EvalPrediction, config, tokenizer):\n",
    "    \"\"\"Computes metrics from trainer's predictions.\"\"\"\n",
    "    # Predictions are logit scores, labels are the input_ids\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions, ignoring padding tokens\n",
    "    # We only care about the generated part, but for simplicity, we decode all.\n",
    "    # The JSON extraction will find the relevant part.\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode labels to get the expected JSON output\n",
    "    # Replace -100 (ignored by loss function) with pad token before decoding\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    verdict_expected, verdict_predicted = [], []\n",
    "    eln_expected, eln_predicted = [], []\n",
    "    parse_failures = 0\n",
    "\n",
    "    for pred_text, label_text in zip(decoded_preds, decoded_labels):\n",
    "        pred_json = extract_json_from_response(pred_text)\n",
    "        expected_json = extract_json_from_response(label_text)\n",
    "\n",
    "        if not pred_json:\n",
    "            parse_failures += 1\n",
    "        \n",
    "        verdict_expected.append(expected_json.get(\"verdict\", \"unknown\"))\n",
    "        verdict_predicted.append(pred_json.get(\"verdict\", \"unknown\"))\n",
    "        \n",
    "        if config[\"include_eln\"]:\n",
    "            key = \"erroneous_line\" if config[\"solution_format\"] == \"nl\" else \"erroneous_line_number\"\n",
    "            expected_line = str(expected_json.get(key, \"\"))\n",
    "            predicted_line = str(pred_json.get(key, \"\"))\n",
    "            eln_expected.append(normalize_text(expected_line))\n",
    "            eln_predicted.append(normalize_text(predicted_line))\n",
    "\n",
    "    # Calculate metrics\n",
    "    verdict_accuracy = accuracy_score(verdict_expected, verdict_predicted)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(verdict_expected, verdict_predicted, average='macro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        \"verdict_accuracy\": verdict_accuracy,\n",
    "        \"verdict_precision\": precision,\n",
    "        \"verdict_recall\": recall,\n",
    "        \"verdict_f1\": f1,\n",
    "        \"parse_failures\": parse_failures\n",
    "    }\n",
    "    \n",
    "    if config[\"include_eln\"]:\n",
    "        metrics[\"eln_accuracy\"] = accuracy_score(eln_expected, eln_predicted)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Print simple metrics summary.\"\"\"\n",
    "    print(f\"\\n{stage_name} Results:\")\n",
    "    print(f\"Total samples: {metrics['total_samples']} (Parse failures: {metrics['parse_failures']})\")\n",
    "    print(f\"Verdict - Accuracy: {metrics['verdict_accuracy']:.3f}, Precision: {metrics['verdict_precision']:.3f}, Recall: {metrics['verdict_recall']:.3f}, F1: {metrics['verdict_f1']:.3f}\")\n",
    "    \n",
    "    if \"eln_accuracy\" in metrics:\n",
    "        print(f\"ELN Accuracy: {metrics['eln_accuracy']:.3f}\")\n",
    "\n",
    "# Simple evaluation function\n",
    "def evaluate_results(results, config, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Evaluate results with simple metrics.\"\"\"\n",
    "    metrics = compute_metrics(results, config)\n",
    "    print_metrics(metrics, stage_name)\n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Simplified metrics functions loaded (with text normalization)!\")\n",
    "print(\"\\nTo evaluate your baseline results, run:\")\n",
    "print(\"baseline_metrics = evaluate_results(baseline_results, CONFIG, 'Baseline')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95700a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "from functools import partial\n",
    "\n",
    "def setup_trainer(model, tokenizer, train_dataset, eval_dataset, config):\n",
    "    \"\"\"Sets up the Trainer for fine-tuning.\"\"\"\n",
    "    \n",
    "    # Data collator for language modeling (pads batches dynamically)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    \n",
    "    output_dir = Path(config[\"output_dir\"]) / \"training\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        num_train_epochs=config[\"num_epochs\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"],\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        eval_accumulation_steps=config[\"gradient_accumulation_steps\"],  # No accumulation during eval\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "\n",
    "        # Evaluation and saving strategy\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=25,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=25,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eln_accuracy\", # Make sure this metric exists\n",
    "        greater_is_better=True,\n",
    "\n",
    "        # Other settings\n",
    "        logging_steps=25,\n",
    "        fp16=True,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Use a partial function to pass config and tokenizer to the metrics function\n",
    "    compute_metrics_with_config = partial(\n",
    "        compute_metrics_for_trainer, \n",
    "        config=config, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics_with_config, # Use the new metrics function\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Trainer initialized successfully!\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Main Execution Block\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== 1. SETUP ===\")\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(CONFIG)\n",
    "# Setup output directory\n",
    "output_dir = setup_output_directory(CONFIG)\n",
    "CONFIG[\"output_dir\"] = str(output_dir)\n",
    "\n",
    "print(\"\\n=== 2. DATA PREPARATION ===\")\n",
    "# Load and process datasets\n",
    "train_dataset, eval_dataset, examples = prepare_dataset(CONFIG, tokenizer)\n",
    "\n",
    "print(\"\\n=== 3. TRAINER SETUP ===\")\n",
    "# Initialize the Trainer\n",
    "trainer = setup_trainer(model, tokenizer, train_dataset, eval_dataset, CONFIG)\n",
    "\n",
    "print(\"\\n=== 4. BASELINE EVALUATION ===\")\n",
    "# Evaluate the base, untrained model\n",
    "baseline_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Baseline Metrics:\", baseline_metrics)\n",
    "# Save baseline metrics\n",
    "with open(Path(output_dir) / \"baseline\" / \"baseline_metrics.json\", 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n=== 5. STARTING TRAINING ===\")\n",
    "# Start fine-tuning\n",
    "train_result = trainer.train()\n",
    "print(\"✅ Training completed!\")\n",
    "\n",
    "# Save training metrics and final model\n",
    "trainer.save_model() # Saves the best model to the output_dir\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(\"\\n=== 6. FINAL EVALUATION ===\")\n",
    "# Evaluate the fine-tuned model\n",
    "final_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Final Metrics:\", final_metrics)\n",
    "# Save final metrics\n",
    "trainer.log_metrics(\"eval\", final_metrics)\n",
    "trainer.save_metrics(\"eval\", final_metrics)\n",
    "\n",
    "print(\"\\n✅✅✅ Experiment pipeline finished successfully! ✅✅✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset and model for baseline inference\n",
    "# print(\"=== LOADING DATASET AND MODEL ===\")\n",
    "\n",
    "# # Load and prepare dataset\n",
    "# train_data, eval_data, examples = prepare_dataset(CONFIG)\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model, tokenizer = load_model_and_tokenizer(CONFIG)\n",
    "\n",
    "# print(f\"✅ Dataset loaded: {len(train_data)} train, {len(eval_data)} eval\")\n",
    "# print(f\"✅ Model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform baseline inference on evaluation set\n",
    "# print(\"=== BASELINE INFERENCE ===\")\n",
    "\n",
    "# # Prepare inference data (use first 50 samples for faster testing)\n",
    "# eval_subset = eval_data[:50]  # Adjust size as needed\n",
    "# messages_batch = [sample[\"messages\"] for sample in eval_subset]\n",
    "\n",
    "# # Prepare inference batch\n",
    "# prepared_inputs = prepare_inference_batch(\n",
    "#     messages_batch, \n",
    "#     tokenizer, \n",
    "#     max_length=CONFIG[\"max_length\"]\n",
    "# )\n",
    "\n",
    "# # Apply padding\n",
    "# padded_inputs = apply_batch_padding(prepared_inputs, tokenizer)\n",
    "\n",
    "# # Run inference\n",
    "# baseline_responses, baseline_metadata = run_inference(\n",
    "#     model, tokenizer, padded_inputs\n",
    "# )\n",
    "\n",
    "# # Format results\n",
    "# baseline_results = []\n",
    "# for i, sample in enumerate(eval_subset):\n",
    "#     baseline_results.append({\n",
    "#         \"id\": sample[\"id\"],\n",
    "#         \"expected_output\": sample[\"expected_output\"],\n",
    "#         \"model_response\": baseline_responses[i],\n",
    "#         \"metadata\": sample[\"metadata\"]\n",
    "#     })\n",
    "\n",
    "# print(f\"✅ Baseline inference completed on {len(baseline_results)} samples\")\n",
    "# print(f\"Avg inference time: {baseline_metadata['avg_inference_time_per_sample']:.2f}s\")\n",
    "\n",
    "# # Evaluate baseline\n",
    "# baseline_metrics = evaluate_results(baseline_results, CONFIG, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save baseline results\n",
    "# print(\"=== SAVING BASELINE RESULTS ===\")\n",
    "\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Create output directory\n",
    "# output_dir = Path(f\"./baseline_results_{CONFIG['experiment_id']}\")\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Save results\n",
    "# baseline_results_path = output_dir / \"baseline_results.json\"\n",
    "# with open(baseline_results_path, 'w') as f:\n",
    "#     json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "# # Save metadata  \n",
    "# baseline_metadata_path = output_dir / \"baseline_metadata.json\"\n",
    "# baseline_metadata.update(baseline_metrics)  # Include metrics in metadata\n",
    "# with open(baseline_metadata_path, 'w') as f:\n",
    "#     json.dump(baseline_metadata, f, indent=2, default=str)\n",
    "\n",
    "# # Save config\n",
    "# config_path = output_dir / \"config.json\"\n",
    "# with open(config_path, 'w') as f:\n",
    "#     json.dump(CONFIG, f, indent=2, default=str)\n",
    "\n",
    "# print(f\"✅ Baseline results saved to: {output_dir}\")\n",
    "# print(f\"   - Results: {baseline_results_path}\")\n",
    "# print(f\"   - Metadata: {baseline_metadata_path}\")\n",
    "# print(f\"   - Config: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68016b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup and perform training\n",
    "# print(\"=== SETTING UP TRAINING ===\")\n",
    "\n",
    "# # Setup training components\n",
    "# trainer, train_dataset, eval_dataset = setup_training_components(\n",
    "#     model, tokenizer, train_data, eval_data, CONFIG\n",
    "# )\n",
    "\n",
    "# print(\"=== STARTING TRAINING ===\")\n",
    "\n",
    "# # Start training\n",
    "# training_results = trainer.train()\n",
    "\n",
    "# print(\"✅ Training completed!\")\n",
    "# print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
    "# print(f\"Training steps: {training_results.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save training results and model\n",
    "# print(\"=== SAVING TRAINING RESULTS ===\")\n",
    "\n",
    "# # Create training output directory\n",
    "# training_output_dir = Path(f\"./training_results_{CONFIG['experiment_id']}\")\n",
    "# training_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Save training history\n",
    "# training_history_path = training_output_dir / \"training_history.json\"\n",
    "# training_history = {\n",
    "#     \"training_loss\": training_results.training_loss,\n",
    "#     \"global_step\": training_results.global_step,\n",
    "#     \"training_time\": str(training_results.metrics.get('train_runtime', 'unknown')),\n",
    "#     \"log_history\": trainer.state.log_history\n",
    "# }\n",
    "\n",
    "# with open(training_history_path, 'w') as f:\n",
    "#     json.dump(training_history, f, indent=2, default=str)\n",
    "\n",
    "# # Save model locally\n",
    "# local_model_path = training_output_dir / \"trained_model\"\n",
    "# trainer.save_model(str(local_model_path))\n",
    "# tokenizer.save_pretrained(str(local_model_path))\n",
    "\n",
    "# print(f\"✅ Training results saved to: {training_output_dir}\")\n",
    "# print(f\"   - Training history: {training_history_path}\")\n",
    "# print(f\"   - Model: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform final inference with trained model\n",
    "# print(\"=== FINAL INFERENCE ===\")\n",
    "\n",
    "# # The trainer already has the best model loaded\n",
    "# # Prepare the same evaluation subset\n",
    "# prepared_inputs_final = prepare_inference_batch(\n",
    "#     messages_batch, \n",
    "#     tokenizer, \n",
    "#     max_length=CONFIG[\"max_length\"]\n",
    "# )\n",
    "\n",
    "# # Apply padding\n",
    "# padded_inputs_final = apply_batch_padding(prepared_inputs_final, tokenizer)\n",
    "\n",
    "# # Run final inference\n",
    "# final_responses, final_metadata = run_inference(\n",
    "#     trainer.model, tokenizer, padded_inputs_final\n",
    "# )\n",
    "\n",
    "# # Format final results\n",
    "# final_results = []\n",
    "# for i, sample in enumerate(eval_subset):\n",
    "#     final_results.append({\n",
    "#         \"id\": sample[\"id\"],\n",
    "#         \"expected_output\": sample[\"expected_output\"],\n",
    "#         \"model_response\": final_responses[i],\n",
    "#         \"metadata\": sample[\"metadata\"]\n",
    "#     })\n",
    "\n",
    "# print(f\"✅ Final inference completed on {len(final_results)} samples\")\n",
    "# print(f\"Avg inference time: {final_metadata['avg_inference_time_per_sample']:.2f}s\")\n",
    "\n",
    "# # Evaluate final results\n",
    "# final_metrics = evaluate_results(final_results, CONFIG, \"Final\")\n",
    "\n",
    "# # Compare with baseline\n",
    "# print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "# print(f\"Baseline Accuracy: {baseline_metrics['verdict_accuracy']:.3f}\")\n",
    "# print(f\"Final Accuracy: {final_metrics['verdict_accuracy']:.3f}\")\n",
    "# print(f\"Improvement: {final_metrics['verdict_accuracy'] - baseline_metrics['verdict_accuracy']:.3f}\")\n",
    "\n",
    "# if CONFIG[\"include_eln\"]:\n",
    "#     print(f\"Baseline ELN Accuracy: {baseline_metrics.get('eln_accuracy', 'N/A')}\")\n",
    "#     print(f\"Final ELN Accuracy: {final_metrics.get('eln_accuracy', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save final results\n",
    "# print(\"=== SAVING FINAL RESULTS ===\")\n",
    "\n",
    "# # Save final inference results\n",
    "# final_results_path = training_output_dir / \"final_results.json\"\n",
    "# with open(final_results_path, 'w') as f:\n",
    "#     json.dump(final_results, f, indent=2)\n",
    "\n",
    "# # Save final metadata\n",
    "# final_metadata_path = training_output_dir / \"final_metadata.json\"\n",
    "# final_metadata.update(final_metrics)  # Include metrics\n",
    "# with open(final_metadata_path, 'w') as f:\n",
    "#     json.dump(final_metadata, f, indent=2, default=str)\n",
    "\n",
    "# # Save comparison results\n",
    "# comparison_results = {\n",
    "#     \"baseline_metrics\": baseline_metrics,\n",
    "#     \"final_metrics\": final_metrics,\n",
    "#     \"improvement\": {\n",
    "#         \"verdict_accuracy\": final_metrics['verdict_accuracy'] - baseline_metrics['verdict_accuracy'],\n",
    "#         \"verdict_f1\": final_metrics['verdict_f1'] - baseline_metrics['verdict_f1']\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# if CONFIG[\"include_eln\"]:\n",
    "#     comparison_results[\"improvement\"][\"eln_accuracy\"] = final_metrics.get('eln_accuracy', 0) - baseline_metrics.get('eln_accuracy', 0)\n",
    "\n",
    "# comparison_path = training_output_dir / \"performance_comparison.json\"\n",
    "# with open(comparison_path, 'w') as f:\n",
    "#     json.dump(comparison_results, f, indent=2, default=str)\n",
    "\n",
    "# print(f\"✅ Final results saved:\")\n",
    "# print(f\"   - Results: {final_results_path}\")\n",
    "# print(f\"   - Metadata: {final_metadata_path}\")\n",
    "# print(f\"   - Comparison: {comparison_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
