{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d6924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq transformers==4.53.2\n",
    "!pip install -Uq peft\n",
    "!pip install -Uq trl\n",
    "!pip install -Uq accelerate\n",
    "!pip install -Uq datasets\n",
    "!pip install -Uq bitsandbytes\n",
    "\n",
    "# Install Flash Attention 2\n",
    "!pip install flash-attn==2.7.4.post1 \\\n",
    "  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "  --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa10bfe",
   "metadata": {},
   "source": [
    "Cell 1: Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14c05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: gene_ter_4N_exp_eln_nl_phi4_20250731_152617\n",
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===== EXPERIMENT CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",  # \"discriminative\" or \"generative\"\n",
    "    \"classification_type\": \"ternary\",   # \"binary\" or \"ternary\"\n",
    "    \"dataset_strategy\": \"4N\",          # \"4N\" or \"3N\" (generative only)\n",
    "    \"include_explanation\": True,      # True or False (generative only)\n",
    "    \"include_eln\": True,              # True or False (generative only)\n",
    "    \"solution_format\": \"nl\",        # \"dict\" or \"nl\" (generative only)\n",
    "    \"model_name\": \"microsoft/phi-4-mini-instruct\",  # or \"Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # Prompting configuration\n",
    "    \"system_prompt\": None,  # Will auto-generate if None, or use custom string\n",
    "    \"include_examples\": False,\n",
    "    \"num_examples\": 3,\n",
    "    \"example_strategy\": \"balanced\",  # \"balanced\", \"error_focused\", \"custom\"\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": 1600,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    # Infrastructure\n",
    "    \"use_lora\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Paths and tokens\n",
    "    # \"base_dataset_dir\": \"/content/drive/MyDrive/sft_datasets\",\n",
    "    \"base_dataset_dir\": \"../data/base-datasets-sanitized\",\n",
    "    \"output_base_dir\": \"/content/drive/MyDrive/sft_experiments\",\n",
    "    # \"hf_token\": \"your_huggingface_token_here\",\n",
    "    # \"wandb_project\": \"math_error_classification\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "    \"use_wandb\": False\n",
    "}\n",
    "\n",
    "# Generate experiment ID\n",
    "import datetime\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],  # \"gene\" or \"disc\"\n",
    "    CONFIG[\"classification_type\"][:3],  # \"bin\" or \"ter\"\n",
    "    CONFIG[\"dataset_strategy\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"qwen\" if \"Qwen\" in CONFIG[\"model_name\"] else \"phi4\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766c573",
   "metadata": {},
   "source": [
    "Cell 2: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c7a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported and seeds set!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"Dependencies imported and seeds set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd140f7",
   "metadata": {},
   "source": [
    "Cell 3: System Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ef498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Classify as 'correct', 'conceptual_error', or 'computational_error'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.\n",
      "\n",
      "To customize the system prompt, run:\n",
      "CONFIG[\"system_prompt\"] = \"Your custom prompt here\"\n"
     ]
    }
   ],
   "source": [
    "def generate_system_prompt(config):\n",
    "    \"\"\"Auto-generates appropriate system prompt based on config\"\"\"\n",
    "    \n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        return \"You are a mathematics tutor. Classify the given solution.\"\n",
    "    \n",
    "    # Generative prompts\n",
    "    base_prompt = \"You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format.\"\n",
    "    \n",
    "    # Add classification instructions\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        base_prompt += \" Determine if the solution is 'correct' or 'flawed'.\"\n",
    "    else:\n",
    "        base_prompt += \" Classify as 'correct', 'conceptual_error', or 'computational_error'.\"\n",
    "    \n",
    "    # Add field instructions\n",
    "    fields = []\n",
    "    if config[\"include_eln\"]:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            fields.append(\"identify the erroneous line number (e.g., 'L1', 'FA')\")\n",
    "        else:\n",
    "            fields.append(\"quote the full erroneous line text\")\n",
    "    \n",
    "    if config[\"include_explanation\"]:\n",
    "        fields.append(\"provide a brief explanation of any error\")\n",
    "    \n",
    "    if fields:\n",
    "        base_prompt += f\" Also {', and '.join(fields)}.\"\n",
    "    \n",
    "    base_prompt += \" Respond only with valid JSON.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Auto-generate system prompt if not provided\n",
    "if CONFIG[\"system_prompt\"] is None:\n",
    "    CONFIG[\"system_prompt\"] = generate_system_prompt(CONFIG)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(CONFIG[\"system_prompt\"])\n",
    "print()\n",
    "\n",
    "# Allow manual override\n",
    "print(\"To customize the system prompt, run:\")\n",
    "print('CONFIG[\"system_prompt\"] = \"Your custom prompt here\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41b208",
   "metadata": {},
   "source": [
    "Cell 4: Example Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3522a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ExampleManager class loaded!\n"
     ]
    }
   ],
   "source": [
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset, config):\n",
    "        # Convert DataFrame to list of dicts if needed\n",
    "        if hasattr(base_dataset, 'to_dict'):  # It's a DataFrame\n",
    "            self.samples = base_dataset.to_dict('records')\n",
    "        else:\n",
    "            self.samples = base_dataset  # Already a list of dicts\n",
    "            \n",
    "        self.config = config\n",
    "        self._prepare_examples_by_problem()\n",
    "    \n",
    "    def _prepare_examples_by_problem(self):\n",
    "        \"\"\"Organizes samples by problem index and error type\"\"\"\n",
    "        self.problems_by_type = {\n",
    "            \"correct\": {},\n",
    "            \"conceptual_error\": {},\n",
    "            \"computational_error\": {}\n",
    "        }\n",
    "        \n",
    "        # Group samples by problem index and error type\n",
    "        for sample in self.samples:\n",
    "            problem_index = sample[\"index\"]\n",
    "            error_type = sample[\"error_type\"]\n",
    "            \n",
    "            if problem_index not in self.problems_by_type[error_type]:\n",
    "                self.problems_by_type[error_type][problem_index] = []\n",
    "            self.problems_by_type[error_type][problem_index].append(sample)\n",
    "        \n",
    "        print(f\"Problems by type: correct={len(self.problems_by_type['correct'])}, \"\n",
    "              f\"conceptual={len(self.problems_by_type['conceptual_error'])}, \"\n",
    "              f\"computational={len(self.problems_by_type['computational_error'])}\")\n",
    "    \n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns examples based on dataset strategy\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        \n",
    "        num_examples = self.config[\"num_examples\"]\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        \n",
    "        import random\n",
    "        \n",
    "        if dataset_strategy == \"3N\":\n",
    "            # Choose num_examples distinct problem indices that have all 3 versions\n",
    "            available_problems = set(self.problems_by_type[\"correct\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"conceptual_error\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            \n",
    "            if not available_problems:\n",
    "                print(\"Warning: No problems found with all 3 versions (correct/conceptual/computational)\")\n",
    "                return []\n",
    "            \n",
    "            # Sample problem indices\n",
    "            selected_problems = random.sample(\n",
    "                list(available_problems), \n",
    "                min(num_examples, len(available_problems))\n",
    "            )\n",
    "            \n",
    "            examples = []\n",
    "            for problem_index in selected_problems:\n",
    "                # Add all 3 versions: correct, conceptual_error, computational_error\n",
    "                examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "            \n",
    "        elif dataset_strategy == \"4N\":\n",
    "            import math\n",
    "            \n",
    "            # Get problems that have conceptual errors (with correct versions)\n",
    "            conceptual_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"conceptual_error\"].keys())\n",
    "            )\n",
    "            \n",
    "            # Get problems that have computational errors (with correct versions)\n",
    "            computational_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            )\n",
    "            \n",
    "            # Calculate splits: floor(n/2) conceptual, ceil(n/2) computational\n",
    "            n_conceptual = num_examples // 2  # This is floor(n/2)\n",
    "            n_computational = math.ceil(num_examples / 2)\n",
    "            \n",
    "            examples = []\n",
    "            \n",
    "            # Sample conceptual problems\n",
    "            if conceptual_problems and n_conceptual > 0:\n",
    "                selected_conceptual = random.sample(\n",
    "                    conceptual_problems,\n",
    "                    min(n_conceptual, len(conceptual_problems))\n",
    "                )\n",
    "                \n",
    "                for problem_index in selected_conceptual:\n",
    "                    # Add correct + conceptual_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "            \n",
    "            # Sample computational problems\n",
    "            if computational_problems and n_computational > 0:\n",
    "                selected_computational = random.sample(\n",
    "                    computational_problems,\n",
    "                    min(n_computational, len(computational_problems))\n",
    "                )\n",
    "                \n",
    "                for problem_index in selected_computational:\n",
    "                    # Add correct + computational_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "        \n",
    "        else:\n",
    "            print(f\"Warning: Unknown dataset strategy '{dataset_strategy}'\")\n",
    "            return []\n",
    "\n",
    "print(\"Updated ExampleManager class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246c873",
   "metadata": {},
   "source": [
    "Cell 5: Dataset Loading and Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc8d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated formatting functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_base_dataset(config):\n",
    "    \"\"\"Loads the appropriate base dataset\"\"\"\n",
    "    dataset_strategy = config[\"dataset_strategy\"]\n",
    "    base_dir = Path(config[\"base_dataset_dir\"])\n",
    "    \n",
    "    dataset_file = base_dir / f\"base_{dataset_strategy}_dataset_sanitized.csv\"\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        raise FileNotFoundError(f\"Base dataset not found: {dataset_file}\")\n",
    "\n",
    "    data = pd.read_csv(dataset_file)\n",
    "\n",
    "    print(f\"Loaded base {dataset_strategy} dataset with {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "def format_solution(sample, config):\n",
    "    \"\"\"Formats solution according to config - updated for CSV structure\"\"\"\n",
    "    # Use wrong_answer for the solution (this contains the solution steps)\n",
    "    solution_text = sample.get('wrong_answer', sample.get('correct_answer', ''))\n",
    "    \n",
    "    if config[\"solution_format\"] == \"dict\":\n",
    "        # Split solution into lines and format as dict\n",
    "        lines = solution_text.strip().split('\\n')\n",
    "        solution = {}\n",
    "        for i, line in enumerate(lines[:-1]):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                solution[f\"L{i+1}\"] = line.strip()\n",
    "        if lines and lines[-1].strip():\n",
    "            solution[\"FA\"] = lines[-1].strip()\n",
    "        return json.dumps(solution, indent=2)\n",
    "    else:\n",
    "        return solution_text.strip()\n",
    "\n",
    "def format_expected_output(sample, config):\n",
    "    \"\"\"Creates the expected JSON output for a sample - updated for CSV structure\"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    # Verdict\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        output[\"verdict\"] = \"correct\" if sample[\"error_type\"] == \"correct\" else \"flawed\"\n",
    "    else:\n",
    "        output[\"verdict\"] = sample[\"error_type\"]\n",
    "    \n",
    "    # ELN (Erroneous Line Number)\n",
    "    if config[\"include_eln\"]:\n",
    "        if sample[\"error_type\"] != \"correct\":\n",
    "            if config[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = sample.get(\"erroneous_line_number\", None)\n",
    "            else:\n",
    "                # For natural language format, try to extract the actual erroneous line text\n",
    "                eln = sample.get(\"erroneous_line_number\")\n",
    "                if eln and pd.notna(eln):\n",
    "                    # Extract line number (e.g., \"L3\" -> 3)\n",
    "                    try:\n",
    "                        if eln.startswith('L'):\n",
    "                            line_num = int(eln[1:]) - 1\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            if 0 <= line_num < len(solution_lines):\n",
    "                                output[\"erroneous_line\"] = solution_lines[line_num].strip()\n",
    "                            else:\n",
    "                                output[\"erroneous_line\"] = eln  # Fallback to the ELN itself\n",
    "                        elif eln == 'FA':\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            output[\"erroneous_line\"] = solution_lines[-1].strip() if solution_lines else None\n",
    "                        else:\n",
    "                            output[\"erroneous_line\"] = eln\n",
    "                    except:\n",
    "                        output[\"erroneous_line\"] = eln\n",
    "                else:\n",
    "                    output[\"erroneous_line\"] = None\n",
    "        else:\n",
    "            key = \"erroneous_line_number\" if config[\"solution_format\"] == \"dict\" else \"erroneous_line\"\n",
    "            output[key] = None\n",
    "    \n",
    "    # Explanation\n",
    "    if config[\"include_explanation\"]:\n",
    "        explanation = sample.get(\"explanation\")\n",
    "        output[\"explanation\"] = explanation if pd.notna(explanation) and sample[\"error_type\"] != \"correct\" else None\n",
    "    \n",
    "    return json.dumps(output)\n",
    "\n",
    "def format_user_message(sample, config):\n",
    "    \"\"\"Format a sample into a user message.\"\"\"\n",
    "    return f\"### Question:\\n{sample['question']}\\n\\n### Answer:\\n{format_solution(sample, config)}\"\n",
    "\n",
    "def create_sample_messages(sample, examples, config):\n",
    "    \"\"\"Create complete message list for a sample.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # System message\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": config[\"system_prompt\"]\n",
    "    })\n",
    "    \n",
    "    # Few-shot examples\n",
    "    if config[\"include_examples\"]:\n",
    "        for example in examples:\n",
    "            user_content = format_user_message(example, config)\n",
    "            assistant_content = format_expected_output(example, config)\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "    \n",
    "    # Actual sample\n",
    "    user_content = format_user_message(sample, config)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "print(\"Updated formatting functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455adbd",
   "metadata": {},
   "source": [
    "Cell 6: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(config):\n",
    "    \"\"\"Loads and formats complete dataset for training/evaluation\"\"\"\n",
    "    \n",
    "    # Load base dataset\n",
    "    base_samples = load_base_dataset(config)\n",
    "    \n",
    "    # Initialize example manager - convert DataFrame to list of dicts\n",
    "    example_manager = ExampleManager(base_samples, config)\n",
    "    examples = example_manager.get_examples()\n",
    "    \n",
    "    print(f\"Using {len(examples)} few-shot examples\")\n",
    "    \n",
    "    # Format all samples\n",
    "    formatted_data = []\n",
    "    \n",
    "    # Iterate over DataFrame rows\n",
    "    for idx, sample in base_samples.iterrows():\n",
    "        # Convert pandas Series to dict for easier access\n",
    "        sample_dict = sample.to_dict()\n",
    "        \n",
    "        # Use modular function to create messages\n",
    "        messages = create_sample_messages(sample_dict, examples, config)\n",
    "        \n",
    "        # Expected output\n",
    "        expected_output = format_expected_output(sample_dict, config)\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"id\": sample_dict.get(\"id\", f\"sample_{len(formatted_data)}\"),\n",
    "            \"messages\": messages,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"metadata\": {\n",
    "                \"error_type\": sample_dict[\"error_type\"],\n",
    "                \"tier\": sample_dict.get(\"tier\", \"unknown\"),\n",
    "                \"source\": sample_dict.get(\"source\", \"unknown\")\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Split into train/eval\n",
    "    split_point = int(0.8 * len(formatted_data))\n",
    "    train_data = formatted_data[:split_point]\n",
    "    eval_data = formatted_data[split_point:]\n",
    "    \n",
    "    print(f\"Dataset prepared: {len(train_data)} training, {len(eval_data)} evaluation samples\")\n",
    "    \n",
    "    return train_data, eval_data, examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3af4e",
   "metadata": {},
   "source": [
    "Cell 7: Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4817613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer with proper configuration\"\"\"\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\"  # Ensure left padding for causal models\n",
    "    \n",
    "    print(f\"✓ Tokenizer loaded successfully!\")\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(model_name, config):\n",
    "    \"\"\"Loads model with appropriate configuration\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Configure quantization if using LoRA\n",
    "    bnb_config = None\n",
    "    if config[\"use_lora\"]:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "    \n",
    "    # Load model based on experiment type\n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        # For discriminative, we need a classification model\n",
    "        num_labels = 2 if config[\"classification_type\"] == \"binary\" else 3\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        # For generative, use causal LM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    \n",
    "    # Apply LoRA if configured\n",
    "    if config[\"use_lora\"]:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Configure LoRA based on experiment type\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            task_type = TaskType.SEQ_CLS\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        else:\n",
    "            task_type = TaskType.CAUSAL_LM\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            r=config[\"lora_rank\"],\n",
    "            lora_alpha=config[\"lora_alpha\"],\n",
    "            lora_dropout=config[\"lora_dropout\"],\n",
    "            target_modules=target_modules,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def apply_chat_template(\n",
    "        messages, \n",
    "        tokenizer, \n",
    "        add_generation_prompt=False, \n",
    "        tokenize=True, \n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Applies chat template to messages with consistent interface\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' keys\n",
    "        tokenizer: The tokenizer to use for formatting\n",
    "        add_generation_prompt: Whether to add generation prompt (for inference)\n",
    "        tokenize: Whether to return tokens (True) or text (False)\n",
    "        **kwargs: Additional arguments for tokenizer (like return_tensors, max_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        If tokenize=True: tokenizer output dict with input_ids, attention_mask, etc.\n",
    "        If tokenize=False: formatted text string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if this is a Qwen3 model and disable thinking if so\n",
    "    template_kwargs = {}\n",
    "    if CONFIG[\"model_name\"].startswith(\"Qwen\"):\n",
    "        template_kwargs['enable_thinking'] = False\n",
    "    \n",
    "    # Apply chat template to get formatted text\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        **template_kwargs\n",
    "    )\n",
    "    \n",
    "    # Return text if not tokenizing\n",
    "    if not tokenize:\n",
    "        return formatted_text\n",
    "    \n",
    "    # Tokenize and return tensor format\n",
    "    return tokenizer(formatted_text, **kwargs)\n",
    "\n",
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"\n",
    "    Convenience function that loads both model and tokenizer\n",
    "    Uses the modular functions above\n",
    "    \"\"\"\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    # Load components separately\n",
    "    tokenizer = load_tokenizer(model_name)\n",
    "    model = load_model(model_name, config)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d9eb1",
   "metadata": {},
   "source": [
    "Cell 8: Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a089c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_output_directory(config):\n",
    "#     \"\"\"Creates organized output directory structure\"\"\"\n",
    "    \n",
    "#     output_dir = Path(config[\"output_base_dir\"]) / config[\"experiment_id\"]\n",
    "    \n",
    "#     # Create subdirectories\n",
    "#     subdirs = [\"baseline\", \"training\", \"final\", \"checkpoints\"]\n",
    "#     for subdir in subdirs:\n",
    "#         (output_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Save configuration\n",
    "#     config_path = output_dir / \"config.json\"\n",
    "#     with open(config_path, 'w') as f:\n",
    "#         json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "#     print(f\"Output directory created: {output_dir}\")\n",
    "#     return output_dir\n",
    "\n",
    "# # Setup output directory\n",
    "# output_dir = setup_output_directory(CONFIG)\n",
    "# CONFIG[\"output_dir\"] = str(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f1ffe",
   "metadata": {},
   "source": [
    "Cell 9: Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc7def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def prepare_inference_batch(messages_batch, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Prepares a batch of messages for inference by applying chat templates and tokenizing.\n",
    "    \n",
    "    Args:\n",
    "        messages_batch: List of message conversations (each is a list of message dicts)\n",
    "        tokenizer: The tokenizer to use\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        dict: Batch with input_ids, attention_mask, and metadata\n",
    "    \"\"\"\n",
    "    batch_data = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"metadata\": {\n",
    "            \"formatted_prompts\": [],\n",
    "            \"input_token_counts\": [],\n",
    "            \"conversation_lengths\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for messages in messages_batch:\n",
    "        # Use our custom apply_chat_template function\n",
    "        formatted_prompt = apply_chat_template(\n",
    "            messages,\n",
    "            tokenizer,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted prompt\n",
    "        tokenized = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False  # We'll pad at batch level\n",
    "        )\n",
    "        \n",
    "        # Store batch data\n",
    "        batch_data[\"input_ids\"].append(tokenized[\"input_ids\"].squeeze(0))\n",
    "        batch_data[\"attention_mask\"].append(tokenized[\"attention_mask\"].squeeze(0))\n",
    "        \n",
    "        # Store metadata\n",
    "        batch_data[\"metadata\"][\"formatted_prompts\"].append(formatted_prompt)\n",
    "        batch_data[\"metadata\"][\"input_token_counts\"].append(len(tokenized[\"input_ids\"][0]))\n",
    "        batch_data[\"metadata\"][\"conversation_lengths\"].append(len(messages))\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "def apply_batch_padding(batch_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Applies padding to a batch of tokenized sequences.\n",
    "    \n",
    "    Args:\n",
    "        batch_data: Output from prepare_inference_batch\n",
    "        tokenizer: The tokenizer (for pad_token_id)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Padded tensors ready for model input\n",
    "    \"\"\"\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    input_ids_padded = pad_sequence(\n",
    "        batch_data[\"input_ids\"],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    attention_mask_padded = pad_sequence(\n",
    "        batch_data[\"attention_mask\"],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"metadata\": batch_data[\"metadata\"]\n",
    "    }\n",
    "\n",
    "def decode_batch_outputs(outputs, input_lengths, tokenizer):\n",
    "    \"\"\"\n",
    "    Decodes model outputs for a batch, extracting only the generated portions.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Model generation outputs (batch_size, sequence_length)\n",
    "        input_lengths: List of input sequence lengths for each item in batch\n",
    "        tokenizer: The tokenizer for decoding\n",
    "        \n",
    "    Returns:\n",
    "        list: Decoded responses (only the generated parts)\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for i, output_sequence in enumerate(outputs):\n",
    "        # Extract only the generated tokens (after input)\n",
    "        input_length = input_lengths[i]\n",
    "        generated_tokens = output_sequence[input_length:]\n",
    "        \n",
    "        # Decode to text\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        responses.append(response.strip())\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def create_attention_masks(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates attention masks for tokenized inputs.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Tokenized input sequences\n",
    "        tokenizer: The tokenizer (for pad_token_id)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Attention masks\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids = torch.stack(input_ids)\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return attention_mask\n",
    "\n",
    "def run_inference(model, tokenizer, prepared_inputs, batch_size=1):\n",
    "    \"\"\"\n",
    "    Pure inference function that accepts pre-processed inputs and returns results.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for inference\n",
    "        tokenizer: The tokenizer (only used for pad_token_id in generation)\n",
    "        prepared_inputs: Pre-processed batch of inputs with input_ids, attention_mask, metadata\n",
    "        batch_size: Batch size for processing (legacy parameter for compatibility)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (responses, generation_metadata)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move inputs to model device\n",
    "        input_ids = prepared_inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = prepared_inputs[\"attention_mask\"].to(model.device)\n",
    "        \n",
    "        # Generate responses\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode outputs\n",
    "        input_lengths = prepared_inputs[\"metadata\"][\"input_token_counts\"]\n",
    "        responses = decode_batch_outputs(outputs.sequences, input_lengths, tokenizer)\n",
    "        \n",
    "        # Calculate generation metadata\n",
    "        generation_metadata = {\n",
    "            \"total_inference_time\": end_time - start_time,\n",
    "            \"batch_size\": len(responses),\n",
    "            \"avg_inference_time_per_sample\": (end_time - start_time) / len(responses),\n",
    "            \"input_token_counts\": input_lengths,\n",
    "            \"output_token_counts\": [len(outputs.sequences[i]) - input_lengths[i] for i in range(len(responses))],\n",
    "            \"total_tokens_generated\": sum(len(outputs.sequences[i]) - input_lengths[i] for i in range(len(responses)))\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            generation_metadata[\"gpu_memory_used\"] = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "    \n",
    "    return responses, generation_metadata\n",
    "\n",
    "def save_results(results, metadata, stage, config):\n",
    "    \"\"\"Saves results and metadata to appropriate locations\"\"\"\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(config[\"output_dir\"]) / stage\n",
    "    \n",
    "    # Save results\n",
    "    results_path = output_dir / f\"results_{timestamp}.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = output_dir / f\"metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return results_path, metadata_path\n",
    "\n",
    "print(\"Inference functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b884d",
   "metadata": {},
   "source": [
    "Cell 10: Compute metrics function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69987512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simplified metrics functions loaded (with text normalization)!\n",
      "\n",
      "To evaluate your baseline results, run:\n",
      "baseline_metrics = evaluate_results(baseline_results, CONFIG, 'Baseline')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for flexible comparison (removes spaces, converts to lowercase).\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', '', str(text).lower().strip())\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"Extract JSON from model response, handling various formatting issues.\"\"\"\n",
    "    if not response:\n",
    "        return {}\n",
    "    \n",
    "    response = response.strip()\n",
    "    \n",
    "    # Look for JSON patterns\n",
    "    json_patterns = [\n",
    "        r'\\{.*\\}',  # Basic JSON pattern\n",
    "        r'```json\\s*(\\{.*\\})\\s*```',  # Markdown code block\n",
    "        r'```\\s*(\\{.*\\})\\s*```',  # Generic code block\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Try parsing the whole response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def compute_metrics(results, config):\n",
    "    \"\"\"\n",
    "    Compute simple metrics: verdict accuracy/precision/recall/F1 and ELN accuracy.\n",
    "    \n",
    "    Args:\n",
    "        results: List of result dictionaries with expected_output and model_response\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        dict: Simple metrics dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse responses\n",
    "    verdict_expected = []\n",
    "    verdict_predicted = []\n",
    "    eln_expected = []\n",
    "    eln_predicted = []\n",
    "    \n",
    "    parse_failures = 0\n",
    "    \n",
    "    for result in results:\n",
    "        expected = json.loads(result[\"expected_output\"])\n",
    "        predicted = extract_json_from_response(result[\"model_response\"])\n",
    "        \n",
    "        if not predicted:\n",
    "            parse_failures += 1\n",
    "        \n",
    "        # Verdict\n",
    "        verdict_expected.append(expected.get(\"verdict\", \"unknown\"))\n",
    "        verdict_predicted.append(predicted.get(\"verdict\", \"unknown\"))\n",
    "        \n",
    "        # Erroneous line (if applicable)\n",
    "        if config[\"include_eln\"]:\n",
    "            if config[\"solution_format\"] == \"dict\":\n",
    "                eln_expected.append(expected.get(\"erroneous_line_number\"))\n",
    "                eln_predicted.append(predicted.get(\"erroneous_line_number\"))\n",
    "            else:\n",
    "                eln_expected.append(expected.get(\"erroneous_line\"))\n",
    "                eln_predicted.append(predicted.get(\"erroneous_line\"))\n",
    "    \n",
    "    # Compute verdict metrics\n",
    "    verdict_accuracy = accuracy_score(verdict_expected, verdict_predicted)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        verdict_expected, verdict_predicted, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        \"verdict_accuracy\": verdict_accuracy,\n",
    "        \"verdict_precision\": precision,\n",
    "        \"verdict_recall\": recall,\n",
    "        \"verdict_f1\": f1,\n",
    "        \"parse_failures\": parse_failures,\n",
    "        \"total_samples\": len(results)\n",
    "    }\n",
    "    \n",
    "    # Compute ELN accuracy if applicable\n",
    "    if config[\"include_eln\"] and eln_expected:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            # For dict format, exact match\n",
    "            eln_accuracy = accuracy_score(eln_expected, eln_predicted)\n",
    "        else:\n",
    "            # For nl format, use normalized comparison for flexibility\n",
    "            normalized_expected = [normalize_text(eln) for eln in eln_expected]\n",
    "            normalized_predicted = [normalize_text(eln) for eln in eln_predicted]\n",
    "            eln_accuracy = accuracy_score(normalized_expected, normalized_predicted)\n",
    "        \n",
    "        metrics[\"eln_accuracy\"] = eln_accuracy\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Print simple metrics summary.\"\"\"\n",
    "    print(f\"\\n{stage_name} Results:\")\n",
    "    print(f\"Total samples: {metrics['total_samples']} (Parse failures: {metrics['parse_failures']})\")\n",
    "    print(f\"Verdict - Accuracy: {metrics['verdict_accuracy']:.3f}, Precision: {metrics['verdict_precision']:.3f}, Recall: {metrics['verdict_recall']:.3f}, F1: {metrics['verdict_f1']:.3f}\")\n",
    "    \n",
    "    if \"eln_accuracy\" in metrics:\n",
    "        print(f\"ELN Accuracy: {metrics['eln_accuracy']:.3f}\")\n",
    "\n",
    "# Simple evaluation function\n",
    "def evaluate_results(results, config, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Evaluate results with simple metrics.\"\"\"\n",
    "    metrics = compute_metrics(results, config)\n",
    "    print_metrics(metrics, stage_name)\n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Simplified metrics functions loaded (with text normalization)!\")\n",
    "print(\"\\nTo evaluate your baseline results, run:\")\n",
    "print(\"baseline_metrics = evaluate_results(baseline_results, CONFIG, 'Baseline')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95700a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_training_data(train_data, tokenizer, config):\n",
    "    \"\"\"\n",
    "    Prepares training data for generative fine-tuning with chat templates.\n",
    "    \n",
    "    Args:\n",
    "        train_data: List of training samples with messages and expected outputs\n",
    "        tokenizer: The tokenizer to use\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Formatted dataset ready for training\n",
    "    \"\"\"\n",
    "    formatted_samples = []\n",
    "    \n",
    "    for sample in train_data:\n",
    "        # Get the messages (includes system, examples, and user message)\n",
    "        messages = sample[\"messages\"]\n",
    "        expected_output = sample[\"expected_output\"]\n",
    "        \n",
    "        # Add the assistant response to complete the conversation\n",
    "        complete_messages = messages + [{\"role\": \"assistant\", \"content\": expected_output}]\n",
    "        \n",
    "        # Use our custom apply_chat_template function\n",
    "        formatted_text = apply_chat_template(\n",
    "            complete_messages,\n",
    "            tokenizer,\n",
    "            add_generation_prompt=False,  # We have the complete conversation\n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        formatted_samples.append({\n",
    "            \"text\": formatted_text,\n",
    "            \"sample_id\": sample[\"id\"],\n",
    "            \"metadata\": sample[\"metadata\"]\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(formatted_samples)\n",
    "\n",
    "def setup_training_components(model, tokenizer, train_data, eval_data, config):\n",
    "    \"\"\"\n",
    "    Sets up training arguments, data collator, and trainer for generative fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model\n",
    "        tokenizer: The loaded tokenizer  \n",
    "        train_data: Training samples\n",
    "        eval_data: Evaluation samples\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trainer, train_dataset, eval_dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare datasets\n",
    "    print(\"Preparing training datasets...\")\n",
    "    train_dataset = prepare_training_data(train_data, tokenizer, config)\n",
    "    eval_dataset = prepare_training_data(eval_data, tokenizer, config)\n",
    "    \n",
    "    print(f\"✓ Training dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"✓ Evaluation dataset: {len(eval_dataset)} samples\")\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=config[\"max_length\"],\n",
    "            padding=False,  # Dynamic padding handled by data collator\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    print(\"Tokenizing datasets...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    eval_dataset = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Datasets tokenized\")\n",
    "    \n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    output_dir = Path(config[\"output_dir\"]) / \"training\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        \n",
    "        # Training schedule\n",
    "        num_train_epochs=config[\"num_epochs\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"],\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # FREQUENT EVALUATION & SAVING\n",
    "        eval_strategy=\"steps\",        # Evaluate every 25 steps\n",
    "        eval_steps=25,\n",
    "        save_strategy=\"steps\",       # Save every 25 steps\n",
    "        save_steps=25,\n",
    "        save_total_limit=1,          # Keep only the most recent checkpoint\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eln_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=25,\n",
    "        report_to=None,  # Disable wandb/tensorboard for now\n",
    "        save_safetensors=False,\n",
    "        \n",
    "        # Mixed precision\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    # Initialize early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,    # Stop after 10 evaluations without improvement\n",
    "        early_stopping_threshold=0.0  # Any improvement counts\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Trainer initialized successfully!\")\n",
    "    print(f\"✓ Training arguments configured\")\n",
    "    print(f\"✓ Data collator ready\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Model: {config['model_name']}\")\n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Evaluation samples: {len(eval_dataset)}\")\n",
    "    print(f\"  Epochs: {config['num_epochs']}\")\n",
    "    print(f\"  Batch size: {config['batch_size']}\")\n",
    "    print(f\"  Gradient accumulation: {config['gradient_accumulation_steps']}\")\n",
    "    print(f\"  Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
    "    print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"  Max length: {config['max_length']}\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    \n",
    "    return trainer, train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66020c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: microsoft/phi-4-mini-instruct\n",
      "✓ Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6cb6727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset and model for baseline inference\n",
    "# print(\"=== LOADING DATASET AND MODEL ===\")\n",
    "\n",
    "# # Load and prepare dataset\n",
    "# train_data, eval_data, examples = prepare_dataset(CONFIG)\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model, tokenizer = load_model_and_tokenizer(CONFIG)\n",
    "\n",
    "# print(f\"✅ Dataset loaded: {len(train_data)} train, {len(eval_data)} eval\")\n",
    "# print(f\"✅ Model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform baseline inference on evaluation set\n",
    "# print(\"=== BASELINE INFERENCE ===\")\n",
    "\n",
    "# # Prepare inference data (use first 50 samples for faster testing)\n",
    "# eval_subset = eval_data[:50]  # Adjust size as needed\n",
    "# messages_batch = [sample[\"messages\"] for sample in eval_subset]\n",
    "\n",
    "# # Prepare inference batch\n",
    "# prepared_inputs = prepare_inference_batch(\n",
    "#     messages_batch, \n",
    "#     tokenizer, \n",
    "#     max_length=CONFIG[\"max_length\"]\n",
    "# )\n",
    "\n",
    "# # Apply padding\n",
    "# padded_inputs = apply_batch_padding(prepared_inputs, tokenizer)\n",
    "\n",
    "# # Run inference\n",
    "# baseline_responses, baseline_metadata = run_inference(\n",
    "#     model, tokenizer, padded_inputs\n",
    "# )\n",
    "\n",
    "# # Format results\n",
    "# baseline_results = []\n",
    "# for i, sample in enumerate(eval_subset):\n",
    "#     baseline_results.append({\n",
    "#         \"id\": sample[\"id\"],\n",
    "#         \"expected_output\": sample[\"expected_output\"],\n",
    "#         \"model_response\": baseline_responses[i],\n",
    "#         \"metadata\": sample[\"metadata\"]\n",
    "#     })\n",
    "\n",
    "# print(f\"✅ Baseline inference completed on {len(baseline_results)} samples\")\n",
    "# print(f\"Avg inference time: {baseline_metadata['avg_inference_time_per_sample']:.2f}s\")\n",
    "\n",
    "# # Evaluate baseline\n",
    "# baseline_metrics = evaluate_results(baseline_results, CONFIG, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save baseline results\n",
    "# print(\"=== SAVING BASELINE RESULTS ===\")\n",
    "\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Create output directory\n",
    "# output_dir = Path(f\"./baseline_results_{CONFIG['experiment_id']}\")\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Save results\n",
    "# baseline_results_path = output_dir / \"baseline_results.json\"\n",
    "# with open(baseline_results_path, 'w') as f:\n",
    "#     json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "# # Save metadata  \n",
    "# baseline_metadata_path = output_dir / \"baseline_metadata.json\"\n",
    "# baseline_metadata.update(baseline_metrics)  # Include metrics in metadata\n",
    "# with open(baseline_metadata_path, 'w') as f:\n",
    "#     json.dump(baseline_metadata, f, indent=2, default=str)\n",
    "\n",
    "# # Save config\n",
    "# config_path = output_dir / \"config.json\"\n",
    "# with open(config_path, 'w') as f:\n",
    "#     json.dump(CONFIG, f, indent=2, default=str)\n",
    "\n",
    "# print(f\"✅ Baseline results saved to: {output_dir}\")\n",
    "# print(f\"   - Results: {baseline_results_path}\")\n",
    "# print(f\"   - Metadata: {baseline_metadata_path}\")\n",
    "# print(f\"   - Config: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68016b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup and perform training\n",
    "# print(\"=== SETTING UP TRAINING ===\")\n",
    "\n",
    "# # Setup training components\n",
    "# trainer, train_dataset, eval_dataset = setup_training_components(\n",
    "#     model, tokenizer, train_data, eval_data, CONFIG\n",
    "# )\n",
    "\n",
    "# print(\"=== STARTING TRAINING ===\")\n",
    "\n",
    "# # Start training\n",
    "# training_results = trainer.train()\n",
    "\n",
    "# print(\"✅ Training completed!\")\n",
    "# print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
    "# print(f\"Training steps: {training_results.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save training results and model\n",
    "# print(\"=== SAVING TRAINING RESULTS ===\")\n",
    "\n",
    "# # Create training output directory\n",
    "# training_output_dir = Path(f\"./training_results_{CONFIG['experiment_id']}\")\n",
    "# training_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Save training history\n",
    "# training_history_path = training_output_dir / \"training_history.json\"\n",
    "# training_history = {\n",
    "#     \"training_loss\": training_results.training_loss,\n",
    "#     \"global_step\": training_results.global_step,\n",
    "#     \"training_time\": str(training_results.metrics.get('train_runtime', 'unknown')),\n",
    "#     \"log_history\": trainer.state.log_history\n",
    "# }\n",
    "\n",
    "# with open(training_history_path, 'w') as f:\n",
    "#     json.dump(training_history, f, indent=2, default=str)\n",
    "\n",
    "# # Save model locally\n",
    "# local_model_path = training_output_dir / \"trained_model\"\n",
    "# trainer.save_model(str(local_model_path))\n",
    "# tokenizer.save_pretrained(str(local_model_path))\n",
    "\n",
    "# print(f\"✅ Training results saved to: {training_output_dir}\")\n",
    "# print(f\"   - Training history: {training_history_path}\")\n",
    "# print(f\"   - Model: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform final inference with trained model\n",
    "# print(\"=== FINAL INFERENCE ===\")\n",
    "\n",
    "# # The trainer already has the best model loaded\n",
    "# # Prepare the same evaluation subset\n",
    "# prepared_inputs_final = prepare_inference_batch(\n",
    "#     messages_batch, \n",
    "#     tokenizer, \n",
    "#     max_length=CONFIG[\"max_length\"]\n",
    "# )\n",
    "\n",
    "# # Apply padding\n",
    "# padded_inputs_final = apply_batch_padding(prepared_inputs_final, tokenizer)\n",
    "\n",
    "# # Run final inference\n",
    "# final_responses, final_metadata = run_inference(\n",
    "#     trainer.model, tokenizer, padded_inputs_final\n",
    "# )\n",
    "\n",
    "# # Format final results\n",
    "# final_results = []\n",
    "# for i, sample in enumerate(eval_subset):\n",
    "#     final_results.append({\n",
    "#         \"id\": sample[\"id\"],\n",
    "#         \"expected_output\": sample[\"expected_output\"],\n",
    "#         \"model_response\": final_responses[i],\n",
    "#         \"metadata\": sample[\"metadata\"]\n",
    "#     })\n",
    "\n",
    "# print(f\"✅ Final inference completed on {len(final_results)} samples\")\n",
    "# print(f\"Avg inference time: {final_metadata['avg_inference_time_per_sample']:.2f}s\")\n",
    "\n",
    "# # Evaluate final results\n",
    "# final_metrics = evaluate_results(final_results, CONFIG, \"Final\")\n",
    "\n",
    "# # Compare with baseline\n",
    "# print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "# print(f\"Baseline Accuracy: {baseline_metrics['verdict_accuracy']:.3f}\")\n",
    "# print(f\"Final Accuracy: {final_metrics['verdict_accuracy']:.3f}\")\n",
    "# print(f\"Improvement: {final_metrics['verdict_accuracy'] - baseline_metrics['verdict_accuracy']:.3f}\")\n",
    "\n",
    "# if CONFIG[\"include_eln\"]:\n",
    "#     print(f\"Baseline ELN Accuracy: {baseline_metrics.get('eln_accuracy', 'N/A')}\")\n",
    "#     print(f\"Final ELN Accuracy: {final_metrics.get('eln_accuracy', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save final results\n",
    "# print(\"=== SAVING FINAL RESULTS ===\")\n",
    "\n",
    "# # Save final inference results\n",
    "# final_results_path = training_output_dir / \"final_results.json\"\n",
    "# with open(final_results_path, 'w') as f:\n",
    "#     json.dump(final_results, f, indent=2)\n",
    "\n",
    "# # Save final metadata\n",
    "# final_metadata_path = training_output_dir / \"final_metadata.json\"\n",
    "# final_metadata.update(final_metrics)  # Include metrics\n",
    "# with open(final_metadata_path, 'w') as f:\n",
    "#     json.dump(final_metadata, f, indent=2, default=str)\n",
    "\n",
    "# # Save comparison results\n",
    "# comparison_results = {\n",
    "#     \"baseline_metrics\": baseline_metrics,\n",
    "#     \"final_metrics\": final_metrics,\n",
    "#     \"improvement\": {\n",
    "#         \"verdict_accuracy\": final_metrics['verdict_accuracy'] - baseline_metrics['verdict_accuracy'],\n",
    "#         \"verdict_f1\": final_metrics['verdict_f1'] - baseline_metrics['verdict_f1']\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# if CONFIG[\"include_eln\"]:\n",
    "#     comparison_results[\"improvement\"][\"eln_accuracy\"] = final_metrics.get('eln_accuracy', 0) - baseline_metrics.get('eln_accuracy', 0)\n",
    "\n",
    "# comparison_path = training_output_dir / \"performance_comparison.json\"\n",
    "# with open(comparison_path, 'w') as f:\n",
    "#     json.dump(comparison_results, f, indent=2, default=str)\n",
    "\n",
    "# print(f\"✅ Final results saved:\")\n",
    "# print(f\"   - Results: {final_results_path}\")\n",
    "# print(f\"   - Metadata: {final_metadata_path}\")\n",
    "# print(f\"   - Comparison: {comparison_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
