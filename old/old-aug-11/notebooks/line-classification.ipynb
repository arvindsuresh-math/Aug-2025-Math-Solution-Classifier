{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378d511b",
   "metadata": {},
   "source": [
    "### ## Experiment 2: Per-Line Error Detection\n",
    "\n",
    "This notebook refines our approach to error detection. Instead of simply classifying a solution as `correct` or `incorrect`, the goal is to pinpoint the **exact line** where the first logical error occurs.\n",
    "\n",
    "### ### Previous Strategy: Sequence Classification\n",
    "\n",
    "Our initial method treated the task as a standard binary sequence classification problem.\n",
    "\n",
    "* **Process**: The model processed the entire `problem + solution` text and used the hidden state of the **final token** as a representation of the whole sequence. A classifier head then predicted one of two labels: `0` (correct) or `1` (incorrect).\n",
    "* **Limitation**: This approach tells us *if* a solution is flawed, but provides no information about *where* the error is.\n",
    "\n",
    "### ### New Strategy: Per-Line Classification\n",
    "\n",
    "The new strategy re-frames the problem as a **sequence labeling** task, enabling a more granular analysis.\n",
    "\n",
    "* **Process**:\n",
    "    1.  The model processes the full `problem + solution` text in a single forward pass.\n",
    "    2.  We identify and select the hidden state at the end of **each line** (specifically, at each `\\n` token).\n",
    "    3.  A single, shared classifier head is applied in parallel to each of these selected hidden states.\n",
    "    4.  This yields a sequence of logits, one for each line. Each logit represents the model's confidence that its corresponding line contains the first error.\n",
    "    5.  The model is trained using a per-line binary loss, learning to output a high value for the single correct error line and low values for all other lines.\n",
    "\n",
    "### ### Key Differences\n",
    "\n",
    "| Feature | Previous Strategy (Sequence Classification) | New Strategy (Per-Line Classification) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Goal** | Is the solution correct or incorrect? | Which line contains the first error? |\n",
    "| **Output** | A single prediction for the entire solution. | A prediction *for each line* of the solution. |\n",
    "| **Model Input** | Hidden state of the **final token**. | Hidden states of **all line-end tokens**. |\n",
    "| **Label Format** | A single integer (`0` or `1`). | A sequence of binary labels (`[0, 0, 1, 0, ...]`). |\n",
    "| **Advantage**| Simple to implement. | Provides granular, interpretable feedback. |\n",
    "\n",
    "> **Note**: This advanced strategy requires a dataset with line-level labels. The code assumes your dataset contains a `first_error_line` column indicating the index of the first incorrect line, or `-1` if the solution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95419eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # Cell 1: Setup and Installations\n",
    "# # (No changes from your original script)\n",
    "# # ==============================================================================\n",
    "# # 1.2 Install required libraries\n",
    "# # Note: TRL is included for consistency with your original script, but is not\n",
    "# # strictly required for this sequence classification task.\n",
    "# !pip install -Uq transformers\n",
    "# !pip install -Uq peft\n",
    "# !pip install -Uq trl\n",
    "# !pip install -Uq accelerate\n",
    "# !pip install -Uq datasets\n",
    "# !pip install -Uq bitsandbytes\n",
    "\n",
    "# # Install Flash Attention 2\n",
    "# !pip install flash-attn==2.7.4.post1 \\\n",
    "#   --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "#   --no-build-isolation\n",
    "\n",
    "# # !unzip -q -o /content/drive/My\\ Drive/level-1-binary.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88bab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: Project Configuration\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds all static configuration for the project.\n",
    "    \"\"\"\n",
    "    # Model ID from Hugging Face Hub\n",
    "    MODEL_ID = \"microsoft/phi-4-mini-instruct\"\n",
    "\n",
    "    # Local path to the unzipped dataset\n",
    "    DATASET_PATH = \"../data/line-classification/flawed-only/flawed_only_line_classification_dataset.csv\"\n",
    "\n",
    "    # Directory for saving the final model adapter\n",
    "    OUTPUT_DIR = \"/content/level1-line-classifier-output\"\n",
    "\n",
    "    # The head outputs one logit per line for binary (is/is_not_error) classification\n",
    "    NUM_LABELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ea9871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 special tokens to tokenizer\n",
      "Line separator token '<|LINE_SEP|>' has ID: 200029\n",
      "Loading flawed-only line classification dataset...\n",
      "✅ Dataset loaded successfully: 3487 samples\n",
      "Tokenizer and raw dataset loaded successfully.\n",
      "Dataset columns: ['text', 'correct_answer', 'line_labels', 'error_type', 'index', 'tier', 'source', 'relative_line_position', 'solution_length']\n",
      "Dataset size: 3487\n",
      "Vocabulary size after adding special tokens: 200030\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Enhanced Tokenizer Setup with Special Token Support\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Special token for reliable line separation (matching dataset creation approach)\n",
    "LINE_SEP_TOKEN = \"<|LINE_SEP|>\"\n",
    "\n",
    "# Add the special line separator token to the tokenizer\n",
    "# This avoids inconsistent newline tokenization issues we discovered\n",
    "special_tokens_dict = {\"additional_special_tokens\": [LINE_SEP_TOKEN]}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f\"Added {num_added_tokens} special tokens to tokenizer\")\n",
    "\n",
    "# Get the token ID for our special line separator\n",
    "line_sep_token_id = tokenizer.convert_tokens_to_ids(LINE_SEP_TOKEN)\n",
    "print(f\"Line separator token '{LINE_SEP_TOKEN}' has ID: {line_sep_token_id}\")\n",
    "\n",
    "# Load the CSV dataset (not using load_from_disk for CSV files)\n",
    "print(\"Loading flawed-only line classification dataset...\")\n",
    "df = pd.read_csv(Config.DATASET_PATH)\n",
    "print(f\"✅ Dataset loaded successfully: {len(df)} samples\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(\"Tokenizer and raw dataset loaded successfully.\")\n",
    "print(f\"Dataset columns: {raw_dataset.column_names}\")\n",
    "print(f\"Dataset size: {len(raw_dataset)}\")\n",
    "print(f\"Vocabulary size after adding special tokens: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd57589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Analyze the following mathematical problem and solution to identify the line containing the error.\\n\\n### Problem:\\nWeng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\\n\\n### Solution:\\nWeng earns 12/60 = $0.2 per minute.<|LINE_SEP|>Working 50 minutes, she earned 50 x 50 = $2500.<|LINE_SEP|>#### 2500<|LINE_SEP|>', 'correct_answer': 'Weng earns 12/60 = $0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $10.\\n#### 10', 'line_labels': '[0, 1, 0]', 'error_type': 'conceptual_error', 'index': 1, 'tier': 'tier4', 'source': 'programmatic', 'relative_line_position': 0.5, 'solution_length': 3}\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "print(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbd698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: UPDATED Preprocessing Function for Special Tokens\n",
    "# ==============================================================================\n",
    "def preprocess_for_line_detection(examples):\n",
    "    \"\"\"\n",
    "    Prepares the flawed-only dataset for line-level error detection.\n",
    "    \n",
    "    This function uses the pre-preprocessed text with special tokens,\n",
    "    finds the special token indices, and uses the pre-computed line_labels.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): A batch of examples from the flawed-only dataset.\n",
    "                         Expected columns: 'text', 'line_labels'\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized inputs, attention masks,\n",
    "              the calculated line-end indices, and the per-line labels.\n",
    "    \"\"\"\n",
    "    # Use the pre-processed text directly (already contains special tokens)\n",
    "    input_texts = examples[\"text\"]\n",
    "    \n",
    "    tokenized_outputs = tokenizer(\n",
    "        input_texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # Find special token indices (instead of newlines)\n",
    "    all_line_end_indices = []\n",
    "    for input_ids in tokenized_outputs[\"input_ids\"]:\n",
    "        indices = [i for i, token_id in enumerate(input_ids) if token_id == line_sep_token_id]\n",
    "        all_line_end_indices.append(indices)\n",
    "    \n",
    "    tokenized_outputs[\"line_end_indices\"] = all_line_end_indices\n",
    "\n",
    "    # Use the pre-computed line_labels from the dataset\n",
    "    # Convert string representation to list if needed\n",
    "    per_line_labels = []\n",
    "    for line_labels_raw in examples[\"line_labels\"]:\n",
    "        if isinstance(line_labels_raw, str):\n",
    "            # Parse string representation like \"[0, 0, 1, 0]\"\n",
    "            import ast\n",
    "            line_labels = ast.literal_eval(line_labels_raw)\n",
    "        else:\n",
    "            line_labels = line_labels_raw\n",
    "        per_line_labels.append(line_labels)\n",
    "    \n",
    "    tokenized_outputs[\"labels\"] = per_line_labels\n",
    "    \n",
    "    # For metrics computation, also compute first_error_line\n",
    "    first_error_lines = []\n",
    "    for line_labels in per_line_labels:\n",
    "        try:\n",
    "            first_error_line = line_labels.index(1)  # Find first occurrence of 1\n",
    "        except ValueError:\n",
    "            first_error_line = -1  # No error found (shouldn't happen in flawed-only dataset)\n",
    "        first_error_lines.append(first_error_line)\n",
    "    \n",
    "    tokenized_outputs[\"first_error_line\"] = first_error_lines\n",
    "    \n",
    "    return tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d5c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 COMPREHENSIVE SPECIAL TOKEN VALIDATION\n",
      "======================================================================\n",
      "🎯 Testing ALL 3487 samples\n",
      "📍 Special token: <|LINE_SEP|> (ID: 200029)\n",
      "\n",
      "📊 COMPREHENSIVE RESULTS:\n",
      "   Total samples: 3,487\n",
      "   Perfect matches (exact count): 3,487 (100.0%)\n",
      "   Sufficient matches (>= expected): 3,487 (100.0%)\n",
      "   Total expected tokens: 15,477\n",
      "   Total detected tokens: 15,477\n",
      "   Detection ratio: 1.000\n",
      "\n",
      "✅ NO ALIGNMENT ISSUES: All samples have sufficient tokens!\n",
      "\n",
      "🎯 FINAL ASSESSMENT:\n",
      "   ✅ EXCELLENT: 100.0% success rate - Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# UPDATED: Comprehensive Special Token Validation (All Samples)\n",
    "# ==============================================================================\n",
    "def validate_special_token_detection_full():\n",
    "    \"\"\"\n",
    "    Validate special token detection across ALL samples in the dataset.\n",
    "    \"\"\"\n",
    "    print(\"🔬 COMPREHENSIVE SPECIAL TOKEN VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"🎯 Testing ALL {len(df)} samples\")\n",
    "    print(f\"📍 Special token: {LINE_SEP_TOKEN} (ID: {line_sep_token_id})\")\n",
    "    \n",
    "    # Statistics tracking\n",
    "    perfect_matches = 0\n",
    "    sufficient_matches = 0  # Has >= expected tokens\n",
    "    total_expected_tokens = 0\n",
    "    total_detected_tokens = 0\n",
    "    \n",
    "    # Track alignment issues\n",
    "    alignment_issues = []\n",
    "    \n",
    "    for idx, (_, row) in enumerate(df.iterrows()):\n",
    "        full_text = row['text']\n",
    "        line_labels = eval(row['line_labels']) if isinstance(row['line_labels'], str) else row['line_labels']\n",
    "        expected_lines = len(line_labels)\n",
    "        \n",
    "        # Tokenize the full text\n",
    "        tokens = tokenizer(full_text, truncation=True, max_length=512)\n",
    "        input_ids = tokens['input_ids']\n",
    "        \n",
    "        # Count special tokens\n",
    "        special_token_count = sum(1 for token_id in input_ids if token_id == line_sep_token_id)\n",
    "        \n",
    "        # Update statistics\n",
    "        total_expected_tokens += expected_lines\n",
    "        total_detected_tokens += special_token_count\n",
    "        \n",
    "        if special_token_count == expected_lines:\n",
    "            perfect_matches += 1\n",
    "        \n",
    "        if special_token_count >= expected_lines:\n",
    "            sufficient_matches += 1\n",
    "        else:\n",
    "            # Track alignment issues\n",
    "            alignment_issues.append({\n",
    "                'index': idx,\n",
    "                'expected': expected_lines,\n",
    "                'detected': special_token_count,\n",
    "                'error_type': row['error_type'],\n",
    "                'text_preview': full_text[:100] + \"...\"\n",
    "            })\n",
    "    \n",
    "    # Calculate success rates\n",
    "    perfect_rate = (perfect_matches / len(df)) * 100\n",
    "    sufficient_rate = (sufficient_matches / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n📊 COMPREHENSIVE RESULTS:\")\n",
    "    print(f\"   Total samples: {len(df):,}\")\n",
    "    print(f\"   Perfect matches (exact count): {perfect_matches:,} ({perfect_rate:.1f}%)\")\n",
    "    print(f\"   Sufficient matches (>= expected): {sufficient_matches:,} ({sufficient_rate:.1f}%)\")\n",
    "    print(f\"   Total expected tokens: {total_expected_tokens:,}\")\n",
    "    print(f\"   Total detected tokens: {total_detected_tokens:,}\")\n",
    "    print(f\"   Detection ratio: {total_detected_tokens/total_expected_tokens:.3f}\")\n",
    "    \n",
    "    # Show alignment issues if any\n",
    "    if alignment_issues:\n",
    "        print(f\"\\n⚠️ ALIGNMENT ISSUES: {len(alignment_issues)} samples\")\n",
    "        print(f\"   Showing first 5 problematic samples:\")\n",
    "        for i, issue in enumerate(alignment_issues[:5]):\n",
    "            print(f\"   {i+1}. Sample {issue['index']}: expected {issue['expected']}, got {issue['detected']} ({issue['error_type']})\")\n",
    "    else:\n",
    "        print(f\"\\n✅ NO ALIGNMENT ISSUES: All samples have sufficient tokens!\")\n",
    "    \n",
    "    # Error type breakdown for issues\n",
    "    if alignment_issues:\n",
    "        error_type_issues = {}\n",
    "        for issue in alignment_issues:\n",
    "            error_type = issue['error_type']\n",
    "            error_type_issues[error_type] = error_type_issues.get(error_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n📈 ISSUES BY ERROR TYPE:\")\n",
    "        for error_type, count in error_type_issues.items():\n",
    "            percentage = (count / len(alignment_issues)) * 100\n",
    "            print(f\"   {error_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(f\"\\n🎯 FINAL ASSESSMENT:\")\n",
    "    if sufficient_rate >= 95:\n",
    "        print(f\"   ✅ EXCELLENT: {sufficient_rate:.1f}% success rate - Ready for training!\")\n",
    "    elif sufficient_rate >= 85:\n",
    "        print(f\"   ✅ GOOD: {sufficient_rate:.1f}% success rate - Should work well for training\")\n",
    "    elif sufficient_rate >= 70:\n",
    "        print(f\"   ⚠️ ACCEPTABLE: {sufficient_rate:.1f}% success rate - May need optimization\")\n",
    "    else:\n",
    "        print(f\"   ❌ PROBLEMATIC: {sufficient_rate:.1f}% success rate - Needs investigation\")\n",
    "    \n",
    "    return {\n",
    "        'total_samples': len(df),\n",
    "        'perfect_matches': perfect_matches,\n",
    "        'sufficient_matches': sufficient_matches,\n",
    "        'perfect_rate': perfect_rate,\n",
    "        'sufficient_rate': sufficient_rate,\n",
    "        'alignment_issues': len(alignment_issues),\n",
    "        'detection_ratio': total_detected_tokens/total_expected_tokens\n",
    "    }\n",
    "\n",
    "# Run the comprehensive validation\n",
    "validation_results = validate_special_token_detection_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ab5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TESTING SUITE: Preprocessing Testing Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def test_preprocessing_function(df: pd.DataFrame, tokenizer, sample_size: int = 5):\n",
    "    \"\"\"Test 4: Validate preprocessing function end-to-end\"\"\"\n",
    "    print(\"\\n🧪 TEST 4: Preprocessing Function Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Create a small sample dataset\n",
    "        test_df = df.sample(sample_size, random_state=42)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        processed = test_dataset.map(\n",
    "            preprocess_for_line_detection,\n",
    "            batched=True,\n",
    "            batch_size=sample_size\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Preprocessing completed on {len(processed)} samples\")\n",
    "        print(f\"📋 Processed columns: {processed.column_names}\")\n",
    "        \n",
    "        # Validate each sample\n",
    "        for i in range(len(processed)):\n",
    "            sample = processed[i]\n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            \n",
    "            # Check required fields\n",
    "            required_fields = ['input_ids', 'attention_mask', 'line_end_indices', 'labels', 'first_error_line']\n",
    "            for field in required_fields:\n",
    "                if field in sample:\n",
    "                    print(f\"   ✅ {field}: {len(sample[field]) if isinstance(sample[field], list) else 'present'}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ {field}: missing\")\n",
    "            \n",
    "            # Check alignment between line_end_indices and labels\n",
    "            line_indices = sample['line_end_indices']\n",
    "            labels = sample['labels']\n",
    "            first_error = sample['first_error_line']\n",
    "            \n",
    "            print(f\"   📏 Line end indices: {len(line_indices)} positions\")\n",
    "            print(f\"   🏷️ Labels: {len(labels)} labels\")\n",
    "            print(f\"   🎯 First error line: {first_error}\")\n",
    "            print(f\"   🔢 Labels sum: {sum(labels)}\")\n",
    "            \n",
    "            # Verify first_error_line matches labels\n",
    "            if first_error != -1 and first_error < len(labels):\n",
    "                expected_label = labels[first_error]\n",
    "                print(f\"   ✅ Label at first_error_line ({first_error}): {expected_label}\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Preprocessing validation failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48cf8283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1902c8aea62471ebd84ee67300a2574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6358ac2c56a042adb7f43f6e756cc87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing for flawed-only line detection complete ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'correct_answer', 'line_labels', 'error_type', 'index', 'tier', 'source', 'relative_line_position', 'solution_length', 'input_ids', 'attention_mask', 'line_end_indices', 'labels', 'first_error_line'],\n",
      "        num_rows: 2789\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'correct_answer', 'line_labels', 'error_type', 'index', 'tier', 'source', 'relative_line_position', 'solution_length', 'input_ids', 'attention_mask', 'line_end_indices', 'labels', 'first_error_line'],\n",
      "        num_rows: 698\n",
      "    })\n",
      "})\n",
      "Train samples: 2789\n",
      "Test samples: 698\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: Apply Preprocessing and Finalize Dataset\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split into train/test (80/20 split)\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(\"Applying preprocessing to the dataset...\")\n",
    "tokenized_dataset = split_dataset.map(\n",
    "    preprocess_for_line_detection,\n",
    "    batched=True,\n",
    "    # Keep all original columns for convenience - Trainer will select what it needs\n",
    "    remove_columns=None\n",
    ")\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": tokenized_dataset[\"train\"],\n",
    "    \"test\": tokenized_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Preprocessing for flawed-only line detection complete ---\")\n",
    "print(final_dataset)\n",
    "print(f\"Train samples: {len(final_dataset['train'])}\")\n",
    "print(f\"Test samples: {len(final_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: Custom Data Collator\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForLineClassification:\n",
    "    \"\"\"\n",
    "    A data collator that handles padding for our line-level task.\n",
    "    Updated to work with variable-length line_labels from flawed-only dataset.\n",
    "    \"\"\"\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding_value: int = -100  # Standard value to ignore in loss functions\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "        \n",
    "        # Use the tokenizer's default padding for inputs and attention mask\n",
    "        padded_inputs = self.tokenizer.pad(\n",
    "            [{\"input_ids\": f[\"input_ids\"], \"attention_mask\": f[\"attention_mask\"]} for f in features],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"input_ids\"] = padded_inputs[\"input_ids\"]\n",
    "        batch[\"attention_mask\"] = padded_inputs[\"attention_mask\"]\n",
    "        \n",
    "        # Manually pad our custom fields\n",
    "        max_lines = max(len(f[\"line_end_indices\"]) for f in features)\n",
    "        max_labels = max(len(f[\"labels\"]) for f in features)\n",
    "        \n",
    "        # Ensure line_end_indices and labels have the same max length\n",
    "        max_length = max(max_lines, max_labels)\n",
    "        \n",
    "        padded_line_indices = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for f in features:\n",
    "            line_indices = f[\"line_end_indices\"]\n",
    "            labels = f[\"labels\"]\n",
    "            \n",
    "            # Pad line_end_indices\n",
    "            padded_line_indices.append(\n",
    "                line_indices + [self.padding_value] * (max_length - len(line_indices))\n",
    "            )\n",
    "            \n",
    "            # Pad labels\n",
    "            padded_labels.append(\n",
    "                labels + [self.padding_value] * (max_length - len(labels))\n",
    "            )\n",
    "\n",
    "        batch[\"line_end_indices\"] = torch.tensor(padded_line_indices, dtype=torch.long)\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.float)\n",
    "        \n",
    "        # Keep the computed first_error_line for metrics\n",
    "        if \"first_error_line\" in features[0]:\n",
    "            batch[\"first_error_line\"] = torch.tensor([f[\"first_error_line\"] for f in features], dtype=torch.long)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e820397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # TESTING SUITE: Data Pipeline Testing Functions\n",
    "# # ==============================================================================\n",
    "\n",
    "# def test_data_collator_detailed(data_collator, processed_dataset, batch_size: int = 3):\n",
    "#     \"\"\"Enhanced Test: Detailed data collator validation with full content inspection\"\"\"\n",
    "#     print(\"\\n🧪 ENHANCED DATA COLLATOR VALIDATION\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     try:\n",
    "#         # Create a small batch\n",
    "#         sample_indices = list(range(min(batch_size, len(processed_dataset))))\n",
    "#         batch_samples = [processed_dataset[i] for i in sample_indices]\n",
    "        \n",
    "#         print(f\"🔄 Testing collator with batch size: {len(batch_samples)}\")\n",
    "        \n",
    "#         # Show raw samples before collation\n",
    "#         print(f\"\\n📋 RAW SAMPLES BEFORE COLLATION:\")\n",
    "#         for i, sample in enumerate(batch_samples):\n",
    "#             print(f\"\\n--- Sample {i+1} ---\")\n",
    "#             print(f\"   🔢 Input IDs length: {len(sample['input_ids'])}\")\n",
    "#             print(f\"   📍 Line end indices: {sample['line_end_indices']}\")\n",
    "#             print(f\"   🏷️ Labels: {sample['labels']}\")\n",
    "#             print(f\"   🎯 First error line: {sample['first_error_line']}\")\n",
    "            \n",
    "#             # Decode and show the actual text with line boundaries marked\n",
    "#             input_ids = sample['input_ids']\n",
    "#             decoded_text = \"\"\n",
    "#             for j, token_id in enumerate(input_ids):\n",
    "#                 if token_id == line_sep_token_id:\n",
    "#                     decoded_text += f\" <|LINE_{len([k for k in input_ids[:j+1] if k == line_sep_token_id])}|> \"\n",
    "#                 else:\n",
    "#                     token_text = tokenizer.decode([token_id])\n",
    "#                     decoded_text += token_text\n",
    "            \n",
    "#             print(f\"   📝 Decoded text with line markers:\")\n",
    "#             print(f\"      {decoded_text}{'...' if len(decoded_text) > 200 else ''}\")\n",
    "        \n",
    "#         # Apply data collator\n",
    "#         print(f\"\\n🔄 APPLYING DATA COLLATOR...\")\n",
    "#         collated_batch = data_collator(batch_samples)\n",
    "        \n",
    "#         print(f\"✅ Collation successful\")\n",
    "#         print(f\"📦 Batch keys: {list(collated_batch.keys())}\")\n",
    "        \n",
    "#         # Check tensor shapes\n",
    "#         print(f\"\\n🔍 TENSOR SHAPES:\")\n",
    "#         for key, tensor in collated_batch.items():\n",
    "#             if isinstance(tensor, torch.Tensor):\n",
    "#                 print(f\"   {key}: {tensor.shape} (dtype: {tensor.dtype})\")\n",
    "#             else:\n",
    "#                 print(f\"   {key}: {type(tensor)}\")\n",
    "        \n",
    "#         # Detailed padding analysis\n",
    "#         input_ids = collated_batch['input_ids']\n",
    "#         line_end_indices = collated_batch['line_end_indices']\n",
    "#         labels = collated_batch['labels']\n",
    "#         first_error_lines = collated_batch['first_error_line']\n",
    "        \n",
    "#         print(f\"\\n DETAILED PADDING ANALYSIS:\")\n",
    "#         print(f\"   Input IDs shape: {input_ids.shape}\")\n",
    "#         print(f\"   Line indices shape: {line_end_indices.shape}\")\n",
    "#         print(f\"   Labels shape: {labels.shape}\")\n",
    "        \n",
    "#         # Check for padding values\n",
    "#         padding_count_indices = (line_end_indices == -100).sum().item()\n",
    "#         padding_count_labels = (labels == -100).sum().item()\n",
    "        \n",
    "#         print(f\"   Padding tokens in line_end_indices: {padding_count_indices}\")\n",
    "#         print(f\"   Padding tokens in labels: {padding_count_labels}\")\n",
    "        \n",
    "#         # Show detailed content for each sample in the batch\n",
    "#         print(f\"\\n🔬 SAMPLE-BY-SAMPLE ANALYSIS:\")\n",
    "#         for i in range(input_ids.shape[0]):\n",
    "#             print(f\"\\n--- Collated Sample {i+1} ---\")\n",
    "            \n",
    "#             # Show line end indices and their validity\n",
    "#             sample_line_indices = line_end_indices[i]\n",
    "#             sample_labels = labels[i]\n",
    "#             sample_first_error = first_error_lines[i]\n",
    "            \n",
    "#             valid_line_mask = (sample_line_indices != -100)\n",
    "#             valid_label_mask = (sample_labels != -100)\n",
    "            \n",
    "#             valid_line_indices = sample_line_indices[valid_line_mask]\n",
    "#             valid_labels = sample_labels[valid_label_mask]\n",
    "            \n",
    "#             print(f\"   📍 Valid line indices: {valid_line_indices.tolist()}\")\n",
    "#             print(f\"   🏷️ Valid labels: {valid_labels.tolist()}\")\n",
    "#             print(f\"   🎯 First error line: {sample_first_error.item()}\")\n",
    "#             print(f\"   🔢 Sum of valid labels: {valid_labels.sum().item()}\")\n",
    "            \n",
    "#             # Verify that first_error_line corresponds to a label=1\n",
    "#             if sample_first_error.item() >= 0 and sample_first_error.item() < len(valid_labels):\n",
    "#                 expected_label = valid_labels[sample_first_error.item()]\n",
    "#                 print(f\"   ✅ Label at first_error_line ({sample_first_error.item()}): {expected_label.item()}\")\n",
    "            \n",
    "#             # Show the actual tokens at line boundaries\n",
    "#             sample_input_ids = input_ids[i]\n",
    "#             print(f\"   🔍 Line boundary tokens:\")\n",
    "#             for j, line_idx in enumerate(valid_line_indices):\n",
    "#                 if line_idx < len(sample_input_ids):\n",
    "#                     # Show context around the line separator\n",
    "#                     start_ctx = max(0, line_idx - 3)\n",
    "#                     end_ctx = min(len(sample_input_ids), line_idx + 4)\n",
    "#                     context_ids = sample_input_ids[start_ctx:end_ctx]\n",
    "#                     context_text = tokenizer.decode(context_ids)\n",
    "#                     label_text = \"ERROR\" if j < len(valid_labels) and valid_labels[j] == 1 else \"OK\"\n",
    "#                     print(f\"      Line {j} [{label_text}]: ...{context_text}...\")\n",
    "        \n",
    "#         # Final validation checks\n",
    "#         print(f\"\\n✅ VALIDATION CHECKS:\")\n",
    "        \n",
    "#         # Check that line_end_indices and labels have matching valid lengths\n",
    "#         all_valid = True\n",
    "#         for i in range(input_ids.shape[0]):\n",
    "#             valid_line_count = (line_end_indices[i] != -100).sum().item()\n",
    "#             valid_label_count = (labels[i] != -100).sum().item()\n",
    "#             if valid_line_count != valid_label_count:\n",
    "#                 print(f\"   ❌ Sample {i+1}: Mismatch in valid counts (lines: {valid_line_count}, labels: {valid_label_count})\")\n",
    "#                 all_valid = False\n",
    "#             else:\n",
    "#                 print(f\"   ✅ Sample {i+1}: Valid counts match ({valid_line_count} lines/labels)\")\n",
    "        \n",
    "#         # Check that each sample has exactly one error label\n",
    "#         for i in range(input_ids.shape[0]):\n",
    "#             valid_labels = labels[i][labels[i] != -100]\n",
    "#             error_count = (valid_labels == 1).sum().item()\n",
    "#             if error_count != 1:\n",
    "#                 print(f\"   ⚠️ Sample {i+1}: Expected 1 error label, found {error_count}\")\n",
    "#                 all_valid = False\n",
    "#             else:\n",
    "#                 print(f\"   ✅ Sample {i+1}: Exactly 1 error label found\")\n",
    "        \n",
    "#         return all_valid\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Enhanced data collator test failed: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bfaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then run the test function like this:\n",
    "# # First, create a small processed dataset for testing\n",
    "# test_df = df.sample(5, random_state=42)\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n",
    "# processed_test_dataset = test_dataset.map(preprocess_for_line_detection, batched=True)\n",
    "\n",
    "# # Create the data collator\n",
    "# data_collator = DataCollatorForLineClassification(tokenizer=tokenizer)\n",
    "\n",
    "# # Run the enhanced test\n",
    "# enhanced_result = test_data_collator_detailed(data_collator, processed_test_dataset, batch_size=2)\n",
    "# print(f\"\\n🎯 Enhanced test result: {enhanced_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8244a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 7: Custom Model Definition\n",
    "# ==============================================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "class GPTLineErrorDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom model wrapper for line-level error detection.\n",
    "\n",
    "    This model uses a pre-trained transformer backbone and applies a shared\n",
    "    linear classifier head to the hidden state of each line-ending token.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: PeftModel, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels, bias=True)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, line_end_indices=None, labels=None, **kw):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Padded token IDs for the batch.\n",
    "            attention_mask (torch.Tensor): Attention mask for the batch.\n",
    "            line_end_indices (torch.Tensor): Padded indices of line-end tokens.\n",
    "            labels (torch.Tensor): Padded per-line binary labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the loss (if labels are provided)\n",
    "                  and the logits for each line.\n",
    "        \"\"\"\n",
    "        outputs = self.base(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "        batch_size, max_lines = line_end_indices.shape\n",
    "        hidden_dim = last_hidden_state.shape[-1]\n",
    "        \n",
    "        # Create a mask to avoid gathering from padded indices (-100)\n",
    "        valid_indices_mask = (line_end_indices != -100)\n",
    "        clamped_indices = line_end_indices.clamp(min=0)\n",
    "        \n",
    "        expanded_indices = clamped_indices.unsqueeze(-1).expand(batch_size, max_lines, hidden_dim)\n",
    "        line_end_hidden_states = torch.gather(last_hidden_state, 1, expanded_indices)\n",
    "\n",
    "        logits = self.classifier(line_end_hidden_states).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Mask the logits and labels to compute loss only on valid lines\n",
    "            valid_logits = logits[valid_indices_mask]\n",
    "            valid_labels = labels[valid_indices_mask]\n",
    "            loss = F.binary_cross_entropy_with_logits(valid_logits, valid_labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 8: Model Initialization\n",
    "# ==============================================================================\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "quant_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Configuration for LoRA adapters\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\"\n",
    ")\n",
    "\n",
    "# Load the base model with quantization\n",
    "backbone = AutoModelForCausalLM.from_pretrained(\n",
    "    Config.MODEL_ID,\n",
    "    quantization_config=quant_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "backbone.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply LoRA adapters to the base model\n",
    "peft_backbone = get_peft_model(backbone, lora_cfg)\n",
    "\n",
    "# Create the final custom model\n",
    "model = GPTLineErrorDetector(peft_backbone, Config.NUM_LABELS)\n",
    "\n",
    "model.base.print_trainable_parameters()\n",
    "print(\"\\n--- Line detection model ready for training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e49b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TESTING SUITE: Model Testing Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def test_model_forward_pass(model, data_collator, processed_dataset, device='cpu'):\n",
    "    \"\"\"Test 6: Validate model forward pass\"\"\"\n",
    "    print(\"\\n🧪 TEST 6: Model Forward Pass Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Create a small batch\n",
    "        batch_samples = [processed_dataset[i] for i in range(min(2, len(processed_dataset)))]\n",
    "        batch = data_collator(batch_samples)\n",
    "        \n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        print(f\"🚀 Testing forward pass with batch size: {batch['input_ids'].shape[0]}\")\n",
    "        \n",
    "        # Model forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        print(f\"✅ Forward pass successful\")\n",
    "        print(f\"📤 Output keys: {list(outputs.keys())}\")\n",
    "        \n",
    "        # Check output shapes\n",
    "        if 'logits' in outputs:\n",
    "            logits = outputs['logits']\n",
    "            print(f\"   Logits shape: {logits.shape}\")\n",
    "            print(f\"   Logits dtype: {logits.dtype}\")\n",
    "            print(f\"   Logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]\")\n",
    "        \n",
    "        if 'loss' in outputs and outputs['loss'] is not None:\n",
    "            loss = outputs['loss']\n",
    "            print(f\"   Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Test predictions\n",
    "        if 'logits' in outputs:\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            true_labels = batch['first_error_line']\n",
    "            print(f\"   Predictions: {predictions.tolist()}\")\n",
    "            print(f\"   True labels: {true_labels.tolist()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model forward pass failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "def test_model_inference_local(model_path_or_model, tokenizer, sample_text: str, device='cpu'):\n",
    "    \"\"\"Test model inference on a single sample (for local testing)\"\"\"\n",
    "    print(\"\\n🧪 INFERENCE TEST: Single Sample\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # This function can be called after model is loaded\n",
    "        print(f\"📝 Testing inference on sample text\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Text preview: {sample_text[:100]}...\")\n",
    "        \n",
    "        # TODO: Add actual inference code here\n",
    "        print(\"   (Implementation pending model loading)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Inference test failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dda72a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cf0ca0285e437da06e534d317ac33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 TEST 6: Model Forward Pass Validation\n",
      "============================================================\n",
      "🚀 Testing forward pass with batch size: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m data_collator = DataCollatorForLineClassification(tokenizer=tokenizer)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Test the model forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m test_result = \u001b[43mtest_model_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎯 Model forward pass test result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtest_model_forward_pass\u001b[39m\u001b[34m(model, data_collator, processed_dataset, device)\u001b[39m\n\u001b[32m     21\u001b[39m model.eval()\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Forward pass successful\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📤 Output keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(outputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mGPTLineErrorDetector.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, line_end_indices, labels, **kw)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids=\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, line_end_indices=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    Defines the forward pass of the model.\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m \u001b[33;03m              and the logits for each line.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     last_hidden_state = outputs.hidden_states[-\u001b[32m1\u001b[39m]\n\u001b[32m     43\u001b[39m     batch_size, max_lines = line_end_indices.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/peft/peft_model.py:878\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    877\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:913\u001b[39m, in \u001b[36mPhi3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    910\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:636\u001b[39m, in \u001b[36mPhi3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    624\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    625\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    626\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m         position_embeddings,\n\u001b[32m    634\u001b[39m     )\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:296\u001b[39m, in \u001b[36mPhi3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    293\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hidden_states = residual + \u001b[38;5;28mself\u001b[39m.resid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[32m    309\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:191\u001b[39m, in \u001b[36mPhi3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m value_states = value_states.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    190\u001b[39m cos, sin = position_embeddings\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m query_states, key_states = \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    195\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:145\u001b[39m, in \u001b[36mapply_rotary_pos_emb\u001b[39m\u001b[34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[39m\n\u001b[32m    142\u001b[39m q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n\u001b[32m    143\u001b[39m k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m q_embed = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_rot\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_rot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_pass\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m k_embed = torch.cat([(k_rot * cos) + (rotate_half(k_rot) * sin), k_pass], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test_df = df.sample(5, random_state=42)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "processed_test_dataset = test_dataset.map(preprocess_for_line_detection, batched=True)\n",
    "\n",
    "# Recreate the data collator and test the model forward pass\n",
    "data_collator = DataCollatorForLineClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Test the model forward pass\n",
    "test_result = test_model_forward_pass(model, data_collator, processed_test_dataset, device=device)\n",
    "print(f\"\\n🎯 Model forward pass test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 9: Custom Metrics Function\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_for_line_detection(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy for the flawed-only line detection task.\n",
    "    \n",
    "    Since all samples have exactly one error line, we compare the predicted \n",
    "    error line (argmax of logits) to the true error line index.\n",
    "    \"\"\"\n",
    "    logits, true_error_lines = eval_pred\n",
    "    \n",
    "    # Find the predicted line index by taking the argmax over the line logits\n",
    "    predicted_error_lines = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted_error_lines == true_error_lines).mean()\n",
    "    \n",
    "    # Additional metrics for better evaluation\n",
    "    total_samples = len(true_error_lines)\n",
    "    correct_predictions = (predicted_error_lines == true_error_lines).sum()\n",
    "    \n",
    "    return {\n",
    "        \"line_accuracy\": accuracy,\n",
    "        \"correct_predictions\": correct_predictions,\n",
    "        \"total_samples\": total_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # Cell 10: Training Setup\n",
    "# # ==============================================================================\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # Define training arguments optimized for flawed-only dataset\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=Config.OUTPUT_DIR,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=8,  # Effective batch size = 32\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     learning_rate=2e-4,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.1,\n",
    "#     bf16=True,\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=25,\n",
    "#     eval_strategy=\"epoch\",  # Added evaluation during training\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=1,\n",
    "#     load_best_model_at_end=True,  # Load best model based on eval metric\n",
    "#     metric_for_best_model=\"line_accuracy\",  # Use our custom metric\n",
    "#     greater_is_better=True,\n",
    "#     report_to=\"none\",\n",
    "#     save_safetensors=False,\n",
    "#     label_names=[\"first_error_line\"]  # For metrics computation\n",
    "# )\n",
    "\n",
    "# # Instantiate the updated data collator\n",
    "# data_collator = DataCollatorForLineClassification(tokenizer=tokenizer)\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=final_dataset[\"train\"],\n",
    "#     eval_dataset=final_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics_for_line_detection,\n",
    "# )\n",
    "\n",
    "# print(\"--- Trainer initialized for flawed-only line detection ---\")\n",
    "# print(f\"Training samples: {len(final_dataset['train'])}\")\n",
    "# print(f\"Evaluation samples: {len(final_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5efb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 11: Execute Training\n",
    "# ==============================================================================\n",
    "print(\"Starting model training...\")\n",
    "# trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 12: Evaluation and Saving\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Evaluating on the test set ---\")\n",
    "# test_results = trainer.evaluate()\n",
    "# print(\"Test set performance:\")\n",
    "# print(test_results)\n",
    "\n",
    "print(f\"\\nSaving final model adapter to {Config.OUTPUT_DIR}...\")\n",
    "# trainer.save_model(Config.OUTPUT_DIR)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # TESTING SUITE: Comprehensive Pipeline Validation\n",
    "# # ==============================================================================\n",
    "\n",
    "# def run_comprehensive_test_suite():\n",
    "#     \"\"\"Run all tests in sequence\"\"\"\n",
    "#     print(\"🚀 RUNNING COMPREHENSIVE TEST SUITE\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Load dataset first\n",
    "#     dataset_path = Config.DATASET_PATH\n",
    "#     df = pd.read_csv(dataset_path)\n",
    "    \n",
    "#     # Initialize tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, trust_remote_code=True)\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "#     # Initialize data collator\n",
    "#     data_collator = DataCollatorForLineClassification(tokenizer=tokenizer)\n",
    "    \n",
    "#     test_results = {}\n",
    "    \n",
    "#     # Run tests\n",
    "#     test_results['dataset_loading'] = test_dataset_loading_and_format()\n",
    "#     test_results['line_labels'] = test_line_labels_validation(df)\n",
    "    \n",
    "#     # Prepare sample texts for tokenization test\n",
    "#     sample_texts = [\n",
    "#         f\"Problem: {row['question']}\\n\\nSolution: {row['solution']}\"\n",
    "#         for _, row in df.sample(3, random_state=42).iterrows()\n",
    "#     ]\n",
    "#     test_results['tokenization'] = test_tokenization_and_line_detection(tokenizer, sample_texts)\n",
    "    \n",
    "#     test_results['preprocessing'] = test_preprocessing_function(df, tokenizer)\n",
    "    \n",
    "#     # Create processed dataset for remaining tests\n",
    "#     small_df = df.sample(5, random_state=42)\n",
    "#     small_dataset = Dataset.from_pandas(small_df)\n",
    "#     processed_dataset = small_dataset.map(preprocess_for_line_detection, batched=True)\n",
    "    \n",
    "#     test_results['data_collator'] = test_data_collator(data_collator, processed_dataset)\n",
    "    \n",
    "#     # Skip model tests for now (will be added when model is loaded)\n",
    "#     test_results['solution_alignment'] = test_solution_line_alignment(df)\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"🏁 TEST SUITE SUMMARY\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     for test_name, result in test_results.items():\n",
    "#         status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "#         print(f\"{status} {test_name}\")\n",
    "    \n",
    "#     overall_success = all(test_results.values())\n",
    "#     print(f\"\\n🎯 Overall Status: {'✅ ALL TESTS PASSED' if overall_success else '❌ SOME TESTS FAILED'}\")\n",
    "    \n",
    "#     return test_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
