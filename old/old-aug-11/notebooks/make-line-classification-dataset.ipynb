{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8aa16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Data directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data\n",
      "Output directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification\n",
      "Random seed set to: 42\n",
      "Line separator token: <|LINE_SEP|>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the git repository.\"\"\"\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "# --- Global Constants and Paths ---\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = DATA_DIR / \"line-classification\"\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "RANDOM_SEED = 42\n",
    "LINE_SEP_TOKEN = \"<|LINE_SEP|>\"  # Special token for line separation\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"Line separator token: {LINE_SEP_TOKEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00008429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded master catalog with 24,652 records\n",
      "Columns: ['index', 'tier', 'question', 'correct_answer', 'wrong_answer', 'error_type', 'erroneous_line_number', 'explanation', 'error_subtype', 'source', 'solution_length', 'relative_line_position']\n",
      "\n",
      "=== Master Catalog Overview ===\n",
      "Total samples: 24,652\n",
      "Unique indices: 6,777\n",
      "\n",
      "--- Error Type Distribution ---\n",
      "error_type\n",
      "computational_error    22542\n",
      "conceptual_error        2110\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Source Distribution ---\n",
      "source\n",
      "programmatic    22912\n",
      "manual           1740\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the master catalog\n",
    "master_catalog_path = DATA_DIR / \"master_catalog_sanitized.csv\"\n",
    "if not master_catalog_path.exists():\n",
    "    raise FileNotFoundError(f\"Master catalog not found: {master_catalog_path}\")\n",
    "\n",
    "master_df = pd.read_csv(master_catalog_path)\n",
    "print(f\"Loaded master catalog with {len(master_df):,} records\")\n",
    "print(f\"Columns: {list(master_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== Master Catalog Overview ===\")\n",
    "print(f\"Total samples: {len(master_df):,}\")\n",
    "print(f\"Unique indices: {master_df['index'].nunique():,}\")\n",
    "\n",
    "print(\"\\n--- Error Type Distribution ---\")\n",
    "error_type_counts = master_df['error_type'].value_counts()\n",
    "print(error_type_counts)\n",
    "\n",
    "print(\"\\n--- Source Distribution ---\") \n",
    "source_counts = master_df['source'].value_counts()\n",
    "print(source_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c525c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Disjoint Sets with Simple Sampling ===\n",
      "Set A (Conceptual errors): 1881 unique indices\n",
      "=== Simple Computational Error Sampling ===\n",
      "Available computational error samples: 16287\n",
      "Available unique indices: 4896\n",
      "Position tertiles: 33%=0.250, 67%=0.600\n",
      "Early group: 5599 samples, 3676 unique indices\n",
      "Middle group: 5680 samples, 3654 unique indices\n",
      "Late group: 5008 samples, 2866 unique indices\n",
      "Early group tier 4 unique indices: 181\n",
      "Middle group tier 4 unique indices: 183\n",
      "Late group tier 4 unique indices: 180\n",
      "Total tier 4 indices selected: 544\n",
      "Need 1337 more indices from other tiers\n",
      "Available non-tier4 indices: 4627\n",
      "Final selection: 1881 unique indices\n",
      "ERROR: Duplicate indices detected!\n",
      "After deduplication: 1606 unique indices\n",
      "\n",
      "--- Final Distribution ---\n",
      "Tier distribution:\n",
      "  tier1: 581 indices (36.2%)\n",
      "  tier2: 164 indices (10.2%)\n",
      "  tier3: 590 indices (36.7%)\n",
      "  tier4: 269 indices (16.7%)\n",
      "\n",
      "Position distribution:\n",
      "  Early: 1156 indices (72.0%)\n",
      "  Middle: 387 indices (24.1%)\n",
      "  Late: 157 indices (9.8%)\n",
      "Set B (Computational errors): 1606 unique indices\n",
      "Available indices for correct samples: 3290\n",
      "Set C (Correct samples): 1606 unique indices\n",
      "\n",
      "--- Disjoint Verification ---\n",
      "Conceptual ‚à© Computational: 0 indices (should be 0)\n",
      "Conceptual ‚à© Correct: 0 indices (should be 0)\n",
      "Computational ‚à© Correct: 0 indices (should be 0)\n",
      "Total unique indices: 5093 (should equal 5093)\n",
      "‚úì All sets are properly disjoint\n"
     ]
    }
   ],
   "source": [
    "def select_balanced_computational_indices_simple(master_df, conceptual_indices, target_count):\n",
    "    \"\"\"\n",
    "    Simplified computational error sampling:\n",
    "    1. Form 3 groups based on relative line position (early, middle, late)\n",
    "    2. From each group, take as many distinct tier 4 samples as possible\n",
    "    3. For remaining, randomly sample distinct indices from other tiers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Simple Computational Error Sampling ===\")\n",
    "    \n",
    "    # Get available computational errors (excluding conceptual indices)\n",
    "    computational_df = master_df[\n",
    "        (master_df['error_type'] == 'computational_error') & \n",
    "        (~master_df['index'].isin(conceptual_indices))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(computational_df) == 0:\n",
    "        print(\"Warning: No computational errors available after excluding conceptual indices\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Available computational error samples: {len(computational_df)}\")\n",
    "    print(f\"Available unique indices: {computational_df['index'].nunique()}\")\n",
    "    \n",
    "    # Step 1: Form 3 groups based on relative line position\n",
    "    # Simple tertile split\n",
    "    positions = computational_df['relative_line_position'].dropna()\n",
    "    tertile_33 = positions.quantile(0.33)\n",
    "    tertile_67 = positions.quantile(0.67)\n",
    "    \n",
    "    print(f\"Position tertiles: 33%={tertile_33:.3f}, 67%={tertile_67:.3f}\")\n",
    "    \n",
    "    # Create groups\n",
    "    early_group = computational_df[computational_df['relative_line_position'] <= tertile_33]\n",
    "    middle_group = computational_df[\n",
    "        (computational_df['relative_line_position'] > tertile_33) & \n",
    "        (computational_df['relative_line_position'] <= tertile_67)\n",
    "    ]\n",
    "    late_group = computational_df[computational_df['relative_line_position'] > tertile_67]\n",
    "    \n",
    "    print(f\"Early group: {len(early_group)} samples, {early_group['index'].nunique()} unique indices\")\n",
    "    print(f\"Middle group: {len(middle_group)} samples, {middle_group['index'].nunique()} unique indices\")\n",
    "    print(f\"Late group: {len(late_group)} samples, {late_group['index'].nunique()} unique indices\")\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    # Step 2: From each group, take as many distinct tier 4 samples as possible\n",
    "    for group_name, group_df in [(\"Early\", early_group), (\"Middle\", middle_group), (\"Late\", late_group)]:\n",
    "        tier4_indices = group_df[group_df['tier'] == 'tier4']['index'].unique()\n",
    "        print(f\"{group_name} group tier 4 unique indices: {len(tier4_indices)}\")\n",
    "        selected_indices.extend(tier4_indices.tolist())\n",
    "    \n",
    "    print(f\"Total tier 4 indices selected: {len(selected_indices)}\")\n",
    "    \n",
    "    # Step 3: For remaining, randomly sample distinct indices from other tiers\n",
    "    remaining_needed = target_count - len(selected_indices)\n",
    "    \n",
    "    if remaining_needed > 0:\n",
    "        print(f\"Need {remaining_needed} more indices from other tiers\")\n",
    "        \n",
    "        # Get all non-tier4 indices that aren't already selected\n",
    "        non_tier4_df = computational_df[computational_df['tier'] != 'tier4']\n",
    "        available_indices = non_tier4_df['index'].unique()\n",
    "        available_indices = [idx for idx in available_indices if idx not in selected_indices]\n",
    "        \n",
    "        print(f\"Available non-tier4 indices: {len(available_indices)}\")\n",
    "        \n",
    "        if len(available_indices) >= remaining_needed:\n",
    "            additional_indices = random.sample(available_indices, remaining_needed)\n",
    "            selected_indices.extend(additional_indices)\n",
    "        else:\n",
    "            print(f\"Warning: Only {len(available_indices)} available, adding all\")\n",
    "            selected_indices.extend(available_indices)\n",
    "    \n",
    "    print(f\"Final selection: {len(selected_indices)} unique indices\")\n",
    "    \n",
    "    # Verify uniqueness\n",
    "    if len(selected_indices) != len(set(selected_indices)):\n",
    "        print(f\"ERROR: Duplicate indices detected!\")\n",
    "        selected_indices = list(set(selected_indices))\n",
    "        print(f\"After deduplication: {len(selected_indices)} unique indices\")\n",
    "    \n",
    "    # Show final distribution\n",
    "    selected_df = computational_df[computational_df['index'].isin(selected_indices)]\n",
    "    selected_unique = selected_df.drop_duplicates('index')\n",
    "    \n",
    "    print(f\"\\n--- Final Distribution ---\")\n",
    "    tier_counts = selected_unique['tier'].value_counts().sort_index()\n",
    "    print(\"Tier distribution:\")\n",
    "    for tier in ['tier1', 'tier2', 'tier3', 'tier4']:\n",
    "        count = tier_counts.get(tier, 0)\n",
    "        pct = (count / len(selected_unique)) * 100\n",
    "        print(f\"  {tier}: {count} indices ({pct:.1f}%)\")\n",
    "    \n",
    "    # Position distribution of selected indices\n",
    "    print(\"\\nPosition distribution:\")\n",
    "    for group_name, (min_pos, max_pos) in [\n",
    "        (\"Early\", (0, tertile_33)),\n",
    "        (\"Middle\", (tertile_33, tertile_67)), \n",
    "        (\"Late\", (tertile_67, 1.0))\n",
    "    ]:\n",
    "        group_selected = selected_unique[\n",
    "            (selected_unique['relative_line_position'] >= min_pos) & \n",
    "            (selected_unique['relative_line_position'] <= max_pos)\n",
    "        ]\n",
    "        count = len(group_selected)\n",
    "        pct = (count / len(selected_unique)) * 100\n",
    "        print(f\"  {group_name}: {count} indices ({pct:.1f}%)\")\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "def get_disjoint_sets_simple(master_df):\n",
    "    \"\"\"\n",
    "    Create three disjoint sets with simplified computational sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating Disjoint Sets with Simple Sampling ===\")\n",
    "    \n",
    "    # Set A: All indices with conceptual errors\n",
    "    conceptual_indices = master_df[\n",
    "        master_df['error_type'] == 'conceptual_error'\n",
    "    ]['index'].unique()\n",
    "    conceptual_indices = sorted(conceptual_indices)\n",
    "    N = len(conceptual_indices)\n",
    "    \n",
    "    print(f\"Set A (Conceptual errors): {N} unique indices\")\n",
    "    \n",
    "    # Set B: Simple computational error sampling\n",
    "    computational_indices = select_balanced_computational_indices_simple(\n",
    "        master_df, conceptual_indices, N\n",
    "    )\n",
    "    \n",
    "    print(f\"Set B (Computational errors): {len(computational_indices)} unique indices\")\n",
    "    \n",
    "    # Set C: Correct samples from remaining disjoint indices\n",
    "    used_indices = set(conceptual_indices) | set(computational_indices)\n",
    "    all_indices = set(master_df['index'].unique())\n",
    "    remaining_indices = list(all_indices - used_indices)\n",
    "    \n",
    "    print(f\"Available indices for correct samples: {len(remaining_indices)}\")\n",
    "    \n",
    "    if len(remaining_indices) < len(computational_indices):\n",
    "        print(f\"WARNING: Only {len(remaining_indices)} remaining indices available, need {len(computational_indices)}\")\n",
    "        correct_indices = remaining_indices\n",
    "    else:\n",
    "        correct_indices = random.sample(remaining_indices, len(computational_indices))\n",
    "    \n",
    "    print(f\"Set C (Correct samples): {len(correct_indices)} unique indices\")\n",
    "    \n",
    "    # Final verification that all sets are truly disjoint\n",
    "    set_A = set(conceptual_indices)\n",
    "    set_B = set(computational_indices)\n",
    "    set_C = set(correct_indices)\n",
    "    \n",
    "    overlap_AB = len(set_A & set_B)\n",
    "    overlap_AC = len(set_A & set_C)\n",
    "    overlap_BC = len(set_B & set_C)\n",
    "    \n",
    "    print(f\"\\n--- Disjoint Verification ---\")\n",
    "    print(f\"Conceptual ‚à© Computational: {overlap_AB} indices (should be 0)\")\n",
    "    print(f\"Conceptual ‚à© Correct: {overlap_AC} indices (should be 0)\")\n",
    "    print(f\"Computational ‚à© Correct: {overlap_BC} indices (should be 0)\")\n",
    "    \n",
    "    total_unique_indices = len(set_A | set_B | set_C)\n",
    "    expected_total = len(conceptual_indices) + len(computational_indices) + len(correct_indices)\n",
    "    print(f\"Total unique indices: {total_unique_indices} (should equal {expected_total})\")\n",
    "    \n",
    "    if total_unique_indices != expected_total:\n",
    "        print(\"ERROR: Sets are not properly disjoint!\")\n",
    "    else:\n",
    "        print(\"‚úì All sets are properly disjoint\")\n",
    "    \n",
    "    return conceptual_indices, computational_indices, correct_indices\n",
    "\n",
    "conceptual_indices, computational_indices, correct_indices = get_disjoint_sets_simple(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line_separator_tokens(solution_text: str, line_sep_token: str = LINE_SEP_TOKEN) -> str:\n",
    "    \"\"\"\n",
    "    Since preprocessing is already done in master catalog, just add line separator tokens.\n",
    "    \n",
    "    Args:\n",
    "        solution_text: Already preprocessed solution text from sanitized master catalog\n",
    "        line_sep_token: Special token to use for line separation\n",
    "        \n",
    "    Returns:\n",
    "        Solution text with line separator tokens for tokenization\n",
    "    \"\"\"\n",
    "    if not isinstance(solution_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Split by newlines, filter empty lines, and join with special token\n",
    "    lines = solution_text.split('\\n')\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]  # Filter empty lines\n",
    "    \n",
    "    processed = line_sep_token.join(non_empty_lines) + line_sep_token\n",
    "    return processed\n",
    "\n",
    "def create_user_prompt_simple(question: str, solution: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates user prompt from already sanitized question and solution.\n",
    "    \n",
    "    Args:\n",
    "        question: Already sanitized GSM8K problem statement\n",
    "        solution: Already preprocessed solution text with line separators\n",
    "        \n",
    "    Returns:\n",
    "        Formatted user prompt for line-level error detection\n",
    "    \"\"\"\n",
    "    system_instruction = \"Analyze the following mathematical problem and solution to identify the line containing the error.\"\n",
    "    \n",
    "    user_prompt = f\"\"\"{system_instruction}\n",
    "\n",
    "### Problem:\n",
    "{question}\n",
    "\n",
    "### Solution:\n",
    "{solution}\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6205c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SIMPLE LINE SEPARATOR DEBUG\n",
      "==================================================\n",
      "\n",
      "--- INDEX 143 ---\n",
      "Stored solution_length: 4\n",
      "\n",
      "RAW SOLUTION:\n",
      "'There are 60 goldfish because 15 / .25 = 60\\n75% of the fish are below the surface because 100 - 25 = 75\\nThere are 35 goldfish below the surface because 60 x .75 = 35\\n#### 35'\n",
      "\n",
      "AFTER LINE SEPARATOR TOKENS:\n",
      "'There are 60 goldfish because 15 / .25 = 60<|LINE_SEP|>75% of the fish are below the surface because 100 - 25 = 75<|LINE_SEP|>There are 35 goldfish below the surface because 60 x .75 = 35<|LINE_SEP|>#### 35<|LINE_SEP|>'\n",
      "\n",
      "TOKEN COUNT: 4\n",
      "STORED LENGTH: 4\n",
      "DIFFERENCE: 0\n",
      "\n",
      "MANUAL LINE SPLIT:\n",
      "  0: 'There are 60 goldfish because 15 / .25 = 60'\n",
      "  1: '75% of the fish are below the surface because 100 - 25 = 75'\n",
      "  2: 'There are 35 goldfish below the surface because 60 x .75 = 35'\n",
      "  3: '#### 35'\n",
      "\n",
      "NON-EMPTY LINES: 4\n",
      "  0: 'There are 60 goldfish because 15 / .25 = 60'\n",
      "  1: '75% of the fish are below the surface because 100 - 25 = 75'\n",
      "  2: 'There are 35 goldfish below the surface because 60 x .75 = 35'\n",
      "  3: '#### 35'\n",
      "\n",
      "--- INDEX 140 ---\n",
      "Stored solution_length: 5\n",
      "\n",
      "RAW SOLUTION:\n",
      "'First find how many kids from Riverside High are rejected: 20% * 120 kids = 24 kids\\nThen find how many kids from West Side High are rejected: 70% * 90 kids = 63 kids\\nThen find how many kids from Mountaintop High are rejected: 50 kids / 2 = 25 kids\\nThen add the number of rejected kids from each school to find the total number of rejected kids: 24 kids + 63 kids + 25 kids = 112 kids\\n#### 112'\n",
      "\n",
      "AFTER LINE SEPARATOR TOKENS:\n",
      "'First find how many kids from Riverside High are rejected: 20% * 120 kids = 24 kids<|LINE_SEP|>Then find how many kids from West Side High are rejected: 70% * 90 kids = 63 kids<|LINE_SEP|>Then find how many kids from Mountaintop High are rejected: 50 kids / 2 = 25 kids<|LINE_SEP|>Then add the number of rejected kids from each school to find the total number of rejected kids: 24 kids + 63 kids + 25 kids = 112 kids<|LINE_SEP|>#### 112<|LINE_SEP|>'\n",
      "\n",
      "TOKEN COUNT: 5\n",
      "STORED LENGTH: 5\n",
      "DIFFERENCE: 0\n",
      "\n",
      "MANUAL LINE SPLIT:\n",
      "  0: 'First find how many kids from Riverside High are rejected: 20% * 120 kids = 24 kids'\n",
      "  1: 'Then find how many kids from West Side High are rejected: 70% * 90 kids = 63 kids'\n",
      "  2: 'Then find how many kids from Mountaintop High are rejected: 50 kids / 2 = 25 kids'\n",
      "  3: 'Then add the number of rejected kids from each school to find the total number of rejected kids: 24 kids + 63 kids + 25 kids = 112 kids'\n",
      "  4: '#### 112'\n",
      "\n",
      "NON-EMPTY LINES: 5\n",
      "  0: 'First find how many kids from Riverside High are rejected: 20% * 120 kids = 24 kids'\n",
      "  1: 'Then find how many kids from West Side High are rejected: 70% * 90 kids = 63 kids'\n",
      "  2: 'Then find how many kids from Mountaintop High are rejected: 50 kids / 2 = 25 kids'\n",
      "  3: 'Then add the number of rejected kids from each school to find the total number of rejected kids: 24 kids + 63 kids + 25 kids = 112 kids'\n",
      "  4: '#### 112'\n",
      "\n",
      "--- INDEX 145 ---\n",
      "Stored solution_length: 5\n",
      "\n",
      "RAW SOLUTION:\n",
      "'He watched 2*20=40 minutes of Jeopardy.\\nWheel of Fortune is 2*20=40 minutes each.\\nSo he watched it for 40*2=80 minutes.\\nSo he watched 40+80=120 minutes of TV.\\n#### 120'\n",
      "\n",
      "AFTER LINE SEPARATOR TOKENS:\n",
      "'He watched 2*20=40 minutes of Jeopardy.<|LINE_SEP|>Wheel of Fortune is 2*20=40 minutes each.<|LINE_SEP|>So he watched it for 40*2=80 minutes.<|LINE_SEP|>So he watched 40+80=120 minutes of TV.<|LINE_SEP|>#### 120<|LINE_SEP|>'\n",
      "\n",
      "TOKEN COUNT: 5\n",
      "STORED LENGTH: 5\n",
      "DIFFERENCE: 0\n",
      "\n",
      "MANUAL LINE SPLIT:\n",
      "  0: 'He watched 2*20=40 minutes of Jeopardy.'\n",
      "  1: 'Wheel of Fortune is 2*20=40 minutes each.'\n",
      "  2: 'So he watched it for 40*2=80 minutes.'\n",
      "  3: 'So he watched 40+80=120 minutes of TV.'\n",
      "  4: '#### 120'\n",
      "\n",
      "NON-EMPTY LINES: 5\n",
      "  0: 'He watched 2*20=40 minutes of Jeopardy.'\n",
      "  1: 'Wheel of Fortune is 2*20=40 minutes each.'\n",
      "  2: 'So he watched it for 40*2=80 minutes.'\n",
      "  3: 'So he watched 40+80=120 minutes of TV.'\n",
      "  4: '#### 120'\n"
     ]
    }
   ],
   "source": [
    "# Simple debugging: print raw solution vs line sep version\n",
    "print(\"üîç SIMPLE LINE SEPARATOR DEBUG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get a few samples that showed mismatches\n",
    "problem_indices = [143, 140, 145]\n",
    "\n",
    "for idx in problem_indices:\n",
    "    sample = master_df[master_df['index'] == idx].iloc[0]\n",
    "    \n",
    "    print(f\"\\n--- INDEX {idx} ---\")\n",
    "    print(f\"Stored solution_length: {sample['solution_length']}\")\n",
    "    \n",
    "    # Raw solution from master catalog\n",
    "    raw_solution = sample['wrong_answer']\n",
    "    print(f\"\\nRAW SOLUTION:\")\n",
    "    print(repr(raw_solution))\n",
    "    \n",
    "    # After adding line separators\n",
    "    solution_with_tokens = add_line_separator_tokens(raw_solution)\n",
    "    print(f\"\\nAFTER LINE SEPARATOR TOKENS:\")\n",
    "    print(repr(solution_with_tokens))\n",
    "    \n",
    "    # Count tokens\n",
    "    token_count = solution_with_tokens.count(LINE_SEP_TOKEN)\n",
    "    print(f\"\\nTOKEN COUNT: {token_count}\")\n",
    "    print(f\"STORED LENGTH: {sample['solution_length']}\")\n",
    "    print(f\"DIFFERENCE: {token_count - sample['solution_length']}\")\n",
    "    \n",
    "    # Split manually to see what's happening\n",
    "    lines = raw_solution.split('\\n')\n",
    "    print(f\"\\nMANUAL LINE SPLIT:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"  {i}: {repr(line)}\")\n",
    "    \n",
    "    non_empty = [line.strip() for line in lines if line.strip()]\n",
    "    print(f\"\\nNON-EMPTY LINES: {len(non_empty)}\")\n",
    "    for i, line in enumerate(non_empty):\n",
    "        print(f\"  {i}: {repr(line)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af0d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Updated Dataset Creation Functions\n",
    "# ==============================================================================\n",
    "def create_line_labels_optimized(solution_with_tokens, relative_line_position, solution_length, sample_index=None):\n",
    "    \"\"\"\n",
    "    Create one-hot encoded line labels using pre-computed relative position and length.\n",
    "    Updated to work with sanitized master catalog and line separator tokens.\n",
    "    \n",
    "    Args:\n",
    "        solution_with_tokens (str): Solution text with line separator tokens added\n",
    "        relative_line_position (float or None): Relative position of error (0.0-1.0), \n",
    "                                              or None for correct solutions\n",
    "        solution_length (int): Pre-computed number of lines (from sanitized catalog)\n",
    "        sample_index (int, optional): Sample index for debugging mismatch reports\n",
    "    \n",
    "    Returns:\n",
    "        list: One-hot encoded labels where 1 indicates the error line, 0 otherwise.\n",
    "              For correct solutions, all labels are 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(solution_with_tokens) or solution_with_tokens is None or solution_with_tokens == \"\":\n",
    "        return []\n",
    "    \n",
    "    # Use the pre-computed solution_length from sanitized catalog\n",
    "    num_lines = int(solution_length)\n",
    "    \n",
    "    # Verify actual line count matches stored length (for debugging)\n",
    "    if LINE_SEP_TOKEN in solution_with_tokens:\n",
    "        # Count line separator tokens to verify consistency\n",
    "        actual_line_count = solution_with_tokens.count(LINE_SEP_TOKEN)\n",
    "        if actual_line_count != num_lines:\n",
    "            index_info = f\" (Index: {sample_index})\" if sample_index is not None else \"\"\n",
    "            print(f\"Warning: Line count mismatch{index_info}! Stored: {num_lines}, Actual: {actual_line_count}\")\n",
    "            # Use actual count to prevent index errors\n",
    "            num_lines = actual_line_count\n",
    "    \n",
    "    # Handle edge case of empty solutions\n",
    "    if num_lines <= 0:\n",
    "        return []\n",
    "    \n",
    "    # Initialize all labels as 0 (no error)\n",
    "    line_labels = [0] * num_lines\n",
    "    \n",
    "    # For error samples, set the error line to 1\n",
    "    if relative_line_position is not None and not pd.isna(relative_line_position):\n",
    "        # Convert relative position (0.0-1.0) to absolute line number (0-based)\n",
    "        if num_lines == 1:\n",
    "            error_line_number = 0  # Only one line, must be line 0\n",
    "        else:\n",
    "            error_line_number = int(relative_line_position * (num_lines - 1))\n",
    "        \n",
    "        # Ensure error line is within valid bounds\n",
    "        error_line_number = max(0, min(error_line_number, num_lines - 1))\n",
    "        line_labels[error_line_number] = 1\n",
    "    \n",
    "    return line_labels\n",
    "\n",
    "def select_best_sample(idx_samples):\n",
    "    \"\"\"\n",
    "    Select the best sample from a group of samples for the same index.\n",
    "    Priority: manual > programmatic\n",
    "    \"\"\"\n",
    "    # Prioritize manual over programmatic\n",
    "    manual_samples = idx_samples[idx_samples['source'] == 'manual']\n",
    "    if len(manual_samples) > 0:\n",
    "        return manual_samples.iloc[0]  # Take first manual sample\n",
    "    else:\n",
    "        return idx_samples.iloc[0]  # Take first programmatic sample\n",
    "\n",
    "def create_flawed_only_line_classification_dataset_simple(master_df, conceptual_indices, computational_indices):\n",
    "    \"\"\"\n",
    "    SIMPLIFIED: Create dataset from preprocessed master catalog.\n",
    "    No additional preprocessing needed since master catalog is already sanitized.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating Flawed-Only Dataset from Sanitized Master Catalog ===\")\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Process conceptual error samples\n",
    "    print(f\"Processing {len(conceptual_indices)} conceptual error samples...\")\n",
    "    for idx in tqdm(conceptual_indices, desc=\"Conceptual errors\"):\n",
    "        idx_samples = master_df[\n",
    "            (master_df['index'] == idx) & \n",
    "            (master_df['error_type'] == 'conceptual_error')\n",
    "        ]\n",
    "        \n",
    "        if len(idx_samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        selected_sample = select_best_sample(idx_samples)\n",
    "        \n",
    "        # Text is already preprocessed, just add line separator tokens\n",
    "        preprocessed_solution = selected_sample['wrong_answer']  # Already sanitized!\n",
    "        solution_with_tokens = add_line_separator_tokens(preprocessed_solution)\n",
    "        \n",
    "        # Question is already preprocessed too\n",
    "        preprocessed_question = selected_sample['question']  # Already sanitized!\n",
    "        user_prompt = create_user_prompt_simple(preprocessed_question, solution_with_tokens)\n",
    "        \n",
    "        # Correct answer is also already preprocessed\n",
    "        correct_answer = selected_sample['correct_answer']  # Already sanitized!\n",
    "        \n",
    "        # Use the stored values directly (they're already calculated correctly)\n",
    "        line_labels = create_line_labels_optimized(\n",
    "            solution_with_tokens, \n",
    "            selected_sample['relative_line_position'],\n",
    "            selected_sample['solution_length'],\n",
    "            sample_index=idx  # Add this parameter\n",
    "        )\n",
    "        \n",
    "        # This should now work perfectly since everything is pre-aligned\n",
    "        if len(line_labels) > 0 and sum(line_labels) == 1:\n",
    "            sample = {\n",
    "                'text': user_prompt,\n",
    "                'correct_answer': correct_answer,\n",
    "                'line_labels': line_labels,\n",
    "                'error_type': 'conceptual_error',\n",
    "                'index': idx,\n",
    "                'tier': selected_sample['tier'],\n",
    "                'source': selected_sample['source'],\n",
    "                'relative_line_position': selected_sample['relative_line_position'],\n",
    "                'solution_length': selected_sample['solution_length']\n",
    "            }\n",
    "            dataset.append(sample)\n",
    "    \n",
    "    # Process computational error samples\n",
    "    print(f\"Processing {len(computational_indices)} computational error samples...\")\n",
    "    for idx in tqdm(computational_indices, desc=\"Computational errors\"):\n",
    "        idx_samples = master_df[\n",
    "            (master_df['index'] == idx) & \n",
    "            (master_df['error_type'] == 'computational_error')\n",
    "        ]\n",
    "        \n",
    "        if len(idx_samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        selected_sample = select_best_sample(idx_samples)\n",
    "        \n",
    "        # Text is already preprocessed, just add line separator tokens\n",
    "        preprocessed_solution = selected_sample['wrong_answer']  # Already sanitized!\n",
    "        solution_with_tokens = add_line_separator_tokens(preprocessed_solution)\n",
    "        \n",
    "        # Question is already preprocessed too\n",
    "        preprocessed_question = selected_sample['question']  # Already sanitized!\n",
    "        user_prompt = create_user_prompt_simple(preprocessed_question, solution_with_tokens)\n",
    "        \n",
    "        # Correct answer is also already preprocessed\n",
    "        correct_answer = selected_sample['correct_answer']  # Already sanitized!\n",
    "        \n",
    "        # Use the stored values directly (they're already calculated correctly)\n",
    "        line_labels = create_line_labels_optimized(\n",
    "            solution_with_tokens, \n",
    "            selected_sample['relative_line_position'],\n",
    "            selected_sample['solution_length'],\n",
    "            sample_index=idx  # Add this parameter\n",
    "        )\n",
    "        \n",
    "        # This should now work perfectly since everything is pre-aligned\n",
    "        if len(line_labels) > 0 and sum(line_labels) == 1:\n",
    "            sample = {\n",
    "                'text': user_prompt,\n",
    "                'correct_answer': correct_answer,\n",
    "                'line_labels': line_labels,\n",
    "                'error_type': 'computational_error',\n",
    "                'index': idx,\n",
    "                'tier': selected_sample['tier'],\n",
    "                'source': selected_sample['source'],\n",
    "                'relative_line_position': selected_sample['relative_line_position'],\n",
    "                'solution_length': selected_sample['solution_length']\n",
    "            }\n",
    "            dataset.append(sample)\n",
    "    \n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    \n",
    "    print(f\"\\nTotal samples created: {len(dataset_df)}\")\n",
    "    \n",
    "    # This validation should now show 100% success!\n",
    "    print(\"\\n--- Label Validation ---\")\n",
    "    label_sums = dataset_df['line_labels'].apply(sum)\n",
    "    valid_samples = (label_sums == 1).sum()\n",
    "    print(f\"Samples with exactly 1 error line: {valid_samples}/{len(dataset_df)} ({valid_samples/len(dataset_df)*100:.1f}%)\")\n",
    "    \n",
    "    if (label_sums != 1).any():\n",
    "        print(f\"WARNING: {(label_sums != 1).sum()} samples don't have exactly 1 error line\")\n",
    "        invalid_samples = dataset_df[label_sums != 1]\n",
    "        print(\"This shouldn't happen with sanitized catalog!\")\n",
    "        \n",
    "    print(\"‚úÖ SUCCESS: Using preprocessed master catalog eliminates all preprocessing inconsistencies!\")\n",
    "    \n",
    "    return dataset_df\n",
    "\n",
    "def save_flawed_only_dataset(dataset_df, output_dir):\n",
    "    \"\"\"\n",
    "    Save the flawed-only line classification dataset with updated metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Saving Flawed-Only Line Classification Dataset ===\")\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save main dataset\n",
    "    dataset_path = output_dir / \"flawed_only_line_classification_dataset.csv\"\n",
    "    dataset_df.to_csv(dataset_path, index=False)\n",
    "    print(f\"‚úì Dataset saved to: {dataset_path}\")\n",
    "    print(f\"  Size: {len(dataset_df):,} samples\")\n",
    "    print(f\"  Unique indices: {dataset_df['index'].nunique():,}\")\n",
    "    \n",
    "    # Calculate class balance for metadata\n",
    "    total_line_labels = sum(len(labels) for labels in dataset_df['line_labels'])\n",
    "    total_error_lines = sum(sum(labels) for labels in dataset_df['line_labels'])\n",
    "    total_correct_lines = total_line_labels - total_error_lines\n",
    "    imbalance_ratio = total_correct_lines / total_error_lines if total_error_lines > 0 else 0\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"creation_info\": {\n",
    "            \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"source_catalog\": \"master_catalog.csv\",\n",
    "            \"creator\": \"make-line-classification-dataset.ipynb\",\n",
    "            \"dataset_strategy\": \"flawed_only_with_preprocessing\"\n",
    "        },\n",
    "        \"preprocessing_applied\": {\n",
    "            \"newline_fixes\": f\"Converted literal \\\\n to actual newlines, then replaced with special token: {LINE_SEP_TOKEN}\",\n",
    "            \"line_separation\": f\"Used special token '{LINE_SEP_TOKEN}' for reliable line boundary detection\",\n",
    "            \"unicode_sanitization\": \"Converted problematic Unicode chars to ASCII equivalents\",\n",
    "            \"user_prompt_formatting\": \"Combined question + solution into standardized user prompt format\",\n",
    "            \"text_cleaning\": \"Removed comma separators from numbers\"\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"description\": \"Flawed-only line-level error detection dataset with comprehensive preprocessing\",\n",
    "            \"strategy\": \"flawed_only_balanced_preprocessed\", \n",
    "            \"total_samples\": len(dataset_df),\n",
    "            \"unique_problems\": dataset_df['index'].nunique(),\n",
    "            \"composition\": dict(dataset_df['error_type'].value_counts()),\n",
    "            \"source_distribution\": dict(dataset_df['source'].value_counts()),\n",
    "            \"tier_distribution\": dict(dataset_df['tier'].value_counts()),\n",
    "            \"class_balance\": {\n",
    "                \"total_line_positions\": total_line_labels,\n",
    "                \"error_line_positions\": total_error_lines,\n",
    "                \"correct_line_positions\": total_correct_lines,\n",
    "                \"imbalance_ratio\": f\"{imbalance_ratio:.1f}:1\",\n",
    "                \"error_percentage\": f\"{total_error_lines/total_line_labels*100:.1f}%\"\n",
    "            }\n",
    "        },\n",
    "        \"column_descriptions\": {\n",
    "            \"text\": \"Complete user prompt: system instruction + problem + preprocessed solution\",\n",
    "            \"correct_answer\": \"Original GSM8K correct answer for reference\",\n",
    "            \"line_labels\": \"One-hot encoded labels [0,0,1,0,...] indicating error line\",\n",
    "            \"error_type\": \"'conceptual_error' or 'computational_error' (no correct samples)\",\n",
    "            \"index\": \"Original GSM8K problem index\",\n",
    "            \"tier\": \"Problem difficulty tier (tier1-tier4)\",\n",
    "            \"source\": \"'manual' or 'programmatic'\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = output_dir / \"flawed_only_dataset_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    print(f\"‚úì Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = f\"\"\"# Flawed-Only Line Classification Dataset (Preprocessed)\n",
    "\n",
    "Created on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Random seed: {RANDOM_SEED}\n",
    "Strategy: **Flawed-Only with Comprehensive Preprocessing**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This dataset addresses class imbalance by using **only flawed samples** for line-level error detection, with comprehensive preprocessing applied to fix tokenization issues and standardize input format.\n",
    "\n",
    "## Key Improvements\n",
    "\n",
    "### 1. Class Balance\n",
    "- **Original approach**: ~85-90% zeros (severe imbalance, ratio ~5-6:1)\n",
    "- **Flawed-only approach**: ~{100-total_error_lines/total_line_labels*100:.1f}% zeros (better balance, ratio ~{imbalance_ratio:.1f}:1)\n",
    "\n",
    "### 2. Preprocessing Applied\n",
    "- **Line separation**: Replaced newlines with special token `{LINE_SEP_TOKEN}` for reliable tokenization\n",
    "- **Unicode sanitization**: Converted problematic Unicode characters to ASCII equivalents\n",
    "- **User prompt formatting**: Combined question + solution into standardized format\n",
    "- **Text cleaning**: Removed comma separators from numbers\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "- **Total samples**: {len(dataset_df):,} (all flawed)\n",
    "- **Unique problems**: {dataset_df['index'].nunique():,}\n",
    "- **Strategy**: Balanced conceptual + computational errors with preprocessing\n",
    "\n",
    "### Composition\n",
    "- **Conceptual errors**: {dict(dataset_df['error_type'].value_counts()).get('conceptual_error', 0)} samples\n",
    "- **Computational errors**: {dict(dataset_df['error_type'].value_counts()).get('computational_error', 0)} samples\n",
    "- **Correct solutions**: 0 samples (removed to eliminate imbalance)\n",
    "\n",
    "### Class Balance Analysis\n",
    "- **Total line positions**: {total_line_labels:,}\n",
    "- **Error lines (label=1)**: {total_error_lines:,} ({total_error_lines/total_line_labels*100:.1f}%)\n",
    "- **Correct lines (label=0)**: {total_correct_lines:,} ({total_correct_lines/total_line_labels*100:.1f}%)\n",
    "- **Imbalance ratio**: {imbalance_ratio:.1f}:1\n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "- `text`: Complete user prompt with system instruction + problem + preprocessed solution\n",
    "- `correct_answer`: Original GSM8K correct answer for reference\n",
    "- `line_labels`: One-hot encoded labels `[0,0,1,0,...]` indicating error line\n",
    "- `error_type`: 'conceptual_error' or 'computational_error'\n",
    "- `index`: Original GSM8K problem index\n",
    "- `tier`: Problem difficulty tier (tier1-tier4)\n",
    "- `source`: 'manual' or 'programmatic'\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('flawed_only_line_classification_dataset.csv')\n",
    "\n",
    "# Parse line_labels from string representation\n",
    "df['line_labels'] = df['line_labels'].apply(ast.literal_eval)\n",
    "\n",
    "# Example: Check a sample with error\n",
    "sample = df.iloc[0]\n",
    "print(\"User prompt:\")\n",
    "print(sample['text'])\n",
    "print(f\"\\\\nError type: {{sample['error_type']}}\")\n",
    "print(f\"Error line position: {{sample['line_labels'].index(1)}}\")\n",
    "```\n",
    "\n",
    "## Training Considerations\n",
    "\n",
    "- **Tokenization**: Preprocessing ensures reliable newline token detection\n",
    "- **Loss function**: Standard cross-entropy works well (no complex weighting needed)\n",
    "- **Data collator**: Custom collator for variable-length `line_labels`\n",
    "- **Evaluation**: Line-level accuracy, precision, recall, F1\n",
    "- **Much more stable training** than severely imbalanced dataset\"\"\"\n",
    "    \n",
    "    readme_path = output_dir / \"README.md\"\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"‚úì README saved to: {readme_path}\")\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae3ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating flawed-only line classification dataset...\n",
      "=== Creating Flawed-Only Dataset from Sanitized Master Catalog ===\n",
      "Processing 1881 conceptual error samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9f40e7535b4e16ad736e973872ac59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conceptual errors:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1606 computational error samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c042585ac3434c8105e46188d4a1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computational errors:   0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples created: 3487\n",
      "\n",
      "--- Label Validation ---\n",
      "Samples with exactly 1 error line: 3487/3487 (100.0%)\n",
      "‚úÖ SUCCESS: Using preprocessed master catalog eliminates all preprocessing inconsistencies!\n",
      "=== Saving Flawed-Only Line Classification Dataset ===\n",
      "‚úì Dataset saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/flawed-only/flawed_only_line_classification_dataset.csv\n",
      "  Size: 3,487 samples\n",
      "  Unique indices: 3,487\n",
      "‚úì Metadata saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/flawed-only/flawed_only_dataset_metadata.json\n",
      "‚úì README saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/flawed-only/README.md\n",
      "\n",
      "üìÅ Flawed-only dataset saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/flawed-only\n"
     ]
    }
   ],
   "source": [
    "# Execute the flawed-only strategy\n",
    "print(\"üöÄ Creating flawed-only line classification dataset...\")\n",
    "flawed_only_dataset_df = create_flawed_only_line_classification_dataset_simple(  # ‚Üê NEW FUNCTION NAME\n",
    "    master_df, conceptual_indices, computational_indices\n",
    ")\n",
    "\n",
    "# Save the flawed-only dataset\n",
    "flawed_only_output_dir = OUTPUT_DIR / \"flawed-only\"\n",
    "saved_flawed_dir = save_flawed_only_dataset(flawed_only_dataset_df, flawed_only_output_dir)\n",
    "print(f\"\\nüìÅ Flawed-only dataset saved to: {saved_flawed_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5e86bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSING LINE COUNT MISMATCHES\n",
      "============================================================\n",
      "\n",
      "--- INDEX 140 ---\n",
      "Stored solution_length: 5\n",
      "Actual line count: 5\n",
      "Difference: 0\n",
      "Sample solution text:\n",
      "'First find how many kids from Riverside High are rejected: 20% * 120 kids = 24 kids\\nThen find how many kids from West Side High are rejected: 70% * 90 kids = 63 kids\\nThen find how many kids from Mountaintop High are rejected: 50 kids / 2 = 25 kids\\nThen add the number of rejected kids from each school to find the total number of rejected kids: 24 kids + 63 kids + 25 kids = 112 kids\\n#### 112'\n",
      "\n",
      "--- INDEX 143 ---\n",
      "Stored solution_length: 4\n",
      "Actual line count: 4\n",
      "Difference: 0\n",
      "Sample solution text:\n",
      "'There are 60 goldfish because 15 / .25 = 60\\n75% of the fish are below the surface because 100 - 25 = 75\\nThere are 35 goldfish below the surface because 60 x .75 = 35\\n#### 35'\n",
      "\n",
      "--- INDEX 145 ---\n",
      "Stored solution_length: 5\n",
      "Actual line count: 5\n",
      "Difference: 0\n",
      "Sample solution text:\n",
      "'He watched 2*20=40 minutes of Jeopardy.\\nWheel of Fortune is 2*20=40 minutes each.\\nSo he watched it for 40*2=80 minutes.\\nSo he watched 40+80=120 minutes of TV.\\n#### 120'\n",
      "\n",
      "--- OVERALL MISMATCH CHECK ---\n",
      "Mismatch rate in first 50 samples: 0/50 (0.0%)\n",
      "‚úÖ Low mismatch rate - issue might be elsewhere\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnosis of the mismatch issue\n",
    "print(\"üîç DIAGNOSING LINE COUNT MISMATCHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check a few samples to see what's happening\n",
    "sample_indices = [140, 143, 145, 7193, 7339]\n",
    "\n",
    "for idx in sample_indices[:3]:\n",
    "    sample = master_df[master_df['index'] == idx].iloc[0]\n",
    "    \n",
    "    print(f\"\\n--- INDEX {idx} ---\")\n",
    "    print(f\"Stored solution_length: {sample['solution_length']}\")\n",
    "    \n",
    "    # Count actual lines in the sanitized text\n",
    "    raw_solution = sample['wrong_answer']\n",
    "    lines = raw_solution.split('\\n')\n",
    "    actual_lines = [line.strip() for line in lines if line.strip()]\n",
    "    actual_count = len(actual_lines)\n",
    "    \n",
    "    print(f\"Actual line count: {actual_count}\")\n",
    "    print(f\"Difference: {actual_count - sample['solution_length']}\")\n",
    "    \n",
    "    print(f\"Sample solution text:\")\n",
    "    print(repr(raw_solution))\n",
    "\n",
    "# Check overall mismatch rate\n",
    "print(f\"\\n--- OVERALL MISMATCH CHECK ---\")\n",
    "mismatches = 0\n",
    "total_checked = 0\n",
    "\n",
    "for _, sample in master_df.head(50).iterrows():\n",
    "    raw_solution = sample['wrong_answer']\n",
    "    stored_length = int(sample['solution_length'])\n",
    "    \n",
    "    lines = raw_solution.split('\\n')\n",
    "    actual_lines = [line.strip() for line in lines if line.strip()]\n",
    "    actual_length = len(actual_lines)\n",
    "    \n",
    "    if actual_length != stored_length:\n",
    "        mismatches += 1\n",
    "    total_checked += 1\n",
    "\n",
    "mismatch_rate = (mismatches / total_checked) * 100\n",
    "print(f\"Mismatch rate in first 50 samples: {mismatches}/{total_checked} ({mismatch_rate:.1f}%)\")\n",
    "\n",
    "if mismatch_rate > 10:\n",
    "    print(\"‚ùå HIGH MISMATCH RATE - The solution_length field in master_catalog_sanitized.csv is inconsistent!\")\n",
    "    print(\"üí° SOLUTION: You need to recalculate solution_length after text sanitization\")\n",
    "else:\n",
    "    print(\"‚úÖ Low mismatch rate - issue might be elsewhere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eedaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Sample Inspection ===\n",
      "Dataset shape: (3487, 9)\n",
      "Columns: ['text', 'correct_answer', 'line_labels', 'error_type', 'index', 'tier', 'source', 'relative_line_position', 'solution_length']\n",
      "\n",
      "============================================================\n",
      "SAMPLE 1 (Index: 1)\n",
      "============================================================\n",
      "Error Type: conceptual_error\n",
      "Source: programmatic\n",
      "Tier: tier4\n",
      "Solution Length: 3 lines\n",
      "Relative Error Position: 0.500\n",
      "Error Line Index: 1 (0-based)\n",
      "Line Labels: [0, 1, 0]\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Analyze the following mathematical problem and solution to identify the line containing the error.\n",
      "\n",
      "### Problem:\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "### Solution:\n",
      "Weng earns 12/60 = $0.2 per minute.<|LINE_SEP|>Working 50 minutes, she earned 50 x 50 = $2500.<|LINE_SEP|>#### 2500<|LINE_SEP|>\n",
      "\n",
      "--- CORRECT ANSWER (GSM8K) ---\n",
      "Weng earns 12/60 = $0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $10.\n",
      "#### 10\n",
      "\n",
      "--- SOLUTION LINES WITH ERROR MARKING ---\n",
      "Line 0: Weng earns 12/60 = $0.2 per minute.<|LINE_SEP|>Working 50 minutes, she earned 50 x 50 = $2500.<|LINE_SEP|>#### 2500<|LINE_SEP|>\n",
      "\n",
      "============================================================\n",
      "SAMPLE 2 (Index: 2)\n",
      "============================================================\n",
      "Error Type: conceptual_error\n",
      "Source: manual\n",
      "Tier: tier3\n",
      "Solution Length: 4 lines\n",
      "Relative Error Position: 0.667\n",
      "Error Line Index: 2 (0-based)\n",
      "Line Labels: [0, 0, 1, 0]\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Analyze the following mathematical problem and solution to identify the line containing the error.\n",
      "\n",
      "### Problem:\n",
      "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "\n",
      "### Solution:\n",
      "In the beginning, Betty has only 100 / 2 = $50.<|LINE_SEP|>Betty's grandparents gave her 15 * 2 = $30.<|LINE_SEP|>This means, Betty needs 100 - 50 - 30 = $20 more.<|LINE_SEP|>#### 20<|LINE_SEP|>\n",
      "\n",
      "--- CORRECT ANSWER (GSM8K) ---\n",
      "In the beginning, Betty has only 100 / 2 = $50.\n",
      "Betty's grandparents gave her 15 * 2 = $30.\n",
      "This means, Betty needs 100 - 50 - 30 - 15 = $5 more.\n",
      "#### 5\n",
      "\n",
      "--- SOLUTION LINES WITH ERROR MARKING ---\n",
      "Line 0: In the beginning, Betty has only 100 / 2 = $50.<|LINE_SEP|>Betty's grandparents gave her 15 * 2 = $30.<|LINE_SEP|>This means, Betty needs 100 - 50 - 30 = $20 more.<|LINE_SEP|>#### 20<|LINE_SEP|>\n",
      "\n",
      "============================================================\n",
      "SAMPLE 3 (Index: 4)\n",
      "============================================================\n",
      "Error Type: conceptual_error\n",
      "Source: manual\n",
      "Tier: tier1\n",
      "Solution Length: 4 lines\n",
      "Relative Error Position: 0.667\n",
      "Error Line Index: 2 (0-based)\n",
      "Line Labels: [0, 0, 1, 0]\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Analyze the following mathematical problem and solution to identify the line containing the error.\n",
      "\n",
      "### Problem:\n",
      "James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
      "\n",
      "### Solution:\n",
      "He writes each friend 3*2=6 pages a week<|LINE_SEP|>So he writes 6*2=12 pages every week<|LINE_SEP|>That means he writes 12*48=576 pages a year<|LINE_SEP|>#### 576<|LINE_SEP|>\n",
      "\n",
      "--- CORRECT ANSWER (GSM8K) ---\n",
      "He writes each friend 3*2=6 pages a week\n",
      "So he writes 6*2=12 pages every week\n",
      "That means he writes 12*52=624 pages a year\n",
      "#### 624\n",
      "\n",
      "--- SOLUTION LINES WITH ERROR MARKING ---\n",
      "Line 0: He writes each friend 3*2=6 pages a week<|LINE_SEP|>So he writes 6*2=12 pages every week<|LINE_SEP|>That means he writes 12*48=576 pages a year<|LINE_SEP|>#### 576<|LINE_SEP|>\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING VERIFICATION\n",
      "============================================================\n",
      "Calculator annotations found: False (should be False)\n",
      "Literal \\n found: False (should be False)\n",
      "\n",
      "--- UNICODE CHARACTER DETECTION ---\n",
      "‚úÖ No problematic Unicode characters found\n",
      "\n",
      "--- COMPREHENSIVE NON-ASCII SCAN ---\n",
      "Scanning all 3487 samples for non-ASCII characters...\n",
      "Non-ASCII characters found:\n",
      "\n",
      "--- NON-ASCII CHARACTER SUMMARY ---\n",
      "  U+00BE ('¬æ'): 1 occurrences in 1+ samples\n",
      "    Found in problem indices: [2141]\n",
      "  U+2028 ('‚Ä®'): 2 occurrences in 1+ samples\n",
      "    Found in problem indices: [2381]\n",
      "\n",
      "--- SCAN SUMMARY ---\n",
      "Total non-ASCII characters found: 3\n",
      "Unique Unicode characters: 2\n",
      "Samples affected: 2+ out of 3487 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "# Print sample inspection code\n",
    "print(\"=== Dataset Sample Inspection ===\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(f\"Dataset shape: {flawed_only_dataset_df.shape}\")\n",
    "print(f\"Columns: {list(flawed_only_dataset_df.columns)}\")\n",
    "\n",
    "# Print first 3 samples with detailed formatting\n",
    "for i in range(min(3, len(flawed_only_dataset_df))):\n",
    "    sample = flawed_only_dataset_df.iloc[i]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAMPLE {i+1} (Index: {sample['index']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"Error Type: {sample['error_type']}\")\n",
    "    print(f\"Source: {sample['source']}\")\n",
    "    print(f\"Tier: {sample['tier']}\")\n",
    "    print(f\"Solution Length: {sample['solution_length']} lines\")\n",
    "    print(f\"Relative Error Position: {sample['relative_line_position']:.3f}\")\n",
    "    \n",
    "    # Parse line labels\n",
    "    import ast\n",
    "    line_labels = ast.literal_eval(sample['line_labels']) if isinstance(sample['line_labels'], str) else sample['line_labels']\n",
    "    error_line_idx = line_labels.index(1) if 1 in line_labels else -1\n",
    "    print(f\"Error Line Index: {error_line_idx} (0-based)\")\n",
    "    print(f\"Line Labels: {line_labels}\")\n",
    "    \n",
    "    print(f\"\\n--- USER PROMPT ---\")\n",
    "    print(sample['text'])\n",
    "    \n",
    "    print(f\"\\n--- CORRECT ANSWER (GSM8K) ---\")\n",
    "    print(sample['correct_answer'][:200] + \"...\" if len(sample['correct_answer']) > 200 else sample['correct_answer'])\n",
    "    \n",
    "    # Show which specific line contains the error\n",
    "    solution_section = sample['text'].split(\"### Solution:\")[-1].strip()\n",
    "    solution_lines = solution_section.split('\\n')\n",
    "    \n",
    "    print(f\"\\n--- SOLUTION LINES WITH ERROR MARKING ---\")\n",
    "    for idx, line in enumerate(solution_lines):\n",
    "        if line.strip():  # Only show non-empty lines\n",
    "            marker = \" ‚Üê ERROR\" if idx == error_line_idx else \"\"\n",
    "            print(f\"Line {idx}: {line.strip()}{marker}\")\n",
    "\n",
    "# Show preprocessing effects\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREPROCESSING VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check for calculator annotations removal\n",
    "has_calculator_annotations = flawed_only_dataset_df['text'].str.contains(r'<<.*?>>', regex=True).any()\n",
    "print(f\"Calculator annotations found: {has_calculator_annotations} (should be False)\")\n",
    "\n",
    "# Check for literal \\n vs actual newlines\n",
    "has_literal_newlines = flawed_only_dataset_df['text'].str.contains(r'\\\\n', regex=False).any()\n",
    "print(f\"Literal \\\\n found: {has_literal_newlines} (should be False)\")\n",
    "\n",
    "# Check for Unicode characters with detailed reporting\n",
    "unicode_chars = {\n",
    "    \"\\u2212\": \"Minus Sign (‚àí)\",\n",
    "    \"\\u00d7\": \"Multiplication Sign (√ó)\", \n",
    "    \"\\u00f7\": \"Division Sign (√∑)\",\n",
    "    \"\\u22c5\": \"Dot Operator (‚ãÖ)\",\n",
    "    \"\\u201c\": \"Left Double Quotation Mark\",\n",
    "    \"\\u201d\": \"Right Double Quotation Mark\", \n",
    "    \"\\u2018\": \"Left Single Quotation Mark\",\n",
    "    \"\\u2019\": \"Right Single Quotation Mark\",\n",
    "    \"\\u2014\": \"Em Dash (‚Äî)\",\n",
    "    \"\\u2013\": \"En Dash (‚Äì)\", \n",
    "    \"\\u2026\": \"Horizontal Ellipsis (‚Ä¶)\",\n",
    "    \"\\u00a0\": \"No-Break Space\"\n",
    "}\n",
    "\n",
    "print(f\"\\n--- UNICODE CHARACTER DETECTION ---\")\n",
    "found_unicode = []\n",
    "for char, description in unicode_chars.items():\n",
    "    char_found = flawed_only_dataset_df['text'].str.contains(char, regex=False).any()\n",
    "    if char_found:\n",
    "        # Count occurrences\n",
    "        total_occurrences = flawed_only_dataset_df['text'].str.count(char).sum()\n",
    "        samples_with_char = flawed_only_dataset_df['text'].str.contains(char, regex=False).sum()\n",
    "        found_unicode.append(f\"  ‚ùå {description}: {total_occurrences} occurrences in {samples_with_char} samples\")\n",
    "        \n",
    "        # Show first few examples\n",
    "        examples = flawed_only_dataset_df[flawed_only_dataset_df['text'].str.contains(char, regex=False)]['text'].head(2)\n",
    "        for idx, example in enumerate(examples):\n",
    "            excerpt = example.replace(char, f\"**{char}**\")[:150] + \"...\"\n",
    "            print(f\"      Example {idx+1}: {excerpt}\")\n",
    "\n",
    "if found_unicode:\n",
    "    print(\"Unicode characters found:\")\n",
    "    for issue in found_unicode:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"‚úÖ No problematic Unicode characters found\")\n",
    "\n",
    "# Additional Unicode scan for any non-ASCII characters\n",
    "print(f\"\\n--- COMPREHENSIVE NON-ASCII SCAN ---\")\n",
    "non_ascii_found = False\n",
    "non_ascii_summary = {}\n",
    "\n",
    "print(f\"Scanning all {len(flawed_only_dataset_df)} samples for non-ASCII characters...\")\n",
    "\n",
    "for idx, text in enumerate(flawed_only_dataset_df['text']):\n",
    "    non_ascii_chars = [char for char in text if ord(char) > 127]\n",
    "    if non_ascii_chars:\n",
    "        unique_non_ascii = list(set(non_ascii_chars))\n",
    "        if not non_ascii_found:\n",
    "            print(\"Non-ASCII characters found:\")\n",
    "            non_ascii_found = True\n",
    "        \n",
    "        # Track summary statistics\n",
    "        for char in unique_non_ascii:\n",
    "            unicode_point = f\"U+{ord(char):04X}\"\n",
    "            if unicode_point not in non_ascii_summary:\n",
    "                non_ascii_summary[unicode_point] = {\n",
    "                    'char': char,\n",
    "                    'count': 0,\n",
    "                    'samples': []\n",
    "                }\n",
    "            non_ascii_summary[unicode_point]['count'] += text.count(char)\n",
    "            if len(non_ascii_summary[unicode_point]['samples']) < 3:  # Store first 3 sample indices\n",
    "                non_ascii_summary[unicode_point]['samples'].append(idx)\n",
    "\n",
    "if non_ascii_found:\n",
    "    print(f\"\\n--- NON-ASCII CHARACTER SUMMARY ---\")\n",
    "    for unicode_point, info in sorted(non_ascii_summary.items()):\n",
    "        char = info['char']\n",
    "        count = info['count']\n",
    "        sample_count = len(info['samples'])\n",
    "        print(f\"  {unicode_point} ('{char}'): {count} occurrences in {sample_count}+ samples\")\n",
    "        \n",
    "        # Show first few sample indices\n",
    "        if info['samples']:\n",
    "            sample_indices = [flawed_only_dataset_df.iloc[i]['index'] for i in info['samples'][:3]]\n",
    "            print(f\"    Found in problem indices: {sample_indices}\")\n",
    "else:\n",
    "    print(\"‚úÖ No non-ASCII characters found in any samples\")\n",
    "\n",
    "# Summary statistics\n",
    "if non_ascii_found:\n",
    "    total_non_ascii_chars = sum(info['count'] for info in non_ascii_summary.values())\n",
    "    total_affected_samples = len(set(sample_idx for info in non_ascii_summary.values() for sample_idx in info['samples']))\n",
    "    print(f\"\\n--- SCAN SUMMARY ---\")\n",
    "    print(f\"Total non-ASCII characters found: {total_non_ascii_chars}\")\n",
    "    print(f\"Unique Unicode characters: {len(non_ascii_summary)}\")\n",
    "    print(f\"Samples affected: {total_affected_samples}+ out of {len(flawed_only_dataset_df)} ({total_affected_samples/len(flawed_only_dataset_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9229b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING LINE COUNT MISMATCH\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 9\n",
      "============================================================\n",
      "üìç Error type: computational_error\n",
      "üìç Source: manual\n",
      "üìç Tier: tier2\n",
      "üìç Stored solution_length: 9\n",
      "üìç Stored relative_line_position: 0.75\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift'\n",
      "  Line 1: 'She works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime'\n",
      "  Line 2: 'Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00'\n",
      "  Line 3: 'Her overtime pay is 18+9 = $27.00'\n",
      "  Line 4: 'Her base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00'\n",
      "  Line 5: 'Her overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay'\n",
      "  Line 6: '2 hours of overtime pay for 5 days means she makes 54*5 = $250.00'\n",
      "  Line 7: 'In 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00'\n",
      "  Line 8: '#### 970'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift'\n",
      "  Line 1: 'She works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime'\n",
      "  Line 2: 'Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00'\n",
      "  Line 3: 'Her overtime pay is 18+9 = $27.00'\n",
      "  Line 4: 'Her base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00'\n",
      "  Line 5: 'Her overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay'\n",
      "  Line 6: '2 hours of overtime pay for 5 days means she makes 54*5 = $250.00'\n",
      "  Line 7: 'In 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00'\n",
      "  Line 8: '#### 970'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "üìä Stored solution_length: 9\n",
      "üìä Actual preprocessed lines: 9\n",
      "üìä Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 9\n",
      "Lines before calculator removal: 9\n",
      "Lines after calculator removal: 9\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 6\n",
      "Corrected error line (using actual length): 6\n",
      "Valid range for actual lines: 0 to 8\n",
      "‚úÖ Original position 6 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 18\n",
      "============================================================\n",
      "üìç Error type: computational_error\n",
      "üìç Source: programmatic\n",
      "üìç Tier: tier1\n",
      "üìç Stored solution_length: 6\n",
      "üìç Stored relative_line_position: 0.2\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'Roque takes 2*3 = 6 hours a week to walk to work.'\n",
      "  Line 1: 'Roque takes 6*2 = 21 hours a week to walk to and from work.'\n",
      "  Line 2: 'Roque takes 1*2 = 2 hours a week to bike to work.'\n",
      "  Line 3: 'Roque takes 2*2 = 4 hours a week to bike to and from work.'\n",
      "  Line 4: 'In total, Roque takes 21+4 = 25 hour a week to go to and from work.'\n",
      "  Line 5: '#### 25'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'Roque takes 2*3 = 6 hours a week to walk to work.'\n",
      "  Line 1: 'Roque takes 6*2 = 21 hours a week to walk to and from work.'\n",
      "  Line 2: 'Roque takes 1*2 = 2 hours a week to bike to work.'\n",
      "  Line 3: 'Roque takes 2*2 = 4 hours a week to bike to and from work.'\n",
      "  Line 4: 'In total, Roque takes 21+4 = 25 hour a week to go to and from work.'\n",
      "  Line 5: '#### 25'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "üìä Stored solution_length: 6\n",
      "üìä Actual preprocessed lines: 6\n",
      "üìä Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 6\n",
      "Lines before calculator removal: 6\n",
      "Lines after calculator removal: 6\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 1\n",
      "Corrected error line (using actual length): 1\n",
      "Valid range for actual lines: 0 to 5\n",
      "‚úÖ Original position 1 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 25\n",
      "============================================================\n",
      "üìç Error type: computational_error\n",
      "üìç Source: programmatic\n",
      "üìç Tier: tier3\n",
      "üìç Stored solution_length: 4\n",
      "üìç Stored relative_line_position: 0.0\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "Step 2 - After calculator annotation removal:\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: \"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 1: \"Out of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\"\n",
      "  Line 2: \"Combined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 3: '#### 650/3'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: \"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 1: \"Out of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\"\n",
      "  Line 2: \"Combined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 3: '#### 650/3'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "üìä Stored solution_length: 4\n",
      "üìä Actual preprocessed lines: 4\n",
      "üìä Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 4\n",
      "Lines before calculator removal: 4\n",
      "Lines after calculator removal: 4\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 0\n",
      "Corrected error line (using actual length): 0\n",
      "Valid range for actual lines: 0 to 3\n",
      "‚úÖ Original position 0 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 29\n",
      "============================================================\n",
      "üìç Error type: conceptual_error\n",
      "üìç Source: manual\n",
      "üìç Tier: tier3\n",
      "üìç Stored solution_length: 9\n",
      "üìç Stored relative_line_position: 0.25\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'Let her previous monthly income be p'\n",
      "  Line 1: 'The cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5'\n",
      "  Line 2: 'Her income was increased by $900 so it is now p+$900'\n",
      "  Line 3: 'The cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4'\n",
      "  Line 4: 'Equating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4'\n",
      "  Line 5: 'Multiplying both sides of the equation by 20 gives 8p = 5p+$4500'\n",
      "  Line 6: 'Subtracting 5p from both sides gives: 3p = $4500'\n",
      "  Line 7: 'Dividing both sides by 3 gives p = $1500'\n",
      "  Line 8: '#### 1500'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'Let her previous monthly income be p'\n",
      "  Line 1: 'The cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5'\n",
      "  Line 2: 'Her income was increased by $900 so it is now p+$900'\n",
      "  Line 3: 'The cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4'\n",
      "  Line 4: 'Equating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4'\n",
      "  Line 5: 'Multiplying both sides of the equation by 20 gives 8p = 5p+$4500'\n",
      "  Line 6: 'Subtracting 5p from both sides gives: 3p = $4500'\n",
      "  Line 7: 'Dividing both sides by 3 gives p = $1500'\n",
      "  Line 8: '#### 1500'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "üìä Stored solution_length: 9\n",
      "üìä Actual preprocessed lines: 9\n",
      "üìä Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 9\n",
      "Lines before calculator removal: 9\n",
      "Lines after calculator removal: 9\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 2\n",
      "Corrected error line (using actual length): 2\n",
      "Valid range for actual lines: 0 to 8\n",
      "‚úÖ Original position 2 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 30\n",
      "============================================================\n",
      "üìç Error type: computational_error\n",
      "üìç Source: programmatic\n",
      "üìç Tier: tier2\n",
      "üìç Stored solution_length: 6\n",
      "üìç Stored relative_line_position: 0.0\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'In total, there are 4 x 4 = 61 pizza pieces.'\n",
      "  Line 1: 'Bill and Dale eat 2 x 4 x 50% = 4 pieces.'\n",
      "  Line 2: 'Ann and Cate eat 2 x 4 x 75% = 6 pieces.'\n",
      "  Line 3: 'The four of them eat 4 + 6 = 10 pieces.'\n",
      "  Line 4: 'There are 61 - 10 = 51 pizza pieces uneaten.'\n",
      "  Line 5: '#### 51'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'In total, there are 4 x 4 = 61 pizza pieces.'\n",
      "  Line 1: 'Bill and Dale eat 2 x 4 x 50% = 4 pieces.'\n",
      "  Line 2: 'Ann and Cate eat 2 x 4 x 75% = 6 pieces.'\n",
      "  Line 3: 'The four of them eat 4 + 6 = 10 pieces.'\n",
      "  Line 4: 'There are 61 - 10 = 51 pizza pieces uneaten.'\n",
      "  Line 5: '#### 51'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "üìä Stored solution_length: 6\n",
      "üìä Actual preprocessed lines: 6\n",
      "üìä Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 6\n",
      "Lines before calculator removal: 6\n",
      "Lines after calculator removal: 6\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 0\n",
      "Corrected error line (using actual length): 0\n",
      "Valid range for actual lines: 0 to 5\n",
      "‚úÖ Original position 0 is within bounds\n",
      "\n",
      "======================================================================\n",
      "SUMMARY ANALYSIS\n",
      "======================================================================\n",
      "üìä Analyzed 5 problematic cases\n",
      "üìä Line count differences: [0, 0, 0, 0, 0]\n",
      "üìä Average difference: 0.00\n",
      "üìä Most common difference: 0\n",
      "\n",
      "--- PATTERNS BY ERROR TYPE ---\n",
      "Conceptual errors: avg difference = 0.00\n",
      "Computational errors: avg difference = 0.00\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DEBUGGING: Line Count Mismatch Investigation\n",
    "# ==============================================================================\n",
    "def debug_line_count_mismatch(master_df, sample_indices=None, max_samples=20):\n",
    "    \"\"\"\n",
    "    Debug line count mismatches between stored solution_length and actual preprocessed lines.\n",
    "    \"\"\"\n",
    "    print(\"üîç DEBUGGING LINE COUNT MISMATCH\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if sample_indices is None:\n",
    "        # Use the problematic indices we found earlier\n",
    "        sample_indices = [9, 18, 25, 29, 30]  # From the debugging output\n",
    "    \n",
    "    problematic_cases = []\n",
    "    \n",
    "    for idx in sample_indices[:max_samples]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DEBUGGING INDEX {idx}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get the sample from master catalog\n",
    "        sample_data = master_df[master_df['index'] == idx]\n",
    "        if len(sample_data) == 0:\n",
    "            print(f\"No data found for index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Take the first available sample (conceptual or computational)\n",
    "        sample = sample_data.iloc[0]\n",
    "        \n",
    "        print(f\"üìç Error type: {sample['error_type']}\")\n",
    "        print(f\"üìç Source: {sample['source']}\")\n",
    "        print(f\"üìç Tier: {sample['tier']}\")\n",
    "        print(f\"üìç Stored solution_length: {sample['solution_length']}\")\n",
    "        print(f\"üìç Stored relative_line_position: {sample['relative_line_position']}\")\n",
    "        \n",
    "        # Get the raw solution\n",
    "        raw_solution = sample['wrong_answer']\n",
    "        print(f\"\\n--- RAW SOLUTION ---\")\n",
    "        print(repr(raw_solution))\n",
    "        \n",
    "        # Process the solution step by step\n",
    "        print(f\"\\n--- PREPROCESSING STEPS ---\")\n",
    "        \n",
    "        # Step 1: Convert literal \\n to actual newlines\n",
    "        step1 = raw_solution.replace('\\\\n', '\\n')\n",
    "        print(f\"Step 1 - After \\\\n conversion:\")\n",
    "        print(repr(step1))\n",
    "        \n",
    "        # Step 2: Remove calculator annotations\n",
    "        import re\n",
    "        step2 = re.sub(r'<<.*?>>', '', step1)\n",
    "        print(f\"Step 2 - After calculator annotation removal:\")\n",
    "        print(repr(step2))\n",
    "        \n",
    "        # Step 3: Sanitize Unicode and commas (simplified)\n",
    "        step3 = step2  # Skip for now to focus on line counting\n",
    "        \n",
    "        # Step 4: Split by newlines and count\n",
    "        lines_before_filter = step3.split('\\n')\n",
    "        print(f\"Step 4 - Lines before filtering:\")\n",
    "        for i, line in enumerate(lines_before_filter):\n",
    "            print(f\"  Line {i}: {repr(line)}\")\n",
    "        \n",
    "        # Step 5: Filter empty lines\n",
    "        non_empty_lines = [line.strip() for line in lines_before_filter if line.strip()]\n",
    "        print(f\"Step 5 - Non-empty lines after filtering:\")\n",
    "        for i, line in enumerate(non_empty_lines):\n",
    "            print(f\"  Line {i}: {repr(line)}\")\n",
    "        \n",
    "        # Calculate actual vs expected\n",
    "        actual_line_count = len(non_empty_lines)\n",
    "        stored_line_count = int(sample['solution_length'])\n",
    "        \n",
    "        print(f\"\\n--- LINE COUNT COMPARISON ---\")\n",
    "        print(f\"üìä Stored solution_length: {stored_line_count}\")\n",
    "        print(f\"üìä Actual preprocessed lines: {actual_line_count}\")\n",
    "        print(f\"üìä Difference: {actual_line_count - stored_line_count}\")\n",
    "        \n",
    "        # Show what the original line counting logic would have done\n",
    "        print(f\"\\n--- ORIGINAL LINE COUNTING SIMULATION ---\")\n",
    "        # The original logic probably counted lines in the raw text\n",
    "        original_lines = raw_solution.replace('\\\\n', '\\n').split('\\n')\n",
    "        original_non_empty = [line.strip() for line in original_lines if line.strip()]\n",
    "        print(f\"Original raw line count: {len(original_non_empty)}\")\n",
    "        \n",
    "        # Check if calculator annotations affected line count\n",
    "        with_calc_lines = step1.split('\\n')\n",
    "        without_calc_lines = step2.split('\\n')\n",
    "        calc_lines_before = len([line.strip() for line in with_calc_lines if line.strip()])\n",
    "        calc_lines_after = len([line.strip() for line in without_calc_lines if line.strip()])\n",
    "        \n",
    "        print(f\"Lines before calculator removal: {calc_lines_before}\")\n",
    "        print(f\"Lines after calculator removal: {calc_lines_after}\")\n",
    "        print(f\"Calculator annotations removed {calc_lines_before - calc_lines_after} lines\")\n",
    "        \n",
    "        # Calculate what the error position should be\n",
    "        if sample['relative_line_position'] is not None:\n",
    "            # Original calculation (wrong)\n",
    "            original_error_line = int(sample['relative_line_position'] * (stored_line_count - 1))\n",
    "            \n",
    "            # Corrected calculation\n",
    "            corrected_error_line = int(sample['relative_line_position'] * (actual_line_count - 1))\n",
    "            \n",
    "            print(f\"\\n--- ERROR POSITION COMPARISON ---\")\n",
    "            print(f\"Original error line (using stored length): {original_error_line}\")\n",
    "            print(f\"Corrected error line (using actual length): {corrected_error_line}\")\n",
    "            print(f\"Valid range for actual lines: 0 to {actual_line_count - 1}\")\n",
    "            \n",
    "            # Check if original position is out of bounds\n",
    "            if original_error_line >= actual_line_count:\n",
    "                print(f\"‚ùå Original position {original_error_line} is OUT OF BOUNDS for {actual_line_count} lines!\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Original position {original_error_line} is within bounds\")\n",
    "        \n",
    "        problematic_cases.append({\n",
    "            'index': idx,\n",
    "            'stored_length': stored_line_count,\n",
    "            'actual_length': actual_line_count,\n",
    "            'difference': actual_line_count - stored_line_count,\n",
    "            'error_type': sample['error_type'],\n",
    "            'relative_position': sample['relative_line_position']\n",
    "        })\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    differences = [case['difference'] for case in problematic_cases]\n",
    "    \n",
    "    print(f\"üìä Analyzed {len(problematic_cases)} problematic cases\")\n",
    "    print(f\"üìä Line count differences: {differences}\")\n",
    "    print(f\"üìä Average difference: {np.mean(differences):.2f}\")\n",
    "    print(f\"üìä Most common difference: {max(set(differences), key=differences.count)}\")\n",
    "    \n",
    "    # Show patterns by error type\n",
    "    conceptual_diffs = [case['difference'] for case in problematic_cases if case['error_type'] == 'conceptual_error']\n",
    "    computational_diffs = [case['difference'] for case in problematic_cases if case['error_type'] == 'computational_error']\n",
    "    \n",
    "    print(f\"\\n--- PATTERNS BY ERROR TYPE ---\")\n",
    "    if conceptual_diffs:\n",
    "        print(f\"Conceptual errors: avg difference = {np.mean(conceptual_diffs):.2f}\")\n",
    "    if computational_diffs:\n",
    "        print(f\"Computational errors: avg difference = {np.mean(computational_diffs):.2f}\")\n",
    "    \n",
    "    return problematic_cases\n",
    "\n",
    "# Run the debugging\n",
    "debug_results = debug_line_count_mismatch(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fd7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE SOLUTION LENGTH ANALYSIS\n",
      "======================================================================\n",
      "Analyzing 24652 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f1f4d34de645eea479890105b5a578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing:   0%|          | 0/24652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ANALYSIS RESULTS:\n",
      "   Perfect matches: 24652/24652 (100.0%)\n",
      "   Mismatches: 0/24652 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPREHENSIVE SOLUTION LENGTH ANALYSIS\n",
    "# ==============================================================================\n",
    "def analyze_solution_length_accuracy(master_df, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyze how accurate the stored solution_length values are across the entire dataset.\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE SOLUTION LENGTH ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Sample from the dataset\n",
    "    sample_data = master_df.sample(min(sample_size, len(master_df)), random_state=42)\n",
    "    \n",
    "    mismatches = []\n",
    "    perfect_matches = 0\n",
    "    \n",
    "    print(f\"Analyzing {len(sample_data)} samples...\")\n",
    "    \n",
    "    for _, sample in tqdm(sample_data.iterrows(), total=len(sample_data), desc=\"Analyzing\"):\n",
    "        raw_solution = sample['wrong_answer']\n",
    "        stored_length = int(sample['solution_length'])\n",
    "        \n",
    "        # Apply the same preprocessing as in the dataset creation\n",
    "        processed = raw_solution.replace('\\\\n', '\\n')\n",
    "        processed = re.sub(r'<<.*?>>', '', processed)  # Remove calculator annotations\n",
    "        \n",
    "        # Count actual lines\n",
    "        lines = processed.split('\\n')\n",
    "        actual_lines = [line.strip() for line in lines if line.strip()]\n",
    "        actual_length = len(actual_lines)\n",
    "        \n",
    "        if actual_length != stored_length:\n",
    "            mismatches.append({\n",
    "                'index': sample['index'],\n",
    "                'error_type': sample['error_type'],\n",
    "                'source': sample['source'],\n",
    "                'tier': sample['tier'],\n",
    "                'stored_length': stored_length,\n",
    "                'actual_length': actual_length,\n",
    "                'difference': actual_length - stored_length\n",
    "            })\n",
    "        else:\n",
    "            perfect_matches += 1\n",
    "    \n",
    "    # Analysis results\n",
    "    mismatch_rate = len(mismatches) / len(sample_data) * 100\n",
    "    perfect_rate = perfect_matches / len(sample_data) * 100\n",
    "    \n",
    "    print(f\"\\nüìä ANALYSIS RESULTS:\")\n",
    "    print(f\"   Perfect matches: {perfect_matches}/{len(sample_data)} ({perfect_rate:.1f}%)\")\n",
    "    print(f\"   Mismatches: {len(mismatches)}/{len(sample_data)} ({mismatch_rate:.1f}%)\")\n",
    "    \n",
    "    if mismatches:\n",
    "        differences = [m['difference'] for m in mismatches]\n",
    "        print(f\"\\nüìä MISMATCH STATISTICS:\")\n",
    "        print(f\"   Average difference: {np.mean(differences):.2f}\")\n",
    "        print(f\"   Std deviation: {np.std(differences):.2f}\")\n",
    "        print(f\"   Range: {min(differences)} to {max(differences)}\")\n",
    "        print(f\"   Most common difference: {max(set(differences), key=differences.count)}\")\n",
    "        \n",
    "        # Show distribution of differences\n",
    "        from collections import Counter\n",
    "        diff_counts = Counter(differences)\n",
    "        print(f\"\\nüìä DIFFERENCE DISTRIBUTION:\")\n",
    "        for diff, count in sorted(diff_counts.items()):\n",
    "            percentage = count / len(mismatches) * 100\n",
    "            print(f\"   Difference {diff:+d}: {count} cases ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show patterns by error type\n",
    "        print(f\"\\nüìä PATTERNS BY ERROR TYPE:\")\n",
    "        for error_type in ['conceptual_error', 'computational_error']:\n",
    "            type_mismatches = [m for m in mismatches if m['error_type'] == error_type]\n",
    "            if type_mismatches:\n",
    "                type_diffs = [m['difference'] for m in type_mismatches]\n",
    "                print(f\"   {error_type}: {len(type_mismatches)} mismatches, avg diff = {np.mean(type_diffs):.2f}\")\n",
    "        \n",
    "        # Show a few examples\n",
    "        print(f\"\\nüìã EXAMPLE MISMATCHES:\")\n",
    "        for i, mismatch in enumerate(mismatches[:5]):\n",
    "            print(f\"   {i+1}. Index {mismatch['index']} ({mismatch['error_type']}): \"\n",
    "                  f\"stored={mismatch['stored_length']}, actual={mismatch['actual_length']}, \"\n",
    "                  f\"diff={mismatch['difference']:+d}\")\n",
    "    \n",
    "    return mismatches, perfect_matches\n",
    "\n",
    "# Run comprehensive analysis\n",
    "mismatches, perfect_matches = analyze_solution_length_accuracy(master_df, sample_size=len(master_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9ba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
