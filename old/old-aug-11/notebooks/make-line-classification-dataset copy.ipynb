{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8aa16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Data directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data\n",
      "Output directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification\n",
      "Random seed set to: 42\n",
      "Line separator token: <|LINE_SEP|>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the git repository.\"\"\"\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "# --- Global Constants and Paths ---\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = DATA_DIR / \"line-classification\"\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "RANDOM_SEED = 42\n",
    "LINE_SEP_TOKEN = \"<|LINE_SEP|>\"  # Special token for line separation\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"Line separator token: {LINE_SEP_TOKEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00008429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded master catalog with 24,652 records\n",
      "Columns: ['index', 'tier', 'question', 'correct_answer', 'wrong_answer', 'error_type', 'erroneous_line_number', 'explanation', 'error_subtype', 'source', 'solution_length', 'relative_line_position']\n",
      "\n",
      "=== Master Catalog Overview ===\n",
      "Total samples: 24,652\n",
      "Unique indices: 6,777\n",
      "\n",
      "--- Error Type Distribution ---\n",
      "error_type\n",
      "computational_error    22542\n",
      "conceptual_error        2110\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Source Distribution ---\n",
      "source\n",
      "programmatic    22912\n",
      "manual           1740\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the master catalog\n",
    "master_catalog_path = DATA_DIR / \"master_catalog_sanitized.csv\"\n",
    "if not master_catalog_path.exists():\n",
    "    raise FileNotFoundError(f\"Master catalog not found: {master_catalog_path}\")\n",
    "\n",
    "master_df = pd.read_csv(master_catalog_path)\n",
    "print(f\"Loaded master catalog with {len(master_df):,} records\")\n",
    "print(f\"Columns: {list(master_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== Master Catalog Overview ===\")\n",
    "print(f\"Total samples: {len(master_df):,}\")\n",
    "print(f\"Unique indices: {master_df['index'].nunique():,}\")\n",
    "\n",
    "print(\"\\n--- Error Type Distribution ---\")\n",
    "error_type_counts = master_df['error_type'].value_counts()\n",
    "print(error_type_counts)\n",
    "\n",
    "print(\"\\n--- Source Distribution ---\") \n",
    "source_counts = master_df['source'].value_counts()\n",
    "print(source_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c525c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Disjoint Sets with Simple Sampling ===\n",
      "Set A (Conceptual errors): 1881 unique indices\n",
      "=== Simple Computational Error Sampling ===\n",
      "Available computational error samples: 16287\n",
      "Available unique indices: 4896\n",
      "Position tertiles: 33%=0.250, 67%=0.600\n",
      "Early group: 5599 samples, 3676 unique indices\n",
      "Middle group: 5680 samples, 3654 unique indices\n",
      "Late group: 5008 samples, 2866 unique indices\n",
      "Early group tier 4 unique indices: 181\n",
      "Middle group tier 4 unique indices: 183\n",
      "Late group tier 4 unique indices: 180\n",
      "Total tier 4 indices selected: 544\n",
      "Need 1337 more indices from other tiers\n",
      "Available non-tier4 indices: 4627\n",
      "Final selection: 1881 unique indices\n",
      "ERROR: Duplicate indices detected!\n",
      "After deduplication: 1606 unique indices\n",
      "\n",
      "--- Final Distribution ---\n",
      "Tier distribution:\n",
      "  tier1: 581 indices (36.2%)\n",
      "  tier2: 164 indices (10.2%)\n",
      "  tier3: 590 indices (36.7%)\n",
      "  tier4: 269 indices (16.7%)\n",
      "\n",
      "Position distribution:\n",
      "  Early: 1156 indices (72.0%)\n",
      "  Middle: 387 indices (24.1%)\n",
      "  Late: 157 indices (9.8%)\n",
      "Set B (Computational errors): 1606 unique indices\n",
      "Available indices for correct samples: 3290\n",
      "Set C (Correct samples): 1606 unique indices\n",
      "\n",
      "--- Disjoint Verification ---\n",
      "Conceptual âˆ© Computational: 0 indices (should be 0)\n",
      "Conceptual âˆ© Correct: 0 indices (should be 0)\n",
      "Computational âˆ© Correct: 0 indices (should be 0)\n",
      "Total unique indices: 5093 (should equal 5093)\n",
      "âœ“ All sets are properly disjoint\n"
     ]
    }
   ],
   "source": [
    "def select_balanced_computational_indices_simple(master_df, conceptual_indices, target_count):\n",
    "    \"\"\"\n",
    "    Simplified computational error sampling:\n",
    "    1. Form 3 groups based on relative line position (early, middle, late)\n",
    "    2. From each group, take as many distinct tier 4 samples as possible\n",
    "    3. For remaining, randomly sample distinct indices from other tiers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Simple Computational Error Sampling ===\")\n",
    "    \n",
    "    # Get available computational errors (excluding conceptual indices)\n",
    "    computational_df = master_df[\n",
    "        (master_df['error_type'] == 'computational_error') & \n",
    "        (~master_df['index'].isin(conceptual_indices))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(computational_df) == 0:\n",
    "        print(\"Warning: No computational errors available after excluding conceptual indices\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Available computational error samples: {len(computational_df)}\")\n",
    "    print(f\"Available unique indices: {computational_df['index'].nunique()}\")\n",
    "    \n",
    "    # Step 1: Form 3 groups based on relative line position\n",
    "    # Simple tertile split\n",
    "    positions = computational_df['relative_line_position'].dropna()\n",
    "    tertile_33 = positions.quantile(0.33)\n",
    "    tertile_67 = positions.quantile(0.67)\n",
    "    \n",
    "    print(f\"Position tertiles: 33%={tertile_33:.3f}, 67%={tertile_67:.3f}\")\n",
    "    \n",
    "    # Create groups\n",
    "    early_group = computational_df[computational_df['relative_line_position'] <= tertile_33]\n",
    "    middle_group = computational_df[\n",
    "        (computational_df['relative_line_position'] > tertile_33) & \n",
    "        (computational_df['relative_line_position'] <= tertile_67)\n",
    "    ]\n",
    "    late_group = computational_df[computational_df['relative_line_position'] > tertile_67]\n",
    "    \n",
    "    print(f\"Early group: {len(early_group)} samples, {early_group['index'].nunique()} unique indices\")\n",
    "    print(f\"Middle group: {len(middle_group)} samples, {middle_group['index'].nunique()} unique indices\")\n",
    "    print(f\"Late group: {len(late_group)} samples, {late_group['index'].nunique()} unique indices\")\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    # Step 2: From each group, take as many distinct tier 4 samples as possible\n",
    "    for group_name, group_df in [(\"Early\", early_group), (\"Middle\", middle_group), (\"Late\", late_group)]:\n",
    "        tier4_indices = group_df[group_df['tier'] == 'tier4']['index'].unique()\n",
    "        print(f\"{group_name} group tier 4 unique indices: {len(tier4_indices)}\")\n",
    "        selected_indices.extend(tier4_indices.tolist())\n",
    "    \n",
    "    print(f\"Total tier 4 indices selected: {len(selected_indices)}\")\n",
    "    \n",
    "    # Step 3: For remaining, randomly sample distinct indices from other tiers\n",
    "    remaining_needed = target_count - len(selected_indices)\n",
    "    \n",
    "    if remaining_needed > 0:\n",
    "        print(f\"Need {remaining_needed} more indices from other tiers\")\n",
    "        \n",
    "        # Get all non-tier4 indices that aren't already selected\n",
    "        non_tier4_df = computational_df[computational_df['tier'] != 'tier4']\n",
    "        available_indices = non_tier4_df['index'].unique()\n",
    "        available_indices = [idx for idx in available_indices if idx not in selected_indices]\n",
    "        \n",
    "        print(f\"Available non-tier4 indices: {len(available_indices)}\")\n",
    "        \n",
    "        if len(available_indices) >= remaining_needed:\n",
    "            additional_indices = random.sample(available_indices, remaining_needed)\n",
    "            selected_indices.extend(additional_indices)\n",
    "        else:\n",
    "            print(f\"Warning: Only {len(available_indices)} available, adding all\")\n",
    "            selected_indices.extend(available_indices)\n",
    "    \n",
    "    print(f\"Final selection: {len(selected_indices)} unique indices\")\n",
    "    \n",
    "    # Verify uniqueness\n",
    "    if len(selected_indices) != len(set(selected_indices)):\n",
    "        print(f\"ERROR: Duplicate indices detected!\")\n",
    "        selected_indices = list(set(selected_indices))\n",
    "        print(f\"After deduplication: {len(selected_indices)} unique indices\")\n",
    "    \n",
    "    # Show final distribution\n",
    "    selected_df = computational_df[computational_df['index'].isin(selected_indices)]\n",
    "    selected_unique = selected_df.drop_duplicates('index')\n",
    "    \n",
    "    print(f\"\\n--- Final Distribution ---\")\n",
    "    tier_counts = selected_unique['tier'].value_counts().sort_index()\n",
    "    print(\"Tier distribution:\")\n",
    "    for tier in ['tier1', 'tier2', 'tier3', 'tier4']:\n",
    "        count = tier_counts.get(tier, 0)\n",
    "        pct = (count / len(selected_unique)) * 100\n",
    "        print(f\"  {tier}: {count} indices ({pct:.1f}%)\")\n",
    "    \n",
    "    # Position distribution of selected indices\n",
    "    print(\"\\nPosition distribution:\")\n",
    "    for group_name, (min_pos, max_pos) in [\n",
    "        (\"Early\", (0, tertile_33)),\n",
    "        (\"Middle\", (tertile_33, tertile_67)), \n",
    "        (\"Late\", (tertile_67, 1.0))\n",
    "    ]:\n",
    "        group_selected = selected_unique[\n",
    "            (selected_unique['relative_line_position'] >= min_pos) & \n",
    "            (selected_unique['relative_line_position'] <= max_pos)\n",
    "        ]\n",
    "        count = len(group_selected)\n",
    "        pct = (count / len(selected_unique)) * 100\n",
    "        print(f\"  {group_name}: {count} indices ({pct:.1f}%)\")\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "def get_disjoint_sets_simple(master_df):\n",
    "    \"\"\"\n",
    "    Create three disjoint sets with simplified computational sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating Disjoint Sets with Simple Sampling ===\")\n",
    "    \n",
    "    # Set A: All indices with conceptual errors\n",
    "    conceptual_indices = master_df[\n",
    "        master_df['error_type'] == 'conceptual_error'\n",
    "    ]['index'].unique()\n",
    "    conceptual_indices = sorted(conceptual_indices)\n",
    "    N = len(conceptual_indices)\n",
    "    \n",
    "    print(f\"Set A (Conceptual errors): {N} unique indices\")\n",
    "    \n",
    "    # Set B: Simple computational error sampling\n",
    "    computational_indices = select_balanced_computational_indices_simple(\n",
    "        master_df, conceptual_indices, N\n",
    "    )\n",
    "    \n",
    "    print(f\"Set B (Computational errors): {len(computational_indices)} unique indices\")\n",
    "    \n",
    "    # Set C: Correct samples from remaining disjoint indices\n",
    "    used_indices = set(conceptual_indices) | set(computational_indices)\n",
    "    all_indices = set(master_df['index'].unique())\n",
    "    remaining_indices = list(all_indices - used_indices)\n",
    "    \n",
    "    print(f\"Available indices for correct samples: {len(remaining_indices)}\")\n",
    "    \n",
    "    if len(remaining_indices) < len(computational_indices):\n",
    "        print(f\"WARNING: Only {len(remaining_indices)} remaining indices available, need {len(computational_indices)}\")\n",
    "        correct_indices = remaining_indices\n",
    "    else:\n",
    "        correct_indices = random.sample(remaining_indices, len(computational_indices))\n",
    "    \n",
    "    print(f\"Set C (Correct samples): {len(correct_indices)} unique indices\")\n",
    "    \n",
    "    # Final verification that all sets are truly disjoint\n",
    "    set_A = set(conceptual_indices)\n",
    "    set_B = set(computational_indices)\n",
    "    set_C = set(correct_indices)\n",
    "    \n",
    "    overlap_AB = len(set_A & set_B)\n",
    "    overlap_AC = len(set_A & set_C)\n",
    "    overlap_BC = len(set_B & set_C)\n",
    "    \n",
    "    print(f\"\\n--- Disjoint Verification ---\")\n",
    "    print(f\"Conceptual âˆ© Computational: {overlap_AB} indices (should be 0)\")\n",
    "    print(f\"Conceptual âˆ© Correct: {overlap_AC} indices (should be 0)\")\n",
    "    print(f\"Computational âˆ© Correct: {overlap_BC} indices (should be 0)\")\n",
    "    \n",
    "    total_unique_indices = len(set_A | set_B | set_C)\n",
    "    expected_total = len(conceptual_indices) + len(computational_indices) + len(correct_indices)\n",
    "    print(f\"Total unique indices: {total_unique_indices} (should equal {expected_total})\")\n",
    "    \n",
    "    if total_unique_indices != expected_total:\n",
    "        print(\"ERROR: Sets are not properly disjoint!\")\n",
    "    else:\n",
    "        print(\"âœ“ All sets are properly disjoint\")\n",
    "    \n",
    "    return conceptual_indices, computational_indices, correct_indices\n",
    "\n",
    "conceptual_indices, computational_indices, correct_indices = get_disjoint_sets_simple(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line_separator_tokens(solution_text: str, line_sep_token: str = LINE_SEP_TOKEN) -> str:\n",
    "    \"\"\"\n",
    "    Since preprocessing is already done in master catalog, just add line separator tokens.\n",
    "    \n",
    "    Args:\n",
    "        solution_text: Already preprocessed solution text from sanitized master catalog\n",
    "        line_sep_token: Special token to use for line separation\n",
    "        \n",
    "    Returns:\n",
    "        Solution text with line separator tokens for tokenization\n",
    "    \"\"\"\n",
    "    if not isinstance(solution_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Split by newlines, filter empty lines, and join with special token\n",
    "    lines = solution_text.split('\\n')\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]  # Filter empty lines\n",
    "    \n",
    "    processed = line_sep_token.join(non_empty_lines) + line_sep_token\n",
    "    return processed\n",
    "\n",
    "def create_user_prompt_simple(question: str, solution: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates user prompt from already sanitized question and solution.\n",
    "    \n",
    "    Args:\n",
    "        question: Already sanitized GSM8K problem statement\n",
    "        solution: Already preprocessed solution text with line separators\n",
    "        \n",
    "    Returns:\n",
    "        Formatted user prompt for line-level error detection\n",
    "    \"\"\"\n",
    "    system_instruction = \"You are an expert in mathematical problem solving. Analyze the following mathematical problem and solution to identify whether there are any errors. At the end of every line, ask yourself- 'Is the solution correct so far? In particular, have there been any computational errors, or misinterpretations of the problem, or any flaws in the logic?' You must carefully examine it line-by-line because it is important to detect the VERY FIRST LINE WITH AN ERROR.\"\n",
    "    \n",
    "    user_prompt = f\"\"\"{system_instruction}\n",
    "\n",
    "### Problem:\n",
    "{question}\n",
    "\n",
    "### Solution:\n",
    "{solution}\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615dd4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0ba102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_progressive_line_labels(solution_with_tokens, relative_line_position, solution_length, sample_index=None, is_correct=False):\n",
    "    \"\"\"\n",
    "    Create PROGRESSIVE line labels for both error and correct samples.\n",
    "    \n",
    "    This answers the question: \"Is the solution correct up to this point?\"\n",
    "    - 0 = Solution is correct up to and including this line\n",
    "    - 1 = Solution becomes incorrect starting from this line (and remains incorrect)\n",
    "    \n",
    "    Args:\n",
    "        solution_with_tokens (str): Solution text with line separator tokens added\n",
    "        relative_line_position (float or None): Relative position of error (0.0-1.0), \n",
    "                                              or None for correct solutions\n",
    "        solution_length (int): Pre-computed number of lines (from sanitized catalog)\n",
    "        sample_index (int, optional): Sample index for debugging mismatch reports\n",
    "        is_correct (bool): Whether this is a correct solution (all labels should be 0)\n",
    "    \n",
    "    Returns:\n",
    "        list: Progressive labels [0, 0, 1, 1, 1, ...] where 1s start from first error\n",
    "              For correct solutions, all labels are 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(solution_with_tokens) or solution_with_tokens is None or solution_with_tokens == \"\":\n",
    "        return []\n",
    "    \n",
    "    # Use the pre-computed solution_length from sanitized catalog\n",
    "    num_lines = int(solution_length)\n",
    "    \n",
    "    # Verify actual line count matches stored length (for debugging)\n",
    "    if LINE_SEP_TOKEN in solution_with_tokens:\n",
    "        actual_line_count = solution_with_tokens.count(LINE_SEP_TOKEN)\n",
    "        if actual_line_count != num_lines:\n",
    "            index_info = f\" (Index: {sample_index})\" if sample_index is not None else \"\"\n",
    "            print(f\"Warning: Line count mismatch{index_info}! Stored: {num_lines}, Actual: {actual_line_count}\")\n",
    "            # Use actual count to prevent index errors\n",
    "            num_lines = actual_line_count\n",
    "    \n",
    "    # Handle edge case of empty solutions\n",
    "    if num_lines <= 0:\n",
    "        return []\n",
    "    \n",
    "    # Initialize all labels as 0 (correct)\n",
    "    line_labels = [0] * num_lines\n",
    "    \n",
    "    # For correct samples, all labels stay 0\n",
    "    if is_correct:\n",
    "        return line_labels\n",
    "    \n",
    "    # For error samples, set error line and ALL SUBSEQUENT LINES to 1\n",
    "    if relative_line_position is not None and not pd.isna(relative_line_position):\n",
    "        # Convert relative position (0.0-1.0) to absolute line number (0-based)\n",
    "        if num_lines == 1:\n",
    "            error_line_number = 0  # Only one line, must be line 0\n",
    "        else:\n",
    "            error_line_number = int(relative_line_position * (num_lines - 1))\n",
    "        \n",
    "        # Ensure error line is within valid bounds\n",
    "        error_line_number = max(0, min(error_line_number, num_lines - 1))\n",
    "        \n",
    "        # Set error line and ALL SUBSEQUENT LINES to 1\n",
    "        for i in range(error_line_number, num_lines):\n",
    "            line_labels[i] = 1\n",
    "    \n",
    "    return line_labels\n",
    "\n",
    "\n",
    "def select_best_sample(idx_samples):\n",
    "    \"\"\"\n",
    "    Select the best sample from a group of samples for the same index.\n",
    "    Priority: manual > programmatic\n",
    "    \"\"\"\n",
    "    # Prioritize manual over programmatic\n",
    "    manual_samples = idx_samples[idx_samples['source'] == 'manual']\n",
    "    if len(manual_samples) > 0:\n",
    "        return manual_samples.iloc[0]  # Take first manual sample\n",
    "    else:\n",
    "        return idx_samples.iloc[0]  # Take first programmatic sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e621073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3n_progressive_line_classification_dataset(master_df, conceptual_indices, computational_indices, correct_indices):\n",
    "    \"\"\"\n",
    "    Create 3N dataset with PROGRESSIVE labeling strategy (no tokenization).\n",
    "    Labels answer: \"Is the solution correct up to this point?\"\n",
    "    \n",
    "    Args:\n",
    "        master_df: Master catalog dataframe\n",
    "        conceptual_indices: List of indices for conceptual errors\n",
    "        computational_indices: List of indices for computational errors  \n",
    "        correct_indices: List of indices for correct solutions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating 3N Progressive Line Classification Dataset ===\")\n",
    "    print(\"Strategy: 0s for correct lines, 1s from first error onwards\")\n",
    "    print(f\"Dataset composition: {len(conceptual_indices)} conceptual + {len(computational_indices)} computational + {len(correct_indices)} correct\")\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Process conceptual error samples\n",
    "    print(f\"Processing {len(conceptual_indices)} conceptual error samples...\")\n",
    "    for idx in tqdm(conceptual_indices, desc=\"Conceptual errors\"):\n",
    "        idx_samples = master_df[\n",
    "            (master_df['index'] == idx) & \n",
    "            (master_df['error_type'] == 'conceptual_error')\n",
    "        ]\n",
    "        \n",
    "        if len(idx_samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        selected_sample = select_best_sample(idx_samples)\n",
    "        \n",
    "        # Use preprocessed text from master catalog\n",
    "        preprocessed_solution = selected_sample['wrong_answer']\n",
    "        solution_with_tokens = add_line_separator_tokens(preprocessed_solution)\n",
    "        preprocessed_question = selected_sample['question']\n",
    "        user_prompt = create_user_prompt_simple(preprocessed_question, solution_with_tokens)\n",
    "        \n",
    "        # Create PROGRESSIVE labels for error sample\n",
    "        line_labels = create_progressive_line_labels(\n",
    "            solution_with_tokens, \n",
    "            selected_sample['relative_line_position'],\n",
    "            selected_sample['solution_length'],\n",
    "            sample_index=idx,\n",
    "            is_correct=False\n",
    "        )\n",
    "        \n",
    "        # Verify we have a valid progressive sequence\n",
    "        if len(line_labels) > 0 and any(label == 1 for label in line_labels):\n",
    "            first_error_idx = line_labels.index(1)\n",
    "            labels_after_error = line_labels[first_error_idx:]\n",
    "            \n",
    "            if all(label == 1 for label in labels_after_error):\n",
    "                sample = {\n",
    "                    'text': user_prompt,\n",
    "                    'correct_answer': selected_sample['correct_answer'],\n",
    "                    'line_labels': line_labels,  # Progressive labels [0,0,1,1,1...]\n",
    "                    'error_type': 'conceptual_error',\n",
    "                    'index': idx,\n",
    "                    'tier': selected_sample['tier'],\n",
    "                    'source': selected_sample['source'],\n",
    "                    'relative_line_position': selected_sample['relative_line_position'],\n",
    "                    'solution_length': selected_sample['solution_length'],\n",
    "                    'first_error_line': first_error_idx\n",
    "                }\n",
    "                dataset.append(sample)\n",
    "    \n",
    "    # Process computational error samples\n",
    "    print(f\"Processing {len(computational_indices)} computational error samples...\")\n",
    "    for idx in tqdm(computational_indices, desc=\"Computational errors\"):\n",
    "        idx_samples = master_df[\n",
    "            (master_df['index'] == idx) & \n",
    "            (master_df['error_type'] == 'computational_error')\n",
    "        ]\n",
    "        \n",
    "        if len(idx_samples) == 0:\n",
    "            continue\n",
    "            \n",
    "        selected_sample = select_best_sample(idx_samples)\n",
    "        \n",
    "        preprocessed_solution = selected_sample['wrong_answer']\n",
    "        solution_with_tokens = add_line_separator_tokens(preprocessed_solution)\n",
    "        preprocessed_question = selected_sample['question']\n",
    "        user_prompt = create_user_prompt_simple(preprocessed_question, solution_with_tokens)\n",
    "        \n",
    "        line_labels = create_progressive_line_labels(\n",
    "            solution_with_tokens, \n",
    "            selected_sample['relative_line_position'],\n",
    "            selected_sample['solution_length'],\n",
    "            sample_index=idx,\n",
    "            is_correct=False\n",
    "        )\n",
    "        \n",
    "        if len(line_labels) > 0 and any(label == 1 for label in line_labels):\n",
    "            first_error_idx = line_labels.index(1)\n",
    "            labels_after_error = line_labels[first_error_idx:]\n",
    "            \n",
    "            if all(label == 1 for label in labels_after_error):\n",
    "                sample = {\n",
    "                    'text': user_prompt,\n",
    "                    'correct_answer': selected_sample['correct_answer'],\n",
    "                    'line_labels': line_labels,  # Progressive labels\n",
    "                    'error_type': 'computational_error',\n",
    "                    'index': idx,\n",
    "                    'tier': selected_sample['tier'],\n",
    "                    'source': selected_sample['source'],\n",
    "                    'relative_line_position': selected_sample['relative_line_position'],\n",
    "                    'solution_length': selected_sample['solution_length'],\n",
    "                    'first_error_line': first_error_idx\n",
    "                }\n",
    "                dataset.append(sample)\n",
    "    \n",
    "    # Process correct samples\n",
    "    print(f\"Processing {len(correct_indices)} correct solution samples...\")\n",
    "    for idx in tqdm(correct_indices, desc=\"Correct solutions\"):\n",
    "        idx_samples = master_df[master_df['index'] == idx]\n",
    "        \n",
    "        if len(idx_samples) == 0:\n",
    "            print(f\"Warning: No data found for correct index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        reference_sample = idx_samples.iloc[0]\n",
    "        preprocessed_question = reference_sample['question']\n",
    "        correct_solution = reference_sample['correct_answer']\n",
    "        \n",
    "        solution_with_tokens = add_line_separator_tokens(correct_solution)\n",
    "        user_prompt = create_user_prompt_simple(preprocessed_question, solution_with_tokens)\n",
    "        \n",
    "        # Calculate solution length for correct answer\n",
    "        lines = correct_solution.split('\\n')\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "        solution_length = len(non_empty_lines)\n",
    "        \n",
    "        # Create all-zero labels for correct solution\n",
    "        line_labels = create_progressive_line_labels(\n",
    "            solution_with_tokens,\n",
    "            relative_line_position=None,\n",
    "            solution_length=solution_length,\n",
    "            sample_index=idx,\n",
    "            is_correct=True\n",
    "        )\n",
    "        \n",
    "        if len(line_labels) > 0:\n",
    "            sample = {\n",
    "                'text': user_prompt,\n",
    "                'correct_answer': correct_solution,\n",
    "                'line_labels': line_labels,  # All zeros [0,0,0,0...]\n",
    "                'error_type': 'correct',\n",
    "                'index': idx,\n",
    "                'tier': reference_sample['tier'],\n",
    "                'source': 'original_gsm8k',\n",
    "                'relative_line_position': None,\n",
    "                'solution_length': solution_length,\n",
    "                'first_error_line': -1  # No error\n",
    "            }\n",
    "            dataset.append(sample)\n",
    "    \n",
    "    dataset_df = pd.DataFrame(dataset)\n",
    "    \n",
    "    print(f\"\\nTotal samples created: {len(dataset_df)}\")\n",
    "    \n",
    "    # Validation\n",
    "    print(\"\\n--- Progressive Label Validation ---\")\n",
    "    valid_progressive = 0\n",
    "    for _, row in dataset_df.iterrows():\n",
    "        line_labels = row['line_labels']\n",
    "        if row['error_type'] == 'correct':\n",
    "            # Should be all zeros\n",
    "            if all(label == 0 for label in line_labels):\n",
    "                valid_progressive += 1\n",
    "        else:\n",
    "            # Should have progressive pattern [0,0,1,1,1...]\n",
    "            if any(label == 1 for label in line_labels):\n",
    "                first_error = line_labels.index(1)\n",
    "                if all(label == 1 for label in line_labels[first_error:]):\n",
    "                    valid_progressive += 1\n",
    "    \n",
    "    print(f\"Valid progressive patterns: {valid_progressive}/{len(dataset_df)} ({valid_progressive/len(dataset_df)*100:.1f}%)\")\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908f762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_3n_progressive_dataset(dataset_df, output_dir):\n",
    "    \"\"\"\n",
    "    Save the 3N progressive line classification dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Saving 3N Progressive Line Classification Dataset ===\")\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save main dataset\n",
    "    dataset_path = output_dir / \"3n_progressive_line_classification_dataset_2.csv\"\n",
    "    dataset_df.to_csv(dataset_path, index=False)\n",
    "    print(f\"âœ“ Dataset saved to: {dataset_path}\")\n",
    "    print(f\"  Size: {len(dataset_df):,} samples\")\n",
    "    print(f\"  Unique indices: {dataset_df['index'].nunique():,}\")\n",
    "    \n",
    "    # Calculate class balance for metadata\n",
    "    total_line_labels = sum(len(labels) for labels in dataset_df['line_labels'])\n",
    "    total_error_lines = sum(sum(labels) for labels in dataset_df['line_labels'])\n",
    "    total_correct_lines = total_line_labels - total_error_lines\n",
    "    imbalance_ratio = total_correct_lines / total_error_lines if total_error_lines > 0 else float('inf')\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"creation_info\": {\n",
    "            \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"source_catalog\": \"master_catalog_sanitized.csv\",\n",
    "            \"creator\": \"make-line-classification-dataset.ipynb\",\n",
    "            \"dataset_strategy\": \"3n_progressive_labeling\"\n",
    "        },\n",
    "        \"labeling_strategy\": {\n",
    "            \"approach\": \"progressive\",\n",
    "            \"description\": \"Labels answer: 'Is the solution correct up to this point?'\",\n",
    "            \"correct_samples\": \"All labels are 0 (solution is correct throughout)\",\n",
    "            \"error_samples\": \"Progressive pattern [0,0,1,1,1...] starting from first error\",\n",
    "            \"advantages\": [\"Better class balance\", \"More training signal\", \"Conceptually aligned\"]\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"description\": \"3N progressive line-level error detection dataset_2\",\n",
    "            \"strategy\": \"3n_progressive_balanced\", \n",
    "            \"total_samples\": len(dataset_df),\n",
    "            \"unique_problems\": dataset_df['index'].nunique(),\n",
    "            \"composition\": dict(dataset_df['error_type'].value_counts()),\n",
    "            \"source_distribution\": dict(dataset_df['source'].value_counts()),\n",
    "            \"tier_distribution\": dict(dataset_df['tier'].value_counts()),\n",
    "            \"class_balance\": {\n",
    "                \"total_line_positions\": total_line_labels,\n",
    "                \"error_line_positions\": total_error_lines,\n",
    "                \"correct_line_positions\": total_correct_lines,\n",
    "                \"imbalance_ratio\": f\"{imbalance_ratio:.1f}:1\",\n",
    "                \"error_percentage\": f\"{total_error_lines/total_line_labels*100:.1f}%\"\n",
    "            }\n",
    "        },\n",
    "        \"column_descriptions\": {\n",
    "            \"text\": \"Complete user prompt: system instruction + problem + solution\",\n",
    "            \"correct_answer\": \"Original GSM8K correct answer for reference\",\n",
    "            \"line_labels\": \"Progressive labels [0,0,1,1,1...] indicating correctness up to each line\",\n",
    "            \"error_type\": \"'conceptual_error', 'computational_error', or 'correct'\",\n",
    "            \"index\": \"Original GSM8K problem index\",\n",
    "            \"tier\": \"Problem difficulty tier (tier1-tier4)\",\n",
    "            \"source\": \"'manual', 'programmatic', or 'original_gsm8k'\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = output_dir / \"3n_progressive_dataset_metadata_2.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    print(f\"âœ“ Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae3ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Creating 3N progressive line classification dataset...\n",
      "=== Creating 3N Progressive Line Classification Dataset ===\n",
      "Strategy: 0s for correct lines, 1s from first error onwards\n",
      "Dataset composition: 1881 conceptual + 1606 computational + 1606 correct\n",
      "Processing 1881 conceptual error samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d55455095e4188a898f6b5e8bfd055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conceptual errors:   0%|          | 0/1881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1606 computational error samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d12d14b248467a9727d981948e2545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computational errors:   0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1606 correct solution samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c1bc809a7e419f93d7a6530677926c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Correct solutions:   0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples created: 5093\n",
      "\n",
      "--- Progressive Label Validation ---\n",
      "Valid progressive patterns: 5093/5093 (100.0%)\n",
      "=== Saving 3N Progressive Line Classification Dataset ===\n",
      "âœ“ Dataset saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/3n_progressive_line_classification_dataset_2.csv\n",
      "  Size: 5,093 samples\n",
      "  Unique indices: 5,093\n",
      "âœ“ Metadata saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/3n_progressive_dataset_metadata_2.json\n",
      "\n",
      "ðŸ“ 3N progressive dataset saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification\n",
      "\n",
      "ðŸ” QUICK VALIDATION:\n",
      "Dataset length: 5093\n",
      "Error type distribution:\n",
      "error_type\n",
      "conceptual_error       1881\n",
      "computational_error    1606\n",
      "correct                1606\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“‹ SAMPLE EXAMPLES:\n",
      "  correct: 5 lines, first error at -1, labels: [0, 0, 0, 0, 0]\n",
      "  conceptual_error: 3 lines, first error at 1, labels: [0, 1, 1]\n",
      "  computational_error: 4 lines, first error at 1, labels: [0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Execute the 3N progressive strategy\n",
    "print(\"ðŸš€ Creating 3N progressive line classification dataset...\")\n",
    "progressive_3n_dataset_df = create_3n_progressive_line_classification_dataset(\n",
    "    master_df, conceptual_indices, computational_indices, correct_indices\n",
    ")\n",
    "\n",
    "# Save the 3N progressive dataset\n",
    "saved_3n_dir = save_3n_progressive_dataset(progressive_3n_dataset_df, OUTPUT_DIR)\n",
    "print(f\"\\nðŸ“ 3N progressive dataset saved to: {saved_3n_dir}\")\n",
    "\n",
    "# Quick validation\n",
    "print(f\"\\nðŸ” QUICK VALIDATION:\")\n",
    "print(f\"Dataset length: {len(progressive_3n_dataset_df)}\")\n",
    "print(f\"Error type distribution:\")\n",
    "print(progressive_3n_dataset_df['error_type'].value_counts())\n",
    "\n",
    "# Show a few examples\n",
    "print(f\"\\nðŸ“‹ SAMPLE EXAMPLES:\")\n",
    "for error_type in ['correct', 'conceptual_error', 'computational_error']:\n",
    "    sample = progressive_3n_dataset_df[progressive_3n_dataset_df['error_type'] == error_type].iloc[0]\n",
    "    line_labels = sample['line_labels']\n",
    "    first_one = line_labels.index(1) if 1 in line_labels else -1\n",
    "    print(f\"  {error_type}: {len(line_labels)} lines, first error at {first_one}, labels: {line_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9229b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DEBUGGING LINE COUNT MISMATCH\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 9\n",
      "============================================================\n",
      "ðŸ“ Error type: computational_error\n",
      "ðŸ“ Source: manual\n",
      "ðŸ“ Tier: tier2\n",
      "ðŸ“ Stored solution_length: 9\n",
      "ðŸ“ Stored relative_line_position: 0.75\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift\\nShe works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime\\nOvertime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00\\nHer overtime pay is 18+9 = $27.00\\nHer base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00\\nHer overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay\\n2 hours of overtime pay for 5 days means she makes 54*5 = $250.00\\nIn 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00\\n#### 970'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift'\n",
      "  Line 1: 'She works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime'\n",
      "  Line 2: 'Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00'\n",
      "  Line 3: 'Her overtime pay is 18+9 = $27.00'\n",
      "  Line 4: 'Her base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00'\n",
      "  Line 5: 'Her overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay'\n",
      "  Line 6: '2 hours of overtime pay for 5 days means she makes 54*5 = $250.00'\n",
      "  Line 7: 'In 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00'\n",
      "  Line 8: '#### 970'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'She works 8 hours a day for $18 per hour so she makes 8*18 = $144.00 per 8-hour shift'\n",
      "  Line 1: 'She works 10 hours a day and anything over 8 hours is eligible for overtime, so she gets 10-8 = 2 hours of overtime'\n",
      "  Line 2: 'Overtime is calculated as time and a half so and she makes $18/hour so her overtime pay is 18*.5 = $9.00'\n",
      "  Line 3: 'Her overtime pay is 18+9 = $27.00'\n",
      "  Line 4: 'Her base pay is $144.00 per 8-hour shift and she works 5 days and makes 5 * $144 = $720.00'\n",
      "  Line 5: 'Her overtime pay is $27.00 per hour and she works 2 hours of overtime per day and makes 27*2 = $54.00 in overtime pay'\n",
      "  Line 6: '2 hours of overtime pay for 5 days means she makes 54*5 = $250.00'\n",
      "  Line 7: 'In 5 days her base pay is $720.00 and she makes $250.00 in overtime pay so she makes $720 + $250 = $970.00'\n",
      "  Line 8: '#### 970'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "ðŸ“Š Stored solution_length: 9\n",
      "ðŸ“Š Actual preprocessed lines: 9\n",
      "ðŸ“Š Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 9\n",
      "Lines before calculator removal: 9\n",
      "Lines after calculator removal: 9\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 6\n",
      "Corrected error line (using actual length): 6\n",
      "Valid range for actual lines: 0 to 8\n",
      "âœ… Original position 6 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 18\n",
      "============================================================\n",
      "ðŸ“ Error type: computational_error\n",
      "ðŸ“ Source: programmatic\n",
      "ðŸ“ Tier: tier1\n",
      "ðŸ“ Stored solution_length: 6\n",
      "ðŸ“ Stored relative_line_position: 0.2\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'Roque takes 2*3 = 6 hours a week to walk to work.\\nRoque takes 6*2 = 21 hours a week to walk to and from work.\\nRoque takes 1*2 = 2 hours a week to bike to work.\\nRoque takes 2*2 = 4 hours a week to bike to and from work.\\nIn total, Roque takes 21+4 = 25 hour a week to go to and from work.\\n#### 25'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'Roque takes 2*3 = 6 hours a week to walk to work.'\n",
      "  Line 1: 'Roque takes 6*2 = 21 hours a week to walk to and from work.'\n",
      "  Line 2: 'Roque takes 1*2 = 2 hours a week to bike to work.'\n",
      "  Line 3: 'Roque takes 2*2 = 4 hours a week to bike to and from work.'\n",
      "  Line 4: 'In total, Roque takes 21+4 = 25 hour a week to go to and from work.'\n",
      "  Line 5: '#### 25'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'Roque takes 2*3 = 6 hours a week to walk to work.'\n",
      "  Line 1: 'Roque takes 6*2 = 21 hours a week to walk to and from work.'\n",
      "  Line 2: 'Roque takes 1*2 = 2 hours a week to bike to work.'\n",
      "  Line 3: 'Roque takes 2*2 = 4 hours a week to bike to and from work.'\n",
      "  Line 4: 'In total, Roque takes 21+4 = 25 hour a week to go to and from work.'\n",
      "  Line 5: '#### 25'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "ðŸ“Š Stored solution_length: 6\n",
      "ðŸ“Š Actual preprocessed lines: 6\n",
      "ðŸ“Š Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 6\n",
      "Lines before calculator removal: 6\n",
      "Lines after calculator removal: 6\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 1\n",
      "Corrected error line (using actual length): 1\n",
      "Valid range for actual lines: 0 to 5\n",
      "âœ… Original position 1 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 25\n",
      "============================================================\n",
      "ðŸ“ Error type: computational_error\n",
      "ðŸ“ Source: programmatic\n",
      "ðŸ“ Tier: tier3\n",
      "ðŸ“ Stored solution_length: 4\n",
      "ðŸ“ Stored relative_line_position: 0.0\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "Step 2 - After calculator annotation removal:\n",
      "\"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\\nOut of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\\nCombined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\\n#### 650/3\"\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: \"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 1: \"Out of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\"\n",
      "  Line 2: \"Combined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 3: '#### 650/3'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: \"Out of the first 100 balls, Ralph was able to hit 2/5 of them and not able to hit 3/5 of them, 3/5 x 100 = 500/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 1: \"Out of the next 75 balls, Ralph was able to hit 1/3 of them and not able to hit 2/3 of them, 2/3 x 75 = 50 tennis balls that Ralph didn't hit.\"\n",
      "  Line 2: \"Combined, Ralph was not able to hit 500/3 + 50 = 650/3 tennis balls Ralph didn't hit.\"\n",
      "  Line 3: '#### 650/3'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "ðŸ“Š Stored solution_length: 4\n",
      "ðŸ“Š Actual preprocessed lines: 4\n",
      "ðŸ“Š Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 4\n",
      "Lines before calculator removal: 4\n",
      "Lines after calculator removal: 4\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 0\n",
      "Corrected error line (using actual length): 0\n",
      "Valid range for actual lines: 0 to 3\n",
      "âœ… Original position 0 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 29\n",
      "============================================================\n",
      "ðŸ“ Error type: conceptual_error\n",
      "ðŸ“ Source: manual\n",
      "ðŸ“ Tier: tier3\n",
      "ðŸ“ Stored solution_length: 9\n",
      "ðŸ“ Stored relative_line_position: 0.25\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'Let her previous monthly income be p\\nThe cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\\nHer income was increased by $900 so it is now p+$900\\nThe cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4\\nEquating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4\\nMultiplying both sides of the equation by 20 gives 8p = 5p+$4500\\nSubtracting 5p from both sides gives: 3p = $4500\\nDividing both sides by 3 gives p = $1500\\n#### 1500'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'Let her previous monthly income be p'\n",
      "  Line 1: 'The cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5'\n",
      "  Line 2: 'Her income was increased by $900 so it is now p+$900'\n",
      "  Line 3: 'The cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4'\n",
      "  Line 4: 'Equating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4'\n",
      "  Line 5: 'Multiplying both sides of the equation by 20 gives 8p = 5p+$4500'\n",
      "  Line 6: 'Subtracting 5p from both sides gives: 3p = $4500'\n",
      "  Line 7: 'Dividing both sides by 3 gives p = $1500'\n",
      "  Line 8: '#### 1500'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'Let her previous monthly income be p'\n",
      "  Line 1: 'The cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5'\n",
      "  Line 2: 'Her income was increased by $900 so it is now p+$900'\n",
      "  Line 3: 'The cost of her rent and utilities now amount to 25% of (p+$900) which is (25/100)*(p+$900) = (p+$900)/4'\n",
      "  Line 4: 'Equating both expressions for cost of rent and utilities: 2p/5 = (p+$900)/4'\n",
      "  Line 5: 'Multiplying both sides of the equation by 20 gives 8p = 5p+$4500'\n",
      "  Line 6: 'Subtracting 5p from both sides gives: 3p = $4500'\n",
      "  Line 7: 'Dividing both sides by 3 gives p = $1500'\n",
      "  Line 8: '#### 1500'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "ðŸ“Š Stored solution_length: 9\n",
      "ðŸ“Š Actual preprocessed lines: 9\n",
      "ðŸ“Š Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 9\n",
      "Lines before calculator removal: 9\n",
      "Lines after calculator removal: 9\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 2\n",
      "Corrected error line (using actual length): 2\n",
      "Valid range for actual lines: 0 to 8\n",
      "âœ… Original position 2 is within bounds\n",
      "\n",
      "============================================================\n",
      "DEBUGGING INDEX 30\n",
      "============================================================\n",
      "ðŸ“ Error type: computational_error\n",
      "ðŸ“ Source: programmatic\n",
      "ðŸ“ Tier: tier2\n",
      "ðŸ“ Stored solution_length: 6\n",
      "ðŸ“ Stored relative_line_position: 0.0\n",
      "\n",
      "--- RAW SOLUTION ---\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "\n",
      "--- PREPROCESSING STEPS ---\n",
      "Step 1 - After \\n conversion:\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "Step 2 - After calculator annotation removal:\n",
      "'In total, there are 4 x 4 = 61 pizza pieces.\\nBill and Dale eat 2 x 4 x 50% = 4 pieces.\\nAnn and Cate eat 2 x 4 x 75% = 6 pieces.\\nThe four of them eat 4 + 6 = 10 pieces.\\nThere are 61 - 10 = 51 pizza pieces uneaten.\\n#### 51'\n",
      "Step 4 - Lines before filtering:\n",
      "  Line 0: 'In total, there are 4 x 4 = 61 pizza pieces.'\n",
      "  Line 1: 'Bill and Dale eat 2 x 4 x 50% = 4 pieces.'\n",
      "  Line 2: 'Ann and Cate eat 2 x 4 x 75% = 6 pieces.'\n",
      "  Line 3: 'The four of them eat 4 + 6 = 10 pieces.'\n",
      "  Line 4: 'There are 61 - 10 = 51 pizza pieces uneaten.'\n",
      "  Line 5: '#### 51'\n",
      "Step 5 - Non-empty lines after filtering:\n",
      "  Line 0: 'In total, there are 4 x 4 = 61 pizza pieces.'\n",
      "  Line 1: 'Bill and Dale eat 2 x 4 x 50% = 4 pieces.'\n",
      "  Line 2: 'Ann and Cate eat 2 x 4 x 75% = 6 pieces.'\n",
      "  Line 3: 'The four of them eat 4 + 6 = 10 pieces.'\n",
      "  Line 4: 'There are 61 - 10 = 51 pizza pieces uneaten.'\n",
      "  Line 5: '#### 51'\n",
      "\n",
      "--- LINE COUNT COMPARISON ---\n",
      "ðŸ“Š Stored solution_length: 6\n",
      "ðŸ“Š Actual preprocessed lines: 6\n",
      "ðŸ“Š Difference: 0\n",
      "\n",
      "--- ORIGINAL LINE COUNTING SIMULATION ---\n",
      "Original raw line count: 6\n",
      "Lines before calculator removal: 6\n",
      "Lines after calculator removal: 6\n",
      "Calculator annotations removed 0 lines\n",
      "\n",
      "--- ERROR POSITION COMPARISON ---\n",
      "Original error line (using stored length): 0\n",
      "Corrected error line (using actual length): 0\n",
      "Valid range for actual lines: 0 to 5\n",
      "âœ… Original position 0 is within bounds\n",
      "\n",
      "======================================================================\n",
      "SUMMARY ANALYSIS\n",
      "======================================================================\n",
      "ðŸ“Š Analyzed 5 problematic cases\n",
      "ðŸ“Š Line count differences: [0, 0, 0, 0, 0]\n",
      "ðŸ“Š Average difference: 0.00\n",
      "ðŸ“Š Most common difference: 0\n",
      "\n",
      "--- PATTERNS BY ERROR TYPE ---\n",
      "Conceptual errors: avg difference = 0.00\n",
      "Computational errors: avg difference = 0.00\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DEBUGGING: Line Count Mismatch Investigation\n",
    "# ==============================================================================\n",
    "def debug_line_count_mismatch(master_df, sample_indices=None, max_samples=20):\n",
    "    \"\"\"\n",
    "    Debug line count mismatches between stored solution_length and actual preprocessed lines.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” DEBUGGING LINE COUNT MISMATCH\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if sample_indices is None:\n",
    "        # Use the problematic indices we found earlier\n",
    "        sample_indices = [9, 18, 25, 29, 30]  # From the debugging output\n",
    "    \n",
    "    problematic_cases = []\n",
    "    \n",
    "    for idx in sample_indices[:max_samples]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DEBUGGING INDEX {idx}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get the sample from master catalog\n",
    "        sample_data = master_df[master_df['index'] == idx]\n",
    "        if len(sample_data) == 0:\n",
    "            print(f\"No data found for index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Take the first available sample (conceptual or computational)\n",
    "        sample = sample_data.iloc[0]\n",
    "        \n",
    "        print(f\"ðŸ“ Error type: {sample['error_type']}\")\n",
    "        print(f\"ðŸ“ Source: {sample['source']}\")\n",
    "        print(f\"ðŸ“ Tier: {sample['tier']}\")\n",
    "        print(f\"ðŸ“ Stored solution_length: {sample['solution_length']}\")\n",
    "        print(f\"ðŸ“ Stored relative_line_position: {sample['relative_line_position']}\")\n",
    "        \n",
    "        # Get the raw solution\n",
    "        raw_solution = sample['wrong_answer']\n",
    "        print(f\"\\n--- RAW SOLUTION ---\")\n",
    "        print(repr(raw_solution))\n",
    "        \n",
    "        # Process the solution step by step\n",
    "        print(f\"\\n--- PREPROCESSING STEPS ---\")\n",
    "        \n",
    "        # Step 1: Convert literal \\n to actual newlines\n",
    "        step1 = raw_solution.replace('\\\\n', '\\n')\n",
    "        print(f\"Step 1 - After \\\\n conversion:\")\n",
    "        print(repr(step1))\n",
    "        \n",
    "        # Step 2: Remove calculator annotations\n",
    "        import re\n",
    "        step2 = re.sub(r'<<.*?>>', '', step1)\n",
    "        print(f\"Step 2 - After calculator annotation removal:\")\n",
    "        print(repr(step2))\n",
    "        \n",
    "        # Step 3: Sanitize Unicode and commas (simplified)\n",
    "        step3 = step2  # Skip for now to focus on line counting\n",
    "        \n",
    "        # Step 4: Split by newlines and count\n",
    "        lines_before_filter = step3.split('\\n')\n",
    "        print(f\"Step 4 - Lines before filtering:\")\n",
    "        for i, line in enumerate(lines_before_filter):\n",
    "            print(f\"  Line {i}: {repr(line)}\")\n",
    "        \n",
    "        # Step 5: Filter empty lines\n",
    "        non_empty_lines = [line.strip() for line in lines_before_filter if line.strip()]\n",
    "        print(f\"Step 5 - Non-empty lines after filtering:\")\n",
    "        for i, line in enumerate(non_empty_lines):\n",
    "            print(f\"  Line {i}: {repr(line)}\")\n",
    "        \n",
    "        # Calculate actual vs expected\n",
    "        actual_line_count = len(non_empty_lines)\n",
    "        stored_line_count = int(sample['solution_length'])\n",
    "        \n",
    "        print(f\"\\n--- LINE COUNT COMPARISON ---\")\n",
    "        print(f\"ðŸ“Š Stored solution_length: {stored_line_count}\")\n",
    "        print(f\"ðŸ“Š Actual preprocessed lines: {actual_line_count}\")\n",
    "        print(f\"ðŸ“Š Difference: {actual_line_count - stored_line_count}\")\n",
    "        \n",
    "        # Show what the original line counting logic would have done\n",
    "        print(f\"\\n--- ORIGINAL LINE COUNTING SIMULATION ---\")\n",
    "        # The original logic probably counted lines in the raw text\n",
    "        original_lines = raw_solution.replace('\\\\n', '\\n').split('\\n')\n",
    "        original_non_empty = [line.strip() for line in original_lines if line.strip()]\n",
    "        print(f\"Original raw line count: {len(original_non_empty)}\")\n",
    "        \n",
    "        # Check if calculator annotations affected line count\n",
    "        with_calc_lines = step1.split('\\n')\n",
    "        without_calc_lines = step2.split('\\n')\n",
    "        calc_lines_before = len([line.strip() for line in with_calc_lines if line.strip()])\n",
    "        calc_lines_after = len([line.strip() for line in without_calc_lines if line.strip()])\n",
    "        \n",
    "        print(f\"Lines before calculator removal: {calc_lines_before}\")\n",
    "        print(f\"Lines after calculator removal: {calc_lines_after}\")\n",
    "        print(f\"Calculator annotations removed {calc_lines_before - calc_lines_after} lines\")\n",
    "        \n",
    "        # Calculate what the error position should be\n",
    "        if sample['relative_line_position'] is not None:\n",
    "            # Original calculation (wrong)\n",
    "            original_error_line = int(sample['relative_line_position'] * (stored_line_count - 1))\n",
    "            \n",
    "            # Corrected calculation\n",
    "            corrected_error_line = int(sample['relative_line_position'] * (actual_line_count - 1))\n",
    "            \n",
    "            print(f\"\\n--- ERROR POSITION COMPARISON ---\")\n",
    "            print(f\"Original error line (using stored length): {original_error_line}\")\n",
    "            print(f\"Corrected error line (using actual length): {corrected_error_line}\")\n",
    "            print(f\"Valid range for actual lines: 0 to {actual_line_count - 1}\")\n",
    "            \n",
    "            # Check if original position is out of bounds\n",
    "            if original_error_line >= actual_line_count:\n",
    "                print(f\"âŒ Original position {original_error_line} is OUT OF BOUNDS for {actual_line_count} lines!\")\n",
    "            else:\n",
    "                print(f\"âœ… Original position {original_error_line} is within bounds\")\n",
    "        \n",
    "        problematic_cases.append({\n",
    "            'index': idx,\n",
    "            'stored_length': stored_line_count,\n",
    "            'actual_length': actual_line_count,\n",
    "            'difference': actual_line_count - stored_line_count,\n",
    "            'error_type': sample['error_type'],\n",
    "            'relative_position': sample['relative_line_position']\n",
    "        })\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    differences = [case['difference'] for case in problematic_cases]\n",
    "    \n",
    "    print(f\"ðŸ“Š Analyzed {len(problematic_cases)} problematic cases\")\n",
    "    print(f\"ðŸ“Š Line count differences: {differences}\")\n",
    "    print(f\"ðŸ“Š Average difference: {np.mean(differences):.2f}\")\n",
    "    print(f\"ðŸ“Š Most common difference: {max(set(differences), key=differences.count)}\")\n",
    "    \n",
    "    # Show patterns by error type\n",
    "    conceptual_diffs = [case['difference'] for case in problematic_cases if case['error_type'] == 'conceptual_error']\n",
    "    computational_diffs = [case['difference'] for case in problematic_cases if case['error_type'] == 'computational_error']\n",
    "    \n",
    "    print(f\"\\n--- PATTERNS BY ERROR TYPE ---\")\n",
    "    if conceptual_diffs:\n",
    "        print(f\"Conceptual errors: avg difference = {np.mean(conceptual_diffs):.2f}\")\n",
    "    if computational_diffs:\n",
    "        print(f\"Computational errors: avg difference = {np.mean(computational_diffs):.2f}\")\n",
    "    \n",
    "    return problematic_cases\n",
    "\n",
    "# Run the debugging\n",
    "debug_results = debug_line_count_mismatch(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc9ba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Comparison ===\n",
      "Flawed-only dataset: 3487 samples\n",
      "3N progressive dataset: 5093 samples\n",
      "3N progressive (errors only): 3487 samples\n",
      "\n",
      "=== Comparison Results ===\n",
      "Perfect matches: 0\n",
      "Mismatches: 3487\n",
      "\n",
      "=== Mismatch Details ===\n",
      "\n",
      "Mismatch 1:\n",
      "  Index: 1, Error type: conceptual_error\n",
      "  Issues: ['text_mismatch']\n",
      "\n",
      "Mismatch 2:\n",
      "  Index: 2, Error type: conceptual_error\n",
      "  Issues: ['text_mismatch']\n",
      "\n",
      "Mismatch 3:\n",
      "  Index: 4, Error type: conceptual_error\n",
      "  Issues: ['text_mismatch']\n",
      "\n",
      "Mismatch 4:\n",
      "  Index: 6, Error type: conceptual_error\n",
      "  Issues: ['text_mismatch']\n",
      "\n",
      "Mismatch 5:\n",
      "  Index: 10, Error type: conceptual_error\n",
      "  Issues: ['text_mismatch']\n",
      "\n",
      "=== Summary ===\n",
      "Total comparisons: 3487\n",
      "Perfect matches: 0 (0.0%)\n",
      "Issues found: 3487 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load both datasets\n",
    "flawed_only_df = pd.read_csv('/Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/flawed-only/flawed_only_line_classification_dataset.csv')\n",
    "progressive_3n_df = pd.read_csv('/Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/line-classification/3n_progressive_line_classification_dataset_2.csv')\n",
    "\n",
    "print(\"=== Dataset Comparison ===\")\n",
    "print(f\"Flawed-only dataset: {len(flawed_only_df)} samples\")\n",
    "print(f\"3N progressive dataset: {len(progressive_3n_df)} samples\")\n",
    "\n",
    "# Filter out correct samples from 3N dataset for comparison\n",
    "progressive_errors_df = progressive_3n_df[progressive_3n_df['error_type'] != 'correct'].copy()\n",
    "print(f\"3N progressive (errors only): {len(progressive_errors_df)} samples\")\n",
    "\n",
    "# Parse line_labels from string representation\n",
    "def parse_labels(label_str):\n",
    "    return ast.literal_eval(label_str) if isinstance(label_str, str) else label_str\n",
    "\n",
    "flawed_only_df['parsed_labels'] = flawed_only_df['line_labels'].apply(parse_labels)\n",
    "progressive_errors_df['parsed_labels'] = progressive_errors_df['line_labels'].apply(parse_labels)\n",
    "\n",
    "# Match rows by index and error_type\n",
    "matches = []\n",
    "mismatches = []\n",
    "\n",
    "for _, flawed_row in flawed_only_df.iterrows():\n",
    "    # Find matching row in progressive dataset\n",
    "    matching_rows = progressive_errors_df[\n",
    "        (progressive_errors_df['index'] == flawed_row['index']) & \n",
    "        (progressive_errors_df['error_type'] == flawed_row['error_type'])\n",
    "    ]\n",
    "    \n",
    "    if len(matching_rows) == 0:\n",
    "        mismatches.append({\n",
    "            'type': 'no_match',\n",
    "            'index': flawed_row['index'],\n",
    "            'error_type': flawed_row['error_type'],\n",
    "            'issue': 'No matching row found in progressive dataset'\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    if len(matching_rows) > 1:\n",
    "        mismatches.append({\n",
    "            'type': 'multiple_matches',\n",
    "            'index': flawed_row['index'],\n",
    "            'error_type': flawed_row['error_type'],\n",
    "            'issue': f'Multiple matches found: {len(matching_rows)}'\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    progressive_row = matching_rows.iloc[0]\n",
    "    \n",
    "    # Check text column exact match\n",
    "    text_match = flawed_row['text'] == progressive_row['text']\n",
    "    \n",
    "    # Check label lengths match\n",
    "    flawed_labels = flawed_row['parsed_labels']\n",
    "    progressive_labels = progressive_row['parsed_labels']\n",
    "    length_match = len(flawed_labels) == len(progressive_labels)\n",
    "    \n",
    "    match_info = {\n",
    "        'index': flawed_row['index'],\n",
    "        'error_type': flawed_row['error_type'],\n",
    "        'text_match': text_match,\n",
    "        'length_match': length_match,\n",
    "        'flawed_labels': flawed_labels,\n",
    "        'progressive_labels': progressive_labels,\n",
    "        'label_length': len(flawed_labels)\n",
    "    }\n",
    "    \n",
    "    if text_match and length_match:\n",
    "        matches.append(match_info)\n",
    "    else:\n",
    "        match_info['issues'] = []\n",
    "        if not text_match:\n",
    "            match_info['issues'].append('text_mismatch')\n",
    "        if not length_match:\n",
    "            match_info['issues'].append(f'length_mismatch: {len(flawed_labels)} vs {len(progressive_labels)}')\n",
    "        mismatches.append(match_info)\n",
    "\n",
    "print(f\"\\n=== Comparison Results ===\")\n",
    "print(f\"Perfect matches: {len(matches)}\")\n",
    "print(f\"Mismatches: {len(mismatches)}\")\n",
    "\n",
    "if mismatches:\n",
    "    print(f\"\\n=== Mismatch Details ===\")\n",
    "    for i, mismatch in enumerate(mismatches[:5]):  # Show first 5 mismatches\n",
    "        print(f\"\\nMismatch {i+1}:\")\n",
    "        print(f\"  Index: {mismatch['index']}, Error type: {mismatch['error_type']}\")\n",
    "        if 'issues' in mismatch:\n",
    "            print(f\"  Issues: {mismatch['issues']}\")\n",
    "        else:\n",
    "            print(f\"  Issue: {mismatch['issue']}\")\n",
    "\n",
    "# Show labeling pattern comparison for a few matches\n",
    "if matches:\n",
    "    print(f\"\\n=== Label Pattern Comparison (First 3 Matches) ===\")\n",
    "    for i, match in enumerate(matches[:3]):\n",
    "        print(f\"\\nSample {i+1} (Index {match['index']}):\")\n",
    "        print(f\"  Flawed-only labels:    {match['flawed_labels']}\")\n",
    "        print(f\"  Progressive labels:    {match['progressive_labels']}\")\n",
    "        \n",
    "        # Show the transformation\n",
    "        flawed = match['flawed_labels']\n",
    "        progressive = match['progressive_labels']\n",
    "        if 1 in flawed:\n",
    "            error_pos = flawed.index(1)\n",
    "            print(f\"  Error at position {error_pos}: {flawed} â†’ {progressive}\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total comparisons: {len(flawed_only_df)}\")\n",
    "print(f\"Perfect matches: {len(matches)} ({len(matches)/len(flawed_only_df)*100:.1f}%)\")\n",
    "print(f\"Issues found: {len(mismatches)} ({len(mismatches)/len(flawed_only_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37331db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
