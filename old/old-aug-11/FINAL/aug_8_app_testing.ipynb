{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c72b9179ab97474ea66084d4f88fd0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bd42e5130cf4999b74ef95a8102cbf5",
              "IPY_MODEL_8abb6e0a3d564973a0e0fa580f8d6ecf",
              "IPY_MODEL_a5fabeab016341e8ad20c27fcf21badb"
            ],
            "layout": "IPY_MODEL_6e3e4115744545268b27efbc348ca775"
          }
        },
        "8bd42e5130cf4999b74ef95a8102cbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c46c71a3c674f2b9803b996f5e803eb",
            "placeholder": "​",
            "style": "IPY_MODEL_f6e6f046dd52496b817124500a3b2a25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8abb6e0a3d564973a0e0fa580f8d6ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2545d5ad72fb44d5acf50eaf39ca1a8e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f86edd42fb7455ea5268357dc808081",
            "value": 2
          }
        },
        "a5fabeab016341e8ad20c27fcf21badb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4415f8c2019b419c8d1ed761f8f769ca",
            "placeholder": "​",
            "style": "IPY_MODEL_867904b5fb17458fbd5bdbdd55fb29a8",
            "value": " 2/2 [00:06&lt;00:00,  3.40s/it]"
          }
        },
        "6e3e4115744545268b27efbc348ca775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c46c71a3c674f2b9803b996f5e803eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e6f046dd52496b817124500a3b2a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2545d5ad72fb44d5acf50eaf39ca1a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f86edd42fb7455ea5268357dc808081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4415f8c2019b419c8d1ed761f8f769ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867904b5fb17458fbd5bdbdd55fb29a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb49fa30bf474ad7894425e0cf7cffc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b5e7fd9af884d20a9abbcb48d6bd80c",
              "IPY_MODEL_809876a3b28c4854ad0fc5543fbdf2d9",
              "IPY_MODEL_06a11bb6693f487492e57961565f757e"
            ],
            "layout": "IPY_MODEL_fac726eb95d7400cbed602d298a3853c"
          }
        },
        "2b5e7fd9af884d20a9abbcb48d6bd80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d7e211da7a4154a8a2312386f16908",
            "placeholder": "​",
            "style": "IPY_MODEL_51732895f18b4c1a972a583c62745d7f",
            "value": "Stage 1 Batches: 100%"
          }
        },
        "809876a3b28c4854ad0fc5543fbdf2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a724e95f0747149f61edee299a1327",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b76a8b443af4a80a60ebc545bf0d764",
            "value": 3
          }
        },
        "06a11bb6693f487492e57961565f757e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cd492636bc74a4a993c38ae34b73dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_f331971585224607aacaebbe7eb59726",
            "value": " 3/3 [03:16&lt;00:00, 62.73s/it]"
          }
        },
        "fac726eb95d7400cbed602d298a3853c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d7e211da7a4154a8a2312386f16908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51732895f18b4c1a972a583c62745d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01a724e95f0747149f61edee299a1327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b76a8b443af4a80a60ebc545bf0d764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cd492636bc74a4a993c38ae34b73dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f331971585224607aacaebbe7eb59726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1aa60223f544c59f61ffe390e9869f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98e212e16af143e2a87ac089901d3a33",
              "IPY_MODEL_332aeb618e3342ff964ef2c72d5f8f0e",
              "IPY_MODEL_3f175beb70cb40b2a168b1c20bb86da9"
            ],
            "layout": "IPY_MODEL_b8cce527fb3b47d1a32293c062b250c1"
          }
        },
        "98e212e16af143e2a87ac089901d3a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3292bd8d55b4eab9b62a5bcb1330a79",
            "placeholder": "​",
            "style": "IPY_MODEL_a1bf4d96896746609fad7af7b6ecacd8",
            "value": "Stage 2 Batches:  33%"
          }
        },
        "332aeb618e3342ff964ef2c72d5f8f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef09dbb5bbb14d94a066042c0bed6449",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d4b97bd94424383ab7968b9b6d556db",
            "value": 1
          }
        },
        "3f175beb70cb40b2a168b1c20bb86da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0658364c1da4c72aa8ea09c7dea3816",
            "placeholder": "​",
            "style": "IPY_MODEL_c6d230478fad4d44b964783230c3ffb0",
            "value": " 1/3 [00:03&lt;00:02,  1.28s/it]"
          }
        },
        "b8cce527fb3b47d1a32293c062b250c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3292bd8d55b4eab9b62a5bcb1330a79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1bf4d96896746609fad7af7b6ecacd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef09dbb5bbb14d94a066042c0bed6449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d4b97bd94424383ab7968b9b6d556db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0658364c1da4c72aa8ea09c7dea3816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6d230478fad4d44b964783230c3ffb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-1rW_6ETBLF",
        "outputId": "97a71d64-c52b-4e84-9728-89a1d648c0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.53.2 in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2025.8.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: flash-attn==2.7.4.post1 in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.4.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Installations\n",
        "!pip install -U transformers==4.53.2\n",
        "!pip install -U accelerate\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "!pip install flash-attn==2.7.4.post1 \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
        "  --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUsFe1CrTESy",
        "outputId": "e517fa27-bbe8-4f39-a737-dae1a2a951b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.8.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (1.1.7)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.12.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.8.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "print(\"✅ All modules imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etqRxpwTZtm1",
        "outputId": "90da50b0-48ed-4371-8725-48182ae4b161"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "✅ All modules imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 1. DEFINE CUSTOM CLASSIFIER (Required for Phi-4)\n",
        "# ===================================================================\n",
        "class GPTSequenceClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        hidden_size = base_model.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels, bias=True)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, **kwargs)\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "        pooled_output = last_hidden_state[:, -1, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.functional.cross_entropy(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
      ],
      "metadata": {
        "id": "L5P4Xs-sTJSo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2. LOAD MODELS AND TOKENIZERS\n",
        "# ===================================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Model 1: Equation Extractor (Gemma-3 with Unsloth) ---\n",
        "print(\"\\nLoading Equation Extraction Model...\")\n",
        "extractor_adapter_repo = \"arvindsuresh-math/gemma-3-1b-equation-extractor-lora\"\n",
        "base_gemma_model = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "\n",
        "gemma_model, gemma_tokenizer = FastModel.from_pretrained(\n",
        "    model_name=base_gemma_model,\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "gemma_model = PeftModel.from_pretrained(gemma_model, extractor_adapter_repo)\n",
        "print(\"✅ Equation Extraction Model loaded.\")\n",
        "\n",
        "\n",
        "# --- Model 2: Conceptual Error Classifier (Phi-4) ---\n",
        "print(\"\\nLoading Conceptual Error Classifier Model...\")\n",
        "classifier_adapter_repo = \"arvindsuresh-math/phi-4-error-binary-classifier\"\n",
        "base_phi_model = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "DTYPE = torch.bfloat16\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=DTYPE\n",
        "    )\n",
        "classifier_backbone_base = AutoModelForCausalLM.from_pretrained(\n",
        "    base_phi_model,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "classifier_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_phi_model,\n",
        "    trust_remote_code=True\n",
        "    )\n",
        "classifier_tokenizer.padding_side = \"left\"\n",
        "if classifier_tokenizer.pad_token is None:\n",
        "    classifier_tokenizer.pad_token = classifier_tokenizer.eos_token\n",
        "\n",
        "classifier_backbone_peft = PeftModel.from_pretrained(\n",
        "    classifier_backbone_base,\n",
        "    classifier_adapter_repo\n",
        "    )\n",
        "classifier_model = GPTSequenceClassifier(classifier_backbone_peft, num_labels=2)\n",
        "\n",
        "# Download and load the custom classifier head's state dictionary\n",
        "classifier_head_path = hf_hub_download(repo_id=classifier_adapter_repo, filename=\"classifier_head.pth\")\n",
        "classifier_model.classifier.load_state_dict(torch.load(classifier_head_path, map_location=device))\n",
        "\n",
        "classifier_model.to(device)\n",
        "classifier_model = classifier_model.to(torch.bfloat16)\n",
        "\n",
        "classifier_model.eval() # Set model to evaluation mode\n",
        "print(\"✅ Conceptual Error Classifier Model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "c72b9179ab97474ea66084d4f88fd0de",
            "8bd42e5130cf4999b74ef95a8102cbf5",
            "8abb6e0a3d564973a0e0fa580f8d6ecf",
            "a5fabeab016341e8ad20c27fcf21badb",
            "6e3e4115744545268b27efbc348ca775",
            "8c46c71a3c674f2b9803b996f5e803eb",
            "f6e6f046dd52496b817124500a3b2a25",
            "2545d5ad72fb44d5acf50eaf39ca1a8e",
            "0f86edd42fb7455ea5268357dc808081",
            "4415f8c2019b419c8d1ed761f8f769ca",
            "867904b5fb17458fbd5bdbdd55fb29a8"
          ]
        },
        "id": "Ynp6tx1aTEjf",
        "outputId": "1a946f0d-1fb7-46f0-8ddc-3c92306e5678"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading Equation Extraction Model...\n",
            "==((====))==  Unsloth 2025.8.4: Fast Gemma3 patching. Transformers: 4.53.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:36: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Equation Extraction Model loaded.\n",
            "\n",
            "Loading Conceptual Error Classifier Model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c72b9179ab97474ea66084d4f88fd0de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Conceptual Error Classifier Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts ---\n",
        "\n",
        "# --- Prompts ---\n",
        "EXTRACTOR_SYSTEM_PROMPT = \\\n",
        "\"\"\"[ROLE]\n",
        "You are an expert at parsing mathematical solutions.\n",
        "[TASK]\n",
        "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
        "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
        "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
        "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation. Do not include any surrounding text, units (like `/hour`), or currency symbols (like `$`).\n",
        "- **RULE 3: Use Standard Operators.** Always use `*` for multiplication. Never use `x`.\n",
        "[RESPONSE FORMAT]\n",
        "Your response must be ONLY a single, valid JSON object, adhering strictly to these rules:\n",
        "For each line of the solution, create a key-value pair.\n",
        "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
        "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
        "- If a line contains no calculation, the value must be an empty string.\n",
        "\"\"\"\n",
        "\n",
        "CLASSIFIER_SYSTEM_PROMPT = \\\n",
        "\"\"\"You are a mathematics tutor.\n",
        "You will be given a math word problem and a solution written by a student.\n",
        "Carefully analyze the problem and solution LINE-BY-LINE and determine whether there are any errors in the solution.\"\"\"\n",
        "\n",
        "# --- Example 1 ---\n",
        "FEW_SHOT_EXAMPLE_1_SOLUTION = {\n",
        "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
        "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
        "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
        "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
        "  \"FA\": \"361\"\n",
        "}\n",
        "\n",
        "FEW_SHOT_EXAMPLE_1_EQUATIONS = {\n",
        "  \"L1\": \"(2/100)*90=1.8\",\n",
        "  \"L2\": \"(2/100)*60=1.2\",\n",
        "  \"L3\": \"90+1.8+1.2=39\",\n",
        "  \"L4\": \"400-39=361\",\n",
        "  \"FA\": \"\"\n",
        "}\n",
        "\n",
        "\n",
        "# --- Example 2 ---\n",
        "FEW_SHOT_EXAMPLE_2_SOLUTION = {\n",
        "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
        "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
        "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
        "  \"FA\": \"240\"\n",
        "}\n",
        "\n",
        "FEW_SHOT_EXAMPLE_2_EQUATIONS = {\n",
        "  \"L1\": \"24/2=12\",\n",
        "  \"L2\": \"240/12=20\",\n",
        "  \"L3\": \"12*20=240.00\",\n",
        "  \"FA\": \"\"\n",
        "}\n",
        "\n",
        "def create_extractor_messages(solution_json_str: str) -> list:\n",
        "    \"\"\"\n",
        "    Returns a list of dictionaries representing the conversation history for the prompt.\n",
        "    \"\"\"\n",
        "    # Start with the constant few-shot examples defined globally\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{EXTRACTOR_SYSTEM_PROMPT}\\n\\n### Solution:\\n{json.dumps(FEW_SHOT_EXAMPLE_1_SOLUTION, indent=2)}\"},\n",
        "        {\"role\": \"assistant\", \"content\": json.dumps(FEW_SHOT_EXAMPLE_1_EQUATIONS, indent=2)},\n",
        "        {\"role\": \"user\", \"content\": f\"### Solution:\\n{json.dumps(FEW_SHOT_EXAMPLE_2_SOLUTION, indent=2)}\"},\n",
        "        {\"role\": \"assistant\", \"content\": json.dumps(FEW_SHOT_EXAMPLE_2_EQUATIONS, indent=2)},\n",
        "    ]\n",
        "\n",
        "    # Add the final user query to the end of the conversation\n",
        "    final_user_prompt = f\"### Solution:\\n{solution_json_str}\"\n",
        "    messages.append({\"role\": \"user\", \"content\": final_user_prompt})\n",
        "\n",
        "    return messages"
      ],
      "metadata": {
        "id": "DBEhNWaWouKo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "def format_solution_into_json_str(solution_text: str) -> str:\n",
        "    lines = [line.strip() for line in solution_text.strip().split('\\n') if line.strip()]\n",
        "    final_answer = \"\"\n",
        "    if lines and \"FINAL ANSWER:\" in lines[-1].upper():\n",
        "        final_answer = lines[-1][len(\"FINAL ANSWER:\"):].strip()\n",
        "        lines = lines[:-1]\n",
        "    solution_dict = {f\"L{i+1}\": line for i, line in enumerate(lines)}\n",
        "    solution_dict[\"FA\"] = final_answer\n",
        "    return json.dumps(solution_dict, indent=2)\n",
        "\n",
        "def sanitize_equation_string(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Enhanced version with your requested simplified parenthesis logic.\n",
        "    \"\"\"\n",
        "    if not isinstance(expression, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Your requested parenthesis logic:\n",
        "    if expression.count('(') > expression.count(')') and expression.startswith('('):\n",
        "        expression = expression[1:]\n",
        "    elif expression.count(')') > expression.count('(') and expression.endswith(')'):\n",
        "        expression = expression[:-1]\n",
        "\n",
        "    sanitized = expression.replace(' ', '')\n",
        "    sanitized = sanitized.replace('x', '*').replace('×', '*')\n",
        "    sanitized = re.sub(r'/([a-zA-Z]+)', '', sanitized)\n",
        "    sanitized = re.sub(r'[^\\d.()+\\-*/=]', '', sanitized)\n",
        "    return sanitized\n",
        "\n",
        "def evaluate_equations(eq_dict: dict, sol_dict: dict):\n",
        "    \"\"\"\n",
        "    Evaluates extracted equations and returns a more detailed dictionary for\n",
        "    building clearer explanations.\n",
        "    \"\"\"\n",
        "    for key, eq_str in eq_dict.items():\n",
        "        if not eq_str or \"=\" not in eq_str:\n",
        "            continue\n",
        "        try:\n",
        "            sanitized_eq = sanitize_equation_string(eq_str)\n",
        "\n",
        "            if not sanitized_eq or \"=\" not in sanitized_eq:\n",
        "                continue\n",
        "\n",
        "            lhs, rhs_str = sanitized_eq.split('=', 1)\n",
        "\n",
        "            if not lhs or not rhs_str:\n",
        "                continue\n",
        "\n",
        "            lhs_val = eval(lhs, {\"__builtins__\": None}, {})\n",
        "            rhs_val = eval(rhs_str, {\"__builtins__\": None}, {})\n",
        "\n",
        "            if not math.isclose(lhs_val, rhs_val, rel_tol=1e-2):\n",
        "                correct_rhs_val = round(lhs_val, 4)\n",
        "                correct_rhs_str = f\"{correct_rhs_val:.4f}\".rstrip('0').rstrip('.')\n",
        "\n",
        "                # Return a more detailed dictionary for better explanations\n",
        "                return {\n",
        "                    \"error\": True,\n",
        "                    \"line_key\": key,\n",
        "                    \"line_text\": sol_dict.get(key, \"N/A\"),\n",
        "                    \"original_flawed_calc\": eq_str, # The raw model output\n",
        "                    \"sanitized_lhs\": lhs,           # The clean left side\n",
        "                    \"original_rhs\": rhs_str,        # The clean right side\n",
        "                    \"correct_rhs\": correct_rhs_str, # The correct right side\n",
        "                }\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return {\"error\": False}\n",
        "\n",
        "def extract_json_from_response(response: str) -> dict:\n",
        "    \"\"\"\n",
        "    (Bulletproof Version)\n",
        "    Manually parses the model's output using regex instead of relying on\n",
        "    perfectly-formed JSON. This is robust to syntax errors from the LLM.\n",
        "    \"\"\"\n",
        "    # Regex to find all keys like \"L1\", \"L2\", \"FA\", etc.\n",
        "    keys = re.findall(r'\"(L\\d+|FA)\"\\s*:', response)\n",
        "\n",
        "    # Regex to find all values associated with the keys.\n",
        "    # It looks for text enclosed in double quotes.\n",
        "    values = re.findall(r':\\s*\"([^\"]*)\"', response)\n",
        "\n",
        "    # If the number of keys and values doesn't match, something is very wrong.\n",
        "    # This is a safety check.\n",
        "    if len(keys) != len(values):\n",
        "        # Fallback for cases where the format is extremely broken\n",
        "        if not keys and not values:\n",
        "            return {} # Truly empty\n",
        "        # Try to at least salvage something if possible\n",
        "        min_len = min(len(keys), len(values))\n",
        "        return dict(zip(keys[:min_len], values[:min_len]))\n",
        "\n",
        "    # Combine the extracted keys and values into a dictionary\n",
        "    return dict(zip(keys, values))\n",
        "\n",
        "# def extract_json_from_response(response: str) -> dict:\n",
        "#     match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL)\n",
        "#     json_str = match.group(1) if match else re.search(r'(\\{.*?\\})', response, re.DOTALL)\n",
        "#     if not json_str: return {}\n",
        "#     try:\n",
        "#         return json.loads(json_str.group(0) if hasattr(json_str, 'group') else json_str)\n",
        "#     except (json.JSONDecodeError, AttributeError): return {}"
      ],
      "metadata": {
        "id": "ko7W_ooyTFgJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_analysis_pipeline(math_question, proposed_solution):\n",
        "    \"\"\"\n",
        "    Notebook version of the analysis pipeline for testing.\n",
        "    Prints progress and returns a final dictionary.\n",
        "    \"\"\"\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Testing problem: {math_question[:50]}...\")\n",
        "\n",
        "    # --- STAGE 1: COMPUTATIONAL CHECK ---\n",
        "    print(\"Step 1: Extracting equations...\")\n",
        "    solution_json_str = format_solution_into_json_str(proposed_solution)\n",
        "    messages = [{\"role\": \"user\", \"content\": f\"{EXTRACTOR_SYSTEM_PROMPT}\\n\\n### Solution:\\n{solution_json_str}\"}]\n",
        "    prompt = gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = gemma_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    outputs = gemma_model.generate(**inputs, max_new_tokens=300, use_cache=True)\n",
        "    extracted_text = gemma_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "    extracted_eq_dict = extract_json_from_response(extracted_text)\n",
        "    print(f\"  > Extracted equations: {json.dumps(extracted_eq_dict)}\")\n",
        "\n",
        "    computational_error = evaluate_equations(extracted_eq_dict, json.loads(solution_json_str))\n",
        "\n",
        "    if computational_error[\"error\"]:\n",
        "        explanation = (\n",
        "            f\"A computational error was found.\\n\"\n",
        "            f\"On line: \\\"{computational_error['line_text']}\\\"\\n\"\n",
        "            f\"The calculation was: {computational_error['flawed_calc']}\\n\"\n",
        "            f\"The correct calculation should be: {computational_error['correct_calc']}\"\n",
        "        )\n",
        "        return {\"classification\": \"Computational Error\", \"confidence\": \"100%\", \"explanation\": explanation}\n",
        "\n",
        "    # --- STAGE 2: CONCEPTUAL CHECK ---\n",
        "    print(\"\\nStep 2: Checking for conceptual errors...\")\n",
        "    input_text = f\"{CLASSIFIER_SYSTEM_PROMPT}\\n\\n### Problem:\\n{math_question}\\n\\n### Answer:\\n{proposed_solution}\"\n",
        "    inputs = classifier_tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = classifier_model(**inputs)\n",
        "        logits = outputs[\"logits\"]\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
        "        print(f\"  > Raw probabilities [Correct, Flawed]: {probs}\")\n",
        "\n",
        "    is_flawed_prob = probs[1]\n",
        "    is_correct_prob = probs[0]\n",
        "\n",
        "    if is_flawed_prob > 0.5:\n",
        "        return {\"classification\": \"Conceptual Error\", \"confidence\": f\"{is_flawed_prob:.2%}\", \"explanation\": \"Logic or setup appears to have a conceptual error.\"}\n",
        "    else:\n",
        "        return {\"classification\": \"Correct\", \"confidence\": f\"{is_correct_prob:.2%}\", \"explanation\": \"Solution appears correct.\"}"
      ],
      "metadata": {
        "id": "UFB3JRXdTvgs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_analysis_pipeline(math_question, proposed_solution):\n",
        "    \"\"\"\n",
        "    (Complete, Refactored Version)\n",
        "    Orchestrates the pipeline by calling helper functions and printing the\n",
        "    inputs/outputs at each stage for clear debugging.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"🕵️  STARTING DEBUG FOR: {math_question[:60]}...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- STAGE 1: COMPUTATIONAL CHECK (GEMMA MODEL) ---\n",
        "    # ===================================================================\n",
        "    stage1_start_time = time.monotonic()\n",
        "    print(\"\\n--- STAGE 1: Equation Extraction (Gemma) ---\")\n",
        "\n",
        "    # [1.1] Format the solution\n",
        "    print(\"\\n[1.1] Calling format_solution_into_json_str...\")\n",
        "    solution_json_str = format_solution_into_json_str(proposed_solution)\n",
        "    solution_dict = json.loads(solution_json_str)\n",
        "    print(\"  > Output:\")\n",
        "    print(solution_json_str)\n",
        "\n",
        "    # [1.2] Build the message list using the new helper function\n",
        "    print(\"\\n[1.2] Calling create_extractor_messages...\")\n",
        "    messages = create_extractor_messages(solution_json_str)\n",
        "    print(f\"  > Output: Generated a list of {len(messages)} messages.\")\n",
        "\n",
        "    # [1.3] Apply the chat template to create the full prompt string\n",
        "    prompt = gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    print(\"\\n[1.3] Full Prompt for Gemma Model (with few-shot examples):\")\n",
        "    print(\"--------------------\")\n",
        "    print(prompt)\n",
        "    print(\"--------------------\")\n",
        "\n",
        "    # [1.4] Run inference with the Gemma model\n",
        "    inputs = gemma_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    outputs = gemma_model.generate(**inputs, max_new_tokens=300, use_cache=True)\n",
        "    extracted_text = gemma_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"\\n[1.4] Raw Text Output from Gemma Model:\")\n",
        "    print(\"--------------------\")\n",
        "    print(extracted_text)\n",
        "    print(\"--------------------\")\n",
        "\n",
        "    # [1.5] Parse the raw text using the robust regex-based function\n",
        "    print(\"\\n[1.5] Calling extract_json_from_response...\")\n",
        "    extracted_eq_dict = extract_json_from_response(extracted_text)\n",
        "    print(\"  > Output (Parsed JSON dict):\")\n",
        "    print(json.dumps(extracted_eq_dict, indent=2))\n",
        "\n",
        "    # [1.6] Filter out equations for lines that contain no digits\n",
        "    print(\"\\n[1.6] Filtering out equations for non-numeric lines:\")\n",
        "    final_eq_to_eval = {}\n",
        "    for key, eq_str in extracted_eq_dict.items():\n",
        "        original_line = solution_dict.get(key, \"\")\n",
        "        if any(char.isdigit() for char in original_line):\n",
        "            final_eq_to_eval[key] = eq_str\n",
        "        else:\n",
        "            print(f\"  > Line '{key}' has no digits. Discarding extracted equation: '{eq_str}'\")\n",
        "    print(\"  > Final equations to be evaluated:\", json.dumps(final_eq_to_eval, indent=2))\n",
        "\n",
        "    # [1.7] Evaluate the final, filtered set of equations\n",
        "    print(\"\\n[1.7] Calling evaluate_equations...\")\n",
        "    computational_error = evaluate_equations(final_eq_to_eval, solution_dict)\n",
        "    stage1_end_time = time.monotonic()\n",
        "    print(f\"  > Output (Latency: {stage1_end_time - stage1_start_time:.2f}s):\")\n",
        "    print(f\"  > {computational_error}\")\n",
        "\n",
        "    # [1.8] Check the result and exit if a computational error was found\n",
        "    if computational_error[\"error\"]:\n",
        "        print(\"\\n  > 🔴 Error Found!\")\n",
        "        lhs, rhs, correct_rhs = computational_error['sanitized_lhs'], computational_error['original_rhs'], computational_error['correct_rhs']\n",
        "        explanation = f\"A computational error was found.\\nOn line: \\\"{computational_error['line_text']}\\\"\\nThe student wrote '{lhs} = {rhs}', but the correct result of '{lhs}' is {correct_rhs}.\"\n",
        "        final_result = {\"classification\": \"Computational Error\", \"confidence\": \"100%\", \"explanation\": explanation}\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"🏁 FINAL RESULT: {final_result['classification']}\")\n",
        "        print(\"=\"*80 + \"\\n\\n\")\n",
        "        return final_result\n",
        "    else:\n",
        "        print(\"\\n  > ✅ All calculations are correct. Proceeding to Stage 2.\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- STAGE 2: CONCEPTUAL CHECK (PHI-4 MODEL) ---\n",
        "    # ===================================================================\n",
        "    stage2_start_time = time.monotonic()\n",
        "    print(\"\\n\\n--- STAGE 2: Conceptual Check (Phi-4) ---\")\n",
        "\n",
        "    # [2.1] Create the prompt for the Phi-4 model\n",
        "    input_text = f\"{CLASSIFIER_SYSTEM_PROMPT}\\n\\n### Problem:\\n{math_question}\\n\\n### Answer:\\n{proposed_solution}\"\n",
        "    print(\"\\n[2.1] Full Prompt for Phi-4 Model:\")\n",
        "    print(\"--------------------\")\n",
        "    print(input_text)\n",
        "    print(\"--------------------\")\n",
        "\n",
        "    # [2.2] Run inference with the Phi-4 model\n",
        "    inputs = classifier_tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = classifier_model(**inputs)\n",
        "        logits = outputs[\"logits\"]\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
        "\n",
        "    # [2.3] Display the model's raw outputs\n",
        "    print(f\"\\n[2.3] Raw Logits from Phi-4 Model: {logits.to(torch.float32).cpu().numpy()}\")\n",
        "    print(f\"\\n[2.4] Softmax Probabilities [Correct, Flawed]: {probs}\")\n",
        "\n",
        "    # [2.5] Determine the final classification based on the probabilities\n",
        "    stage2_end_time = time.monotonic()\n",
        "    print(f\"\\n[2.5] Final Decision Logic (Latency: {stage2_end_time - stage2_start_time:.2f}s):\")\n",
        "\n",
        "    is_flawed_prob = probs[1]\n",
        "    is_correct_prob = probs[0]\n",
        "\n",
        "    if is_flawed_prob > 0.5:\n",
        "        print(f\"  > Flawed probability ({is_flawed_prob:.2%}) is > 50%.\")\n",
        "        final_result = {\"classification\": \"Conceptual Error\", \"confidence\": f\"{is_flawed_prob:.2%}\", \"explanation\": \"Logic or setup appears to have a conceptual error.\"}\n",
        "    else:\n",
        "        print(f\"  > Correct probability ({is_correct_prob:.2%}) is > 50%.\")\n",
        "        final_result = {\"classification\": \"Correct\", \"confidence\": f\"{is_correct_prob:.2%}\", \"explanation\": \"Solution appears correct.\"}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"🏁 FINAL RESULT: {final_result['classification']}\")\n",
        "    print(\"=\"*80 + \"\\n\\n\")\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "rCrJoCH2dnOj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"Computational Error\",\n",
        "        \"question\": \"John has three apples and Mary has seven, how many apples do they have together?\",\n",
        "        \"solution\": \"They have 7 + 3 = 11 apples.\\nFINAL ANSWER: 11\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Correct Solution\",\n",
        "        \"question\": \"A grocery store sells apples for $0.50 each and oranges for $0.75 each. If a customer buys 10 apples and 5 oranges, what is the total cost?\",\n",
        "        \"solution\": \"Apple cost: 10 * 0.50 = 5.00\\nOrange cost: 5 * 0.75 = 3.75\\nTotal cost: 5.00 + 3.75 = 8.75\\nFINAL ANSWER: 8.75\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Conceptual Error\",\n",
        "        \"question\": \"A car travels at 60 miles per hour. How long will it take to travel 180 miles?\",\n",
        "        \"solution\": \"Time = Speed / Distance\\nTime = 60 / 180 = 0.33 hours\\nFINAL ANSWER: 0.33\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    result = debug_analysis_pipeline(test[\"question\"], test[\"solution\"])\n",
        "    print(\"\\n--- FINAL RESULT ---\")\n",
        "    print(f\"Case: {test['name']}\")\n",
        "    print(f\"Classification: {result['classification']}\")\n",
        "    print(f\"Confidence: {result['confidence']}\")\n",
        "    print(f\"Explanation: {result['explanation']}\")\n",
        "    print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypuadbs-Tya5",
        "outputId": "d67dd9db-325a-43c5-fc04-b91b0ad90705"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🕵️  STARTING DEBUG FOR: John has three apples and Mary has seven, how many apples do...\n",
            "================================================================================\n",
            "\n",
            "--- STAGE 1: Equation Extraction (Gemma) ---\n",
            "\n",
            "[1.1] Calling format_solution_into_json_str...\n",
            "  > Output:\n",
            "{\n",
            "  \"L1\": \"They have 7 + 3 = 11 apples.\",\n",
            "  \"FA\": \"11\"\n",
            "}\n",
            "\n",
            "[1.2] Calling create_extractor_messages...\n",
            "  > Output: Generated a list of 5 messages.\n",
            "\n",
            "[1.3] Full Prompt for Gemma Model (with few-shot examples):\n",
            "--------------------\n",
            "<bos><start_of_turn>user\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "[TASK]\n",
            "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
            "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
            "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
            "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation. Do not include any surrounding text, units (like `/hour`), or currency symbols (like `$`).\n",
            "- **RULE 3: Use Standard Operators.** Always use `*` for multiplication. Never use `x`.\n",
            "[RESPONSE FORMAT]\n",
            "Your response must be ONLY a single, valid JSON object, adhering strictly to these rules:\n",
            "For each line of the solution, create a key-value pair.\n",
            "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
            "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
            "- If a line contains no calculation, the value must be an empty string.\n",
            "\n",
            "\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
            "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
            "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
            "  \"FA\": \"240\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"24/2=12\",\n",
            "  \"L2\": \"240/12=20\",\n",
            "  \"L3\": \"12*20=240.00\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"They have 7 + 3 = 11 apples.\",\n",
            "  \"FA\": \"11\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "--------------------\n",
            "\n",
            "[1.4] Raw Text Output from Gemma Model:\n",
            "--------------------\n",
            "{\n",
            "  \"L1\": \"7+3=11\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "--------------------\n",
            "\n",
            "[1.5] Calling extract_json_from_response...\n",
            "  > Output (Parsed JSON dict):\n",
            "{\n",
            "  \"L1\": \"7+3=11\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.6] Filtering out equations for non-numeric lines:\n",
            "  > Final equations to be evaluated: {\n",
            "  \"L1\": \"7+3=11\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.7] Calling evaluate_equations...\n",
            "  > Output (Latency: 3.77s):\n",
            "  > {'error': True, 'line_key': 'L1', 'line_text': 'They have 7 + 3 = 11 apples.', 'original_flawed_calc': '7+3=11', 'sanitized_lhs': '7+3', 'original_rhs': '11', 'correct_rhs': '10'}\n",
            "\n",
            "  > 🔴 Error Found!\n",
            "\n",
            "================================================================================\n",
            "🏁 FINAL RESULT: Computational Error\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "--- FINAL RESULT ---\n",
            "Case: Computational Error\n",
            "Classification: Computational Error\n",
            "Confidence: 100%\n",
            "Explanation: A computational error was found.\n",
            "On line: \"They have 7 + 3 = 11 apples.\"\n",
            "The student wrote '7+3 = 11', but the correct result of '7+3' is 10.\n",
            "==================================================\n",
            "\n",
            "================================================================================\n",
            "🕵️  STARTING DEBUG FOR: A grocery store sells apples for $0.50 each and oranges for ...\n",
            "================================================================================\n",
            "\n",
            "--- STAGE 1: Equation Extraction (Gemma) ---\n",
            "\n",
            "[1.1] Calling format_solution_into_json_str...\n",
            "  > Output:\n",
            "{\n",
            "  \"L1\": \"Apple cost: 10 * 0.50 = 5.00\",\n",
            "  \"L2\": \"Orange cost: 5 * 0.75 = 3.75\",\n",
            "  \"L3\": \"Total cost: 5.00 + 3.75 = 8.75\",\n",
            "  \"FA\": \"8.75\"\n",
            "}\n",
            "\n",
            "[1.2] Calling create_extractor_messages...\n",
            "  > Output: Generated a list of 5 messages.\n",
            "\n",
            "[1.3] Full Prompt for Gemma Model (with few-shot examples):\n",
            "--------------------\n",
            "<bos><start_of_turn>user\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "[TASK]\n",
            "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
            "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
            "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
            "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation. Do not include any surrounding text, units (like `/hour`), or currency symbols (like `$`).\n",
            "- **RULE 3: Use Standard Operators.** Always use `*` for multiplication. Never use `x`.\n",
            "[RESPONSE FORMAT]\n",
            "Your response must be ONLY a single, valid JSON object, adhering strictly to these rules:\n",
            "For each line of the solution, create a key-value pair.\n",
            "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
            "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
            "- If a line contains no calculation, the value must be an empty string.\n",
            "\n",
            "\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
            "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
            "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
            "  \"FA\": \"240\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"24/2=12\",\n",
            "  \"L2\": \"240/12=20\",\n",
            "  \"L3\": \"12*20=240.00\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"Apple cost: 10 * 0.50 = 5.00\",\n",
            "  \"L2\": \"Orange cost: 5 * 0.75 = 3.75\",\n",
            "  \"L3\": \"Total cost: 5.00 + 3.75 = 8.75\",\n",
            "  \"FA\": \"8.75\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "--------------------\n",
            "\n",
            "[1.4] Raw Text Output from Gemma Model:\n",
            "--------------------\n",
            "{\n",
            "  \"L1\": \"10*0.50=5.00\",\n",
            "  \"L2\": \"5*0.75=3.75\",\n",
            "  \"L3\": \"5+3.75=8.75\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "--------------------\n",
            "\n",
            "[1.5] Calling extract_json_from_response...\n",
            "  > Output (Parsed JSON dict):\n",
            "{\n",
            "  \"L1\": \"10*0.50=5.00\",\n",
            "  \"L2\": \"5*0.75=3.75\",\n",
            "  \"L3\": \"5+3.75=8.75\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.6] Filtering out equations for non-numeric lines:\n",
            "  > Final equations to be evaluated: {\n",
            "  \"L1\": \"10*0.50=5.00\",\n",
            "  \"L2\": \"5*0.75=3.75\",\n",
            "  \"L3\": \"5+3.75=8.75\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.7] Calling evaluate_equations...\n",
            "  > Output (Latency: 10.66s):\n",
            "  > {'error': False}\n",
            "\n",
            "  > ✅ All calculations are correct. Proceeding to Stage 2.\n",
            "\n",
            "\n",
            "--- STAGE 2: Conceptual Check (Phi-4) ---\n",
            "\n",
            "[2.1] Full Prompt for Phi-4 Model:\n",
            "--------------------\n",
            "You are a mathematics tutor.\n",
            "You will be given a math word problem and a solution written by a student.\n",
            "Carefully analyze the problem and solution LINE-BY-LINE and determine whether there are any errors in the solution.\n",
            "\n",
            "### Problem:\n",
            "A grocery store sells apples for $0.50 each and oranges for $0.75 each. If a customer buys 10 apples and 5 oranges, what is the total cost?\n",
            "\n",
            "### Answer:\n",
            "Apple cost: 10 * 0.50 = 5.00\n",
            "Orange cost: 5 * 0.75 = 3.75\n",
            "Total cost: 5.00 + 3.75 = 8.75\n",
            "FINAL ANSWER: 8.75\n",
            "--------------------\n",
            "\n",
            "[2.3] Raw Logits from Phi-4 Model: [[ 2.421875 -3.546875]]\n",
            "\n",
            "[2.4] Softmax Probabilities [Correct, Flawed]: [0.99609375, 0.0025482177734375]\n",
            "\n",
            "[2.5] Final Decision Logic (Latency: 0.10s):\n",
            "  > Correct probability (99.61%) is > 50%.\n",
            "\n",
            "================================================================================\n",
            "🏁 FINAL RESULT: Correct\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "--- FINAL RESULT ---\n",
            "Case: Correct Solution\n",
            "Classification: Correct\n",
            "Confidence: 99.61%\n",
            "Explanation: Solution appears correct.\n",
            "==================================================\n",
            "\n",
            "================================================================================\n",
            "🕵️  STARTING DEBUG FOR: A car travels at 60 miles per hour. How long will it take to...\n",
            "================================================================================\n",
            "\n",
            "--- STAGE 1: Equation Extraction (Gemma) ---\n",
            "\n",
            "[1.1] Calling format_solution_into_json_str...\n",
            "  > Output:\n",
            "{\n",
            "  \"L1\": \"Time = Speed / Distance\",\n",
            "  \"L2\": \"Time = 60 / 180 = 0.33 hours\",\n",
            "  \"FA\": \"0.33\"\n",
            "}\n",
            "\n",
            "[1.2] Calling create_extractor_messages...\n",
            "  > Output: Generated a list of 5 messages.\n",
            "\n",
            "[1.3] Full Prompt for Gemma Model (with few-shot examples):\n",
            "--------------------\n",
            "<bos><start_of_turn>user\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "[TASK]\n",
            "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
            "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
            "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
            "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation. Do not include any surrounding text, units (like `/hour`), or currency symbols (like `$`).\n",
            "- **RULE 3: Use Standard Operators.** Always use `*` for multiplication. Never use `x`.\n",
            "[RESPONSE FORMAT]\n",
            "Your response must be ONLY a single, valid JSON object, adhering strictly to these rules:\n",
            "For each line of the solution, create a key-value pair.\n",
            "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
            "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
            "- If a line contains no calculation, the value must be an empty string.\n",
            "\n",
            "\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
            "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
            "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
            "  \"FA\": \"240\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"24/2=12\",\n",
            "  \"L2\": \"240/12=20\",\n",
            "  \"L3\": \"12*20=240.00\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"Time = Speed / Distance\",\n",
            "  \"L2\": \"Time = 60 / 180 = 0.33 hours\",\n",
            "  \"FA\": \"0.33\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "--------------------\n",
            "\n",
            "[1.4] Raw Text Output from Gemma Model:\n",
            "--------------------\n",
            "{\n",
            "  \"L1\": \"9/60=0.16\",\n",
            "  \"L2\": \"60/180=0.33\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "--------------------\n",
            "\n",
            "[1.5] Calling extract_json_from_response...\n",
            "  > Output (Parsed JSON dict):\n",
            "{\n",
            "  \"L1\": \"9/60=0.16\",\n",
            "  \"L2\": \"60/180=0.33\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.6] Filtering out equations for non-numeric lines:\n",
            "  > Line 'L1' has no digits. Discarding extracted equation: '9/60=0.16'\n",
            "  > Final equations to be evaluated: {\n",
            "  \"L2\": \"60/180=0.33\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "[1.7] Calling evaluate_equations...\n",
            "  > Output (Latency: 7.20s):\n",
            "  > {'error': False}\n",
            "\n",
            "  > ✅ All calculations are correct. Proceeding to Stage 2.\n",
            "\n",
            "\n",
            "--- STAGE 2: Conceptual Check (Phi-4) ---\n",
            "\n",
            "[2.1] Full Prompt for Phi-4 Model:\n",
            "--------------------\n",
            "You are a mathematics tutor.\n",
            "You will be given a math word problem and a solution written by a student.\n",
            "Carefully analyze the problem and solution LINE-BY-LINE and determine whether there are any errors in the solution.\n",
            "\n",
            "### Problem:\n",
            "A car travels at 60 miles per hour. How long will it take to travel 180 miles?\n",
            "\n",
            "### Answer:\n",
            "Time = Speed / Distance\n",
            "Time = 60 / 180 = 0.33 hours\n",
            "FINAL ANSWER: 0.33\n",
            "--------------------\n",
            "\n",
            "[2.3] Raw Logits from Phi-4 Model: [[-7.      5.9375]]\n",
            "\n",
            "[2.4] Softmax Probabilities [Correct, Flawed]: [2.3990869522094727e-06, 1.0]\n",
            "\n",
            "[2.5] Final Decision Logic (Latency: 0.10s):\n",
            "  > Flawed probability (100.00%) is > 50%.\n",
            "\n",
            "================================================================================\n",
            "🏁 FINAL RESULT: Conceptual Error\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "--- FINAL RESULT ---\n",
            "Case: Conceptual Error\n",
            "Classification: Conceptual Error\n",
            "Confidence: 100.00%\n",
            "Explanation: Logic or setup appears to have a conceptual error.\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "\n",
        "def run_batch_evaluation(df: pd.DataFrame, batch_size: int = 128) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a full, timed, batched evaluation of the two-stage pipeline.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame with columns 'index', 'question', 'correct_answer',\n",
        "            'wrong_answer', and 'error_type'.\n",
        "        batch_size: The number of samples to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing detailed results and metadata for each test sample.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"🚀 Starting Batch Evaluation...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    overall_start_time = time.monotonic()\n",
        "\n",
        "    # 1. Double the test set by preparing correct and incorrect samples\n",
        "    print(f\"[1/5] Preparing {len(df) * 2} test samples...\")\n",
        "    prepared_samples = []\n",
        "    for _, row in df.iterrows():\n",
        "        # Case 1: The correct solution\n",
        "        prepared_samples.append({\n",
        "            'original_index': row['index'],\n",
        "            'question': row['question'],\n",
        "            'solution': row['correct_answer'],\n",
        "            'true_label': 'Correct'\n",
        "        })\n",
        "        # Case 2: The flawed solution\n",
        "        error_map = {'comp': 'Computational Error', 'concep': 'Conceptual Error'}\n",
        "        prepared_samples.append({\n",
        "            'original_index': row['index'],\n",
        "            'question': row['question'],\n",
        "            'solution': row['wrong_answer'],\n",
        "            'true_label': error_map.get(row['error_type'], 'Unknown Error')\n",
        "        })\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- STAGE 1: BATCHED EQUATION EXTRACTION (GEMMA) ---\n",
        "    # ===================================================================\n",
        "    print(f\"[2/5] Running Stage 1 (Gemma Equation Extraction) on {len(prepared_samples)} samples...\")\n",
        "    stage1_start_time = time.monotonic()\n",
        "\n",
        "    # Generate all prompts for Stage 1 first\n",
        "    gemma_prompts = []\n",
        "    for sample in prepared_samples:\n",
        "        solution_json_str = format_solution_into_json_str(sample['solution'])\n",
        "        messages = create_extractor_messages(solution_json_str)\n",
        "        gemma_prompts.append(gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "\n",
        "    # Run batched inference\n",
        "    gemma_raw_outputs = []\n",
        "    for i in tqdm(range(0, len(gemma_prompts), batch_size), desc=\"Stage 1 Batches\"):\n",
        "        batch_prompts = gemma_prompts[i:i + batch_size]\n",
        "        inputs = gemma_tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "        outputs = gemma_model.generate(**inputs, max_new_tokens=300, use_cache=True, pad_token_id=gemma_tokenizer.pad_token_id)\n",
        "        # Decode only the newly generated tokens\n",
        "        decoded_outputs = gemma_tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        gemma_raw_outputs.extend(decoded_outputs)\n",
        "\n",
        "    stage1_latency = time.monotonic() - stage1_start_time\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- STAGE 1 POST-PROCESSING & STAGE 2 PREPARATION ---\n",
        "    # ===================================================================\n",
        "    print(f\"[3/5] Processing Stage 1 results and preparing Stage 2 batch...\")\n",
        "\n",
        "    results_data = []\n",
        "    stage2_batch = [] # This will hold only the samples that need conceptual checking\n",
        "\n",
        "    for i, sample in enumerate(prepared_samples):\n",
        "        # Initial data entry\n",
        "        result_entry = {\n",
        "            'original_index': sample['original_index'],\n",
        "            'question': sample['question'],\n",
        "            'solution': sample['solution'],\n",
        "            'true_label': sample['true_label'],\n",
        "            'gemma_raw_output': gemma_raw_outputs[i]\n",
        "        }\n",
        "\n",
        "        # Process Gemma's output\n",
        "        extracted_eq_dict = extract_json_from_response(gemma_raw_outputs[i])\n",
        "        result_entry['extracted_json_str'] = json.dumps(extracted_eq_dict)\n",
        "\n",
        "        # Filter and evaluate\n",
        "        solution_dict = json.loads(format_solution_into_json_str(sample['solution']))\n",
        "        final_eq_to_eval = {\n",
        "            k: v for k, v in extracted_eq_dict.items()\n",
        "            if any(char.isdigit() for char in solution_dict.get(k, \"\"))\n",
        "        }\n",
        "        computational_error = evaluate_equations(final_eq_to_eval, solution_dict)\n",
        "\n",
        "        # Make a decision: stop or proceed to Stage 2\n",
        "        if computational_error[\"error\"]:\n",
        "            result_entry['predicted_classification'] = 'Computational Error'\n",
        "            result_entry['pipeline_stage_stopped'] = 1\n",
        "        else:\n",
        "            result_entry['predicted_classification'] = None # Placeholder\n",
        "            result_entry['pipeline_stage_stopped'] = 2\n",
        "            # This sample needs to go to Stage 2. We store its prompt and its index in the main results list.\n",
        "            stage2_batch.append({\n",
        "                'prompt': f\"{CLASSIFIER_SYSTEM_PROMPT}\\n\\n### Problem:\\n{sample['question']}\\n\\n### Answer:\\n{sample['solution']}\",\n",
        "                'result_index': i\n",
        "            })\n",
        "\n",
        "        results_data.append(result_entry)\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- STAGE 2: BATCHED CONCEPTUAL CHECK (PHI-4) ---\n",
        "    # ===================================================================\n",
        "    print(f\"[4/5] Running Stage 2 (Phi-4 Conceptual Check) on {len(stage2_batch)} samples...\")\n",
        "    stage2_start_time = time.monotonic()\n",
        "\n",
        "    if stage2_batch:\n",
        "        phi4_prompts = [item['prompt'] for item in stage2_batch]\n",
        "        phi4_logits_list = []\n",
        "\n",
        "        for i in tqdm(range(0, len(phi4_prompts), batch_size), desc=\"Stage 2 Batches\"):\n",
        "            batch_prompts = phi4_prompts[i:i + batch_size]\n",
        "            inputs = classifier_tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = classifier_model(**inputs)\n",
        "                # Move logits to CPU immediately to free up VRAM\n",
        "                phi4_logits_list.extend(outputs['logits'].cpu())\n",
        "\n",
        "        # Update the main results list with the Stage 2 outcomes\n",
        "        for i, item in enumerate(stage2_batch):\n",
        "            result_index = item['result_index']\n",
        "            logits = phi4_logits_list[i]\n",
        "            probs = torch.softmax(logits, dim=-1).tolist()\n",
        "\n",
        "            results_data[result_index]['phi4_correct_proba'] = probs[0]\n",
        "            results_data[result_index]['phi4_flawed_proba'] = probs[1]\n",
        "            results_data[result_index]['phi4_raw_logits'] = str(logits.numpy())\n",
        "\n",
        "            if probs[1] > 0.5: # Flawed\n",
        "                results_data[result_index]['predicted_classification'] = 'Conceptual Error'\n",
        "            else: # Correct\n",
        "                results_data[result_index]['predicted_classification'] = 'Correct'\n",
        "\n",
        "    stage2_latency = time.monotonic() - stage2_start_time\n",
        "\n",
        "    # ===================================================================\n",
        "    # --- FINALIZATION ---\n",
        "    # ===================================================================\n",
        "    print(\"[5/5] Finalizing results and creating DataFrame...\")\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "\n",
        "    overall_latency = time.monotonic() - overall_start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ Batch Evaluation Complete!\")\n",
        "    print(f\"  > Total Samples Processed: {len(results_df)}\")\n",
        "    print(f\"  > Overall Time: {overall_latency:.2f} seconds\")\n",
        "    print(f\"  > Avg Time per Sample: {overall_latency / len(results_df):.3f} seconds\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  > Stage 1 Latency (Gemma): {stage1_latency:.2f} seconds\")\n",
        "    print(f\"  > Stage 2 Latency (Phi-4): {stage2_latency:.2f} seconds\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"  > Predicted Classification Counts:\")\n",
        "    print(results_df['predicted_classification'].value_counts().to_string())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "vCXxSO_Cd0Z7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/final-test-with-wrong-answers.csv')"
      ],
      "metadata": {
        "id": "LsNk4cl7rg8J"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure all your models and helper functions are defined in previous cells\n",
        "evaluation_results_df = run_batch_evaluation(df, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645,
          "referenced_widgets": [
            "eb49fa30bf474ad7894425e0cf7cffc7",
            "2b5e7fd9af884d20a9abbcb48d6bd80c",
            "809876a3b28c4854ad0fc5543fbdf2d9",
            "06a11bb6693f487492e57961565f757e",
            "fac726eb95d7400cbed602d298a3853c",
            "29d7e211da7a4154a8a2312386f16908",
            "51732895f18b4c1a972a583c62745d7f",
            "01a724e95f0747149f61edee299a1327",
            "6b76a8b443af4a80a60ebc545bf0d764",
            "4cd492636bc74a4a993c38ae34b73dfb",
            "f331971585224607aacaebbe7eb59726",
            "ba1aa60223f544c59f61ffe390e9869f",
            "98e212e16af143e2a87ac089901d3a33",
            "332aeb618e3342ff964ef2c72d5f8f0e",
            "3f175beb70cb40b2a168b1c20bb86da9",
            "b8cce527fb3b47d1a32293c062b250c1",
            "e3292bd8d55b4eab9b62a5bcb1330a79",
            "a1bf4d96896746609fad7af7b6ecacd8",
            "ef09dbb5bbb14d94a066042c0bed6449",
            "1d4b97bd94424383ab7968b9b6d556db",
            "f0658364c1da4c72aa8ea09c7dea3816",
            "c6d230478fad4d44b964783230c3ffb0"
          ]
        },
        "id": "-6hJubz-rhRQ",
        "outputId": "dcf9e77b-10c9-4692-97a5-a52f20d9f941"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🚀 Starting Batch Evaluation...\n",
            "================================================================================\n",
            "[1/5] Preparing 302 test samples...\n",
            "[2/5] Running Stage 1 (Gemma Equation Extraction) on 302 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Stage 1 Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb49fa30bf474ad7894425e0cf7cffc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3/5] Processing Stage 1 results and preparing Stage 2 batch...\n",
            "[4/5] Running Stage 2 (Phi-4 Conceptual Check) on 259 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Stage 2 Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1aa60223f544c59f61ffe390e9869f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 39.56 GiB of which 886.88 MiB is free. Process 129420 has 38.68 GiB memory in use. Of the allocated memory 37.57 GiB is allocated by PyTorch, and 605.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-471946676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make sure all your models and helper functions are defined in previous cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluation_results_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_batch_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-749445336.py\u001b[0m in \u001b[0;36mrun_batch_evaluation\u001b[0;34m(df, batch_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# Move logits to CPU immediately to free up VRAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mphi4_logits_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4035225597.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    914\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n\u001b[1;32m    635\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    637\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_mlp_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# main diff with Llama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mup_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_up_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/Linear4bit_peft_forward.py\u001b[0m in \u001b[0;36munsloth_forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactive_adapter\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_variant\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# vanilla LoRA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlora_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrequires_conversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/Linear4bit_peft_forward.py\u001b[0m in \u001b[0;36mlora_forward\u001b[0;34m(result, lora_A, lora_B, dropout, x, scaling)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# output = result + scaling * xA @ lora_B.weight.t()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     output = torch_addmm(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mxA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 39.56 GiB of which 886.88 MiB is free. Process 129420 has 38.68 GiB memory in use. Of the allocated memory 37.57 GiB is allocated by PyTorch, and 605.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the detailed output\n",
        "display(evaluation_results_df.head())\n",
        "\n",
        "# Save the results to a CSV for offline analysis in Excel, etc.\n",
        "evaluation_results_df.to_csv(\"/content/batch_evaluation_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "uDQYTzPErgrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}