{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac80d1a",
   "metadata": {},
   "source": [
    "# Project Summary: Fine-Tuning an LLM for Mathematical Problem Classification\n",
    "\n",
    "The core objective of this project is to fine-tune a small, efficient Large Language Model (LLM) to classify mathematical word problems into three distinct categories based on their solvability.\n",
    "\n",
    "The methodology is divided into three main phases:\n",
    "\n",
    "### 1. Rigorous Dataset Generation via Code Formalization\n",
    "\n",
    "The primary challenge is creating a high-quality, verifiably correct dataset. This is addressed by converting each natural language math problem (from a source like GSM8K) into a parameterized Python function.\n",
    "\n",
    "*   A powerful generator LLM is used to translate the problem's text and step-by-step solution into a generalized `solve()` function.\n",
    "*   This function acts as a formal, executable representation of the problem's underlying logic.\n",
    "*   By making the problem's numerical values the function's arguments, the logic becomes testable and easy to manipulate.\n",
    "\n",
    "### 2. Creating a Labeled Dataset with Three Solvability Classes\n",
    "\n",
    "Using the verified Python functions from Phase 1, the final labeled dataset is constructed by programmatically modifying the original problems to fit into one of three classes:\n",
    "\n",
    "*   **Class 1: Has a Unique Solution**\n",
    "    *   This is the original, verified problem where all parameters are defined, leading to a single correct answer.\n",
    "\n",
    "*   **Class 2: Has Multiple Solutions**\n",
    "    *   Generated by taking a Class 1 problem and removing a key piece of numerical information from the problem statement. This makes the problem underspecified, as different values for the now-missing parameter would lead to different valid solutions.\n",
    "\n",
    "*   **Class 0: Has No Solution**\n",
    "    *   Generated by manipulating the parameters of the Python function to yield a logically or physically absurd result (e.g., a negative count of objects) or by introducing a direct contradiction into the problem statement.\n",
    "\n",
    "### 3. Fine-Tuning the Classifier LLM\n",
    "\n",
    "The resulting dataset, with its high-confidence labels, is used to fine-tune a smaller, more efficient LLM. The final model will be trained to take a new math problem as input and output its classification (Class 1, 2, or 0), having learned the underlying patterns of solvability, ambiguity, and contradiction from the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9d639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_generator import *\n",
    "\n",
    "indices = [310, 3822, 7371]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6df8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 310\n",
      "Code String:\n",
      "def solve(\n",
      "        num_employees: int = 6, # Janet hires six employees\n",
      "        num_warehouse_workers: int = 4, # Four of them are warehouse workers\n",
      "        num_managers: int = 2, # the other two are managers\n",
      "        hourly_wage_warehouse: int = 15, # warehouse workers make $15/hour\n",
      "        hourly_wage_manager: int = 20, # managers make $20/hour\n",
      "        fica_tax_rate: float = 0.1, # FICA tax rate is 10%\n",
      "        days_per_month: int = 25, # everyone works 25 days a month\n",
      "        hours_per_day: int = 8 # everyone works 8 hours a day\n",
      "    ):\n",
      "    \"\"\"Index: 310.\n",
      "    Returns: the monthly total wage bill, including FICA taxes.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    hours_per_month = days_per_month * hours_per_day\n",
      "\n",
      "    #: L2\n",
      "    monthly_wage_warehouse = hourly_wage_warehouse * hours_per_month\n",
      "\n",
      "    #: L3\n",
      "    total_wage_warehouse = monthly_wage_warehouse * num_warehouse_workers\n",
      "\n",
      "    #: L4\n",
      "    monthly_wage_manager = hourly_wage_manager * hours_per_month\n",
      "\n",
      "    #: L5\n",
      "    total_wage_manager = monthly_wage_manager * num_managers\n",
      "\n",
      "    #: L6\n",
      "    total_wages = total_wage_warehouse + total_wage_manager\n",
      "\n",
      "    #: L7\n",
      "    fica_taxes = total_wages * fica_tax_rate\n",
      "\n",
      "    #: L8\n",
      "    grand_total = total_wages + fica_taxes\n",
      "\n",
      "    answer = grand_total # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "----------------------------------------\n",
      "Index: 3822\n",
      "Code String:\n",
      "def solve(\n",
      "        fraction_needed_to_win: float = 3/4,  # f he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him.\n",
      "        fraction_voting_for_him: float = 1/2,  # Half of the class have already said they will vote for him\n",
      "        students_thinking_about_it: int = 5,  # only 5 have said they are thinking about voting for him\n",
      "        total_students: int = 60  # Alec's class has 60 students\n",
      "):    \n",
      "    \"\"\"Index: 3822.\n",
      "    Returns: the number of votes by which Alec is short of his goal.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    students_per_quarter = total_students / 4\n",
      "\n",
      "    #: L2\n",
      "    votes_needed = students_per_quarter * 3\n",
      "\n",
      "    #: L3\n",
      "    votes_for_him = total_students * fraction_voting_for_him\n",
      "\n",
      "    #: L4\n",
      "    votes_so_far = votes_for_him + students_thinking_about_it\n",
      "\n",
      "    #: L5\n",
      "    students_not_voting_for_him = total_students - votes_so_far\n",
      "    \n",
      "    #: L6\n",
      "    new_votes = students_not_voting_for_him / 5\n",
      "\n",
      "    #: L7\n",
      "    total_votes_for_him = votes_so_far + new_votes\n",
      "\n",
      "    #: L8\n",
      "    votes_short_of_goal = votes_needed - total_votes_for_him\n",
      "\n",
      "    answer = votes_short_of_goal  # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "----------------------------------------\n",
      "Index: 7371\n",
      "Code String:\n",
      "def solve(\n",
      "    baseline_bonus: int = 500, # Karen gets a $500 bonus\n",
      "    baseline_avg_score: int = 75, # if their average score is above 75\n",
      "    extra_bonus_per_point: int = 10, # plus an extra $10 bonus for every additional point the average score increases above 75\n",
      "    tests_graded_so_far: int = 8, # So far, Karen has graded 8 tests\n",
      "    avg_so_far: int = 70, # and the average is 70\n",
      "    max_score_per_student: int = 150, # each student can have a maximum score of 150\n",
      "    desired_bonus: int = 600 # Karen wants to earn a $600 bonus\n",
      "):\n",
      "    \"\"\"Index: 7371.\n",
      "    Returns: the combined score needed in the last two tests to ensure that Karen earns a $600 bonus.\"\"\"\n",
      "    #: L1\n",
      "    extra_bonus_needed = desired_bonus - baseline_bonus\n",
      "\n",
      "    #: L2\n",
      "    extra_points_needed = extra_bonus_needed / extra_bonus_per_point\n",
      "\n",
      "    #: L3\n",
      "    target_avg_score = baseline_avg_score + extra_points_needed\n",
      "\n",
      "    #: L4\n",
      "    total_tests = tests_graded_so_far + 2\n",
      "\n",
      "    #: L5\n",
      "    total_points_needed = target_avg_score * total_tests\n",
      "\n",
      "    #: L6\n",
      "    points_earned_so_far = avg_so_far * tests_graded_so_far\n",
      "\n",
      "    #: L7\n",
      "    points_needed_last_two_tests = total_points_needed - points_earned_so_far\n",
      "\n",
      "    answer = points_needed_last_two_tests  # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "code_strings = get_code_strings(indices)\n",
    "for index, code_string in code_strings.items():\n",
    "    print(f\"Index: {index}\")\n",
    "    print(\"Code String:\")\n",
    "    print(code_string)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac10522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Index*: \n",
      "310\n",
      "\n",
      "*Question*: \n",
      "Janet hires six employees. Four of them are warehouse workers who make $15/hour, and the other two are managers who make $20/hour. Janet has to pay 10% of her workers' salaries in FICA taxes. If everyone works 25 days a month and 8 hours a day, how much does Janet owe total for their wages and taxes for one month?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"First figure out how many hours each worker works per month by multiplying the number of days they work by the number of hours a day they work: 25 days * 8 hours/day = [[25*8=200]]200 hours\", \"L2\": \"Then calculate how much one warehouse worker makes per month by multiplying their hourly rate by the number of hours they work: 200 hours * $15/hour = $[[200*15=3000]]3000\", \"L3\": \"Then multiply that number by 4 to find out how much all the warehouse workers make: $3000/worker * 4 workers = $[[3000*4=12000]]12,000\", \"L4\": \"Now multiply the hours each manager works (also 200) by their hourly wage to find out how much one manager makes per month: 200 hours * $20/hour = $[[200*20=4000]]4,000\", \"L5\": \"Now multiply one manager's wages by the number of managers (2) to find their total wage amount: $4,000/manager * 2 managers = $[[4000*2=8000]]8,000\", \"L6\": \"Now add the wages for the managers and the workers to find the total cost of the wages: $8,000 + $12,000 = $[[8000+12000=20000]]20,000\", \"L7\": \"Now multiply the total wage bill by 10% to find how much the FICA taxes are: $20,000 * .1 = $[[20000*.1=2000]]2,000\", \"L8\": \"Now add the total wage bill to the total tax amount to find the grand total: $2,000 + $20,000 = $[[2000+20000=22000]]22,000\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        num_employees: int = 6, # Janet hires six employees\n",
      "        num_warehouse_workers: int = 4, # Four of them are warehouse workers\n",
      "        num_managers: int = 2, # the other two are managers\n",
      "        hourly_wage_warehouse: int = 15, # warehouse workers make $15/hour\n",
      "        hourly_wage_manager: int = 20, # managers make $20/hour\n",
      "        fica_tax_rate: float = 0.1, # FICA tax rate is 10%\n",
      "        days_per_month: int = 25, # everyone works 25 days a month\n",
      "        hours_per_day: int = 8 # everyone works 8 hours a day\n",
      "    ):\n",
      "    \"\"\"Index: 310.\n",
      "    Returns: the monthly total wage bill, including FICA taxes.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    hours_per_month = days_per_month * hours_per_day\n",
      "\n",
      "    #: L2\n",
      "    monthly_wage_warehouse = hourly_wage_warehouse * hours_per_month\n",
      "\n",
      "    #: L3\n",
      "    total_wage_warehouse = monthly_wage_warehouse * num_warehouse_workers\n",
      "\n",
      "    #: L4\n",
      "    monthly_wage_manager = hourly_wage_manager * hours_per_month\n",
      "\n",
      "    #: L5\n",
      "    total_wage_manager = monthly_wage_manager * num_managers\n",
      "\n",
      "    #: L6\n",
      "    total_wages = total_wage_warehouse + total_wage_manager\n",
      "\n",
      "    #: L7\n",
      "    fica_taxes = total_wages * fica_tax_rate\n",
      "\n",
      "    #: L8\n",
      "    grand_total = total_wages + fica_taxes\n",
      "\n",
      "    answer = grand_total # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "example_with_code = format_prompt_query(index=310, \n",
    "                    code_strings=code_strings, \n",
    "                    with_code=True)\n",
    "print(example_with_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9aac2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Guidelines\n",
      "\n",
      "0. **Output wrapping**\n",
      "   Return the code inside a single ```python … ``` block, and nothing else.\n",
      "\n",
      "1.  **Function Naming & Docstring:** The function must be named `solve`. It must begin with a docstring that has exactly two lines:\n",
      "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
      "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
      "\n",
      "2.  **Function Arguments:** The function arguments must be derived from the 'Question' text. \n",
      "    *   Create a distinct argument for every numerical value that is directly stated in the text.\n",
      "    *   The arguments should be created **in the same order in which they appear in the question**.\n",
      "    *   **Note:** Some of these arguments may end up not being used in the function body. This is expected. Do not worry about this and leave the unused arguments in the function signature.\n",
      "\n",
      "3.  **Argument Formatting:** Each argument must include a type-hint (e.g., `int`, `float`) and a default value equal to its value in the 'Question'. You must also add a comment (`#`) next to each argument that quotes or refers to the phrase in the 'Question' it comes from. \n",
      "\n",
      "4.  **Function Body:** The body of the function should follow the logic of the provided 'Solution' dict, which contains the step-by-step solution to the problem. The keys of this dict are strings (e.g. `\"L1\"`, `\"L2\"`) which refer to the line number, and the values of the dict are the corresponding steps in the solution. \n",
      "    * For every relevant line in the 'Solution', you must include a comment in the Python code that indicates the line number (key) from the 'Solution' dict.\n",
      "    * These comments should be formatted as `#: L<n>`, where `<n>` is the line number from the 'Solution' dict.\n",
      "    * Immediately follow the comment with the Python statement that performs the calculation.\n",
      "\n",
      "5.  **Calculator Annotations:** Pay close attention to the calculator annotations (e.g., `[[25*8=200]]`) in the 'Solution' as they reveal the precise mathematical operations to implement. **Note**: Some lines in the solution may not contain calculator annotations, but you should still pay attention to the logic and calculations described in those lines.\n",
      "\n",
      "6.  **Final Answer:** Store the final answer in a variable named 'answer', and on the same line, add the comment `# FINAL ANSWER`. In the next line, return the 'answer' variable.\n",
      "\n",
      "7. **No extra output:** Your output should end with the ``` closing the code block. Do not include any additional text, explanations, or comments outside of the code block.\n",
      "\n",
      "--- EXAMPLES ---\n",
      "\n",
      "*Index*: \n",
      "310\n",
      "\n",
      "*Question*: \n",
      "Janet hires six employees. Four of them are warehouse workers who make $15/hour, and the other two are managers who make $20/hour. Janet has to pay 10% of her workers' salaries in FICA taxes. If everyone works 25 days a month and 8 hours a day, how much does Janet owe total for their wages and taxes for one month?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"First figure out how many hours each worker works per month by multiplying the number of days they work by the number of hours a day they work: 25 days * 8 hours/day = [[25*8=200]]200 hours\", \"L2\": \"Then calculate how much one warehouse worker makes per month by multiplying their hourly rate by the number of hours they work: 200 hours * $15/hour = $[[200*15=3000]]3000\", \"L3\": \"Then multiply that number by 4 to find out how much all the warehouse workers make: $3000/worker * 4 workers = $[[3000*4=12000]]12,000\", \"L4\": \"Now multiply the hours each manager works (also 200) by their hourly wage to find out how much one manager makes per month: 200 hours * $20/hour = $[[200*20=4000]]4,000\", \"L5\": \"Now multiply one manager's wages by the number of managers (2) to find their total wage amount: $4,000/manager * 2 managers = $[[4000*2=8000]]8,000\", \"L6\": \"Now add the wages for the managers and the workers to find the total cost of the wages: $8,000 + $12,000 = $[[8000+12000=20000]]20,000\", \"L7\": \"Now multiply the total wage bill by 10% to find how much the FICA taxes are: $20,000 * .1 = $[[20000*.1=2000]]2,000\", \"L8\": \"Now add the total wage bill to the total tax amount to find the grand total: $2,000 + $20,000 = $[[2000+20000=22000]]22,000\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        num_employees: int = 6, # Janet hires six employees\n",
      "        num_warehouse_workers: int = 4, # Four of them are warehouse workers\n",
      "        num_managers: int = 2, # the other two are managers\n",
      "        hourly_wage_warehouse: int = 15, # warehouse workers make $15/hour\n",
      "        hourly_wage_manager: int = 20, # managers make $20/hour\n",
      "        fica_tax_rate: float = 0.1, # FICA tax rate is 10%\n",
      "        days_per_month: int = 25, # everyone works 25 days a month\n",
      "        hours_per_day: int = 8 # everyone works 8 hours a day\n",
      "    ):\n",
      "    \"\"\"Index: 310.\n",
      "    Returns: the monthly total wage bill, including FICA taxes.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    hours_per_month = days_per_month * hours_per_day\n",
      "\n",
      "    #: L2\n",
      "    monthly_wage_warehouse = hourly_wage_warehouse * hours_per_month\n",
      "\n",
      "    #: L3\n",
      "    total_wage_warehouse = monthly_wage_warehouse * num_warehouse_workers\n",
      "\n",
      "    #: L4\n",
      "    monthly_wage_manager = hourly_wage_manager * hours_per_month\n",
      "\n",
      "    #: L5\n",
      "    total_wage_manager = monthly_wage_manager * num_managers\n",
      "\n",
      "    #: L6\n",
      "    total_wages = total_wage_warehouse + total_wage_manager\n",
      "\n",
      "    #: L7\n",
      "    fica_taxes = total_wages * fica_tax_rate\n",
      "\n",
      "    #: L8\n",
      "    grand_total = total_wages + fica_taxes\n",
      "\n",
      "    answer = grand_total # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "```\n",
      "*Index*: \n",
      "3822\n",
      "\n",
      "*Question*: \n",
      "Alec is running for Class President. He thinks that if he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him. Half of the class have already said they will vote for him but out of the remaining students, only 5 have said they are thinking about voting for him. He surveys the students who are thinking about voting for someone else, and changes his flyers to reflect the issues these students are concerned about. This results in a fifth of these students saying they'll vote for him. If Alec's class has 60 students and everyone who said they will vote for him does so, how many more votes does Alec need to reach his goal number of votes?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"To calculate Alec's goal number of votes, we need to know that 60 students / 4 = [[60/4=15]]15 students is equal to one-quarter of the class students.\", \"L2\": \"Alec's goal is therefore 15 students * 3 quarters = [[15*3=45]]45 votes.\", \"L3\": \"Half of the class said they will vote for him, so there are already 60 students / 2 = [[60/2=30]]30 votes.\", \"L4\": \"Another 5 students are thinking about voting for him which leaves a total so far of 30 + 5 = [[30+5=35]]35 votes.\", \"L5\": \"This means there are 60 students - 35 voting for Alec = [[60-35=25]]25 students not voting for Alec.\", \"L6\": \"A fifth of these decided to vote, so this is a further 25 students / 5 = [[25/5=5]]5 votes.\", \"L7\": \"Alec is therefore receiving a total of 35 + 5 = [[35+5=40]]40 votes.\", \"L8\": \"So he has missed his goal by 45 goal votes - 40 actual votes = [[45-40=5]]5 votes.\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        fraction_needed_to_win: float = 3/4,  # f he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him.\n",
      "        fraction_voting_for_him: float = 1/2,  # Half of the class have already said they will vote for him\n",
      "        students_thinking_about_it: int = 5,  # only 5 have said they are thinking about voting for him\n",
      "        total_students: int = 60  # Alec's class has 60 students\n",
      "):    \n",
      "    \"\"\"Index: 3822.\n",
      "    Returns: the number of votes by which Alec is short of his goal.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    students_per_quarter = total_students / 4\n",
      "\n",
      "    #: L2\n",
      "    votes_needed = students_per_quarter * 3\n",
      "\n",
      "    #: L3\n",
      "    votes_for_him = total_students * fraction_voting_for_him\n",
      "\n",
      "    #: L4\n",
      "    votes_so_far = votes_for_him + students_thinking_about_it\n",
      "\n",
      "    #: L5\n",
      "    students_not_voting_for_him = total_students - votes_so_far\n",
      "    \n",
      "    #: L6\n",
      "    new_votes = students_not_voting_for_him / 5\n",
      "\n",
      "    #: L7\n",
      "    total_votes_for_him = votes_so_far + new_votes\n",
      "\n",
      "    #: L8\n",
      "    votes_short_of_goal = votes_needed - total_votes_for_him\n",
      "\n",
      "    answer = votes_short_of_goal  # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "```\n",
      "*Index*: \n",
      "7371\n",
      "\n",
      "*Question*: \n",
      "Karen's students are about to take a standardized test. Karen gets a $500 bonus if their average score is above 75, plus an extra $10 bonus for every additional point the average score increases above 75. So far, Karen has graded 8 tests, and the average is 70. Given that each student can have a maximum score of 150, what combined score do the last two tests need to have for Karen to earn a $600 bonus?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"First subtract $500 from Karen's goal bonus amount to find how much she makes from the extra $10/point bonus: $600 - $500 = $[[600-500=100]]100\", \"L2\": \"Then divide the extra bonus by the extra rate: $100 / $10/point = [[100/10=10]]10 points\", \"L3\": \"Then add the 10 extra points to the baseline 75 point goal to find the students' average test score: 10 points + 75 points = [[10+75=85]]85 points\", \"L4\": \"Then added the 8 graded tests to the 2 ungraded tests to find the total number of tests: 2 tests + 8 tests = [[2+8=10]]10 tests\", \"L5\": \"Then multiply the 85 point average by the number of tests to find the total number of points the students need to earn: 85 points/test * 10 tests = 850 points\", \"L6\": \"Then multiply the current average by the current number of graded tests to find how many points have been earned so far: 70 points/test * 8 tests = [[70*8=560]]560 points\", \"L7\": \"Then subtract the number of points earned from the number of points needed to find the combine score the last two tests need: 850 points - 560 points = [[850-560=290]]290 points\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "    baseline_bonus: int = 500, # Karen gets a $500 bonus\n",
      "    baseline_avg_score: int = 75, # if their average score is above 75\n",
      "    extra_bonus_per_point: int = 10, # plus an extra $10 bonus for every additional point the average score increases above 75\n",
      "    tests_graded_so_far: int = 8, # So far, Karen has graded 8 tests\n",
      "    avg_so_far: int = 70, # and the average is 70\n",
      "    max_score_per_student: int = 150, # each student can have a maximum score of 150\n",
      "    desired_bonus: int = 600 # Karen wants to earn a $600 bonus\n",
      "):\n",
      "    \"\"\"Index: 7371.\n",
      "    Returns: the combined score needed in the last two tests to ensure that Karen earns a $600 bonus.\"\"\"\n",
      "    #: L1\n",
      "    extra_bonus_needed = desired_bonus - baseline_bonus\n",
      "\n",
      "    #: L2\n",
      "    extra_points_needed = extra_bonus_needed / extra_bonus_per_point\n",
      "\n",
      "    #: L3\n",
      "    target_avg_score = baseline_avg_score + extra_points_needed\n",
      "\n",
      "    #: L4\n",
      "    total_tests = tests_graded_so_far + 2\n",
      "\n",
      "    #: L5\n",
      "    total_points_needed = target_avg_score * total_tests\n",
      "\n",
      "    #: L6\n",
      "    points_earned_so_far = avg_so_far * tests_graded_so_far\n",
      "\n",
      "    #: L7\n",
      "    points_needed_last_two_tests = total_points_needed - points_earned_so_far\n",
      "\n",
      "    answer = points_needed_last_two_tests  # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "```\n",
      "--- TASK ---\n",
      "\n",
      "*Index*: \n",
      "4483\n",
      "\n",
      "*Question*: \n",
      "Peggy is moving and is looking to get rid of her record collection. Sammy says that he will buy all of them for 4 dollars each. Bryan is only interested in half of the records but will offer 6 dollars each for the half that he is interested in and 1 dollar each for the remaining half that he is not interested in with the hopes that he can resell them in bulk later. If Peggy has 200 records, what is the difference in profit between Sammy versus Bryan's deal?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"Sammy is offering to take the whole collection of 200 records and pay Peggy 4 dollars each for them which would net Peggy 200 * 4=[[200*4=800]]800 dollars for her entire record collection.\", \"L2\": \"Bryan is willing to buy Peggy's entire record collection but at two different price points, half at one point and half at another. Half of Peggy's record collection is 200/2=[[200/2=100]]100, which means that 100 records will sell for one price and 100 records will sell for another price.\", \"L3\": \"Bryan is willing to pay more for the half of the record collection that he is interested in so Peggy would net 100 * 6=[[100*6=600]]600 dollars for the first half of her record collection.\", \"L4\": \"For the half of the collection that Bryan is just planning on reselling at a later date, he is willing to offer Peggy 100 *1=[[100*1=100]]100 dollars to take off of her hands.\", \"L5\": \"In total Bryan is willing to offer Peggy 600+100=[[600+100=700]]700 dollars for her entire record collection.\", \"L6\": \"If Sammy is offering 800 dollars to buy Peggy's entire record collection and Bryan is offering 700 dollars for Peggy's entire record collection, then Peggy's net profit would be 800-700=[[800-700=100]]100 dollars more by taking Sammy's deal instead of Bryan's deal.\"}\n",
      "\n",
      "*Code*:\n"
     ]
    }
   ],
   "source": [
    "user_prompt = craft_user_prompt(\n",
    "    index=4483, \n",
    "    example_indices=indices,\n",
    "    code_examples=get_code_strings(indices)\n",
    ")\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19e9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = \\\n",
    "{\n",
    "  \"anthropic\": [\n",
    "    \"claude-3-5-haiku-20241022\"\n",
    "  ],\n",
    "  \"openai\": [\n",
    "    \"o4-mini\",\n",
    "    \"gpt-4.1-mini\"\n",
    "  ],\n",
    "  \"google\": [\n",
    "    \"gemini-2.0-flash-thinking-exp\",\n",
    "    \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9902f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Generation for Index: 3779 ====================\n",
      "Output directory: code_generation_outputs/3779\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.71 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o4-mini ---\n",
      "  Response received in 28.46 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/openai_o4-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 5.11 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.49 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.04 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "==================== Starting Generation for Index: 4483 ====================\n",
      "Output directory: code_generation_outputs/4483\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.09 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o4-mini ---\n",
      "  Response received in 17.33 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/openai_o4-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 2.74 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.17 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.05 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "==================== Starting Generation for Index: 6237 ====================\n",
      "Output directory: code_generation_outputs/6237\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.20 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o4-mini ---\n",
      "  Response received in 26.50 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/openai_o4-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 7.72 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 4.29 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "==================== Starting Generation for Index: 1202 ====================\n",
      "Output directory: code_generation_outputs/1202\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.50 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o4-mini ---\n",
      "  Response received in 21.14 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/openai_o4-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 5.33 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 4.15 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.18 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "==================== Starting Generation for Index: 2345 ====================\n",
      "Output directory: code_generation_outputs/2345\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.96 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o4-mini ---\n",
      "  Response received in 13.11 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/openai_o4-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 4.88 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.91 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.23 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "==================== Generation Complete ====================\n",
      "Performance data successfully saved to code_generation_outputs/generation_performance.csv.\n"
     ]
    }
   ],
   "source": [
    "df = generate_GSM8K_code(\n",
    "    model_dict=model_dict,\n",
    "    indices_to_generate=[3779, 4483, 6237, 1202, 2345],\n",
    "    example_indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20eb898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>3779</td>\n",
       "      <td>5.712822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>3779</td>\n",
       "      <td>28.464530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>3779</td>\n",
       "      <td>5.113441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>3779</td>\n",
       "      <td>3.486094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>3779</td>\n",
       "      <td>1.041422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>4483</td>\n",
       "      <td>5.092986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openai</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>4483</td>\n",
       "      <td>17.325825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>4483</td>\n",
       "      <td>2.743717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>4483</td>\n",
       "      <td>3.169473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>4483</td>\n",
       "      <td>1.048883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>6237</td>\n",
       "      <td>5.199077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>openai</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>6237</td>\n",
       "      <td>26.499058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>6237</td>\n",
       "      <td>7.724929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>6237</td>\n",
       "      <td>4.286880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>6237</td>\n",
       "      <td>1.018298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>1202</td>\n",
       "      <td>6.498431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>openai</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>1202</td>\n",
       "      <td>21.137652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>1202</td>\n",
       "      <td>5.326258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>1202</td>\n",
       "      <td>4.146654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>1202</td>\n",
       "      <td>1.176603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>2345</td>\n",
       "      <td>6.964966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>2345</td>\n",
       "      <td>13.109650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>2345</td>\n",
       "      <td>4.881717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>2345</td>\n",
       "      <td>3.913965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>2345</td>\n",
       "      <td>1.227347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider                                model  index  time_taken\n",
       "0   anthropic            claude-3-5-haiku-20241022   3779    5.712822\n",
       "1      openai                              o4-mini   3779   28.464530\n",
       "2      openai                         gpt-4.1-mini   3779    5.113441\n",
       "3      google        gemini-2.0-flash-thinking-exp   3779    3.486094\n",
       "4      google  gemini-2.5-flash-lite-preview-06-17   3779    1.041422\n",
       "5   anthropic            claude-3-5-haiku-20241022   4483    5.092986\n",
       "6      openai                              o4-mini   4483   17.325825\n",
       "7      openai                         gpt-4.1-mini   4483    2.743717\n",
       "8      google        gemini-2.0-flash-thinking-exp   4483    3.169473\n",
       "9      google  gemini-2.5-flash-lite-preview-06-17   4483    1.048883\n",
       "10  anthropic            claude-3-5-haiku-20241022   6237    5.199077\n",
       "11     openai                              o4-mini   6237   26.499058\n",
       "12     openai                         gpt-4.1-mini   6237    7.724929\n",
       "13     google        gemini-2.0-flash-thinking-exp   6237    4.286880\n",
       "14     google  gemini-2.5-flash-lite-preview-06-17   6237    1.018298\n",
       "15  anthropic            claude-3-5-haiku-20241022   1202    6.498431\n",
       "16     openai                              o4-mini   1202   21.137652\n",
       "17     openai                         gpt-4.1-mini   1202    5.326258\n",
       "18     google        gemini-2.0-flash-thinking-exp   1202    4.146654\n",
       "19     google  gemini-2.5-flash-lite-preview-06-17   1202    1.176603\n",
       "20  anthropic            claude-3-5-haiku-20241022   2345    6.964966\n",
       "21     openai                              o4-mini   2345   13.109650\n",
       "22     openai                         gpt-4.1-mini   2345    4.881717\n",
       "23     google        gemini-2.0-flash-thinking-exp   2345    3.913965\n",
       "24     google  gemini-2.5-flash-lite-preview-06-17   2345    1.227347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9a8385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Generation for Index: 3779 ====================\n",
      "Output directory: code_generation_outputs/3779\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.67 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 11.59 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.75 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 22.52 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.33 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3779/google_gemini-2.5-flash.txt\n",
      "\n",
      "==================== Starting Generation for Index: 4483 ====================\n",
      "Output directory: code_generation_outputs/4483\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.08 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 9.69 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.46 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 16.59 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 4.66 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4483/google_gemini-2.5-flash.txt\n",
      "\n",
      "==================== Starting Generation for Index: 6237 ====================\n",
      "Output directory: code_generation_outputs/6237\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.15 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 17.92 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.79 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 17.59 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.65 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6237/google_gemini-2.5-flash.txt\n",
      "\n",
      "==================== Starting Generation for Index: 1202 ====================\n",
      "Output directory: code_generation_outputs/1202\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.46 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 13.52 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.81 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 23.87 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.88 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1202/google_gemini-2.5-flash.txt\n",
      "\n",
      "==================== Starting Generation for Index: 2345 ====================\n",
      "Output directory: code_generation_outputs/2345\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.76 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 13.11 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 3.48 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 21.72 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 4.41 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/2345/google_gemini-2.5-flash.txt\n",
      "\n",
      "==================== Generation Complete ====================\n",
      "Performance data successfully saved to code_generation_outputs/generation_performance.csv.\n"
     ]
    }
   ],
   "source": [
    "model_dict_2 = \\\n",
    "{\n",
    "  \"anthropic\": [\n",
    "    \"claude-3-5-haiku-20241022\"\n",
    "  ],\n",
    "  \"openai\": [\n",
    "    \"o3-mini\",\n",
    "    \"gpt-4.1\"\n",
    "  ],\n",
    "  \"google\": [\n",
    "    \"gemini-2.5-pro\",\n",
    "    \"gemini-2.5-flash\",\n",
    "  ]\n",
    "}\n",
    "\n",
    "df = generate_GSM8K_code(\n",
    "    model_dict=model_dict_2,\n",
    "    indices_to_generate=[3779, 4483, 6237, 1202, 2345],\n",
    "    example_indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e4ee37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>3779</td>\n",
       "      <td>5.670871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai</td>\n",
       "      <td>o3-mini</td>\n",
       "      <td>3779</td>\n",
       "      <td>11.586511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>3779</td>\n",
       "      <td>2.745864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>3779</td>\n",
       "      <td>22.519250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>3779</td>\n",
       "      <td>5.330314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>4483</td>\n",
       "      <td>6.077964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openai</td>\n",
       "      <td>o3-mini</td>\n",
       "      <td>4483</td>\n",
       "      <td>9.689164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4483</td>\n",
       "      <td>2.457451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>4483</td>\n",
       "      <td>16.586376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>4483</td>\n",
       "      <td>4.655331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>6237</td>\n",
       "      <td>5.146466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>openai</td>\n",
       "      <td>o3-mini</td>\n",
       "      <td>6237</td>\n",
       "      <td>17.922746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>6237</td>\n",
       "      <td>2.786614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>6237</td>\n",
       "      <td>17.589984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>6237</td>\n",
       "      <td>5.646524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>1202</td>\n",
       "      <td>6.455634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>openai</td>\n",
       "      <td>o3-mini</td>\n",
       "      <td>1202</td>\n",
       "      <td>13.516382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>1202</td>\n",
       "      <td>2.807676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>1202</td>\n",
       "      <td>23.867050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>1202</td>\n",
       "      <td>5.884178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>2345</td>\n",
       "      <td>6.756890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai</td>\n",
       "      <td>o3-mini</td>\n",
       "      <td>2345</td>\n",
       "      <td>13.107946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>2345</td>\n",
       "      <td>3.478042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>2345</td>\n",
       "      <td>21.720064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>2345</td>\n",
       "      <td>4.406006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider                      model  index  time_taken\n",
       "0   anthropic  claude-3-5-haiku-20241022   3779    5.670871\n",
       "1      openai                    o3-mini   3779   11.586511\n",
       "2      openai                    gpt-4.1   3779    2.745864\n",
       "3      google             gemini-2.5-pro   3779   22.519250\n",
       "4      google           gemini-2.5-flash   3779    5.330314\n",
       "5   anthropic  claude-3-5-haiku-20241022   4483    6.077964\n",
       "6      openai                    o3-mini   4483    9.689164\n",
       "7      openai                    gpt-4.1   4483    2.457451\n",
       "8      google             gemini-2.5-pro   4483   16.586376\n",
       "9      google           gemini-2.5-flash   4483    4.655331\n",
       "10  anthropic  claude-3-5-haiku-20241022   6237    5.146466\n",
       "11     openai                    o3-mini   6237   17.922746\n",
       "12     openai                    gpt-4.1   6237    2.786614\n",
       "13     google             gemini-2.5-pro   6237   17.589984\n",
       "14     google           gemini-2.5-flash   6237    5.646524\n",
       "15  anthropic  claude-3-5-haiku-20241022   1202    6.455634\n",
       "16     openai                    o3-mini   1202   13.516382\n",
       "17     openai                    gpt-4.1   1202    2.807676\n",
       "18     google             gemini-2.5-pro   1202   23.867050\n",
       "19     google           gemini-2.5-flash   1202    5.884178\n",
       "20  anthropic  claude-3-5-haiku-20241022   2345    6.756890\n",
       "21     openai                    o3-mini   2345   13.107946\n",
       "22     openai                    gpt-4.1   2345    3.478042\n",
       "23     google             gemini-2.5-pro   2345   21.720064\n",
       "24     google           gemini-2.5-flash   2345    4.406006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2456db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbb5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Generation for Index: 3331 ====================\n",
      "Output directory: code_generation_outputs/3331\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.18 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 11.44 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.27 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 3.74 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 17.23 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 7.14 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.82 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 4.91 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3331/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 1647 ====================\n",
      "Output directory: code_generation_outputs/1647\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.14 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 11.05 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.67 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 4.70 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 12.86 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.18 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.68 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1647/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 636 ====================\n",
      "Output directory: code_generation_outputs/636\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 4.71 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 26.90 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.06 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 1.84 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 15.15 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 4.45 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.61 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 2.95 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/636/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 399 ====================\n",
      "Output directory: code_generation_outputs/399\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.41 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 22.53 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.66 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 3.89 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 15.56 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 11.88 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 13.25 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/399/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 4670 ====================\n",
      "Output directory: code_generation_outputs/4670\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 3.24 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 11.83 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 1.05 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 1.41 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 13.44 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 3.90 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.82 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.75 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/4670/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 5918 ====================\n",
      "Output directory: code_generation_outputs/5918\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.90 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 10.36 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 1.86 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 3.63 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 13.04 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 4.96 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.95 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 2.66 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5918/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 1531 ====================\n",
      "Output directory: code_generation_outputs/1531\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.73 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 19.25 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.86 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 4.30 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 25.42 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.19 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.92 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.74 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1531/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 7364 ====================\n",
      "Output directory: code_generation_outputs/7364\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.70 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 28.65 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 3.27 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 7.19 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 37.43 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 12.34 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.23 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 7.99 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/7364/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 5464 ====================\n",
      "Output directory: code_generation_outputs/5464\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.68 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 15.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 4.10 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 6.76 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 35.24 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "An API error occurred for google model gemini-2.5-flash: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.\n",
      "  Response received in 16.58 seconds.\n",
      "  No response received. Skipping file save.\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 7.37 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/5464/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 1205 ====================\n",
      "Output directory: code_generation_outputs/1205\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.73 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 15.36 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 3.88 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 4.92 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 18.43 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 8.16 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.05 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 4.50 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/1205/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 3518 ====================\n",
      "Output directory: code_generation_outputs/3518\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 5.73 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 10.36 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 3.06 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 5.65 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 24.59 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.48 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 0.94 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.71 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/3518/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Starting Generation for Index: 6732 ====================\n",
      "Output directory: code_generation_outputs/6732\n",
      "Crafting user prompt...\n",
      "\n",
      "--- Calling Anthropic model: claude-3-5-haiku-20241022 ---\n",
      "  Response received in 6.41 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/anthropic_claude-3-5-haiku-20241022.txt\n",
      "\n",
      "--- Calling Openai model: o3-mini ---\n",
      "  Response received in 12.70 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/openai_o3-mini.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1 ---\n",
      "  Response received in 2.60 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/openai_gpt-4.1.txt\n",
      "\n",
      "--- Calling Openai model: gpt-4.1-mini ---\n",
      "  Response received in 4.63 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/openai_gpt-4.1-mini.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-pro ---\n",
      "  Response received in 15.24 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/google_gemini-2.5-pro.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash ---\n",
      "  Response received in 5.79 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/google_gemini-2.5-flash.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.5-flash-lite-preview-06-17 ---\n",
      "  Response received in 1.02 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "\n",
      "--- Calling Google model: gemini-2.0-flash-thinking-exp ---\n",
      "  Response received in 3.27 seconds.\n",
      "  Successfully saved raw output to: code_generation_outputs/6732/google_gemini-2.0-flash-thinking-exp.txt\n",
      "\n",
      "==================== Generation Complete ====================\n",
      "Performance data successfully saved to code_generation_outputs/generation_performance.csv.\n"
     ]
    }
   ],
   "source": [
    "model_dict_3 = \\\n",
    "{\n",
    "  \"anthropic\": [\n",
    "    \"claude-3-5-haiku-20241022\"\n",
    "  ],\n",
    "  \"openai\": [\n",
    "    \"o3-mini\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4.1-mini\"\n",
    "  ],\n",
    "  \"google\": [\n",
    "    \"gemini-2.5-pro\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"gemini-2.0-flash-thinking-exp\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# [3779, 4483, 6237, 1202, 2345]\n",
    "\n",
    "df_3 = generate_GSM8K_code(\n",
    "    model_dict=model_dict_3,\n",
    "    indices_to_generate=[3331, 1647, 636, 399, 4670, 5918, 1531, 7364, 5464, 1205, 3518, 6732],\n",
    "    example_indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed736f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing from 'code_generation_outputs'...\n",
      "Cleaned files will be saved in 'code_generation_outputs_cleaned'.\n",
      "\n",
      "Processing file: code_generation_outputs/7364/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/google_gemini-2.5-pro.txt\n",
      "  Warning: No python markdown block found. Using raw content as code.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/7364/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/7364/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/3331/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3331/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/399/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/399/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/5918/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5918/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/4670/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4670/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/1531/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1531/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/openai_o4-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/openai_o4-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/3779/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3779/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/1647/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1647/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/3518/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/3518/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/google_gemini-2.5-pro.txt\n",
      "  Warning: No python markdown block found. Using raw content as code.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/5464/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/5464/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/openai_o4-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/openai_o4-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/1202/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1202/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/1205/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/1205/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/6732/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6732/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/openai_o4-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/openai_o4-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/4483/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/4483/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/openai_o4-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/openai_o4-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/2345/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/2345/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/636/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/636/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/openai_o4-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/openai_o4-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/google_gemini-2.5-pro.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/google_gemini-2.5-pro.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/openai_gpt-4.1.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/openai_gpt-4.1.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/openai_o3-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/openai_o3-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/anthropic_claude-3-5-haiku-20241022.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/anthropic_claude-3-5-haiku-20241022.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/openai_gpt-4.1-mini.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/openai_gpt-4.1-mini.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/google_gemini-2.0-flash-thinking-exp.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "Processing file: code_generation_outputs/6237/google_gemini-2.5-flash.txt\n",
      "  Successfully extracted code from markdown block.\n",
      "  Saved cleaned code to: code_generation_outputs_cleaned/6237/google_gemini-2.5-flash.py\n",
      "\n",
      "Processing complete. Processed 140 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def process_and_clean_outputs(\n",
    "    source_dir: str = 'code_generation_outputs',\n",
    "    dest_dir: str = 'code_generation_outputs_cleaned'\n",
    "):\n",
    "    \"\"\"\n",
    "    Traverses a source directory, cleans raw .txt model outputs, and saves\n",
    "    them as .py files in a new destination directory with the same structure.\n",
    "\n",
    "    This function will:\n",
    "    1. Walk through all subdirectories of the source_dir.\n",
    "    2. Find all files ending in .txt.\n",
    "    3. Create a corresponding subdirectory in the dest_dir.\n",
    "    4. Read the .txt content and extract the Python code from ```python ... ``` blocks.\n",
    "    5. Save the cleaned code to a new file with a .py extension in the destination.\n",
    "    6. The original source directory and its files will be left untouched.\n",
    "\n",
    "    Args:\n",
    "        source_dir: The top-level directory containing the raw generated outputs.\n",
    "        dest_dir: The top-level directory where cleaned .py files will be saved.\n",
    "    \"\"\"\n",
    "    print(f\"Starting processing from '{source_dir}'...\")\n",
    "    print(f\"Cleaned files will be saved in '{dest_dir}'.\")\n",
    "    files_processed = 0\n",
    "\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "        return\n",
    "\n",
    "    # os.walk efficiently traverses the entire directory tree\n",
    "    for dirpath, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            # Process only the .txt files\n",
    "            if filename.endswith(\".txt\"):\n",
    "                source_filepath = os.path.join(dirpath, filename)\n",
    "                print(f\"\\nProcessing file: {source_filepath}\")\n",
    "\n",
    "                try:\n",
    "                    # 1. Determine the new directory structure\n",
    "                    # Replaces 'code_generation_outputs' with 'code_generation_outputs_cleaned'\n",
    "                    # The '1' ensures it only replaces the first occurrence at the beginning\n",
    "                    dest_subdir = dirpath.replace(source_dir, dest_dir, 1)\n",
    "                    \n",
    "                    # 2. Create the destination subdirectory if it doesn't exist\n",
    "                    os.makedirs(dest_subdir, exist_ok=True)\n",
    "                    \n",
    "                    # 3. Read the raw content from the source file\n",
    "                    with open(source_filepath, 'r', encoding='utf-8') as f:\n",
    "                        raw_content = f.read()\n",
    "\n",
    "                    # 4. Extract the code from the markdown block\n",
    "                    cleaned_code = \"\"\n",
    "                    code_match = re.search(r\"```python\\n(.*?)\\n```\", raw_content, re.DOTALL)\n",
    "                    \n",
    "                    if code_match:\n",
    "                        cleaned_code = code_match.group(1).strip()\n",
    "                        print(\"  Successfully extracted code from markdown block.\")\n",
    "                    else:\n",
    "                        cleaned_code = raw_content.strip()\n",
    "                        print(\"  Warning: No python markdown block found. Using raw content as code.\")\n",
    "\n",
    "                    # 5. Define the new .py filename and full path\n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    py_filename = f\"{base_name}.py\"\n",
    "                    dest_filepath = os.path.join(dest_subdir, py_filename)\n",
    "\n",
    "                    # 6. Write the cleaned code to the new .py file in the destination\n",
    "                    with open(dest_filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(cleaned_code)\n",
    "                    print(f\"  Saved cleaned code to: {dest_filepath}\")\n",
    "                    \n",
    "                    files_processed += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  An error occurred while processing {source_filepath}: {e}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete. Processed {files_processed} files.\")\n",
    "\n",
    "process_and_clean_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486fb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gsm8k_validator.py\n",
    "==================\n",
    "\n",
    "End-to-end checker for **three independently generated** `solve()` files\n",
    "that formalise the same GSM8K problem.  The script enforces the _compact\n",
    "format_ we finally adopted:\n",
    "\n",
    "* 2-line docstring header (`Index: …`, `Returns: …`)\n",
    "* Semantic argument names (no numeric prefix)\n",
    "* Trace comments that start **exactly** with `#: L<n>`\n",
    "* No calculator annotations inside the code\n",
    "* Final answer stored in `answer` with `# FINAL ANSWER`\n",
    "\n",
    "The pipeline performs four orthogonal tests:\n",
    "\n",
    "UT-0  – Default run returns the official numeric answer  \n",
    "UT-1  – Trace-comment lists are identical across files  \n",
    "UT-2  – Arguments match in default literals **and** comment semantics  \n",
    "UT-3  – Hypothesis fuzzing confirms functional equivalence\n",
    "\n",
    "Files that pass all four tests are optionally rewritten into a *canonical*\n",
    "form (`X1 …, Y1 …`) suitable for automated error-injection experiments.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "black                – whitespace-stable formatting  \n",
    "libcst               – reliable CST traversal with comment access  \n",
    "hypothesis           – property-based fuzzing  \n",
    "sentence-transformers (mpnet-base) – SBERT cosine for comment semantics\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import importlib.util\n",
    "import inspect\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import black\n",
    "import hypothesis.strategies as st\n",
    "import libcst as cst\n",
    "from hypothesis import given, settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Global constants & regex helpers\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "_MODEL = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "_COS_THRESHOLD = 0.90          # SBERT cosine ≥ 0.9 ⇒ semantic match\n",
    "_FUZZ_EXAMPLES  = 60           # Hypothesis draws\n",
    "\n",
    "_TRACE_RE = re.compile(r\"^#: L(\\d+)\\b\")          # matches trace comment\n",
    "_DOC_INDEX_RE = re.compile(r\"^Index:\\s*(\\d+)\")   # first docstring line\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  ParsedFile dataclass\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "@dataclass\n",
    "class ParsedFile:\n",
    "    \"\"\"\n",
    "    Normalised representation of a generated code file.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    index         : str\n",
    "        Problem ID, e.g. ``\"Q310\"``.\n",
    "    path          : pathlib.Path\n",
    "        Path to the original file on disk.\n",
    "    module_code   : str\n",
    "        Black-formatted source code.\n",
    "    trace_keys    : List[str]\n",
    "        Ordered list like ``[\"L1\", \"L2\", …]`` extracted from comments.\n",
    "    arg_defaults  : List[str]\n",
    "        Default literal strings in the function signature.\n",
    "    arg_comments  : List[str]\n",
    "        Trailing comments for each argument.\n",
    "    func          : callable\n",
    "        Dynamically imported ``solve`` function ready for execution.\n",
    "    \"\"\"\n",
    "    index: str\n",
    "    path: Path\n",
    "    module_code: str\n",
    "    trace_keys: List[str]\n",
    "    arg_defaults: List[str]\n",
    "    arg_comments: List[str]\n",
    "    func: Any\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Parsing utilities\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def parse_file(path: Path) -> ParsedFile:\n",
    "    \"\"\"\n",
    "    Parse one generated `.py` file and extract all information needed\n",
    "    by the validators.\n",
    "\n",
    "    Why a dedicated parser?\n",
    "    -----------------------\n",
    "    * Centralises Black formatting (whitespace normalisation).\n",
    "    * Collects trace comments, argument literals, and their semantic\n",
    "      comments in **one** traversal (libcst).\n",
    "    * Dynamically imports the `solve()` function so UT-0 and UT-3 can\n",
    "      execute it in memory without writing temp files.\n",
    "    \"\"\"\n",
    "    src_raw = path.read_text()\n",
    "    src_fmt = black.format_str(src_raw, mode=black.FileMode())\n",
    "\n",
    "    # ------- docstring extraction & index check ------------------- #\n",
    "    doc_match = re.search(r'\"\"\"(.*?)\"\"\"', src_fmt, re.S)\n",
    "    if not doc_match:\n",
    "        raise ValueError(f\"{path}   » missing two-line docstring\")\n",
    "    first_line = doc_match.group(1).strip().splitlines()[0]\n",
    "    idx_match = _DOC_INDEX_RE.match(first_line)\n",
    "    if not idx_match:\n",
    "        raise ValueError(f\"{path}   » first docstring line must start 'Index:'\")\n",
    "    index = f\"Q{idx_match.group(1)}\"\n",
    "\n",
    "    # ------- trace comment list ----------------------------------- #\n",
    "    trace_keys = [\n",
    "        f\"L{m.group(1)}\"\n",
    "        for line in src_fmt.splitlines()\n",
    "        if (m := _TRACE_RE.match(line.lstrip()))\n",
    "    ]\n",
    "    if not trace_keys:\n",
    "        raise ValueError(f\"{path}   » no trace comments found\")\n",
    "\n",
    "    # ------- AST walk for args ------------------------------------ #\n",
    "    mod     = cst.parse_module(src_fmt)\n",
    "    func_nd = next(\n",
    "        n for n in mod.body\n",
    "        if isinstance(n, cst.FunctionDef) and n.name.value == \"solve\"\n",
    "    )\n",
    "    arg_defaults, arg_comments = [], []\n",
    "    for param in func_nd.params.params:\n",
    "        arg_defaults.append(param.default.code if param.default else None)\n",
    "        cmt = (\n",
    "            param.trailing_whitespace.comment.value.lstrip(\"#\").strip()\n",
    "            if param.trailing_whitespace.comment else \"\"\n",
    "        )\n",
    "        arg_comments.append(cmt)\n",
    "\n",
    "    # ------- dynamic import --------------------------------------- #\n",
    "    spec = importlib.util.spec_from_loader(f\"gsm8k_{index}_{hash(path)}\", loader=None)\n",
    "    mod_dyn = importlib.util.module_from_spec(spec)\n",
    "    exec(src_fmt, mod_dyn.__dict__)\n",
    "\n",
    "    return ParsedFile(\n",
    "        index=index,\n",
    "        path=path,\n",
    "        module_code=src_fmt,\n",
    "        trace_keys=trace_keys,\n",
    "        arg_defaults=arg_defaults,\n",
    "        arg_comments=arg_comments,\n",
    "        func=mod_dyn.solve,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Unit Test 0 – default answer match\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def ut0_answer_match(files: List[ParsedFile], gold: int | float) -> List[ParsedFile]:\n",
    "    \"\"\"\n",
    "    Keep only files whose `solve()` returns the official answer when\n",
    "    called *with default arguments*.\n",
    "\n",
    "    Rationale\n",
    "    ---------\n",
    "    Eliminates obvious errors: hard-coded wrong literals, mis-parsed\n",
    "    numbers, or runtime exceptions.\n",
    "    \"\"\"\n",
    "    ok = []\n",
    "    for pf in files:\n",
    "        try:\n",
    "            if pf.func() == gold:\n",
    "                ok.append(pf)\n",
    "        except Exception as e:\n",
    "            print(f\"[UT-0] {pf.path.name} raised {e!r}\", file=sys.stderr)\n",
    "    return ok\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Unit Test 1 – identical trace lists\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def ut1_trace_lists_equal(files: List[ParsedFile]) -> bool:\n",
    "    \"\"\"\n",
    "    Verify that every surviving file has the *same* ordered list of trace\n",
    "    keys (`L1`, `L2`, …).\n",
    "\n",
    "    Why:\n",
    "    ----\n",
    "    Ensures no model skipped or re-ordered steps; makes later\n",
    "    canonicalisation deterministic.\n",
    "    \"\"\"\n",
    "    hashes = {\n",
    "        hashlib.sha1(\",\".join(p.trace_keys).encode()).hexdigest()\n",
    "        for p in files\n",
    "    }\n",
    "    return len(hashes) == 1\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Unit Test 2 – argument semantic alignment\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def ut2_arg_semantics(files: List[ParsedFile]) -> bool:\n",
    "    \"\"\"\n",
    "    Check two things:\n",
    "    1. Argument *default literals* match exactly across files.\n",
    "    2. Trailing comments are semantically equivalent (SBERT cosine ≥ 0.9).\n",
    "\n",
    "    Rationale\n",
    "    ---------\n",
    "    Detects swapped wage vs. hours, wrong default numbers, or comment /\n",
    "    code drift that numeric fuzzing may not catch immediately.\n",
    "    \"\"\"\n",
    "    ref_def  = files[0].arg_defaults\n",
    "    ref_emb  = _MODEL.encode(files[0].arg_comments, normalize_embeddings=True)\n",
    "\n",
    "    for pf in files[1:]:\n",
    "        if pf.arg_defaults != ref_def:\n",
    "            return False\n",
    "        emb = _MODEL.encode(pf.arg_comments, normalize_embeddings=True)\n",
    "        if ((emb * ref_emb).sum(axis=1) < _COS_THRESHOLD).any():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Unit Test 3 – property-based fuzz equivalence\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def ut3_fuzz_equivalent(files: List[ParsedFile], n=_FUZZ_EXAMPLES) -> bool:\n",
    "    \"\"\"\n",
    "    Use Hypothesis to generate random overrides for **all arguments** and\n",
    "    require that every file returns the same value.\n",
    "\n",
    "    Rationale\n",
    "    ---------\n",
    "    Guards against coincidental agreement on default literals.  Divergent\n",
    "    internal logic will surface when you vary inputs.\n",
    "    \"\"\"\n",
    "    funcs = [pf.func for pf in files]\n",
    "    sig   = inspect.signature(funcs[0])\n",
    "    strat_map = {}\n",
    "    for param in sig.parameters.values():\n",
    "        literal = param.default\n",
    "        strat_map[param.name] = (\n",
    "            st.floats(1, 30) if isinstance(literal, float) else st.integers(1, 30)\n",
    "        )\n",
    "\n",
    "    @settings(max_examples=n, deadline=None)\n",
    "    @given(st.fixed_dictionaries(strat_map))\n",
    "    def _check(kwargs):\n",
    "        ref = funcs[0](**kwargs)\n",
    "        for fn in funcs[1:]:\n",
    "            assert fn(**kwargs) == ref\n",
    "\n",
    "    try:\n",
    "        _check()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[UT-3] fuzz divergence: {e!r}\", file=sys.stderr)\n",
    "        return False\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Canonical renamer (optional post-step)\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "_RE_IDENT = re.compile(r\"\\b([A-Za-z_]\\w*)\\b\")\n",
    "\n",
    "def canonicalise(source: str, arg_order: List[str], trace_keys: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Return a *canonical* version of `source` where:\n",
    "\n",
    "    * Arguments become `X1`, `X2`, … following their order in `arg_order`.\n",
    "    * The variable assigned immediately after `#: L<n>` becomes `Y<n>`.\n",
    "\n",
    "    This deterministic form is ideal for later error-injection because it\n",
    "    decouples variable names from human semantics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source     : str\n",
    "        Original source code.\n",
    "    arg_order  : List[str]\n",
    "        Ordered list of argument names in the `solve` signature.\n",
    "    trace_keys : List[str]\n",
    "        Ordered list of trace keys extracted earlier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Source code with canonical identifiers.\n",
    "    \"\"\"\n",
    "    # map args → Xi\n",
    "    arg_map = {old: f\"X{i+1}\" for i, old in enumerate(arg_order)}\n",
    "\n",
    "    # map intermediates via trace comments\n",
    "    lines = source.splitlines()\n",
    "    interm_map: Dict[str, str] = {}\n",
    "    for idx, key in enumerate(trace_keys):\n",
    "        # find the line number of #: L<n>\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.lstrip().startswith(f\"#: {key}\"):\n",
    "                # next line should have assignment\n",
    "                lhs_match = re.match(r\"\\s*([A-Za-z_]\\w*)\\s*=\", lines[i + 1])\n",
    "                if lhs_match:\n",
    "                    interm_map[lhs_match.group(1)] = f\"Y{key[1:]}\"\n",
    "                break\n",
    "\n",
    "    name_map = {**arg_map, **interm_map}\n",
    "    return _RE_IDENT.sub(lambda m: name_map.get(m.group(1), m.group(1)), source)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Orchestration function\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def validate_triplet(paths: List[Path], gold: int | float) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Run UT-0…UT-3 on a list of *three* files that claim to solve the same\n",
    "    GSM8K item.  If at least two survive **all** tests, write canonical\n",
    "    versions (`canon_<name>.py`) and return their original paths.\n",
    "\n",
    "    Side-effects:\n",
    "    -------------\n",
    "    • Prints status lines for each stage.  \n",
    "    • Writes canonical files next to originals.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Path]\n",
    "        Paths of files that passed, or an empty list if quorum fails.\n",
    "    \"\"\"\n",
    "    parsed = [parse_file(p) for p in paths]\n",
    "\n",
    "    parsed = ut0_answer_match(parsed, gold)\n",
    "    if len(parsed) < 2:\n",
    "        print(\"Quorum failed at UT-0\")\n",
    "        return []\n",
    "\n",
    "    if not ut1_trace_lists_equal(parsed):\n",
    "        print(\"Trace lists differ (UT-1)\")\n",
    "        return []\n",
    "\n",
    "    if not ut2_arg_semantics(parsed):\n",
    "        print(\"Arg mismatch (UT-2)\")\n",
    "        return []\n",
    "\n",
    "    if not ut3_fuzz_equivalent(parsed):\n",
    "        print(\"Functional divergence (UT-3)\")\n",
    "        return []\n",
    "\n",
    "    # optional canonicalise\n",
    "    arg_order = list(inspect.signature(parsed[0].func).parameters.keys())\n",
    "    for pf in parsed:\n",
    "        canon = canonicalise(pf.module_code, arg_order, pf.trace_keys)\n",
    "        (pf.path.parent / f\"canon_{pf.path.name}\").write_text(canon)\n",
    "\n",
    "    print(\"All tests passed; canonical files written.\")\n",
    "    return [pf.path for pf in parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99856823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
