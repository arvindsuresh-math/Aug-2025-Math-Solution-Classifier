{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# gsm8k_validator_v2.py\n",
    "# ======================\n",
    "\n",
    "# Performs a flexible, score-based validation of multiple LLM-generated `solve()`\n",
    "# functions for a given GSM8K problem. Instead of a rigid filter, this script\n",
    "# analyzes all pairs of models to find the most robust and comprehensive consensus.\n",
    "\n",
    "# The final output is a confidence score (0.0-1.0+) that reflects:\n",
    "# 1.  The completeness of parameter alignment between models.\n",
    "# 2.  The semantic clarity of the aligned parameters.\n",
    "# 3.  The level of consensus (number of models in agreement).\n",
    "\n",
    "# Core Logic:\n",
    "# -----------\n",
    "# 1.  **Parse & Pre-filter (UT-0):** All generated Python files for a problem are\n",
    "#     parsed and filtered, keeping only those that produce the correct answer\n",
    "#     for the default values.\n",
    "\n",
    "# 2.  **Pairwise Alignment (UT-2):** For every pair of surviving models, find the\n",
    "#     best possible alignment between their function arguments based on semantic\n",
    "#     similarity (SBERT) and matching default values. Order is ignored.\n",
    "\n",
    "# 3.  **Pairwise Fuzzing (UT-3):** For each aligned pair, fuzz-test for functional\n",
    "#     equivalence using the \"Fuzz Aligned, Freeze Unaligned\" strategy. This\n",
    "#     ensures logical soundness even with partially matching signatures.\n",
    "\n",
    "# 4.  **Scoring & Consensus:**\n",
    "#     - Each successfully validated pair receives a `PairwiseQualityScore` based\n",
    "#       on its Alignment Ratio and Semantic Strength.\n",
    "#     - The script identifies the largest \"clique\" of models that are all\n",
    "#       mutually validated.\n",
    "#     - A final `ConfidenceScore` is computed from the clique's average quality,\n",
    "#       boosted by a bonus for the size of the consensus.\n",
    "\n",
    "# Dependencies\n",
    "# ------------\n",
    "# black                – whitespace-stable formatting\n",
    "# libcst               – reliable CST traversal with comment access\n",
    "# hypothesis           – property-based fuzzing\n",
    "# sentence-transformers (mpnet-base) – SBERT cosine for comment semantics\n",
    "# numpy                - for fast vector operations\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "\n",
    "# # --- (inside gsm8k_validator.py) ---\n",
    "\n",
    "# import importlib.util\n",
    "# import inspect\n",
    "# import itertools\n",
    "# import json\n",
    "# import re\n",
    "# import sys\n",
    "# from dataclasses import dataclass, field\n",
    "# from pathlib import Path\n",
    "# from typing import List, Any, Tuple, Dict\n",
    "\n",
    "# import black\n",
    "# import hypothesis.strategies as st\n",
    "# import libcst as cst  # <--- ADD THIS LINE\n",
    "# import numpy as np\n",
    "# from hypothesis import given, settings, HealthCheck\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Global constants & Configuration\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# _MODEL = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "# _COS_THRESHOLD = 0.90  # SBERT cosine ≥ 0.9 ⇒ semantic match\n",
    "# _FUZZ_EXAMPLES = 50  # Hypothesis draws\n",
    "# _MIN_ALIGNMENT_FOR_FUZZ = 1 # A pair must align on at least one arg to be fuzzed\n",
    "\n",
    "# # --- Scoring Weights --- #\n",
    "# W_ALIGNMENT = 0.7\n",
    "# W_SEMANTIC = 0.3\n",
    "\n",
    "# # --- Consensus Bonus Multipliers --- #\n",
    "# CONSENSUS_BONUS = {\n",
    "#     2: 1.0,  # Baseline for a pair\n",
    "#     3: 1.1,  # 10% bonus for a 3-way consensus\n",
    "#     4: 1.2,  # 20% bonus for a 4-way consensus\n",
    "#     5: 1.3,\n",
    "# }\n",
    "\n",
    "# _TRACE_RE = re.compile(r\"^#: L(\\d+)\\b\")\n",
    "# _DOC_INDEX_RE = re.compile(r\"^Index:\\s*(\\d+)\")\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Dataclasses for Structured Data\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class Argument:\n",
    "#     \"\"\"Represents a single argument from a function signature.\"\"\"\n",
    "#     name: str\n",
    "#     default: Any\n",
    "#     comment: str\n",
    "\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class ParsedFile:\n",
    "#     \"\"\"Normalised representation of a generated code file.\"\"\"\n",
    "#     path: Path\n",
    "#     module_code: str\n",
    "#     func: Any\n",
    "#     args: List[Argument] = field(default_factory=list)\n",
    "\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class AlignmentResult:\n",
    "#     \"\"\"Stores the result of aligning two ParsedFiles.\"\"\"\n",
    "#     aligned_pairs: List[Tuple[Argument, Argument]]\n",
    "#     unaligned_A: List[Argument]\n",
    "#     unaligned_B: List[Argument]\n",
    "#     semantic_scores: List[float]\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class PairwiseValidation:\n",
    "#     \"\"\"Stores the full result of a successful pairwise validation.\"\"\"\n",
    "#     file_A: ParsedFile\n",
    "#     file_B: ParsedFile\n",
    "#     alignment: AlignmentResult\n",
    "#     quality_score: float\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Core Logic Implementation\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# def parse_file(path: Path) -> ParsedFile | None:\n",
    "#     \"\"\"Parse one generated .py file into a structured ParsedFile object.\"\"\"\n",
    "#     try:\n",
    "#         src_raw = path.read_text(encoding=\"utf-8\")\n",
    "#         src_fmt = black.format_str(src_raw, mode=black.FileMode())\n",
    "\n",
    "#         mod = cst.parse_module(src_fmt)\n",
    "#         func_nd = next(\n",
    "#             n for n in mod.body if isinstance(n, cst.FunctionDef) and n.name.value == \"solve\"\n",
    "#         )\n",
    "\n",
    "#         args = []\n",
    "#         for param in func_nd.params.params:\n",
    "#             if not param.default:\n",
    "#                 continue\n",
    "\n",
    "#             # --- Argument Name and Default Value ---\n",
    "#             name = param.name.value\n",
    "#             default = eval(mod.code_for_node(param.default))\n",
    "\n",
    "#             # --- Robust Comment Extraction ---\n",
    "#             comment = \"\"\n",
    "#             comment_node = None\n",
    "\n",
    "#             # Check for comment on the parameter's trailing whitespace\n",
    "#             # This handles both simple cases and the 'ParenthesizedWhitespace' edge case\n",
    "#             if hasattr(param, \"trailing_whitespace\") and param.trailing_whitespace:\n",
    "#                 ws_node = param.trailing_whitespace\n",
    "#                 if hasattr(ws_node, \"comment\") and ws_node.comment:\n",
    "#                     comment_node = ws_node.comment\n",
    "#                 elif hasattr(ws_node, \"last_line\") and hasattr(ws_node.last_line, \"comment\") and ws_node.last_line.comment:\n",
    "#                     comment_node = ws_node.last_line.comment\n",
    "\n",
    "#             # If not found, check for comment on the trailing comma (for non-last params)\n",
    "#             if not comment_node and hasattr(param, \"comma\") and param.comma:\n",
    "#                 if hasattr(param.comma, \"whitespace_after\") and param.comma.whitespace_after:\n",
    "#                     ws_node_after_comma = param.comma.whitespace_after\n",
    "#                     if hasattr(ws_node_after_comma, \"comment\") and ws_node_after_comma.comment:\n",
    "#                          comment_node = ws_node_after_comma.comment\n",
    "\n",
    "#             if comment_node:\n",
    "#                 comment = comment_node.value.lstrip(\"#\").strip()\n",
    "\n",
    "\n",
    "#             # --- Append the fully parsed argument to the list ---\n",
    "#             args.append(Argument(\n",
    "#                 name=name,\n",
    "#                 default=default,\n",
    "#                 comment=comment\n",
    "#             ))\n",
    "\n",
    "#         spec = importlib.util.spec_from_loader(f\"gsm8k_{path.stem}_{hash(path)}\", loader=None)\n",
    "#         mod_dyn = importlib.util.module_from_spec(spec)\n",
    "#         exec(src_fmt, mod_dyn.__dict__)\n",
    "\n",
    "#         return ParsedFile(\n",
    "#             path=path,\n",
    "#             module_code=src_fmt,\n",
    "#             func=mod_dyn.solve,\n",
    "#             args=args\n",
    "#         )\n",
    "#     except (FileNotFoundError, StopIteration, SyntaxError, Exception) as e:\n",
    "#         print(f\"[Parser Error] Skipping {path.name}: {e!r}\", file=sys.stderr)\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def ut0_answer_match(files: List[ParsedFile], gold: float) -> List[ParsedFile]:\n",
    "#     \"\"\"Keep only files whose solve() returns the official answer with default args.\"\"\"\n",
    "#     ok_files = []\n",
    "#     for pf in files:\n",
    "#         try:\n",
    "#             if np.isclose(pf.func(), gold):\n",
    "#                 ok_files.append(pf)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[UT-0 Fail] {pf.path.name} raised {e!r}\", file=sys.stderr)\n",
    "#     return ok_files\n",
    "\n",
    "\n",
    "# def find_best_alignment(file_A: ParsedFile, file_B: ParsedFile) -> AlignmentResult:\n",
    "#     \"\"\"Find the best argument alignment between two functions, ignoring order.\"\"\"\n",
    "#     args_A, args_B = file_A.args, file_B.args\n",
    "#     if not args_A or not args_B:\n",
    "#         return AlignmentResult([], args_A, args_B, [])\n",
    "\n",
    "#     comments_B = [arg.comment for arg in args_B]\n",
    "#     embeddings_B = _MODEL.encode(comments_B, normalize_embeddings=True)\n",
    "\n",
    "#     aligned_pairs = []\n",
    "#     semantic_scores = []\n",
    "#     used_b_indices = set()\n",
    "\n",
    "#     for arg_A in args_A:\n",
    "#         embedding_A = _MODEL.encode([arg_A.comment], normalize_embeddings=True)\n",
    "#         similarities = (embedding_A @ embeddings_B.T).flatten()\n",
    "\n",
    "#         best_b_idx = -1\n",
    "#         # Find best match among available B args\n",
    "#         for b_idx in np.argsort(similarities)[::-1]:\n",
    "#             if b_idx not in used_b_indices:\n",
    "#                 best_b_idx = b_idx\n",
    "#                 break\n",
    "        \n",
    "#         if best_b_idx == -1: continue\n",
    "\n",
    "#         best_match_arg_B = args_B[best_b_idx]\n",
    "#         similarity_score = similarities[best_b_idx]\n",
    "\n",
    "#         if similarity_score >= _COS_THRESHOLD and arg_A.default == best_match_arg_B.default:\n",
    "#             aligned_pairs.append((arg_A, best_match_arg_B))\n",
    "#             semantic_scores.append(similarity_score)\n",
    "#             used_b_indices.add(best_b_idx)\n",
    "    \n",
    "#     unaligned_A = [arg for arg in args_A if arg not in [p[0] for p in aligned_pairs]]\n",
    "#     unaligned_B = [args_B[i] for i in range(len(args_B)) if i not in used_b_indices]\n",
    "\n",
    "#     return AlignmentResult(aligned_pairs, unaligned_A, unaligned_B, semantic_scores)\n",
    "\n",
    "\n",
    "# def fuzz_aligned_pair(alignment: AlignmentResult, func_A: callable, func_B: callable) -> bool:\n",
    "#     \"\"\"Fuzz-test an aligned pair using the 'Fuzz Aligned, Freeze Unaligned' strategy.\"\"\"\n",
    "#     if len(alignment.aligned_pairs) < _MIN_ALIGNMENT_FOR_FUZZ:\n",
    "#         return False\n",
    "\n",
    "#     strat_map = {}\n",
    "#     for i, (arg_A, _) in enumerate(alignment.aligned_pairs):\n",
    "#         literal = arg_A.default\n",
    "#         strat = st.floats if isinstance(literal, float) else st.integers\n",
    "#         strat_map[f\"pair_{i}\"] = strat(min_value=1, max_value=50)\n",
    "\n",
    "#     # Freeze unaligned args to their defaults\n",
    "#     frozen_kwargs_A = {arg.name: arg.default for arg in alignment.unaligned_A}\n",
    "#     frozen_kwargs_B = {arg.name: arg.default for arg in alignment.unaligned_B}\n",
    "\n",
    "#     @settings(max_examples=_FUZZ_EXAMPLES, deadline=None, suppress_health_check=[HealthCheck.too_slow])\n",
    "#     @given(st.fixed_dictionaries(strat_map))\n",
    "#     def _check(fuzzed_values):\n",
    "#         kwargs_A = frozen_kwargs_A.copy()\n",
    "#         kwargs_B = frozen_kwargs_B.copy()\n",
    "\n",
    "#         for i, (arg_A, arg_B) in enumerate(alignment.aligned_pairs):\n",
    "#             fuzzed_val = fuzzed_values[f\"pair_{i}\"]\n",
    "#             kwargs_A[arg_A.name] = fuzzed_val\n",
    "#             kwargs_B[arg_B.name] = fuzzed_val\n",
    "\n",
    "#         assert np.isclose(func_A(**kwargs_A), func_B(**kwargs_B))\n",
    "\n",
    "#     try:\n",
    "#         _check()\n",
    "#         return True\n",
    "#     except Exception:\n",
    "#         return False\n",
    "\n",
    "\n",
    "# def calculate_pairwise_score(alignment: AlignmentResult, file_A: ParsedFile, file_B: ParsedFile) -> float:\n",
    "#     \"\"\"Calculate the quality score for a single validated pair.\"\"\"\n",
    "#     num_aligned = len(alignment.aligned_pairs)\n",
    "    \n",
    "#     # Alignment Ratio\n",
    "#     total_args_A = len(file_A.args)\n",
    "#     total_args_B = len(file_B.args)\n",
    "#     max_possible_args = max(total_args_A, total_args_B)\n",
    "#     alignment_ratio = num_aligned / max_possible_args if max_possible_args > 0 else 1.0\n",
    "\n",
    "#     # Semantic Strength\n",
    "#     semantic_strength = np.mean(alignment.semantic_scores) if alignment.semantic_scores else 1.0\n",
    "\n",
    "#     return (W_ALIGNMENT * alignment_ratio) + (W_SEMANTIC * semantic_strength)\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Orchestration and Reporting\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# def analyze_problem_outputs(problem_dir: Path, gold_answer: float):\n",
    "#     \"\"\"Main orchestrator to analyze all model outputs for a single problem.\"\"\"\n",
    "#     print(f\"\\n{'='*20} Analyzing Problem: {problem_dir.name} {'='*20}\")\n",
    "    \n",
    "#     all_files = list(problem_dir.glob(\"*.py\"))\n",
    "#     if not all_files:\n",
    "#         print(\"No Python files found in this directory.\")\n",
    "#         return\n",
    "\n",
    "#     parsed_files = [pf for pf in [parse_file(p) for p in all_files] if pf is not None]\n",
    "#     print(f\"Found and parsed {len(parsed_files)} files.\")\n",
    "\n",
    "#     survivors_ut0 = ut0_answer_match(parsed_files, gold_answer)\n",
    "#     print(f\"{len(survivors_ut0)} files passed UT-0 (correct default answer).\")\n",
    "#     if len(survivors_ut0) < 2:\n",
    "#         print(\"Not enough models passed UT-0 to find a pair. Aborting.\")\n",
    "#         return\n",
    "\n",
    "#     # --- Pairwise Validation ---\n",
    "#     validated_pairs: List[PairwiseValidation] = []\n",
    "#     for file_A, file_B in itertools.combinations(survivors_ut0, 2):\n",
    "#         alignment = find_best_alignment(file_A, file_B)\n",
    "        \n",
    "#         if fuzz_aligned_pair(alignment, file_A.func, file_B.func):\n",
    "#             score = calculate_pairwise_score(alignment, file_A, file_B)\n",
    "#             validated_pairs.append(PairwiseValidation(file_A, file_B, alignment, score))\n",
    "#             print(f\"  ✓ Validated Pair: ({file_A.path.name}, {file_B.path.name}), Score: {score:.3f}\")\n",
    "\n",
    "#     if not validated_pairs:\n",
    "#         print(\"\\nNo functionally equivalent pairs found after fuzzing.\")\n",
    "#         return\n",
    "\n",
    "#     # --- Find Best Consensus Clique ---\n",
    "#     nodes = survivors_ut0\n",
    "#     adj = {pf.path.name: set() for pf in nodes}\n",
    "#     for vp in validated_pairs:\n",
    "#         adj[vp.file_A.path.name].add(vp.file_B.path.name)\n",
    "#         adj[vp.file_B.path.name].add(vp.file_A.path.name)\n",
    "\n",
    "#     best_clique = []\n",
    "#     # Check for cliques of decreasing size\n",
    "#     for size in range(len(nodes), 1, -1):\n",
    "#         for combo in itertools.combinations(nodes, size):\n",
    "#             names = [pf.path.name for pf in combo]\n",
    "#             is_clique = all(\n",
    "#                 names[j] in adj[names[i]] for i in range(size) for j in range(i + 1, size)\n",
    "#             )\n",
    "#             if is_clique:\n",
    "#                 best_clique = list(combo)\n",
    "#                 break\n",
    "#         if best_clique:\n",
    "#             break\n",
    "    \n",
    "#     # --- Calculate Final Score and Report ---\n",
    "#     if not best_clique:\n",
    "#         # Should not happen if validated_pairs is not empty\n",
    "#         best_pair = max(validated_pairs, key=lambda vp: vp.quality_score)\n",
    "#         final_score = best_pair.quality_score\n",
    "#         clique_size = 2\n",
    "#         best_clique_names = [best_pair.file_A.path.name, best_pair.file_B.path.name]\n",
    "#         avg_quality = final_score\n",
    "#     else:\n",
    "#         clique_size = len(best_clique)\n",
    "#         clique_names = [pf.path.name for pf in best_clique]\n",
    "        \n",
    "#         clique_pairs_scores = [\n",
    "#             vp.quality_score for vp in validated_pairs \n",
    "#             if vp.file_A.path.name in clique_names and vp.file_B.path.name in clique_names\n",
    "#         ]\n",
    "#         avg_quality = np.mean(clique_pairs_scores) if clique_pairs_scores else 0\n",
    "#         bonus = CONSENSUS_BONUS.get(clique_size, max(CONSENSUS_BONUS.values()))\n",
    "#         final_score = avg_quality * bonus\n",
    "\n",
    "#     print(\"\\n\" + \"-\"*50)\n",
    "#     print(\"                 VALIDATION SUMMARY\")\n",
    "#     print(\"-\"*50)\n",
    "#     print(f\"Best Consensus Found: {clique_size}-way agreement\")\n",
    "#     print(f\"Models in Consensus: {best_clique_names}\")\n",
    "#     print(f\"Average Pairwise Quality in Clique: {avg_quality:.4f}\")\n",
    "#     print(f\"Consensus Bonus Multiplier: x{CONSENSUS_BONUS.get(clique_size, 'N/A')}\")\n",
    "#     print(f\"FINAL CONFIDENCE SCORE: {final_score:.4f}\")\n",
    "#     print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5affe16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gsm8k_validator_v2.py\n",
    "======================\n",
    "\n",
    "Performs a flexible, score-based validation of multiple LLM-generated `solve()`\n",
    "functions for a given GSM8K problem. Instead of a rigid filter, this script\n",
    "analyzes all pairs of models to find the most robust and comprehensive consensus.\n",
    "\n",
    "The final output is a confidence score (0.0-1.0+) that reflects:\n",
    "1.  The completeness of parameter alignment between models.\n",
    "2.  The semantic clarity of the aligned parameters.\n",
    "3.  The level of consensus (number of models in agreement).\n",
    "\n",
    "Core Logic:\n",
    "-----------\n",
    "1.  **Parse & Pre-filter (UT-0):** All generated Python files for a problem are\n",
    "    parsed and filtered, keeping only those that produce the correct answer\n",
    "    for the default values.\n",
    "\n",
    "2.  **Pairwise Alignment (UT-2):** For every pair of surviving models, find the\n",
    "    best possible alignment between their function arguments based on semantic\n",
    "    similarity (SBERT) and matching default values. Order is ignored.\n",
    "\n",
    "3.  **Pairwise Fuzzing (UT-3):** For each aligned pair, fuzz-test for functional\n",
    "    equivalence using the \"Fuzz Aligned, Freeze Unaligned\" strategy. This\n",
    "    ensures logical soundness even with partially matching signatures.\n",
    "\n",
    "4.  **Scoring & Consensus:**\n",
    "    - Each successfully validated pair receives a `PairwiseQualityScore` based\n",
    "      on its Alignment Ratio and Semantic Strength.\n",
    "    - The script identifies the largest \"clique\" of models that are all\n",
    "      mutually validated.\n",
    "    - A final `ConfidenceScore` is computed from the clique's average quality,\n",
    "      boosted by a bonus for the size of the consensus.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "black                – whitespace-stable formatting\n",
    "libcst               – reliable CST traversal with comment access\n",
    "hypothesis           – property-based fuzzing\n",
    "sentence-transformers (mpnet-base) – SBERT cosine for comment semantics\n",
    "numpy                - for fast vector operations\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- (inside gsm8k_validator.py) ---\n",
    "\n",
    "import importlib.util\n",
    "import inspect\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Any, Tuple, Dict\n",
    "\n",
    "import black\n",
    "import hypothesis.strategies as st\n",
    "import libcst as cst  # <--- ADD THIS LINE\n",
    "import numpy as np\n",
    "from hypothesis import given, settings, HealthCheck\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Global constants & Configuration\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "_MODEL = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "_COS_THRESHOLD = 0.70  # SBERT cosine ≥ 0.7 ⇒ semantic match\n",
    "_FUZZ_EXAMPLES = 50  # Hypothesis draws\n",
    "_MIN_ALIGNMENT_FOR_FUZZ = 1 # A pair must align on at least one arg to be fuzzed\n",
    "\n",
    "# --- Scoring Weights --- #\n",
    "W_ALIGNMENT = 0.7\n",
    "W_SEMANTIC = 0.3\n",
    "\n",
    "# --- Consensus Bonus Multipliers --- #\n",
    "CONSENSUS_BONUS = {\n",
    "    2: 1.0,  # Baseline for a pair\n",
    "    3: 1.1,  # 10% bonus for a 3-way consensus\n",
    "    4: 1.2,  # 20% bonus for a 4-way consensus\n",
    "    5: 1.3,\n",
    "}\n",
    "\n",
    "_TRACE_RE = re.compile(r\"^#: L(\\d+)\\b\")\n",
    "_DOC_INDEX_RE = re.compile(r\"^Index:\\s*(\\d+)\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Dataclasses for Structured Data\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Argument:\n",
    "    \"\"\"Represents a single argument from a function signature.\"\"\"\n",
    "    name: str\n",
    "    arg_type: str  # <--- ADD THIS LINE\n",
    "    default: Any\n",
    "    comment: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParsedFile:\n",
    "    \"\"\"Normalised representation of a generated code file.\"\"\"\n",
    "    path: Path\n",
    "    module_code: str\n",
    "    func: Any\n",
    "    args: List[Argument] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AlignmentResult:\n",
    "    \"\"\"Stores the result of aligning two ParsedFiles.\"\"\"\n",
    "    aligned_pairs: List[Tuple[Argument, Argument]]\n",
    "    unaligned_A: List[Argument]\n",
    "    unaligned_B: List[Argument]\n",
    "    semantic_scores: List[float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PairwiseValidation:\n",
    "    \"\"\"Stores the full result of a successful pairwise validation.\"\"\"\n",
    "    file_A: ParsedFile\n",
    "    file_B: ParsedFile\n",
    "    alignment: AlignmentResult\n",
    "    quality_score: float\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Core Logic Implementation\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# In your validator script, find and replace the parse_file function\n",
    "\n",
    "def parse_file(path: Path) -> ParsedFile | None:\n",
    "    \"\"\"\n",
    "    Parse one generated .py file into a structured ParsedFile object using\n",
    "    a direct, regex-based approach.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src_raw = path.read_text(encoding=\"utf-8\")\n",
    "        src_fmt = black.format_str(src_raw, mode=black.FileMode())\n",
    "\n",
    "        signature_match = re.search(r\"def solve\\s*\\((.*?)\\):\", src_fmt, re.DOTALL)\n",
    "        if not signature_match:\n",
    "            raise ValueError(\"Could not find a 'def solve(...):' signature.\")\n",
    "        \n",
    "        signature_content = signature_match.group(1)\n",
    "\n",
    "        args = []\n",
    "        # --- MODIFIED REGEX: Now captures the type hint in group(2) ---\n",
    "        arg_pattern = re.compile(\n",
    "            r\"^\\s*([a-zA-Z_]\\w*)\\s*:\\s*(\\w+)\\s*=\\s*(.*?)\\s*,?\\s*(?:#\\s*(.*))?$\"\n",
    "        )\n",
    "\n",
    "        for line in signature_content.splitlines():\n",
    "            if not line.strip(): continue\n",
    "            match = arg_pattern.match(line)\n",
    "            if match:\n",
    "                name = match.group(1)\n",
    "                arg_type = match.group(2) # <--- CAPTURE TYPE\n",
    "                default_str = match.group(3).strip()\n",
    "                default_val = eval(default_str)\n",
    "                comment = match.group(4).strip() if match.group(4) else \"\"\n",
    "                \n",
    "                args.append(Argument(\n",
    "                    name=name,\n",
    "                    arg_type=arg_type, # <--- STORE TYPE\n",
    "                    default=default_val,\n",
    "                    comment=comment\n",
    "                ))\n",
    "\n",
    "        spec = importlib.util.spec_from_loader(f\"gsm8k_{path.stem}_{hash(path)}\", loader=None)\n",
    "        mod_dyn = importlib.util.module_from_spec(spec)\n",
    "        exec(src_fmt, mod_dyn.__dict__)\n",
    "\n",
    "        return ParsedFile(\n",
    "            path=path,\n",
    "            module_code=src_fmt,\n",
    "            func=mod_dyn.solve,\n",
    "            args=args\n",
    "        )\n",
    "    except (FileNotFoundError, StopIteration, SyntaxError, Exception) as e:\n",
    "        print(f\"[Parser Error] Skipping {path.name}: {e!r}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "\n",
    "def ut0_answer_match(files: List[ParsedFile], gold: float) -> List[ParsedFile]:\n",
    "    \"\"\"Keep only files whose solve() returns the official answer with default args.\"\"\"\n",
    "    ok_files = []\n",
    "    for pf in files:\n",
    "        try:\n",
    "            if np.isclose(pf.func(), gold):\n",
    "                ok_files.append(pf)\n",
    "        except Exception as e:\n",
    "            print(f\"[UT-0 Fail] {pf.path.name} raised {e!r}\", file=sys.stderr)\n",
    "    return ok_files\n",
    "\n",
    "def find_best_alignment(file_A: ParsedFile, file_B: ParsedFile) -> AlignmentResult:\n",
    "    \"\"\"\n",
    "    Finds the best argument alignment using a 'bucket and match' strategy.\n",
    "    1. Groups args from each file into buckets by (type, default_value).\n",
    "    2. Only performs semantic comparison on args within matching buckets.\n",
    "    \"\"\"\n",
    "    # --- 1. Create buckets for each file's arguments ---\n",
    "    buckets_A = defaultdict(list)\n",
    "    for arg in file_A.args:\n",
    "        buckets_A[(arg.arg_type, arg.default)].append(arg)\n",
    "        \n",
    "    buckets_B = defaultdict(list)\n",
    "    for arg in file_B.args:\n",
    "        buckets_B[(arg.arg_type, arg.default)].append(arg)\n",
    "\n",
    "    aligned_pairs = []\n",
    "    semantic_scores = []\n",
    "    \n",
    "    # --- 2. Iterate through buckets that exist in BOTH files ---\n",
    "    common_keys = set(buckets_A.keys()) & set(buckets_B.keys())\n",
    "    \n",
    "    for key in common_keys:\n",
    "        args_in_bucket_A = buckets_A[key]\n",
    "        args_in_bucket_B = buckets_B[key]\n",
    "        \n",
    "        # --- 3. Perform semantic alignment ONLY within the current bucket ---\n",
    "        texts_A = [arg.name.replace(\"_\", \" \") + \" | \" + arg.comment for arg in args_in_bucket_A]\n",
    "        texts_B = [arg.name.replace(\"_\", \" \") + \" | \" + arg.comment for arg in args_in_bucket_B]\n",
    "        \n",
    "        embeddings_A = _MODEL.encode(texts_A, normalize_embeddings=True)\n",
    "        embeddings_B = _MODEL.encode(texts_B, normalize_embeddings=True)\n",
    "        similarity_matrix = embeddings_A @ embeddings_B.T\n",
    "\n",
    "        # Use a greedy matching strategy within the bucket\n",
    "        sorted_indices = np.argsort(similarity_matrix, axis=None)[::-1]\n",
    "        flat_indices = np.atleast_1d(sorted_indices)\n",
    "        rows, cols = np.unravel_index(flat_indices, similarity_matrix.shape)\n",
    "\n",
    "        used_in_bucket_A = set()\n",
    "        used_in_bucket_B = set()\n",
    "\n",
    "        for i, j in zip(rows, cols):\n",
    "            if i in used_in_bucket_A or j in used_in_bucket_B:\n",
    "                continue\n",
    "\n",
    "            similarity_score = similarity_matrix[i, j]\n",
    "            if similarity_score >= _COS_THRESHOLD:\n",
    "                aligned_pairs.append((args_in_bucket_A[i], args_in_bucket_B[j]))\n",
    "                semantic_scores.append(similarity_score)\n",
    "                used_in_bucket_A.add(i)\n",
    "                used_in_bucket_B.add(j)\n",
    "\n",
    "    # --- 4. Calculate the final unaligned sets ---\n",
    "    final_aligned_A = {p[0] for p in aligned_pairs}\n",
    "    final_aligned_B = {p[1] for p in aligned_pairs}\n",
    "    unaligned_A = [arg for arg in file_A.args if arg not in final_aligned_A]\n",
    "    unaligned_B = [arg for arg in file_B.args if arg not in final_aligned_B]\n",
    "\n",
    "    return AlignmentResult(aligned_pairs, unaligned_A, unaligned_B, semantic_scores)\n",
    "\n",
    "\n",
    "def fuzz_aligned_pair(alignment: AlignmentResult, func_A: callable, func_B: callable) -> bool:\n",
    "    \"\"\"Fuzz-test an aligned pair using the 'Fuzz Aligned, Freeze Unaligned' strategy.\"\"\"\n",
    "    if len(alignment.aligned_pairs) < _MIN_ALIGNMENT_FOR_FUZZ:\n",
    "        return False\n",
    "\n",
    "    strat_map = {}\n",
    "    for i, (arg_A, _) in enumerate(alignment.aligned_pairs):\n",
    "        literal = arg_A.default\n",
    "        strat = st.floats if isinstance(literal, float) else st.integers\n",
    "        strat_map[f\"pair_{i}\"] = strat(min_value=1, max_value=50)\n",
    "\n",
    "    # Freeze unaligned args to their defaults\n",
    "    frozen_kwargs_A = {arg.name: arg.default for arg in alignment.unaligned_A}\n",
    "    frozen_kwargs_B = {arg.name: arg.default for arg in alignment.unaligned_B}\n",
    "\n",
    "    @settings(max_examples=_FUZZ_EXAMPLES, deadline=None, suppress_health_check=[HealthCheck.too_slow])\n",
    "    @given(st.fixed_dictionaries(strat_map))\n",
    "    def _check(fuzzed_values):\n",
    "        kwargs_A = frozen_kwargs_A.copy()\n",
    "        kwargs_B = frozen_kwargs_B.copy()\n",
    "\n",
    "        for i, (arg_A, arg_B) in enumerate(alignment.aligned_pairs):\n",
    "            fuzzed_val = fuzzed_values[f\"pair_{i}\"]\n",
    "            kwargs_A[arg_A.name] = fuzzed_val\n",
    "            kwargs_B[arg_B.name] = fuzzed_val\n",
    "\n",
    "        assert np.isclose(func_A(**kwargs_A), func_B(**kwargs_B))\n",
    "\n",
    "    try:\n",
    "        _check()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def calculate_pairwise_score(alignment: AlignmentResult, file_A: ParsedFile, file_B: ParsedFile) -> float:\n",
    "    \"\"\"Calculate the quality score for a single validated pair.\"\"\"\n",
    "    num_aligned = len(alignment.aligned_pairs)\n",
    "    \n",
    "    # Alignment Ratio\n",
    "    total_args_A = len(file_A.args)\n",
    "    total_args_B = len(file_B.args)\n",
    "    max_possible_args = max(total_args_A, total_args_B)\n",
    "    alignment_ratio = num_aligned / max_possible_args if max_possible_args > 0 else 1.0\n",
    "\n",
    "    # Semantic Strength\n",
    "    semantic_strength = np.mean(alignment.semantic_scores) if alignment.semantic_scores else 1.0\n",
    "\n",
    "    return (W_ALIGNMENT * alignment_ratio) + (W_SEMANTIC * semantic_strength)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Orchestration and Reporting\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def analyze_problem_outputs(problem_dir: Path, gold_answer: float):\n",
    "    \"\"\"Main orchestrator to analyze all model outputs for a single problem.\"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing Problem: {problem_dir.name} {'='*20}\")\n",
    "    \n",
    "    all_files = list(problem_dir.glob(\"*.py\"))\n",
    "    if not all_files:\n",
    "        print(\"No Python files found in this directory.\")\n",
    "        return\n",
    "\n",
    "    parsed_files = [pf for pf in [parse_file(p) for p in all_files] if pf is not None]\n",
    "    print(f\"Found and parsed {len(parsed_files)} files.\")\n",
    "\n",
    "    survivors_ut0 = ut0_answer_match(parsed_files, gold_answer)\n",
    "    print(f\"{len(survivors_ut0)} files passed UT-0 (correct default answer).\")\n",
    "    if len(survivors_ut0) < 2:\n",
    "        print(\"Not enough models passed UT-0 to find a pair. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- Pairwise Validation ---\n",
    "    validated_pairs: List[PairwiseValidation] = []\n",
    "    for file_A, file_B in itertools.combinations(survivors_ut0, 2):\n",
    "        alignment = find_best_alignment(file_A, file_B)\n",
    "        \n",
    "        if fuzz_aligned_pair(alignment, file_A.func, file_B.func):\n",
    "            score = calculate_pairwise_score(alignment, file_A, file_B)\n",
    "            validated_pairs.append(PairwiseValidation(file_A, file_B, alignment, score))\n",
    "            print(f\"  ✓ Validated Pair: ({file_A.path.name}, {file_B.path.name}), Score: {score:.3f}\")\n",
    "\n",
    "    if not validated_pairs:\n",
    "        print(\"\\nNo functionally equivalent pairs found after fuzzing.\")\n",
    "        return\n",
    "\n",
    "    # --- Find Best Consensus Clique ---\n",
    "    nodes = survivors_ut0\n",
    "    adj = {pf.path.name: set() for pf in nodes}\n",
    "    for vp in validated_pairs:\n",
    "        adj[vp.file_A.path.name].add(vp.file_B.path.name)\n",
    "        adj[vp.file_B.path.name].add(vp.file_A.path.name)\n",
    "\n",
    "    best_clique = []\n",
    "    # Check for cliques of decreasing size\n",
    "    for size in range(len(nodes), 1, -1):\n",
    "        for combo in itertools.combinations(nodes, size):\n",
    "            names = [pf.path.name for pf in combo]\n",
    "            is_clique = all(\n",
    "                names[j] in adj[names[i]] for i in range(size) for j in range(i + 1, size)\n",
    "            )\n",
    "            if is_clique:\n",
    "                best_clique = list(combo)\n",
    "                break\n",
    "        if best_clique:\n",
    "            break\n",
    "    \n",
    "    # --- Calculate Final Score and Report ---\n",
    "    if not best_clique:\n",
    "        # Should not happen if validated_pairs is not empty\n",
    "        best_pair = max(validated_pairs, key=lambda vp: vp.quality_score)\n",
    "        final_score = best_pair.quality_score\n",
    "        clique_size = 2\n",
    "        best_clique_names = [best_pair.file_A.path.name, best_pair.file_B.path.name]\n",
    "        avg_quality = final_score\n",
    "    else:\n",
    "        clique_size = len(best_clique)\n",
    "        clique_names = [pf.path.name for pf in best_clique]\n",
    "        \n",
    "        clique_pairs_scores = [\n",
    "            vp.quality_score for vp in validated_pairs \n",
    "            if vp.file_A.path.name in clique_names and vp.file_B.path.name in clique_names\n",
    "        ]\n",
    "        avg_quality = np.mean(clique_pairs_scores) if clique_pairs_scores else 0\n",
    "        bonus = CONSENSUS_BONUS.get(clique_size, max(CONSENSUS_BONUS.values()))\n",
    "        final_score = avg_quality * bonus\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"                 VALIDATION SUMMARY\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Best Consensus Found: {clique_size}-way agreement\")\n",
    "    print(f\"Models in Consensus: {best_clique_names}\")\n",
    "    print(f\"Average Pairwise Quality in Clique: {avg_quality:.4f}\")\n",
    "    print(f\"Consensus Bonus Multiplier: x{CONSENSUS_BONUS.get(clique_size, 'N/A')}\")\n",
    "    print(f\"FINAL CONFIDENCE SCORE: {final_score:.4f}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e4ac29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Bill milked his cow and got 16 gallons of milk. He turned 1/4 into sour cream, 1/4 into butter, and kept the rest as whole milk. It takes 4 gallons of milk to make one gallon of butter and 2 gallons of milk to make 1 gallon of sour cream. If Bill sells butter for $5/gallon, sour cream for $6/gallon, and whole milk for $3/gallon, how much money does he make?', 'answer': 'First find how much milk Bill turned into sour cream and butter: 16 gallons * 1/4 = <<16*1/4=4>>4 gallons\\nThen find how many gallons of butter he makes out of 4 gallons of milk: 4 gallons milk / 4 gallons milk/1 gallon butter = <<4/4/1=1>>1 gallon butter\\nThen find how many gallons of sour cream he makes out of 4 gallons of milk: 4 gallons milk / 2 gallons milk/1 gallon sour cream = <<4/2/1=2>>2 gallon sour cream\\nThen subtract the amount of milk turned into butter and sour cream to find the remaining amount of whole milk: 16 gallons - 4 gallons - 4 gallons = <<16-4-4=8>>8 gallons\\nThen multiply the number of gallons of milk by the price of milk to find the total cost of the milk: 8 gallons * $3/gallon = $<<8*3=24>>24\\nThen multiply the number of gallons of sour cream by the price of sour cream to find the total cost of the sour cream: 2 gallons * $6/gallon = $<<2*6=12>>12\\nThen add the cost of the butter, sour cream and milk to find the total amount of money Bill earns: $24 + $12 + $5 = $<<24+12+5=41>>41\\n#### 41'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset (train split)\n",
    "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "sample_5464 = gsm8k_train[5464]\n",
    "print(sample_5464)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82fa680",
   "metadata": {},
   "source": [
    "### Cell 1: Setup and Imports\n",
    "\n",
    "This cell imports the necessary libraries and the functions from your validator script. It also sets up the problem parameters we'll use for the test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd8878f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Functions imported and configuration set.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Ensure the script's directory is in the Python path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# # Import all the necessary functions and classes from your validator\n",
    "# from gsm8k_validator import (\n",
    "#     parse_file,\n",
    "#     ut0_answer_match,\n",
    "#     find_best_alignment,\n",
    "#     fuzz_aligned_pair,\n",
    "#     calculate_pairwise_score,\n",
    "#     analyze_problem_outputs,\n",
    "#     ParsedFile,\n",
    "#     AlignmentResult,\n",
    "#     PairwiseValidation\n",
    "# )\n",
    "\n",
    "# --- Configuration for our test run ---\n",
    "# Let's use problem 4483 from our previous discussion\n",
    "BASE_DIR = Path(\"code_generation_outputs_cleaned\")\n",
    "PROBLEM_INDEX = 4483\n",
    "GOLD_ANSWER = 100.0  # Sammy: 200*4=800, Bryan: 100*6+100*1=700. Diff=100\n",
    "\n",
    "# Pretty printer for clean output\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "print(\"Setup complete. Functions imported and configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea345920",
   "metadata": {},
   "source": [
    "### Cell 2: Test `parse_file` on a Single File\n",
    "\n",
    "This cell tests the foundational parsing logic. We'll pick one file and inspect the `ParsedFile` object it produces to ensure arguments, comments, and the function itself are extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad007fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing parse_file on: anthropic_claude-3-5-haiku-20241022.py ---\n",
      "\n",
      "Successfully parsed the file. Contents:\n",
      "Path: anthropic_claude-3-5-haiku-20241022.py\n",
      "Callable function found: True\n",
      "\n",
      "Extracted Arguments:\n",
      "  - Name: total_records, Default: 200, Comment: 'Peggy has 200 records'\n",
      "  - Name: sammy_price_per_record, Default: 4, Comment: 'Sammy offers 4 dollars each'\n",
      "  - Name: bryan_high_price_per_record, Default: 6, Comment: 'Bryan offers 6 dollars for half'\n",
      "  - Name: bryan_low_price_per_record, Default: 1, Comment: 'Bryan offers 1 dollar for other half'\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test `parse_file` on a single file\n",
    "\n",
    "def test_parse_file(base_dir, problem_index):\n",
    "    problem_dir = BASE_DIR / str(PROBLEM_INDEX)\n",
    "    # Let's test the Anthropic file, which had a different argument order\n",
    "    file_to_test = problem_dir / \"anthropic_claude-3-5-haiku-20241022.py\"\n",
    "\n",
    "    print(f\"--- Testing parse_file on: {file_to_test.name} ---\")\n",
    "    parsed_file_single = parse_file(file_to_test)\n",
    "\n",
    "    if parsed_file_single:\n",
    "        print(\"\\nSuccessfully parsed the file. Contents:\")\n",
    "        print(f\"Path: {parsed_file_single.path.name}\")\n",
    "        print(f\"Callable function found: {callable(parsed_file_single.func) and parsed_file_single.func.__name__ == 'solve'}\")\n",
    "        \n",
    "        print(\"\\nExtracted Arguments:\")\n",
    "        for arg in parsed_file_single.args:\n",
    "            print(f\"  - Name: {arg.name}, Default: {arg.default}, Comment: '{arg.comment}'\")\n",
    "    else:\n",
    "        print(\"\\nFailed to parse the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc678a",
   "metadata": {},
   "source": [
    "### Cell 3: Load All Files and Run `ut0_answer_match`\n",
    "\n",
    "Now, let's parse all files for the problem and run the first filtering step (UT-0) to see which ones produce the correct default answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b757f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 files in 'code_generation_outputs_cleaned/4483'.\n",
      "Successfully parsed 9 files.\n",
      "\n",
      "--- Running UT-0 (Answer Match against Gold Answer: 100.0) ---\n",
      "\n",
      "9 files passed UT-0:\n",
      "  - anthropic_claude-3-5-haiku-20241022.py\n",
      "  - google_gemini-2.5-flash.py\n",
      "  - google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "  - openai_o3-mini.py\n",
      "  - openai_gpt-4.1.py\n",
      "  - google_gemini-2.0-flash-thinking-exp.py\n",
      "  - openai_o4-mini.py\n",
      "  - openai_gpt-4.1-mini.py\n",
      "  - google_gemini-2.5-pro.py\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load all files and run `ut0_answer_match`\n",
    "\n",
    "def test_ut0(problem_index, gold_answer):\n",
    "    all_files_for_problem = list(problem_dir.glob(\"*.py\"))\n",
    "    print(f\"Found {len(all_files_for_problem)} files in '{problem_dir}'.\")\n",
    "\n",
    "    # Parse all of them\n",
    "    all_parsed_files = [pf for pf in [parse_file(p) for p in all_files_for_problem] if pf]\n",
    "    print(f\"Successfully parsed {len(all_parsed_files)} files.\")\n",
    "\n",
    "    print(f\"\\n--- Running UT-0 (Answer Match against Gold Answer: {GOLD_ANSWER}) ---\")\n",
    "    survivors_ut0 = ut0_answer_match(all_parsed_files, GOLD_ANSWER)\n",
    "\n",
    "    print(f\"\\n{len(survivors_ut0)} files passed UT-0:\")\n",
    "    for pf in survivors_ut0:\n",
    "        print(f\"  - {pf.path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d9fd2",
   "metadata": {},
   "source": [
    "### Cell 4: Test `find_best_alignment` on a Pair\n",
    "\n",
    "This is a critical step. We will manually select two files that we know have different argument structures and see if the alignment logic correctly identifies the matching parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Aligning google_gemini-2.0-flash-thinking-exp.py (A) and anthropic_claude-3-5-haiku-20241022.py (B) ---\n",
      "\n",
      "--- Correct Alignment Result (from find_best_alignment) ---\n",
      "  - Found 4 aligned pairs.\n",
      "    - Pair 1: 'bryan_price_interested' (A) <=> 'bryan_high_price_per_record' (B)\n",
      "    - Pair 2: 'bryan_price_not_interested' (A) <=> 'bryan_low_price_per_record' (B)\n",
      "    - Pair 3: 'sammy_price_per_record' (A) <=> 'sammy_price_per_record' (B)\n",
      "    - Pair 4: 'total_records' (A) <=> 'total_records' (B)\n",
      "\n",
      "==================================================\n",
      "           DETAILED ALIGNMENT DEBUG REPORT\n",
      "==================================================\n",
      "\n",
      "--- Comparing Bucket: type='int', value=6 ---\n",
      "  - Comparing A:'bryan_price_interested' with B:'bryan_high_price_per_record'\n",
      "      Similarity: 0.7210 | ==> Status: PASS\n",
      "\n",
      "--- Comparing Bucket: type='int', value=1 ---\n",
      "  - Comparing A:'bryan_price_not_interested' with B:'bryan_low_price_per_record'\n",
      "      Similarity: 0.7451 | ==> Status: PASS\n",
      "\n",
      "--- Comparing Bucket: type='int', value=200 ---\n",
      "  - Comparing A:'total_records' with B:'total_records'\n",
      "      Similarity: 1.0000 | ==> Status: PASS\n",
      "\n",
      "--- Comparing Bucket: type='int', value=4 ---\n",
      "  - Comparing A:'sammy_price_per_record' with B:'sammy_price_per_record'\n",
      "      Similarity: 0.9663 | ==> Status: PASS\n",
      "\n",
      "==================================================\n",
      "NOTE: Any comparisons not shown were skipped because the arguments were\n",
      "      not in a matching (type, default_value) bucket.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test `find_best_alignment` on a Pair (Corrected Debug Report)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def test_alignment\n",
    "try:\n",
    "    # --- 1. Select the two files to compare ---\n",
    "    file_A = next(pf for pf in survivors_ut0 if \"google_gemini-2.0-flash-thinking-exp\" in pf.path.name)\n",
    "    file_B = next(pf for pf in survivors_ut0 if \"anthropic_claude-3-5-haiku\" in pf.path.name)\n",
    "\n",
    "    print(f\"--- Aligning {file_A.path.name} (A) and {file_B.path.name} (B) ---\")\n",
    "    \n",
    "    # --- 2. Run the real alignment function to see its correct output ---\n",
    "    alignment_result = find_best_alignment(file_A, file_B)\n",
    "\n",
    "    print(\"\\n--- Correct Alignment Result (from find_best_alignment) ---\")\n",
    "    print(f\"  - Found {len(alignment_result.aligned_pairs)} aligned pairs.\")\n",
    "    if alignment_result.aligned_pairs:\n",
    "        sorted_pairs = sorted(alignment_result.aligned_pairs, key=lambda p: p[0].name)\n",
    "        for i, (arg_A, arg_B) in enumerate(sorted_pairs):\n",
    "            print(f\"    - Pair {i+1}: '{arg_A.name}' (A) <=> '{arg_B.name}' (B)\")\n",
    "\n",
    "    # --- 3. DETAILED DEBUG REPORT USING THE \"BUCKET AND MATCH\" STRATEGY ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           DETAILED ALIGNMENT DEBUG REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create buckets for each file's arguments\n",
    "    buckets_A = defaultdict(list)\n",
    "    for arg in file_A.args:\n",
    "        buckets_A[(arg.arg_type, arg.default)].append(arg)\n",
    "        \n",
    "    buckets_B = defaultdict(list)\n",
    "    for arg in file_B.args:\n",
    "        buckets_B[(arg.arg_type, arg.default)].append(arg)\n",
    "    \n",
    "    # Iterate through buckets that exist in BOTH files\n",
    "    common_keys = set(buckets_A.keys()) & set(buckets_B.keys())\n",
    "    \n",
    "    if not common_keys:\n",
    "        print(\"No common (type, default_value) buckets found between files.\")\n",
    "        \n",
    "    for key in common_keys:\n",
    "        print(f\"\\n--- Comparing Bucket: type='{key[0]}', value={key[1]} ---\")\n",
    "        \n",
    "        args_in_bucket_A = buckets_A[key]\n",
    "        args_in_bucket_B = buckets_B[key]\n",
    "        \n",
    "        # Perform semantic comparison only within this bucket\n",
    "        texts_A = [arg.name.replace(\"_\", \" \") + \" | \" + arg.comment for arg in args_in_bucket_A]\n",
    "        texts_B = [arg.name.replace(\"_\", \" \") + \" | \" + arg.comment for arg in args_in_bucket_B]\n",
    "        \n",
    "        embeddings_A = _MODEL.encode(texts_A, normalize_embeddings=True)\n",
    "        embeddings_B = _MODEL.encode(texts_B, normalize_embeddings=True)\n",
    "        similarity_matrix = embeddings_A @ embeddings_B.T\n",
    "\n",
    "        for i, arg_A in enumerate(args_in_bucket_A):\n",
    "            for j, arg_B in enumerate(args_in_bucket_B):\n",
    "                sim_score = similarity_matrix[i, j]\n",
    "                status = \"PASS\" if sim_score >= _COS_THRESHOLD else \"FAIL (Similarity Too Low)\"\n",
    "                \n",
    "                print(f\"  - Comparing A:'{arg_A.name}' with B:'{arg_B.name}'\")\n",
    "                print(f\"      Similarity: {sim_score:.4f} | ==> Status: {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NOTE: Any comparisons not shown were skipped because the arguments were\")\n",
    "    print(\"      not in a matching (type, default_value) bucket.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"Could not find the specified files among UT-0 survivors. Please check filenames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f12bac",
   "metadata": {},
   "source": [
    "### Cell 5: Test `fuzz_aligned_pair` and `calculate_pairwise_score`\n",
    "\n",
    "Now we'll take the `alignment_result` from the previous cell and use it to run the fuzzing test. If it passes, we'll calculate the quality score for this specific pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28678aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test `fuzz_aligned_pair` and `calculate_pairwise_score`\n",
    "\n",
    "# We'll use the alignment_result from the previous cell\n",
    "if 'alignment_result' in locals() and alignment_result:\n",
    "    print(\"--- Running Fuzz Test on the aligned pair ---\")\n",
    "    \n",
    "    is_equivalent = fuzz_aligned_pair(alignment_result, file_A.func, file_B.func)\n",
    "\n",
    "    if is_equivalent:\n",
    "        print(\"\\n✅ FUZZING PASSED: The pair is functionally equivalent.\")\n",
    "        \n",
    "        print(\"\\n--- Calculating Pairwise Quality Score ---\")\n",
    "        score = calculate_pairwise_score(alignment_result, file_A, file_B)\n",
    "        print(f\"Pairwise Quality Score: {score:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ FUZZING FAILED: The pair is NOT functionally equivalent.\")\n",
    "else:\n",
    "    print(\"Alignment result not found. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e153d3",
   "metadata": {},
   "source": [
    "### Cell 6: Run the Full Orchestrator\n",
    "\n",
    "Finally, this cell calls the main function from the script to run the entire analysis end-to-end and print the final summary report. This confirms that all the pieces work together as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf12a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run the Full Orchestrator\n",
    "\n",
    "# This function ties everything together and provides the final report.\n",
    "print(\"--- Running the full analysis pipeline ---\")\n",
    "analyze_problem_outputs(problem_dir, GOLD_ANSWER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
