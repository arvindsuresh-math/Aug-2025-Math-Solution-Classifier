{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc8b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Base output directory set to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned\n",
      "Available models: ['anthropic_claude-3-5-haiku-20241022', 'openai_gpt-4.1-mini', 'google_gemini-2.0-flash-thinking-exp', 'google_gemini-2.5-flash-lite-preview-06-17', 'google_gemini-2.5-flash']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Global constants & Configuration\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "BASE_OUTPUT_DIR = PROJECT_ROOT / 'data' / 'code_gen_outputs_cleaned'\n",
    "print(f\"Project root found: {PROJECT_ROOT}\")\n",
    "print(f\"Base output directory set to: {BASE_OUTPUT_DIR}\")\n",
    "\n",
    "MODEL_DICT = {\n",
    "  \"anthropic\": [\"claude-3-5-haiku-20241022\"], \n",
    "  \"openai\": [\"gpt-4.1-mini\"],\n",
    "  \"google\": [\"gemini-2.0-flash-thinking-exp\", \n",
    "             \"gemini-2.5-flash-lite-preview-06-17\",\n",
    "             \"gemini-2.5-flash\"]\n",
    "}\n",
    "\n",
    "MODELS = [f\"{provider}_{model}\" for provider, sublist in MODEL_DICT.items() for model in sublist]\n",
    "print(f\"Available models: {MODELS}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Reusable Function\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def generate_ast_for_model( \n",
    "    problem_index: int, \n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    ") -> ast.AST | None:\n",
    "    \"\"\"\n",
    "    Locates the Python file for a given model and problem, and returns its AST.\n",
    "\n",
    "    The Abstract Syntax Tree (AST) is a structured, hierarchical representation\n",
    "    of the code that can be programmatically analyzed and modified.\n",
    "\n",
    "    Args:\n",
    "        base_output_dir: The root directory for the cleaned code outputs\n",
    "                         (e.g., '.../data/code_gen_outputs_cleaned').\n",
    "        problem_index: The integer index of the GSM8K problem.\n",
    "        model_name: The name of the model, which corresponds to the file stem\n",
    "                    (e.g., 'openai_gpt-4.1-mini').\n",
    "\n",
    "    Returns:\n",
    "        The root ast.AST node for the file if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    problem_dir = base_output_dir / str(problem_index)\n",
    "    file_path = problem_dir / f\"{model_name}.py\"\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"[AST Generator Error] File not found: {file_path}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        source_code = file_path.read_text(encoding=\"utf-8\")\n",
    "        tree = ast.parse(source_code)\n",
    "        return tree\n",
    "    except SyntaxError as e:\n",
    "        print(f\"[AST Generator Error] Syntax error in {file_path}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"[AST Generator Error] Unexpected error processing {file_path}: {e!r}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9fe9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing problem 0.\n",
      "Finished processing problem 1.\n",
      "Finished processing problem 2.\n",
      "Finished processing problem 3.\n",
      "Finished processing problem 4.\n",
      "Finished processing problem 5.\n",
      "Finished processing problem 6.\n",
      "Finished processing problem 7.\n",
      "Finished processing problem 8.\n",
      "Finished processing problem 9.\n",
      "Finished processing problem 10.\n",
      "Finished processing problem 11.\n",
      "Finished processing problem 12.\n",
      "Finished processing problem 13.\n",
      "Finished processing problem 14.\n",
      "Finished processing problem 15.\n",
      "Finished processing problem 16.\n",
      "Finished processing problem 17.\n",
      "Finished processing problem 18.\n",
      "Finished processing problem 19.\n",
      "Finished processing problem 20.\n",
      "Finished processing problem 21.\n",
      "Finished processing problem 22.\n",
      "Finished processing problem 23.\n",
      "Finished processing problem 24.\n",
      "Finished processing problem 25.\n",
      "Finished processing problem 26.\n",
      "Finished processing problem 27.\n",
      "Finished processing problem 28.\n",
      "Finished processing problem 29.\n",
      "Finished processing problem 30.\n",
      "Finished processing problem 31.\n",
      "Finished processing problem 32.\n",
      "Finished processing problem 33.\n",
      "Finished processing problem 34.\n",
      "Finished processing problem 35.\n",
      "Finished processing problem 36.\n",
      "Finished processing problem 37.\n",
      "Finished processing problem 38.\n",
      "Finished processing problem 39.\n",
      "Finished processing problem 40.\n",
      "Finished processing problem 41.\n",
      "Finished processing problem 42.\n",
      "Finished processing problem 43.\n",
      "Finished processing problem 44.\n",
      "Finished processing problem 45.\n",
      "Finished processing problem 46.\n",
      "Finished processing problem 47.\n",
      "Finished processing problem 48.\n",
      "Finished processing problem 49.\n",
      "Finished processing problem 50.\n",
      "Finished processing problem 51.\n",
      "Finished processing problem 52.\n",
      "Finished processing problem 53.\n",
      "Finished processing problem 54.\n",
      "Finished processing problem 55.\n",
      "Finished processing problem 56.\n",
      "Finished processing problem 57.\n",
      "Finished processing problem 58.\n",
      "Finished processing problem 59.\n",
      "Finished processing problem 60.\n",
      "Finished processing problem 61.\n",
      "Finished processing problem 62.\n",
      "Finished processing problem 63.\n",
      "Finished processing problem 64.\n",
      "Finished processing problem 65.\n",
      "Finished processing problem 66.\n",
      "Finished processing problem 67.\n",
      "Finished processing problem 68.\n",
      "Finished processing problem 69.\n",
      "Finished processing problem 70.\n",
      "Finished processing problem 71.\n",
      "Finished processing problem 72.\n",
      "Finished processing problem 73.\n",
      "Finished processing problem 74.\n",
      "Finished processing problem 75.\n",
      "Finished processing problem 76.\n",
      "Finished processing problem 77.\n",
      "Finished processing problem 78.\n",
      "Finished processing problem 79.\n",
      "Finished processing problem 80.\n",
      "Finished processing problem 81.\n",
      "Finished processing problem 82.\n",
      "Finished processing problem 83.\n",
      "Finished processing problem 84.\n",
      "Finished processing problem 85.\n",
      "Finished processing problem 86.\n",
      "Finished processing problem 87.\n",
      "Finished processing problem 88.\n",
      "Finished processing problem 89.\n",
      "Finished processing problem 90.\n",
      "Finished processing problem 91.\n",
      "Finished processing problem 92.\n",
      "Finished processing problem 93.\n",
      "Finished processing problem 94.\n",
      "Finished processing problem 95.\n",
      "Finished processing problem 96.\n",
      "Finished processing problem 97.\n",
      "Finished processing problem 98.\n",
      "Finished processing problem 99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/52/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/59/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/71/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/76/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/82/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/85/anthropic_claude-3-5-haiku-20241022.py\n",
      "[AST Generator Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/98/anthropic_claude-3-5-haiku-20241022.py\n"
     ]
    }
   ],
   "source": [
    "ast_dict = {}\n",
    "\n",
    "for index in range(100):\n",
    "    index_dict = {}\n",
    "    for model in MODELS:\n",
    "        ast_tree = generate_ast_for_model(index, model)\n",
    "        if ast_tree is not None:\n",
    "            index_dict[model] = ast_tree\n",
    "        else:\n",
    "            index_dict[model] = None\n",
    "    ast_dict[index] = index_dict\n",
    "    print(f'Finished processing problem {index}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09389fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_ast_dict(data: dict, filepath: Path) -> None:\n",
    "    \"\"\"\n",
    "    Saves a dictionary of AST objects to a file using pickle.\n",
    "\n",
    "    Args:\n",
    "        data: The dictionary to save.\n",
    "        filepath: The path to the output file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create parent directories if they don't exist\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Open the file in binary write mode ('wb')\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Successfully saved AST dictionary to: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e!r}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def load_ast_dict(filepath: Path) -> dict | None:\n",
    "    \"\"\"\n",
    "    Loads a dictionary of AST objects from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        filepath: The path to the input file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded dictionary, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"Error: File not found at {filepath}\", file=sys.stderr)\n",
    "        return None\n",
    "    try:\n",
    "        # Open the file in binary read mode ('rb')\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Successfully loaded AST dictionary from: {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e!r}\", file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96780af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved AST dictionary to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/ast_data/ast_dict.pkl\n",
      "Successfully loaded AST dictionary from: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/ast_data/ast_dict.pkl\n",
      "\n",
      "Verification successful: Loaded dictionary keys match original.\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Define a path within your project for the serialized data\n",
    "ast_data_path = PROJECT_ROOT / \"data\" / \"ast_data\" / \"ast_dict.pkl\"\n",
    "\n",
    "# Save the dictionary you created in the previous cell\n",
    "save_ast_dict(ast_dict, ast_data_path)\n",
    "\n",
    "# Load it back to confirm the process works\n",
    "loaded_ast_dict = load_ast_dict(ast_data_path)\n",
    "\n",
    "# Check if the loaded data is the same\n",
    "if loaded_ast_dict is not None and list(loaded_ast_dict.keys()) == list(ast_dict.keys()):\n",
    "    print(\"\\nVerification successful: Loaded dictionary keys match original.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a536c589",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_ast_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m TARGET_INDEX = \u001b[32m52\u001b[39m\n\u001b[32m      5\u001b[39m TARGET_MODEL = \u001b[33m\"\u001b[39m\u001b[33mgoogle_gemini-2.0-flash-thinking-exp\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mloaded_ast_dict\u001b[49m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Safely get the AST, handling cases where it might not exist\u001b[39;00m\n\u001b[32m      9\u001b[39m     tree_to_explore = loaded_ast_dict.get(TARGET_INDEX, {}).get(TARGET_MODEL)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'loaded_ast_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Select a specific AST to inspect ---\n",
    "# Let's choose a problem and model that we know has data.\n",
    "# Replace with any index/model you want to explore.\n",
    "TARGET_INDEX = 52\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "if loaded_ast_dict:\n",
    "    # Safely get the AST, handling cases where it might not exist\n",
    "    tree_to_explore = loaded_ast_dict.get(TARGET_INDEX, {}).get(TARGET_MODEL)\n",
    "else:\n",
    "    tree_to_explore = None\n",
    "    print(\"AST dictionary not loaded. Please run the previous cell.\", file=sys.stderr)\n",
    "\n",
    "if tree_to_explore:\n",
    "    print(f\"--- Exploring AST for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "    # 1. The most direct method: ast.dump()\n",
    "    # This gives a complete, unambiguous representation of the tree's structure.\n",
    "    # It's dense, but it's the ground truth of what's in the tree.\n",
    "    print(\"\\n1. Full Tree Structure using ast.dump():\")\n",
    "    # The `indent` argument makes the output readable\n",
    "    print(ast.dump(tree_to_explore, indent=4))\n",
    "\n",
    "    # 2. Programmatic Traversal: ast.walk()\n",
    "    # This function lets you iterate over every single node in the tree.\n",
    "    # It's how you would programmatically find specific elements.\n",
    "    print(\"\\n2. Finding all Binary Operations using ast.walk():\")\n",
    "    binary_ops_found = 0\n",
    "    for node in ast.walk(tree_to_explore):\n",
    "        if isinstance(node, ast.BinOp):\n",
    "            binary_ops_found += 1\n",
    "            # ast.dump() is useful for inspecting even small sub-nodes\n",
    "            print(f\"  - Found a BinOp node: {ast.dump(node)}\")\n",
    "    if not binary_ops_found:\n",
    "        print(\"  - No binary operations found in this tree.\")\n",
    "\n",
    "    # 3. Converting back to source code: ast.unparse()\n",
    "    # This demonstrates that the AST contains all the information needed\n",
    "    # to reconstruct the original code.\n",
    "    print(\"\\n3. Reconstructed Source Code using ast.unparse():\")\n",
    "    reconstructed_code = ast.unparse(tree_to_explore)\n",
    "    print(reconstructed_code)\n",
    "\n",
    "    # 4. (Optional) For better visualization, install and use `astpretty`\n",
    "    # It provides a cleaner, more readable dump than the default.\n",
    "    # You may need to install it first by running:\n",
    "    # !pip install astpretty\n",
    "    try:\n",
    "        import astpretty\n",
    "        print(\"\\n4. Visualizing with `astpretty` (more readable dump):\")\n",
    "        astpretty.pprint(tree_to_explore)\n",
    "    except ImportError:\n",
    "        print(\"\\n4. `astpretty` not installed. Skipping. (Run '!pip install astpretty' to use)\")\n",
    "else:\n",
    "    print(f\"Could not find a valid AST for index {TARGET_INDEX}, model '{TARGET_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "# from dataclasses import dataclass\n",
    "# import libcst as cst\n",
    "# from libcst.metadata import PositionProvider, MetadataWrapper\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Define a custom data structure to hold the parsed line information\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# @dataclass\n",
    "# class CSTStatement:\n",
    "#     \"\"\"\n",
    "#     A structured representation of a single logical line of code from the CST.\n",
    "    \n",
    "#     This object links a statement node with its associated comment and the\n",
    "#     extracted solution step (e.g., 'L1').\n",
    "#     \"\"\"\n",
    "#     line_number: int         # Physical line number in the source file\n",
    "#     solution_step: str | None # The extracted step, like 'L1', 'L2', etc.\n",
    "#     code_str: str            # The string representation of the code on the line\n",
    "#     comment_str: str | None  # The string representation of the comment\n",
    "#     cst_node: cst.CSTNode    # The actual libcst node for potential modification\n",
    "\n",
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Reusable Function using libcst\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# # Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "# _TRACE_RE = re.compile(r\"^#:\\s*L(\\d+)\")\n",
    "\n",
    "# def generate_cst_data_for_model(\n",
    "#     problem_index: int,\n",
    "#     model_name: str,\n",
    "#     base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "# ) -> list[CSTStatement] | None:\n",
    "#     \"\"\"\n",
    "#     Parses a model's Python file using libcst to preserve comments and\n",
    "#     returns a list of structured statement objects.\n",
    "\n",
    "#     Args:\n",
    "#         problem_index: The integer index of the GSM8K problem.\n",
    "#         model_name: The name of the model corresponding to the file stem.\n",
    "#         base_output_dir: The root directory for the cleaned code outputs.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of CSTStatement objects, one for each relevant line in the\n",
    "#         'solve' function's body, or None if an error occurs.\n",
    "#     \"\"\"\n",
    "#     print(f\"[DEBUG] Entered generate_cst_data_for_model with problem_index={problem_index}, model_name={model_name}\")\n",
    "#     problem_dir = base_output_dir / str(problem_index)\n",
    "#     file_path = problem_dir / f\"{model_name}.py\"\n",
    "#     print(f\"[DEBUG] Looking for file at: {file_path}\")\n",
    "\n",
    "#     if not file_path.exists():\n",
    "#         print(f\"[CST Parser Error] File not found: {file_path}\", file=sys.stderr)\n",
    "#         return None\n",
    "\n",
    "#     try:\n",
    "#         source_code = file_path.read_text(encoding=\"utf-8\")\n",
    "#         print(f\"[DEBUG] Successfully read source code ({len(source_code)} chars)\")\n",
    "#         module_cst = cst.parse_module(source_code)\n",
    "#         print(f\"[DEBUG] Parsed module_cst\")\n",
    "#         wrapper = MetadataWrapper(module_cst)\n",
    "#         print(f\"[DEBUG] Created MetadataWrapper\")\n",
    "#         # Resolve the position metadata for all nodes in the module once\n",
    "#         position_map = wrapper.resolve(PositionProvider)\n",
    "#         print(f\"[DEBUG] Resolved position_map with {len(position_map)} entries\")\n",
    "#         print(\"[DEBUG] position_map keys (types):\", set(type(k) for k in position_map.keys()))\n",
    "#         print(\"[DEBUG] position_map keys (repr):\", [repr(k) for k in list(position_map.keys())[:10]])\n",
    "#     except Exception as e:\n",
    "#         print(f\"[CST Parser Error] Failed to parse {file_path}: {e!r}\", file=sys.stderr)\n",
    "#         return None\n",
    "\n",
    "#     statements = []\n",
    "\n",
    "#     # Iterate through the top-level nodes to find the 'solve' function\n",
    "#     found_solve = False\n",
    "#     for node in module_cst.body:\n",
    "#         print(f\"[DEBUG] Top-level node: {type(node)}\")\n",
    "#         if isinstance(node, cst.FunctionDef) and node.name.value == \"solve\":\n",
    "#             print(f\"[DEBUG] Found 'solve' function\")\n",
    "#             found_solve = True\n",
    "#             function_body = node.body\n",
    "#             if not isinstance(function_body, cst.IndentedBlock):\n",
    "#                 print(f\"[DEBUG] Function body is not an IndentedBlock, skipping\")\n",
    "#                 continue\n",
    "#             # Iterate through each statement in the function body\n",
    "#             for i, stmt_line in enumerate(function_body.body):\n",
    "#                 print(f\"[DEBUG] Statement {i}: {type(stmt_line)}\")\n",
    "#                 if isinstance(stmt_line, cst.SimpleStatementLine):\n",
    "#                     # --- Extract comment from leading_lines or trailing_whitespace ---\n",
    "#                     comment_text = None\n",
    "#                     # Check leading_lines for a comment (step comments like #: L1)\n",
    "#                     for leading in stmt_line.leading_lines:\n",
    "#                         if leading.comment is not None:\n",
    "#                             comment_text = leading.comment.value.strip()\n",
    "#                             break\n",
    "#                     # If not found in leading_lines, check trailing_whitespace\n",
    "#                     if comment_text is None and stmt_line.trailing_whitespace.comment:\n",
    "#                         comment_text = stmt_line.trailing_whitespace.comment.value.strip()\n",
    "#                     # --- Extract solution step if present ---\n",
    "#                     solution_step = None\n",
    "#                     if comment_text:\n",
    "#                         match = _TRACE_RE.match(comment_text)\n",
    "#                         if match:\n",
    "#                             solution_step = f\"L{match.group(1)}\"\n",
    "#                     if stmt_line.body:\n",
    "#                         code_str = module_cst.code_for_node(stmt_line.body[0])\n",
    "#                         print(f\"[DEBUG] Checking stmt_line {i}: {repr(stmt_line)}\")\n",
    "#                         found = False\n",
    "#                         for k in position_map:\n",
    "#                             if isinstance(k, cst.SimpleStatementLine) and k.deep_equals(stmt_line):\n",
    "#                                 pos = position_map[k]\n",
    "#                                 found = True\n",
    "#                                 break\n",
    "#                         if not found:\n",
    "#                             print(f\"[DEBUG] Statement {i} not in position_map (by deep_equals), skipping\")\n",
    "#                             continue\n",
    "#                         print(f\"[DEBUG] Appending CSTStatement for line {pos.start.line}, step={solution_step}, code={code_str}\")\n",
    "#                         statements.append(CSTStatement(\n",
    "#                             line_number=pos.start.line,\n",
    "#                             solution_step=solution_step,\n",
    "#                             code_str=code_str,\n",
    "#                             comment_str=comment_text,\n",
    "#                             cst_node=stmt_line.body[0]\n",
    "#                         ))\n",
    "#             break\n",
    "#     if not found_solve:\n",
    "#         print(f\"[DEBUG] Did not find 'solve' function in file.\")\n",
    "#     print(f\"[DEBUG] Returning {len(statements)} statements\")\n",
    "#     return statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parsing with libcst for Problem 8, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=8, model_name=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/8/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "[DEBUG] Successfully read source code (787 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 270 entries\n",
      "[DEBUG] position_map keys (types): {<class 'libcst._nodes.expression.Name'>, <class 'libcst._nodes.whitespace.Newline'>, <class 'libcst._nodes.expression.Param'>, <class 'libcst._nodes.op.Comma'>, <class 'libcst._nodes.statement.Expr'>, <class 'libcst._nodes.statement.Assign'>, <class 'libcst._nodes.whitespace.SimpleWhitespace'>, <class 'libcst._nodes.whitespace.ParenthesizedWhitespace'>, <class 'libcst._nodes.whitespace.TrailingWhitespace'>, <class 'libcst._nodes.op.AssignEqual'>, <class 'libcst._nodes.expression.SimpleString'>, <class 'libcst._nodes.statement.AssignTarget'>, <class 'libcst._nodes.statement.IndentedBlock'>, <class 'libcst._nodes.module.Module'>, <class 'libcst._nodes.expression.Parameters'>, <class 'libcst._nodes.expression.Annotation'>, <class 'libcst._nodes.statement.SimpleStatementLine'>, <class 'libcst._nodes.op.Subtract'>, <class 'libcst._nodes.statement.Return'>, <class 'libcst._nodes.statement.FunctionDef'>, <class 'libcst._nodes.expression.Integer'>, <class 'libcst._nodes.whitespace.EmptyLine'>, <class 'libcst._nodes.op.Add'>, <class 'libcst._nodes.expression.BinaryOperation'>, <class 'libcst._nodes.whitespace.Comment'>}\n",
      "[DEBUG] position_map keys (repr): [\"SimpleWhitespace(\\n    value=' ',\\n)\", \"Name(\\n    value='solve',\\n    lpar=[],\\n    rpar=[],\\n)\", \"SimpleWhitespace(\\n    value='',\\n)\", \"SimpleWhitespace(\\n    value='',\\n)\", \"SimpleWhitespace(\\n    value='',\\n)\", 'Newline(\\n    value=None,\\n)', \"TrailingWhitespace(\\n    whitespace=SimpleWhitespace(\\n        value='',\\n    ),\\n    comment=None,\\n    newline=Newline(\\n        value=None,\\n    ),\\n)\", \"SimpleWhitespace(\\n    value='    ',\\n)\", \"ParenthesizedWhitespace(\\n    first_line=TrailingWhitespace(\\n        whitespace=SimpleWhitespace(\\n            value='',\\n        ),\\n        comment=None,\\n        newline=Newline(\\n            value=None,\\n        ),\\n    ),\\n    empty_lines=[],\\n    indent=True,\\n    last_line=SimpleWhitespace(\\n        value='    ',\\n    ),\\n)\", \"SimpleWhitespace(\\n    value='',\\n)\"]\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 0: SimpleStatementLine(\n",
      "    body=[\n",
      "        Expr(\n",
      "            value=SimpleString(\n",
      "                value='\"\"\"Index: 8.\\n    Returns: the amount Alexis paid for the shoes.\\n    \"\"\"',\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value='',\n",
      "        ),\n",
      "        comment=None,\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 10, step=None, code=\"\"\"Index: 8.\n",
      "    Returns: the amount Alexis paid for the shoes.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 1: SimpleStatementLine(\n",
      "    body=[\n",
      "        Assign(\n",
      "            targets=[\n",
      "                AssignTarget(\n",
      "                    target=Name(\n",
      "                        value='spent_on_clothes_without_shoes',\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    whitespace_before_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "            ],\n",
      "            value=BinaryOperation(\n",
      "                left=BinaryOperation(\n",
      "                    left=BinaryOperation(\n",
      "                        left=BinaryOperation(\n",
      "                            left=Name(\n",
      "                                value='shirt_cost',\n",
      "                                lpar=[],\n",
      "                                rpar=[],\n",
      "                            ),\n",
      "                            operator=Add(\n",
      "                                whitespace_before=SimpleWhitespace(\n",
      "                                    value=' ',\n",
      "                                ),\n",
      "                                whitespace_after=SimpleWhitespace(\n",
      "                                    value=' ',\n",
      "                                ),\n",
      "                            ),\n",
      "                            right=Name(\n",
      "                                value='pants_cost',\n",
      "                                lpar=[],\n",
      "                                rpar=[],\n",
      "                            ),\n",
      "                            lpar=[],\n",
      "                            rpar=[],\n",
      "                        ),\n",
      "                        operator=Add(\n",
      "                            whitespace_before=SimpleWhitespace(\n",
      "                                value=' ',\n",
      "                            ),\n",
      "                            whitespace_after=SimpleWhitespace(\n",
      "                                value=' ',\n",
      "                            ),\n",
      "                        ),\n",
      "                        right=Name(\n",
      "                            value='coat_cost',\n",
      "                            lpar=[],\n",
      "                            rpar=[],\n",
      "                        ),\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    operator=Add(\n",
      "                        whitespace_before=SimpleWhitespace(\n",
      "                            value=' ',\n",
      "                        ),\n",
      "                        whitespace_after=SimpleWhitespace(\n",
      "                            value=' ',\n",
      "                        ),\n",
      "                    ),\n",
      "                    right=Name(\n",
      "                        value='socks_cost',\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                operator=Add(\n",
      "                    whitespace_before=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "                right=Name(\n",
      "                    value='belt_cost',\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[\n",
      "        EmptyLine(\n",
      "            indent=True,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=Comment(\n",
      "                value='#: L1',\n",
      "            ),\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "        EmptyLine(\n",
      "            indent=True,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=Comment(\n",
      "                value='# Let S be the amount Alexis paid for the shoes.',\n",
      "            ),\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "        EmptyLine(\n",
      "            indent=False,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=None,\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "        EmptyLine(\n",
      "            indent=True,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=Comment(\n",
      "                value='#: L2',\n",
      "            ),\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "    ],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value='',\n",
      "        ),\n",
      "        comment=None,\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 17, step=L1, code=spent_on_clothes_without_shoes = shirt_cost + pants_cost + coat_cost + socks_cost + belt_cost\n",
      "[DEBUG] Statement 2: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 2: SimpleStatementLine(\n",
      "    body=[\n",
      "        Assign(\n",
      "            targets=[\n",
      "                AssignTarget(\n",
      "                    target=Name(\n",
      "                        value='total_spent',\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    whitespace_before_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "            ],\n",
      "            value=BinaryOperation(\n",
      "                left=Name(\n",
      "                    value='budget',\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                operator=Subtract(\n",
      "                    whitespace_before=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "                right=Name(\n",
      "                    value='money_left',\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[\n",
      "        EmptyLine(\n",
      "            indent=False,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=None,\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "        EmptyLine(\n",
      "            indent=True,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=Comment(\n",
      "                value='#: L3',\n",
      "            ),\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "    ],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value='',\n",
      "        ),\n",
      "        comment=None,\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 20, step=L3, code=total_spent = budget - money_left\n",
      "[DEBUG] Statement 3: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 3: SimpleStatementLine(\n",
      "    body=[\n",
      "        Assign(\n",
      "            targets=[\n",
      "                AssignTarget(\n",
      "                    target=Name(\n",
      "                        value='shoes_cost',\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    whitespace_before_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "            ],\n",
      "            value=BinaryOperation(\n",
      "                left=Name(\n",
      "                    value='total_spent',\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                operator=Subtract(\n",
      "                    whitespace_before=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "                right=Name(\n",
      "                    value='spent_on_clothes_without_shoes',\n",
      "                    lpar=[],\n",
      "                    rpar=[],\n",
      "                ),\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[\n",
      "        EmptyLine(\n",
      "            indent=False,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=None,\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "        EmptyLine(\n",
      "            indent=True,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=Comment(\n",
      "                value='#: L4',\n",
      "            ),\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "    ],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value='',\n",
      "        ),\n",
      "        comment=None,\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 23, step=L4, code=shoes_cost = total_spent - spent_on_clothes_without_shoes\n",
      "[DEBUG] Statement 4: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 4: SimpleStatementLine(\n",
      "    body=[\n",
      "        Assign(\n",
      "            targets=[\n",
      "                AssignTarget(\n",
      "                    target=Name(\n",
      "                        value='answer',\n",
      "                        lpar=[],\n",
      "                        rpar=[],\n",
      "                    ),\n",
      "                    whitespace_before_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                    whitespace_after_equal=SimpleWhitespace(\n",
      "                        value=' ',\n",
      "                    ),\n",
      "                ),\n",
      "            ],\n",
      "            value=Name(\n",
      "                value='shoes_cost',\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[\n",
      "        EmptyLine(\n",
      "            indent=False,\n",
      "            whitespace=SimpleWhitespace(\n",
      "                value='',\n",
      "            ),\n",
      "            comment=None,\n",
      "            newline=Newline(\n",
      "                value=None,\n",
      "            ),\n",
      "        ),\n",
      "    ],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value=' ',\n",
      "        ),\n",
      "        comment=Comment(\n",
      "            value='# FINAL ANSWER',\n",
      "        ),\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 25, step=None, code=answer = shoes_cost\n",
      "[DEBUG] Statement 5: <class 'libcst._nodes.statement.SimpleStatementLine'>\n",
      "[DEBUG] Checking stmt_line 5: SimpleStatementLine(\n",
      "    body=[\n",
      "        Return(\n",
      "            value=Name(\n",
      "                value='answer',\n",
      "                lpar=[],\n",
      "                rpar=[],\n",
      "            ),\n",
      "            whitespace_after_return=SimpleWhitespace(\n",
      "                value=' ',\n",
      "            ),\n",
      "            semicolon=MaybeSentinel.DEFAULT,\n",
      "        ),\n",
      "    ],\n",
      "    leading_lines=[],\n",
      "    trailing_whitespace=TrailingWhitespace(\n",
      "        whitespace=SimpleWhitespace(\n",
      "            value='',\n",
      "        ),\n",
      "        comment=None,\n",
      "        newline=Newline(\n",
      "            value=None,\n",
      "        ),\n",
      "    ),\n",
      ")\n",
      "[DEBUG] Appending CSTStatement for line 26, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line 10: Step='N/A', Code='\"\"\"Index: 8.\n",
      "    Returns: the amount Alexis paid for the shoes.\n",
      "    \"\"\"', Comment=''\n",
      "Line 17: Step='L1', Code='spent_on_clothes_without_shoes = shirt_cost + pants_cost + coat_cost + socks_cost + belt_cost', Comment='#: L1'\n",
      "Line 20: Step='L3', Code='total_spent = budget - money_left', Comment='#: L3'\n",
      "Line 23: Step='L4', Code='shoes_cost = total_spent - spent_on_clothes_without_shoes', Comment='#: L4'\n",
      "Line 25: Step='N/A', Code='answer = shoes_cost', Comment='# FINAL ANSWER'\n",
      "Line 26: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Select a target to explore. Let's use the same one as before.\n",
    "TARGET_INDEX = 8\n",
    "TARGET_MODEL = \"google_gemini-2.5-flash-lite-preview-06-17\"\n",
    "\n",
    "print(f\"--- Parsing with libcst for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "# Call the new function\n",
    "cst_data = generate_cst_data_for_model(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "if cst_data:\n",
    "    print(\"Successfully parsed file and extracted statements with comments.\\n\")\n",
    "    print(\"--- Extracted Statement Data ---\")\n",
    "    for statement in cst_data:\n",
    "        print(\n",
    "            f\"Line {statement.line_number:2d}: \"\n",
    "            f\"Step='{statement.solution_step or 'N/A'}', \"\n",
    "            f\"Code='{statement.code_str}', \"\n",
    "            f\"Comment='{statement.comment_str or ''}'\"\n",
    "        )\n",
    "    \n",
    "    # You can also inspect the cst_node directly for a specific step\n",
    "    print(\"\\n--- Inspecting the CST node for step 'L2' ---\")\n",
    "    step_l2_node = next((s.cst_node for s in cst_data if s.solution_step == 'L2'), None)\n",
    "    \n",
    "    if step_l2_node:\n",
    "        # For CST nodes, `step_l2_node.visit(YourVisitor())` is how you'd traverse\n",
    "        # or `step_l2_node.with_changes(...)` is how you'd modify it.\n",
    "        # For now, just printing the type and code is illustrative.\n",
    "        print(f\"Node Type: {type(step_l2_node)}\")\n",
    "        print(f\"Code for node: {cst.parse_module('').code_for_node(step_l2_node)}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve CST data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61a524b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "import libcst as cst\n",
    "from libcst.metadata import PositionProvider, MetadataWrapper\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Define a custom data structure to hold the parsed line information\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "@dataclass\n",
    "class CSTStatement:\n",
    "    \"\"\"\n",
    "    A structured representation of a single logical line of code from the CST.\n",
    "    \n",
    "    This object links a statement node with its associated comment and the\n",
    "    extracted solution step (e.g., 'L1').\n",
    "    \"\"\"\n",
    "    line_number: int         # Physical line number in the source file\n",
    "    solution_step: str | None # The extracted step, like 'L1', 'L2', etc.\n",
    "    code_str: str            # The string representation of the code on the line\n",
    "    comment_str: str | None  # The string representation of the comment\n",
    "    cst_node: cst.CSTNode    # The actual libcst node for potential modification\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Reusable Function using libcst\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"^#:\\s*L(\\d+)\")\n",
    "\n",
    "def generate_cst_data_for_model(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    ") -> list[CSTStatement] | None:\n",
    "    \"\"\"\n",
    "    Parses a model's Python file using libcst to preserve comments and\n",
    "    returns a list of structured statement objects.\n",
    "\n",
    "    Args:\n",
    "        problem_index: The integer index of the GSM8K problem.\n",
    "        model_name: The name of the model corresponding to the file stem.\n",
    "        base_output_dir: The root directory for the cleaned code outputs.\n",
    "\n",
    "    Returns:\n",
    "        A list of CSTStatement objects, one for each relevant line in the\n",
    "        'solve' function's body, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    print(f\"[DEBUG] Entered generate_cst_data_for_model with problem_index={problem_index}, model_name={model_name}\")\n",
    "    problem_dir = base_output_dir / str(problem_index)\n",
    "    file_path = problem_dir / f\"{model_name}.py\"\n",
    "    print(f\"[DEBUG] Looking for file at: {file_path}\")\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"[CST Parser Error] File not found: {file_path}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        source_code = file_path.read_text(encoding=\"utf-8\")\n",
    "        print(f\"[DEBUG] Successfully read source code ({len(source_code)} chars)\")\n",
    "        module_cst = cst.parse_module(source_code)\n",
    "        print(f\"[DEBUG] Parsed module_cst\")\n",
    "        wrapper = MetadataWrapper(module_cst)\n",
    "        print(f\"[DEBUG] Created MetadataWrapper\")\n",
    "        position_map = wrapper.resolve(PositionProvider)\n",
    "        print(f\"[DEBUG] Resolved position_map with {len(position_map)} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CST Parser Error] Failed to parse {file_path}: {e!r}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    statements = []\n",
    "    found_solve = False\n",
    "    pending_step_comment = None\n",
    "\n",
    "    for node in module_cst.body:\n",
    "        print(f\"[DEBUG] Top-level node: {type(node)}\")\n",
    "        if isinstance(node, cst.FunctionDef) and node.name.value == \"solve\":\n",
    "            print(f\"[DEBUG] Found 'solve' function\")\n",
    "            found_solve = True\n",
    "            function_body = node.body\n",
    "            if not isinstance(function_body, cst.IndentedBlock):\n",
    "                print(f\"[DEBUG] Function body is not an IndentedBlock, skipping\")\n",
    "                continue\n",
    "            for i, stmt_line in enumerate(function_body.body):\n",
    "                print(f\"[DEBUG] Statement {i}:\")\n",
    "                if isinstance(stmt_line, cst.SimpleStatementLine):\n",
    "                    # --- Gather all step comments in leading_lines ---\n",
    "                    for leading in stmt_line.leading_lines:\n",
    "                        if leading.comment is not None:\n",
    "                            comment_val = leading.comment.value.strip()\n",
    "                            match = _TRACE_RE.match(comment_val)\n",
    "                            if match:\n",
    "                                # If it's a step comment, store it for the next statement\n",
    "                                pending_step_comment = comment_val\n",
    "                            else:\n",
    "                                # If it's a regular comment, you may want to handle it as well\n",
    "                                pass  # (optional: collect or ignore)\n",
    "                    # --- Extract trailing comment (for non-step comments) ---\n",
    "                    comment_text = None\n",
    "                    if stmt_line.trailing_whitespace.comment:\n",
    "                        comment_text = stmt_line.trailing_whitespace.comment.value.strip()\n",
    "                    # --- Extract solution step if pending ---\n",
    "                    solution_step = None\n",
    "                    if pending_step_comment:\n",
    "                        match = _TRACE_RE.match(pending_step_comment)\n",
    "                        if match:\n",
    "                            solution_step = f\"L{match.group(1)}\"\n",
    "                    if stmt_line.body:\n",
    "                        code_str = module_cst.code_for_node(stmt_line.body[0])\n",
    "                        print(f\"[DEBUG] Checking stmt_line {i}\")\n",
    "                        found = False\n",
    "                        for k in position_map:\n",
    "                            if isinstance(k, cst.SimpleStatementLine) and k.deep_equals(stmt_line):\n",
    "                                pos = position_map[k]\n",
    "                                found = True\n",
    "                                break\n",
    "                        if not found:\n",
    "                            print(f\"[DEBUG] Statement {i} not in position_map (by deep_equals), skipping\")\n",
    "                            continue\n",
    "                        print(f\"[DEBUG] Appending CSTStatement for line {pos.start.line}, step={solution_step}, code={code_str}\")\n",
    "                        statements.append(CSTStatement(\n",
    "                            line_number=pos.start.line,\n",
    "                            solution_step=solution_step,\n",
    "                            code_str=code_str,\n",
    "                            comment_str=comment_text if comment_text else pending_step_comment,\n",
    "                            cst_node=stmt_line.body[0]\n",
    "                        ))\n",
    "                    # Clear the pending step comment after using it\n",
    "                    pending_step_comment = None\n",
    "            break\n",
    "    if not found_solve:\n",
    "        print(f\"[DEBUG] Did not find 'solve' function in file.\")\n",
    "    print(f\"[DEBUG] Returning {len(statements)} statements\")\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b4b5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_cst_data(\n",
    "    target_indices: list[int],\n",
    "    target_models: list[str],\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    ") -> dict[int, dict[str, list[CSTStatement] | None]]:\n",
    "    \"\"\"\n",
    "    Runs generate_cst_data_for_model for each (index, model) pair and returns a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        target_indices: List of problem indices to process.\n",
    "        target_models: List of model names to process.\n",
    "        base_output_dir: The root directory for the cleaned code outputs.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of the form {index: {model: [CSTStatement, ...] or None}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for idx in target_indices:\n",
    "        model_results = {}\n",
    "        for model in target_models:\n",
    "            print(f\"\\n[INFO] Processing index={idx}, model={model}\")\n",
    "            cst_data = generate_cst_data_for_model(idx, model, base_output_dir=base_output_dir)\n",
    "            model_results[model] = cst_data\n",
    "\n",
    "            if cst_data:\n",
    "                print(\"Successfully parsed file and extracted statements with comments.\\n\")\n",
    "                print(\"--- Extracted Statement Data ---\")\n",
    "                for statement in cst_data:\n",
    "                    print(\n",
    "                        f\"Line {statement.line_number:2d}: \"\n",
    "                        f\"Step='{statement.solution_step or 'N/A'}', \"\n",
    "                        f\"Code='{statement.code_str}', \"\n",
    "                        f\"Comment='{statement.comment_str or ''}'\"\n",
    "                    )\n",
    "                \n",
    "                # You can also inspect the cst_node directly for a specific step\n",
    "                print(\"\\n--- Inspecting the CST node for step 'L2' ---\")\n",
    "                step_l2_node = next((s.cst_node for s in cst_data if s.solution_step == 'L2'), None)\n",
    "                \n",
    "                if step_l2_node:\n",
    "                    # For CST nodes, `step_l2_node.visit(YourVisitor())` is how you'd traverse\n",
    "                    # or `step_l2_node.with_changes(...)` is how you'd modify it.\n",
    "                    # For now, just printing the type and code is illustrative.\n",
    "                    print(f\"Node Type: {type(step_l2_node)}\")\n",
    "                    print(f\"Code for node: {cst.parse_module('').code_for_node(step_l2_node)}\")\n",
    "            else:\n",
    "                print(\"Failed to retrieve CST data.\")\n",
    "        results[idx] = model_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a8d2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing index=10, model=anthropic_claude-3-5-haiku-20241022\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=10, model_name=anthropic_claude-3-5-haiku-20241022\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/10/anthropic_claude-3-5-haiku-20241022.py\n",
      "[DEBUG] Successfully read source code (875 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 172 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 5, step=None, code=\"\"\"Index: 10.\n",
      "    Returns: the number of people on the first ship the monster consumed.\"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 19, step=L4, code=total_ship_multiplier = 1 + 2 + 4\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 23, step=L5, code=people_on_first_ship = total_people_consumed / total_ship_multiplier\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 25, step=None, code=answer = people_on_first_ship\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 26, step=None, code=return answer\n",
      "[DEBUG] Returning 5 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line  5: Step='N/A', Code='\"\"\"Index: 10.\n",
      "    Returns: the number of people on the first ship the monster consumed.\"\"\"', Comment=''\n",
      "Line 19: Step='L4', Code='total_ship_multiplier = 1 + 2 + 4', Comment='# S + 2S + 4S'\n",
      "Line 23: Step='L5', Code='people_on_first_ship = total_people_consumed / total_ship_multiplier', Comment='#: L5'\n",
      "Line 25: Step='N/A', Code='answer = people_on_first_ship', Comment='# FINAL ANSWER'\n",
      "Line 26: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n",
      "\n",
      "[INFO] Processing index=10, model=openai_gpt-4.1-mini\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=10, model_name=openai_gpt-4.1-mini\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/10/openai_gpt-4.1-mini.py\n",
      "[DEBUG] Successfully read source code (405 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 101 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 4, step=None, code=\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 8, step=L4, code=total_ships_factor = 7\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 11, step=L5, code=first_ship_people = total_people / total_ships_factor\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 13, step=None, code=answer = first_ship_people\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 14, step=None, code=return answer\n",
      "[DEBUG] Returning 5 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line  4: Step='N/A', Code='\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"', Comment=''\n",
      "Line  8: Step='L4', Code='total_ships_factor = 7', Comment='# S + 2S + 4S = 7S'\n",
      "Line 11: Step='L5', Code='first_ship_people = total_people / total_ships_factor', Comment='#: L5'\n",
      "Line 13: Step='N/A', Code='answer = first_ship_people', Comment='# FINAL ANSWER'\n",
      "Line 14: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n",
      "\n",
      "[INFO] Processing index=10, model=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=10, model_name=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/10/google_gemini-2.0-flash-thinking-exp.py\n",
      "[DEBUG] Successfully read source code (1340 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 221 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 7, step=None, code=\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 16, step=L3, code=multiplier_third_ship = twice_as_many * twice_as_many\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 21, step=L4, code=total_factor = 1 + twice_as_many + multiplier_third_ship\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 25, step=L5, code=people_first_ship = total_people_consumed / total_factor\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 27, step=None, code=answer = people_first_ship\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 28, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line  7: Step='N/A', Code='\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"', Comment=''\n",
      "Line 16: Step='L3', Code='multiplier_third_ship = twice_as_many * twice_as_many', Comment='#: L3'\n",
      "Line 21: Step='L4', Code='total_factor = 1 + twice_as_many + multiplier_third_ship', Comment='#: L4'\n",
      "Line 25: Step='L5', Code='people_first_ship = total_people_consumed / total_factor', Comment='#: L5'\n",
      "Line 27: Step='N/A', Code='answer = people_first_ship', Comment='# FINAL ANSWER'\n",
      "Line 28: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n",
      "\n",
      "[INFO] Processing index=10, model=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=10, model_name=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/10/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "[DEBUG] Successfully read source code (991 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 188 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 6, step=None, code=\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 20, step=L4, code=total_proportions = 1 + 2 + 4\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 24, step=L5, code=people_on_first_ship = total_people_eaten / total_proportions\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 26, step=None, code=answer = people_on_first_ship\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 27, step=None, code=return answer\n",
      "[DEBUG] Returning 5 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line  6: Step='N/A', Code='\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"', Comment=''\n",
      "Line 20: Step='L4', Code='total_proportions = 1 + 2 + 4', Comment='#: L4'\n",
      "Line 24: Step='L5', Code='people_on_first_ship = total_people_eaten / total_proportions', Comment='#: L5'\n",
      "Line 26: Step='N/A', Code='answer = people_on_first_ship', Comment='# FINAL ANSWER'\n",
      "Line 27: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n",
      "\n",
      "[INFO] Processing index=10, model=google_gemini-2.5-flash\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=10, model_name=google_gemini-2.5-flash\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/10/google_gemini-2.5-flash.py\n",
      "[DEBUG] Successfully read source code (1315 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 251 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 7, step=None, code=\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 20, step=L3, code=coefficient_first_ship = 1\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 21, step=None, code=coefficient_second_ship = growth_factor\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 22, step=None, code=coefficient_third_ship = growth_factor * growth_factor\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 26, step=L4, code=total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 30, step=L5, code=people_on_first_ship = total_people_consumed / total_coefficient_s\n",
      "[DEBUG] Statement 6:\n",
      "[DEBUG] Checking stmt_line 6\n",
      "[DEBUG] Appending CSTStatement for line 32, step=None, code=answer = people_on_first_ship\n",
      "[DEBUG] Statement 7:\n",
      "[DEBUG] Checking stmt_line 7\n",
      "[DEBUG] Appending CSTStatement for line 33, step=None, code=return answer\n",
      "[DEBUG] Returning 8 statements\n",
      "Successfully parsed file and extracted statements with comments.\n",
      "\n",
      "--- Extracted Statement Data ---\n",
      "Line  7: Step='N/A', Code='\"\"\"Index: 10.\n",
      "    Returns: the number of people on the ship the monster ate in the first hundred years.\n",
      "    \"\"\"', Comment=''\n",
      "Line 20: Step='L3', Code='coefficient_first_ship = 1', Comment='#: L3'\n",
      "Line 21: Step='N/A', Code='coefficient_second_ship = growth_factor', Comment=''\n",
      "Line 22: Step='N/A', Code='coefficient_third_ship = growth_factor * growth_factor', Comment=''\n",
      "Line 26: Step='L4', Code='total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship', Comment='#: L4'\n",
      "Line 30: Step='L5', Code='people_on_first_ship = total_people_consumed / total_coefficient_s', Comment='#: L5'\n",
      "Line 32: Step='N/A', Code='answer = people_on_first_ship', Comment='# FINAL ANSWER'\n",
      "Line 33: Step='N/A', Code='return answer', Comment=''\n",
      "\n",
      "--- Inspecting the CST node for step 'L2' ---\n"
     ]
    }
   ],
   "source": [
    "results = batch_generate_cst_data([10], MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4be3d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------------------------------------------------- #\n",
    "# #  Example Usage\n",
    "# # ---------------------------------------------------------------------- #\n",
    "\n",
    "# # Select a target to explore. Let's use the same one as before.\n",
    "# TARGET_INDEX = 8\n",
    "# TARGET_MODEL = \"google_gemini-2.5-flash-lite-preview-06-17\"\n",
    "\n",
    "# print(f\"--- Parsing with libcst for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "# # Call the new function\n",
    "# cst_data = generate_cst_data_for_model(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "# if cst_data:\n",
    "#     print(\"Successfully parsed file and extracted statements with comments.\\n\")\n",
    "#     print(\"--- Extracted Statement Data ---\")\n",
    "#     for statement in cst_data:\n",
    "#         print(\n",
    "#             f\"Line {statement.line_number:2d}: \"\n",
    "#             f\"Step='{statement.solution_step or 'N/A'}', \"\n",
    "#             f\"Code='{statement.code_str}', \"\n",
    "#             f\"Comment='{statement.comment_str or ''}'\"\n",
    "#         )\n",
    "    \n",
    "#     # You can also inspect the cst_node directly for a specific step\n",
    "#     print(\"\\n--- Inspecting the CST node for step 'L2' ---\")\n",
    "#     step_l2_node = next((s.cst_node for s in cst_data if s.solution_step == 'L2'), None)\n",
    "    \n",
    "#     if step_l2_node:\n",
    "#         # For CST nodes, `step_l2_node.visit(YourVisitor())` is how you'd traverse\n",
    "#         # or `step_l2_node.with_changes(...)` is how you'd modify it.\n",
    "#         # For now, just printing the type and code is illustrative.\n",
    "#         print(f\"Node Type: {type(step_l2_node)}\")\n",
    "#         print(f\"Code for node: {cst.parse_module('').code_for_node(step_l2_node)}\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve CST data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d23caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing index=8, model=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=8, model_name=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/8/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "[DEBUG] Successfully read source code (787 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 270 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 10, step=None, code=\"\"\"Index: 8.\n",
      "    Returns: the amount Alexis paid for the shoes.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 17, step=L2, code=spent_on_clothes_without_shoes = shirt_cost + pants_cost + coat_cost + socks_cost + belt_cost\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 20, step=L3, code=total_spent = budget - money_left\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 23, step=L4, code=shoes_cost = total_spent - spent_on_clothes_without_shoes\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 25, step=None, code=answer = shoes_cost\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 26, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n",
      "\n",
      "[INFO] Processing index=8, model=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=8, model_name=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/8/google_gemini-2.0-flash-thinking-exp.py\n",
      "[DEBUG] Successfully read source code (701 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 259 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 10, step=None, code=\"\"\"Index: 8.\n",
      "    Returns: the amount Alexis paid for the shoes.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 14, step=L2, code=known_expenses = shirt_cost + pants_cost + coat_cost + socks_cost + belt_cost\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 17, step=L3, code=total_spent = budget - remaining_budget\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 20, step=L4, code=shoes_cost = total_spent - known_expenses\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 22, step=None, code=answer = shoes_cost\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 23, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n",
      "\n",
      "[INFO] Processing index=52, model=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=52, model_name=google_gemini-2.5-flash-lite-preview-06-17\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/52/google_gemini-2.5-flash-lite-preview-06-17.py\n",
      "[DEBUG] Successfully read source code (618 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 184 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 7, step=None, code=\"\"\"Index: 52.\n",
      "    Returns: the number of pencils in each box.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 11, step=L1, code=total_pencils_shared = num_friends * pencils_per_friend\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 14, step=L2, code=total_pencils_all = pencils_kept + total_pencils_shared\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 17, step=L3, code=pencils_per_box = total_pencils_all / num_boxes\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 19, step=None, code=answer = pencils_per_box\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 20, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n",
      "\n",
      "[INFO] Processing index=52, model=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Entered generate_cst_data_for_model with problem_index=52, model_name=google_gemini-2.0-flash-thinking-exp\n",
      "[DEBUG] Looking for file at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/52/google_gemini-2.0-flash-thinking-exp.py\n",
      "[DEBUG] Successfully read source code (596 chars)\n",
      "[DEBUG] Parsed module_cst\n",
      "[DEBUG] Created MetadataWrapper\n",
      "[DEBUG] Resolved position_map with 184 entries\n",
      "[DEBUG] Top-level node: <class 'libcst._nodes.statement.FunctionDef'>\n",
      "[DEBUG] Found 'solve' function\n",
      "[DEBUG] Statement 0:\n",
      "[DEBUG] Checking stmt_line 0\n",
      "[DEBUG] Appending CSTStatement for line 7, step=None, code=\"\"\"Index: 52.\n",
      "    Returns: the number of pencils in each box.\n",
      "    \"\"\"\n",
      "[DEBUG] Statement 1:\n",
      "[DEBUG] Checking stmt_line 1\n",
      "[DEBUG] Appending CSTStatement for line 11, step=L1, code=pencils_shared = num_friends * pencils_per_friend\n",
      "[DEBUG] Statement 2:\n",
      "[DEBUG] Checking stmt_line 2\n",
      "[DEBUG] Appending CSTStatement for line 14, step=L2, code=total_pencils = pencils_kept + pencils_shared\n",
      "[DEBUG] Statement 3:\n",
      "[DEBUG] Checking stmt_line 3\n",
      "[DEBUG] Appending CSTStatement for line 17, step=L3, code=pencils_in_each_box = total_pencils / num_boxes\n",
      "[DEBUG] Statement 4:\n",
      "[DEBUG] Checking stmt_line 4\n",
      "[DEBUG] Appending CSTStatement for line 19, step=None, code=answer = pencils_in_each_box\n",
      "[DEBUG] Statement 5:\n",
      "[DEBUG] Checking stmt_line 5\n",
      "[DEBUG] Appending CSTStatement for line 20, step=None, code=return answer\n",
      "[DEBUG] Returning 6 statements\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d72472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_lines_dict(problem_index: int, model_name: str, base_output_dir: Path = BASE_OUTPUT_DIR) -> dict[int, str] | None:\n",
    "    \"\"\"\n",
    "    Returns a dict mapping line numbers (0-based) to the verbatim lines of code\n",
    "    for the given problem index and model name.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    problem_dir = base_output_dir / str(problem_index)\n",
    "    file_path = problem_dir / f\"{model_name}.py\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"[Error] File not found: {file_path}\", file=sys.stderr)\n",
    "        return None\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return {i: line.rstrip(\"\\n\") for i, line in enumerate(lines)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a184a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'def solve(',\n",
       " 1: '    every_hundred_years: int = 100, # rises from the waters once every hundred years',\n",
       " 2: '    over_three_hundred_years: int = 300, # Over three hundred years',\n",
       " 3: '    total_people_consumed: int = 847, # it has consumed 847 people',\n",
       " 4: '    twice_as_many: int = 2 # each new ship has twice as many people as the last ship',\n",
       " 5: '):',\n",
       " 6: '    \"\"\"Index: 10.',\n",
       " 7: '    Returns: the number of people on the ship the monster ate in the first hundred years.',\n",
       " 8: '    \"\"\"',\n",
       " 9: '    # L1: Let S be the number of people on the first hundred years ship. (Implicitly defined as the variable we solve for)',\n",
       " 10: '    #: L2',\n",
       " 11: '    # The second hundred years ship had twice as many as the first, so it had 2S people.',\n",
       " 12: \"    # This is represented by the factor 'twice_as_many'\",\n",
       " 13: '    #: L3',\n",
       " 14: '    # The third hundred years ship had twice as many as the second, so it had 2 * 2S = 4S people.',\n",
       " 15: '    multiplier_third_ship = twice_as_many * twice_as_many',\n",
       " 16: '',\n",
       " 17: '    #: L4',\n",
       " 18: '    # All the ships had S + 2S + 4S = 7S = 847 people.',\n",
       " 19: '    # The total factor is 1 (for the first ship) + 2 (for the second) + 4 (for the third)',\n",
       " 20: '    total_factor = 1 + twice_as_many + multiplier_third_ship',\n",
       " 21: '',\n",
       " 22: '    #: L5',\n",
       " 23: '    # Thus, the ship that the monster ate in the first hundred years had S = 847 / 7 = 121 people on it.',\n",
       " 24: '    people_first_ship = total_people_consumed / total_factor',\n",
       " 25: '',\n",
       " 26: '    answer = people_first_ship # FINAL ANSWER',\n",
       " 27: '    return answer'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_dict = get_code_lines_dict(10, \"google_gemini-2.0-flash-thinking-exp\")\n",
    "lines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f8d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Normalizing with v2 function ---\n",
      "\n",
      "--- Normalized Code Dictionary (Problem 10) ---\n",
      "{\n",
      "    \"L3\": [\n",
      "        \"multiplier_third_ship = twice_as_many * twice_as_many\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_factor = 1 + twice_as_many + multiplier_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / total_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Normalized Code Dictionary (Problem 8) ---\n",
      "{\n",
      "    \"L2\": [\n",
      "        \"spent_on_clothes_without_shoes = shirt_cost + pants_cost + coat_cost + socks_cost + belt_cost\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_spent = budget - money_left\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"shoes_cost = total_spent - spent_on_clothes_without_shoes\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = shoes_cost # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "def normalize_code_lines(raw_lines_dict: dict[int, str]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Normalizes raw code lines into a canonical format. (Version 2)\n",
    "    \n",
    "    This version correctly separates the final 'answer' and 'return'\n",
    "    statements from the logical solution steps.\n",
    "    \"\"\"\n",
    "    normalized_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(raw_lines_dict.items())\n",
    "    last_seen_step = None\n",
    "\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        line = line_content.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        step_match = _TRACE_RE.match(line)\n",
    "        if step_match:\n",
    "            last_seen_step = f\"L{step_match.group(1)}\"\n",
    "            continue\n",
    "\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        code_part = line\n",
    "        trailing_match = _TRACE_RE.search(line)\n",
    "        if trailing_match:\n",
    "            last_seen_step = f\"L{trailing_match.group(1)}\"\n",
    "            code_part = line[:trailing_match.start()].strip()\n",
    "        \n",
    "        # --- MODIFIED LOGIC ---\n",
    "        # Prioritize checking for final lines BEFORE associating with a step\n",
    "        if 'answer =' in line:\n",
    "            normalized_dict['answer_line'].append(code_part)\n",
    "        elif line.startswith('return'):\n",
    "            normalized_dict['return_line'].append(code_part)\n",
    "        elif last_seen_step:\n",
    "            # Only associate with a step if it's not a final line\n",
    "            normalized_dict[last_seen_step].append(code_part)\n",
    "            \n",
    "    return dict(normalized_dict)\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "print(\"--- Normalizing with v2 function ---\")\n",
    "\n",
    "# Run on the first example\n",
    "raw_lines_1 = get_code_lines_dict(10, \"google_gemini-2.0-flash-thinking-exp\")\n",
    "if raw_lines_1:\n",
    "    normalized_code_1 = normalize_code_lines(raw_lines_1)\n",
    "    print(\"\\n--- Normalized Code Dictionary (Problem 10) ---\")\n",
    "    print(json.dumps(normalized_code_1, indent=4))\n",
    "\n",
    "# Run on the second example\n",
    "raw_lines_2 = get_code_lines_dict(8, \"google_gemini-2.5-flash-lite-preview-06-17\")\n",
    "if raw_lines_2:\n",
    "    normalized_code_2 = normalize_code_lines(raw_lines_2)\n",
    "    print(\"\\n--- Normalized Code Dictionary (Problem 8) ---\")\n",
    "    print(json.dumps(normalized_code_2, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a71c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated master dictionary with 100 problem indices.\n",
      "\n",
      "--- Verifying data for Problem 10 ---\n",
      "\n",
      "Normalized code for 'anthropic_claude-3-5-haiku-20241022':\n",
      "{\n",
      "    \"L4\": [\n",
      "        \"total_ship_multiplier = 1 + 2 + 4  # S + 2S + 4S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_ship_multiplier\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Normalized code for 'openai_gpt-4.1-mini':\n",
      "{\n",
      "    \"L4\": [\n",
      "        \"total_ships_factor = 7  # S + 2S + 4S = 7S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"first_ship_people = total_people / total_ships_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = first_ship_people  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Normalized code for 'google_gemini-2.0-flash-thinking-exp':\n",
      "{\n",
      "    \"L3\": [\n",
      "        \"multiplier_third_ship = twice_as_many * twice_as_many\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_factor = 1 + twice_as_many + multiplier_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / total_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Normalized code for 'google_gemini-2.5-flash-lite-preview-06-17':\n",
      "{\n",
      "    \"L4\": [\n",
      "        \"total_proportions = 1 + 2 + 4\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_eaten / total_proportions\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Normalized code for 'google_gemini-2.5-flash':\n",
      "{\n",
      "    \"L3\": [\n",
      "        \"coefficient_first_ship = 1\",\n",
      "        \"coefficient_second_ship = growth_factor\",\n",
      "        \"coefficient_third_ship = growth_factor * growth_factor\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_coefficient_s\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "No data found for model 'google_gemini-2.5-flash' in problem 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/52/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/59/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/71/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/76/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/82/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/85/anthropic_claude-3-5-haiku-20241022.py\n",
      "[Error] File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_cleaned/98/anthropic_claude-3-5-haiku-20241022.py\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def generate_normalized_code_dict(\n",
    "    indices: range, \n",
    "    models: list[str],\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR\n",
    ") -> dict[int, dict[str, dict[str, list[str]]]]:\n",
    "    \"\"\"\n",
    "    Generates a nested dictionary containing the normalized code for each model\n",
    "    across a range of problem indices.\n",
    "\n",
    "    The structure of the returned dictionary is:\n",
    "    {\n",
    "        problem_index_1: {\n",
    "            \"model_name_A\": { \"L1\": [\"code\"], \"L2\": [\"code\"], ... },\n",
    "            \"model_name_B\": { \"L1\": [\"code\"], ... },\n",
    "            ...\n",
    "        },\n",
    "        problem_index_2: { ... },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        indices: A range of problem indices to process (e.g., range(100)).\n",
    "        models: A list of model name strings.\n",
    "        base_output_dir: The root directory for the cleaned code outputs.\n",
    "\n",
    "    Returns:\n",
    "        A nested dictionary containing all the normalized code.\n",
    "    \"\"\"\n",
    "    master_dict = {}\n",
    "\n",
    "    for index in indices:\n",
    "        index_dict = {}\n",
    "        for model_name in models:\n",
    "            # 1. Get the raw lines for the current model and index\n",
    "            raw_lines = get_code_lines_dict(index, model_name, base_output_dir)\n",
    "            \n",
    "            if raw_lines:\n",
    "                # 2. Normalize the code if raw lines were found\n",
    "                normalized_code = normalize_code_lines(raw_lines)\n",
    "                if normalized_code:  # Ensure the dictionary is not empty\n",
    "                    index_dict[model_name] = normalized_code\n",
    "\n",
    "        if index_dict:  # Only add the index if at least one model had valid code\n",
    "            master_dict[index] = index_dict\n",
    "            \n",
    "    return master_dict\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Use the constants defined in your first cell\n",
    "# We'll run it on a smaller range (0-20) for a quick demonstration\n",
    "INDICES_TO_PROCESS = range(100) \n",
    "\n",
    "# Generate the master dictionary\n",
    "normalized_code_master_dict = generate_normalized_code_dict(\n",
    "    indices=INDICES_TO_PROCESS,\n",
    "    models=MODELS\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated master dictionary with {len(normalized_code_master_dict)} problem indices.\")\n",
    "\n",
    "# --- Inspect the output for a specific problem ---\n",
    "# Let's look at the data for problem 10 again to verify\n",
    "if 10 in normalized_code_master_dict:\n",
    "    print(\"\\n--- Verifying data for Problem 10 ---\")\n",
    "    problem_10_data = normalized_code_master_dict[10]\n",
    "    \n",
    "    # Print the normalized code for one of the models\n",
    "    for model in MODELS:\n",
    "        if model in problem_10_data:\n",
    "            print(f\"\\nNormalized code for '{model}':\")\n",
    "            print(json.dumps(problem_10_data[model], indent=4))\n",
    "    else:\n",
    "        print(f\"No data found for model '{model}' in problem 10.\")\n",
    "else:\n",
    "    print(\"\\nData for problem 10 not found in the generated dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3621b087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthropic_claude-3-5-haiku-20241022': {'L4': ['total_ship_multiplier = 1 + 2 + 4  # S + 2S + 4S'],\n",
       "  'L5': ['people_on_first_ship = total_people_consumed / total_ship_multiplier'],\n",
       "  'answer_line': ['answer = people_on_first_ship  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'openai_gpt-4.1-mini': {'L4': ['total_ships_factor = 7  # S + 2S + 4S = 7S'],\n",
       "  'L5': ['first_ship_people = total_people / total_ships_factor'],\n",
       "  'answer_line': ['answer = first_ship_people  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.0-flash-thinking-exp': {'L3': ['multiplier_third_ship = twice_as_many * twice_as_many'],\n",
       "  'L4': ['total_factor = 1 + twice_as_many + multiplier_third_ship'],\n",
       "  'L5': ['people_first_ship = total_people_consumed / total_factor'],\n",
       "  'answer_line': ['answer = people_first_ship # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash-lite-preview-06-17': {'L4': ['total_proportions = 1 + 2 + 4'],\n",
       "  'L5': ['people_on_first_ship = total_people_eaten / total_proportions'],\n",
       "  'answer_line': ['answer = people_on_first_ship # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash': {'L3': ['coefficient_first_ship = 1',\n",
       "   'coefficient_second_ship = growth_factor',\n",
       "   'coefficient_third_ship = growth_factor * growth_factor'],\n",
       "  'L4': ['total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship'],\n",
       "  'L5': ['people_on_first_ship = total_people_consumed / total_coefficient_s'],\n",
       "  'answer_line': ['answer = people_on_first_ship # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_code_master_dict[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89d8cbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthropic_claude-3-5-haiku-20241022': {'L1': ['sets_of_20_mins_per_hour = minutes_per_hour // 20'],\n",
       "  'L2': ['pages_read_per_hour = pages_read_in_20_mins * sets_of_20_mins_per_hour'],\n",
       "  'L3': ['hours_to_read = total_pages / pages_read_per_hour'],\n",
       "  'answer_line': ['answer = hours_to_read  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'openai_gpt-4.1-mini': {'L1': ['sets_per_hour = 60 / 20'],\n",
       "  'L2': ['pages_per_hour = pages_per_20_min * sets_per_hour'],\n",
       "  'L3': ['hours_needed = total_pages / pages_per_hour'],\n",
       "  'answer_line': ['answer = hours_needed  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.0-flash-thinking-exp': {'L1': ['minutes_in_hour = 60',\n",
       "   'intervals_in_hour = minutes_in_hour / time_in_minutes'],\n",
       "  'L2': ['pages_per_hour = pages_read_in_time * intervals_in_hour'],\n",
       "  'L3': ['time_needed_hours = total_pages_to_read / pages_per_hour'],\n",
       "  'answer_line': ['answer = time_needed_hours # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash-lite-preview-06-17': {'L1': ['minutes_in_an_hour = 60',\n",
       "   'sets_of_20_min_in_an_hour = minutes_in_an_hour / time_interval_minutes'],\n",
       "  'L2': ['pages_read_per_hour = pages_read_in_20_min * sets_of_20_min_in_an_hour'],\n",
       "  'L3': ['hours_to_read_120_pages = pages_to_read / pages_read_per_hour'],\n",
       "  'answer_line': ['answer = hours_to_read_120_pages # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash': {'L1': ['sets_of_minutes_in_hour = 60 / minutes_per_segment'],\n",
       "  'L2': ['pages_per_hour = pages_per_segment * sets_of_minutes_in_hour'],\n",
       "  'L3': ['hours_to_read_target_pages = target_pages / pages_per_hour'],\n",
       "  'answer_line': ['answer = hours_to_read_target_pages # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_code_master_dict[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d361cffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthropic_claude-3-5-haiku-20241022': {'L1': ['part_value = johnson_amount / johnson_share_ratio'],\n",
       "  'L3': ['mike_total_share = mike_share_ratio * part_value'],\n",
       "  'L4': ['mike_remaining_amount = mike_total_share - shirt_cost'],\n",
       "  'answer_line': ['answer = mike_remaining_amount  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'openai_gpt-4.1-mini': {'L2': ['value_per_part = johnson_share / 5'],\n",
       "  'L3': ['mike_share = 2 * value_per_part'],\n",
       "  'L4': ['mike_after_shirt = mike_share - shirt_cost'],\n",
       "  'answer_line': ['answer = mike_after_shirt  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.0-flash-thinking-exp': {'L2': ['value_per_part = johnson_share / johnson_ratio_part'],\n",
       "  'L3': ['mike_share = mike_ratio_part * value_per_part'],\n",
       "  'L4': ['mike_remaining = mike_share - shirt_cost'],\n",
       "  'answer_line': ['answer = mike_remaining # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash-lite-preview-06-17': {'L2': ['value_per_part = johnson_share / ratio_johnson'],\n",
       "  'L3': ['mike_share = ratio_mike * value_per_part'],\n",
       "  'L4': ['mike_remaining_share = mike_share - shirt_cost'],\n",
       "  'answer_line': ['answer = mike_remaining_share # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash': {'L2': ['value_per_part = johnson_share / johnson_ratio_part'],\n",
       "  'L3': ['mike_share = mike_ratio_part * value_per_part'],\n",
       "  'L4': ['mike_remaining_share = mike_share - shirt_cost'],\n",
       "  'answer_line': ['answer = mike_remaining_share # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_code_master_dict[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbe6c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the GSM8K dataset\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0ac679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 16\n",
      "Question: The profit from a business transaction is shared among 2 business partners, Mike and Johnson in the ratio 2:5 respectively. If Johnson got $2500, how much will Mike have after spending some of his share on a shirt that costs $200?\n",
      "Answer: According to the ratio, for every 5 parts that Johnson gets, Mike gets 2 parts\n",
      "Since Johnson got $2500, each part is therefore $2500/5 = $<<2500/5=500>>500\n",
      "Mike will get 2*$500 = $<<2*500=1000>>1000\n",
      "After buying the shirt he will have $1000-$200 = $<<1000-200=800>>800 left\n",
      "#### 800\n"
     ]
    }
   ],
   "source": [
    "sample = gsm8k_dataset[16]\n",
    "print(f\"Index: 16\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aaacb284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthropic_claude-3-5-haiku-20241022': {'L1': ['first_batch_missed_balls = (1 - first_batch_hit_fraction) * first_batch_balls'],\n",
       "  'L2': ['next_batch_missed_balls = (1 - next_batch_hit_fraction) * next_batch_balls'],\n",
       "  'L3': ['total_missed_balls = first_batch_missed_balls + next_batch_missed_balls'],\n",
       "  'answer_line': ['answer = total_missed_balls  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'openai_gpt-4.1-mini': {'L1': ['not_hit_first = (1 - hit_fraction_first) * first_batch'],\n",
       "  'L2': ['not_hit_second = (1 - hit_fraction_second) * second_batch'],\n",
       "  'L3': ['total_not_hit = not_hit_first + not_hit_second'],\n",
       "  'answer_line': ['answer = total_not_hit  # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.0-flash-thinking-exp': {'L1': ['fraction_not_hit_first_batch = 1 - fraction_hit_first_batch',\n",
       "   'not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size'],\n",
       "  'L2': ['fraction_not_hit_second_batch = 1 - fraction_hit_second_batch',\n",
       "   'not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size'],\n",
       "  'L3': ['total_not_hit = not_hit_first_batch + not_hit_second_batch'],\n",
       "  'answer_line': ['answer = total_not_hit # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash-lite-preview-06-17': {'L1': ['first_batch_not_hit_fraction = 1 - first_batch_hit_fraction',\n",
       "   'first_batch_not_hit_count = first_batch_not_hit_fraction * first_batch_count'],\n",
       "  'L2': ['second_batch_not_hit_fraction = 1 - second_batch_hit_fraction',\n",
       "   'second_batch_not_hit_count = second_batch_not_hit_fraction * second_batch_count'],\n",
       "  'L3': ['total_not_hit = first_batch_not_hit_count + second_batch_not_hit_count'],\n",
       "  'answer_line': ['answer = total_not_hit # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']},\n",
       " 'google_gemini-2.5-flash': {'L1': ['fraction_not_hit_first_set = 1 - fraction_hit_first_set',\n",
       "   'balls_not_hit_first_set = fraction_not_hit_first_set * first_set_balls'],\n",
       "  'L2': ['fraction_not_hit_second_set = 1 - fraction_hit_second_set',\n",
       "   'balls_not_hit_second_set = fraction_not_hit_second_set * second_set_balls'],\n",
       "  'L3': ['total_balls_not_hit = balls_not_hit_first_set + balls_not_hit_second_set'],\n",
       "  'answer_line': ['answer = total_balls_not_hit # FINAL ANSWER'],\n",
       "  'return_line': ['return answer']}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_code_master_dict[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72bab998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Step 1: Pre-Normalized Code ---\n",
      "def solve(\n",
      "    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\n",
      "    first_batch_size: int = 100, # Out of the first 100 balls\n",
      "    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\n",
      "    second_batch_size: int = 75, # Of the next 75 tennis balls\n",
      "    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\n",
      "):\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch  # L1\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size  # L1\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch  # L2\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size  # L2\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch  # L3\n",
      "    answer = total_not_hit # FINAL ANSWER\n",
      "    return answer\n",
      "'def solve(\\n    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\\n    first_batch_size: int = 100, # Out of the first 100 balls\\n    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_size: int = 75, # Of the next 75 tennis balls\\n    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\\n):\\n    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch  # L1\\n    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size  # L1\\n    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch  # L2\\n    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size  # L2\\n    total_not_hit = not_hit_first_batch + not_hit_second_batch  # L3\\n    answer = total_not_hit # FINAL ANSWER\\n    return answer'\n",
      "\n",
      "--- Step 2: Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solve(initial_balls: int=175, first_batch_size: int=100, fraction_hit_first_batch: float=0.4, second_batch_size: int=75, fraction_hit_second_batch: float=0.3333333333333333):\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "    answer = total_not_hit\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "# The following is a dictionary of all variable values\n",
      "# when the function is run with its default arguments.\n",
      "\n",
      "\"\"\"\n",
      "{\n",
      "    \"result\": 110.0\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Assumes that BASE_OUTPUT_DIR, get_code_lines_dict, and normalize_code_lines\n",
    "# are defined in previous cells.\n",
    "\n",
    "# Define a new output directory for the final, fully normalized code\n",
    "FINAL_NORMALIZED_DIR = PROJECT_ROOT / \"data\" / \"code_gen_outputs_final\"\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. Function to generate the \"Pre-Normalized\" function string\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def generate_pre_normalized_function(\n",
    "    problem_index: int, model_name: str\n",
    ") -> tuple[str, dict] | None:\n",
    "    \"\"\"\n",
    "    Reads a raw model output, normalizes its step-wise logic, and\n",
    "    reconstructs it into a single, canonical Python function string.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The pre-normalized function code as a single string.\n",
    "        - The normalized_dict mapping steps to code lines.\n",
    "        Returns None if the source file cannot be processed.\n",
    "    \"\"\"\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines:\n",
    "        return None\n",
    "\n",
    "    normalized_dict = normalize_code_lines(raw_lines)\n",
    "    if not normalized_dict:\n",
    "        return None\n",
    "\n",
    "    # --- Reconstruct the function ---\n",
    "    # a. Extract the original function signature\n",
    "    signature_lines = []\n",
    "    in_signature = False\n",
    "    for line in raw_lines.values():\n",
    "        if line.strip().startswith(\"def solve(\"):\n",
    "            in_signature = True\n",
    "        if in_signature:\n",
    "            signature_lines.append(line)\n",
    "        if in_signature and line.strip().endswith(\"):\"):\n",
    "            break\n",
    "    \n",
    "    # b. Reconstruct the body from the normalized dict\n",
    "    body_lines = []\n",
    "    # Ensure a consistent order for reconstruction\n",
    "    sorted_steps = sorted([key for key in normalized_dict if key.startswith('L')])\n",
    "    \n",
    "    for step in sorted_steps:\n",
    "        for code_line in normalized_dict[step]:\n",
    "            body_lines.append(f\"    {code_line}  # {step}\")\n",
    "\n",
    "    # Add the answer and return lines\n",
    "    if 'answer_line' in normalized_dict:\n",
    "        body_lines.append(f\"    {normalized_dict['answer_line'][0]}\")\n",
    "    if 'return_line' in normalized_dict:\n",
    "        body_lines.append(f\"    {normalized_dict['return_line'][0]}\")\n",
    "        \n",
    "    # c. Combine into a full function string\n",
    "    full_code = \"\\n\".join(signature_lines) + \"\\n\" + \"\\n\".join(body_lines)\n",
    "    \n",
    "    return full_code, normalized_dict\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  2. AST Transformers for Final Normalization\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "class ConstantFolder(ast.NodeTransformer):\n",
    "    \"\"\"\n",
    "    An AST transformer that finds and evaluates binary operations on constants.\n",
    "    Example: a tree for `3 + 5` becomes a single Constant node with value `8`.\n",
    "    \"\"\"\n",
    "    def visit_BinOp(self, node):\n",
    "        self.generic_visit(node) # Ensure children are visited first\n",
    "        if isinstance(node.left, ast.Constant) and isinstance(node.right, ast.Constant):\n",
    "            # Both operands are constants, we can evaluate them.\n",
    "            left_val = node.left.value\n",
    "            right_val = node.right.value\n",
    "            op = node.op\n",
    "            \n",
    "            # Use a safe evaluation for the specific operation\n",
    "            if isinstance(op, ast.Add): result = left_val + right_val\n",
    "            elif isinstance(op, ast.Sub): result = left_val - right_val\n",
    "            elif isinstance(op, ast.Mult): result = left_val * right_val\n",
    "            elif isinstance(op, ast.Div): result = left_val / right_val\n",
    "            # Add other operations as needed (Pow, etc.)\n",
    "            else: return node # Cannot evaluate, return original node\n",
    "\n",
    "            return ast.Constant(value=result)\n",
    "        return node\n",
    "\n",
    "\n",
    "def _substitute_step_vars(statements: list[ast.stmt]) -> list[ast.stmt]:\n",
    "    \"\"\"\n",
    "    A helper function to perform intermediate variable substitution on a\n",
    "    list of AST assignment statements for a single logical step.\n",
    "    \"\"\"\n",
    "    # 1. Identify variables defined within this block\n",
    "    defined_vars = {\n",
    "        node.targets[0].id for node in statements if isinstance(node, ast.Assign)\n",
    "    }\n",
    "\n",
    "    # 2. Identify all variables used within this block\n",
    "    used_vars = {\n",
    "        node.id for stmt in statements for node in ast.walk(stmt) if isinstance(node, ast.Name)\n",
    "    }\n",
    "\n",
    "    # 3. An intermediate variable is one that is defined and used, but not used elsewhere\n",
    "    #    (For simplicity here, we assume any var defined in the block is intermediate)\n",
    "    substitution_map = {\n",
    "        node.targets[0].id: node.value \n",
    "        for node in statements if isinstance(node, ast.Assign)\n",
    "    }\n",
    "    \n",
    "    # Transformer to perform the substitution\n",
    "    class Substituter(ast.NodeTransformer):\n",
    "        def visit_Name(self, node):\n",
    "            if node.id in substitution_map and isinstance(node.ctx, ast.Load):\n",
    "                # Replace the variable name with its defining expression\n",
    "                return substitution_map[node.id]\n",
    "            return node\n",
    "\n",
    "    # 4. Apply substitution to all statements\n",
    "    transformer = Substituter()\n",
    "    substituted_statements = [transformer.visit(s) for s in statements]\n",
    "\n",
    "    # 5. Filter out the original assignments for the variables we just substituted\n",
    "    final_statements = [\n",
    "        s for s in substituted_statements if not (\n",
    "            isinstance(s, ast.Assign) and s.targets[0].id in defined_vars and s.targets[0].id != 'answer'\n",
    "        )\n",
    "    ]\n",
    "    # This logic is simplified; a perfect implementation would track usage counts.\n",
    "    # For GSM8K, this should be sufficient.\n",
    "    return final_statements if final_statements else substituted_statements\n",
    "\n",
    "def _execute_and_trace(code_str: str, signature: dict) -> dict:\n",
    "    # Prepare the namespace and define the function\n",
    "    namespace = {}\n",
    "    exec(code_str, namespace)\n",
    "    # Call the function with the default arguments\n",
    "    result = namespace['solve'](**signature)\n",
    "    # Optionally, return just the result or modify the function to return locals()\n",
    "    return {'result': result}\n",
    "\n",
    "# def _execute_and_trace(code_str: str, signature: dict) -> dict:\n",
    "#     \"\"\"Executes a function string and returns a dict of all local variables.\"\"\"\n",
    "#     # We must provide the function with its default args to execute it\n",
    "#     default_args = {name: val for name, val in signature.items()}\n",
    "    \n",
    "#     # A namespace to execute the code in and capture the results\n",
    "#     namespace = {}\n",
    "#     namespace.update(default_args)\n",
    "    \n",
    "#     # Parse the code and find the function body\n",
    "#     tree = ast.parse(code_str)\n",
    "#     function_body_nodes = []\n",
    "#     for node in ast.walk(tree):\n",
    "#         if isinstance(node, ast.FunctionDef) and node.name == 'solve':\n",
    "#             function_body_nodes = node.body\n",
    "#             break\n",
    "            \n",
    "#     # Execute each statement of the function body in the namespace\n",
    "#     for stmt in function_body_nodes:\n",
    "#         # Wrap statement in a module to make it compilable\n",
    "#         module_wrapper = ast.Module(body=[stmt], type_ignores=[])\n",
    "#         code_obj = compile(module_wrapper, '<string>', 'exec')\n",
    "#         exec(code_obj, namespace)\n",
    "        \n",
    "#     # Return all variables defined during execution, excluding builtins\n",
    "#     return {k: v for k, v in namespace.items() if not k.startswith('__')}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  3. Main Orchestrator Function\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def normalize_and_save_function(\n",
    "    pre_normalized_code: str,\n",
    "    original_signature: dict, # Needed for the trace\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a pre-normalized function, applies final AST transformations,\n",
    "    generates a value trace, and saves the results.\n",
    "    \"\"\"\n",
    "    # 1. Parse the pre-normalized code\n",
    "    try:\n",
    "        print(repr(pre_normalized_code))\n",
    "        tree = ast.parse(pre_normalized_code)\n",
    "    except SyntaxError:\n",
    "        return # Cannot process\n",
    "        \n",
    "    # 2. Apply constant folding across the entire tree\n",
    "    folder = ConstantFolder()\n",
    "    tree = folder.visit(tree)\n",
    "    \n",
    "    # 3. Apply intermediate variable substitution (This logic is complex and simplified here)\n",
    "    # A full implementation would be more involved, but this demonstrates the principle.\n",
    "    # For now, we will skip this complex step and focus on the structure.\n",
    "    # A more robust version would apply `_substitute_step_vars` to each step's nodes.\n",
    "\n",
    "    # 4. Unparse to get the final normalized code\n",
    "    final_code_str = ast.unparse(tree)\n",
    "\n",
    "    # 5. Generate the value_dict by executing the final code\n",
    "    value_dict = _execute_and_trace(final_code_str, original_signature)\n",
    "    \n",
    "    # 6. Save the results\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare the value_dict for pretty printing\n",
    "    value_dict_str = json.dumps(value_dict, indent=4)\n",
    "    \n",
    "    final_output_content = (\n",
    "        f\"{final_code_str}\\n\\n\"\n",
    "        f\"# --- EXECUTION TRACE (value_dict) ---\\n\"\n",
    "        f\"# The following is a dictionary of all variable values\\n\"\n",
    "        f\"# when the function is run with its default arguments.\\n\\n\"\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    \n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "    \n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example End-to-End Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Use a model that has both single and multi-line steps\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full normalization pipeline for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "# 1. Generate the pre-normalized function\n",
    "result = generate_pre_normalized_function(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "if result:\n",
    "    pre_norm_code, norm_dict = result\n",
    "    print(\"\\n--- Step 1: Pre-Normalized Code ---\")\n",
    "    print(pre_norm_code)\n",
    "    \n",
    "    # We need the original signature to run the trace\n",
    "    from inspect import signature, Parameter\n",
    "    # This is a bit of a hack to get the signature; a more robust way\n",
    "    # would be to parse it properly.\n",
    "    temp_namespace = {}\n",
    "    exec(pre_norm_code.split('\"\"\"')[0], temp_namespace)\n",
    "    sig = signature(temp_namespace['solve'])\n",
    "    original_signature_defaults = {\n",
    "        p.name: p.default for p in sig.parameters.values() \n",
    "        if p.default is not Parameter.empty\n",
    "    }\n",
    "\n",
    "    # 2. Run the final normalization and save\n",
    "    normalize_and_save_function(\n",
    "        pre_normalized_code=pre_norm_code,\n",
    "        original_signature=original_signature_defaults,\n",
    "        problem_index=TARGET_INDEX,\n",
    "        model_name=TARGET_MODEL\n",
    "    )\n",
    "    \n",
    "    # 3. Verify the saved output\n",
    "    saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "    if saved_file.exists():\n",
    "        print(f\"\\n--- Step 2: Final Normalized File Saved ---\")\n",
    "        print(f\"File created at: {saved_file}\")\n",
    "        print(\"\\n--- File Content ---\")\n",
    "        print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d5c2c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solve(initial_balls: int=175, first_batch_size: int=100, fraction_hit_first_batch: float=2 / 5, second_batch_size: int=75, fraction_hit_second_batch: float=1 / 3):\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "    answer = total_not_hit\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "\"\"\"\n",
      "{\n",
      "    \"initial_balls\": 175,\n",
      "    \"first_batch_size\": 100,\n",
      "    \"fraction_hit_first_batch\": 0.4,\n",
      "    \"second_batch_size\": 75,\n",
      "    \"fraction_hit_second_batch\": 0.3333333333333333,\n",
      "    \"fraction_not_hit_first_batch\": 0.6,\n",
      "    \"not_hit_first_batch\": 60.0,\n",
      "    \"fraction_not_hit_second_batch\": 0.6666666666666667,\n",
      "    \"not_hit_second_batch\": 50.00000000000001,\n",
      "    \"total_not_hit\": 110.0,\n",
      "    \"answer\": 110.0\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Assumes previous cells have defined:\n",
    "# - PROJECT_ROOT\n",
    "# - BASE_OUTPUT_DIR\n",
    "# - get_code_lines_dict\n",
    "# - normalize_code_lines\n",
    "\n",
    "FINAL_NORMALIZED_DIR = PROJECT_ROOT / \"data\" / \"code_gen_outputs_final\"\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. AST Transformers for Normalization (Unchanged)\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "class ConstantFolder(ast.NodeTransformer):\n",
    "    \"\"\"\n",
    "    An AST transformer that finds and evaluates binary operations on constants.\n",
    "    \"\"\"\n",
    "    def visit_BinOp(self, node):\n",
    "        self.generic_visit(node)\n",
    "        if isinstance(node.left, ast.Constant) and isinstance(node.right, ast.Constant):\n",
    "            left_val = node.left.value\n",
    "            right_val = node.right.value\n",
    "            op = node.op\n",
    "            if isinstance(op, ast.Add): result = left_val + right_val\n",
    "            elif isinstance(op, ast.Sub): result = left_val - right_val\n",
    "            elif isinstance(op, ast.Mult): result = left_val * right_val\n",
    "            elif isinstance(op, ast.Div): result = left_val / right_val\n",
    "            else: return node\n",
    "            return ast.Constant(value=result)\n",
    "        return node\n",
    "\n",
    "# Note: A robust intermediate variable substituter is highly complex.\n",
    "# We will omit it for now in favor of a simpler, more reliable pipeline.\n",
    "# The primary normalization will come from constant folding.\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  2. Corrected Tracing and Normalization Logic\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def _instrument_and_trace(tree: ast.AST) -> tuple[ast.AST, str]:\n",
    "    \"\"\"\n",
    "    Instruments a function's AST to trace all variable assignments and\n",
    "    generates the code to execute this trace.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The instrumented AST of the function.\n",
    "        - The name of the dictionary that will hold the trace results.\n",
    "    \"\"\"\n",
    "    trace_dict_name = \"__execution_trace__\"\n",
    "    \n",
    "    class AssignmentTracer(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            # Inject the trace dictionary initialization at the start of the function\n",
    "            trace_init_expr = ast.parse(f\"{trace_dict_name} = {{}}\").body[0]\n",
    "            # Also copy function args into the trace dict\n",
    "            arg_names = [arg.arg for arg in node.args.args]\n",
    "            arg_trace_expr = ast.parse(\n",
    "                f\"{trace_dict_name}.update({{name: value for name, value in locals().items() if name in {arg_names}}})\"\n",
    "            ).body[0]\n",
    "            \n",
    "            # Instrument the rest of the body\n",
    "            new_body = [trace_init_expr, arg_trace_expr]\n",
    "            for stmt in node.body:\n",
    "                new_body.append(stmt)\n",
    "                if isinstance(stmt, ast.Assign):\n",
    "                    # For each assignment, add a line to log the result\n",
    "                    var_name = stmt.targets[0].id\n",
    "                    log_expr = ast.parse(f\"{trace_dict_name}['{var_name}'] = {var_name}\").body[0]\n",
    "                    new_body.append(log_expr)\n",
    "            \n",
    "            # Inject a final return of the trace dictionary\n",
    "            return_trace_expr = ast.parse(f\"return {trace_dict_name}\").body[0]\n",
    "            # Replace the original return statement with this one\n",
    "            final_body = [s for s in new_body if not isinstance(s, ast.Return)]\n",
    "            final_body.append(return_trace_expr)\n",
    "            \n",
    "            node.body = final_body\n",
    "            return node\n",
    "\n",
    "    tracer = AssignmentTracer()\n",
    "    instrumented_tree = tracer.visit(tree)\n",
    "    return instrumented_tree, trace_dict_name\n",
    "\n",
    "\n",
    "def generate_final_normalized_code(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full normalization pipeline for a single model file.\n",
    "    - Reads the original code.\n",
    "    - Normalizes step comments into a canonical body.\n",
    "    - Applies AST transformations (constant folding) to the body.\n",
    "    - Instruments the final AST to produce an execution trace.\n",
    "    - Saves the final code and its trace to a new file.\n",
    "    \"\"\"\n",
    "    # 1. Get original source code\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines: return\n",
    "    source_code = \"\\n\".join(raw_lines.values())\n",
    "    \n",
    "    # 2. Get normalized step-wise dictionary\n",
    "    normalized_steps = normalize_code_lines(raw_lines)\n",
    "    if not normalized_steps: return\n",
    "\n",
    "    # 3. Parse the original code to get the original function definition\n",
    "    try:\n",
    "        original_tree = ast.parse(source_code)\n",
    "        original_func_def = next(\n",
    "            (n for n in original_tree.body if isinstance(n, ast.FunctionDef)), None\n",
    "        )\n",
    "        if not original_func_def: return\n",
    "    except SyntaxError:\n",
    "        return\n",
    "        \n",
    "    # 4. Reconstruct the body from the normalized steps\n",
    "    body_str_parts = []\n",
    "    sorted_steps = sorted([key for key in normalized_steps if key.startswith('L')])\n",
    "    for step in sorted_steps:\n",
    "        body_str_parts.extend(normalized_steps[step])\n",
    "    body_str_parts.extend(normalized_steps.get('answer_line', []))\n",
    "    body_str_parts.extend(normalized_steps.get('return_line', []))\n",
    "    \n",
    "    body_str = \"\\n\".join(body_str_parts)\n",
    "    \n",
    "    # 5. Parse the reconstructed body and apply AST transformations\n",
    "    try:\n",
    "        # We need to wrap it in a dummy function to parse it\n",
    "        body_tree = ast.parse(f\"def dummy():\\n    {body_str.replace('\\n', '\\n    ')}\")\n",
    "        body_nodes = body_tree.body[0].body\n",
    "        \n",
    "        # Apply constant folding\n",
    "        folder = ConstantFolder()\n",
    "        # We must visit each node individually\n",
    "        transformed_body_nodes = [folder.visit(node) for node in body_nodes]\n",
    "        \n",
    "    except SyntaxError:\n",
    "        # If the reconstructed body is invalid, we can't proceed\n",
    "        return\n",
    "\n",
    "    # 6. Replace the original function's body with the new transformed body\n",
    "    original_func_def.body = transformed_body_nodes\n",
    "    final_tree = ast.Module(body=[original_func_def], type_ignores=[])\n",
    "    final_code_str = ast.unparse(final_tree)\n",
    "    \n",
    "    # 7. Instrument the final tree to get the execution trace\n",
    "    instrumented_tree, trace_dict_name = _instrument_and_trace(ast.parse(final_code_str))\n",
    "    instrumented_code_str = ast.unparse(instrumented_tree)\n",
    "    \n",
    "    # 8. Execute the instrumented code to get the value_dict\n",
    "    try:\n",
    "        namespace = {}\n",
    "        exec(instrumented_code_str, namespace)\n",
    "        solve_func = namespace['solve']\n",
    "        value_dict = solve_func() # No args needed as defaults are in the code\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing trace for {model_name}: {e!r}\", file=sys.stderr)\n",
    "        value_dict = {\"__error__\": f\"Execution failed: {e!r}\"}\n",
    "        \n",
    "    # 9. Save the results\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    value_dict_str = json.dumps(value_dict, indent=4)\n",
    "    final_output_content = (\n",
    "        f\"{final_code_str}\\n\\n\"\n",
    "        f'# --- EXECUTION TRACE (value_dict) ---\\n'\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example End-to-End Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full normalization pipeline for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "# This single function now handles the entire process.\n",
    "generate_final_normalized_code(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "# Verify the saved output\n",
    "saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "if saved_file.exists():\n",
    "    print(f\"\\n--- Final Normalized File Saved ---\")\n",
    "    print(f\"File created at: {saved_file}\")\n",
    "    print(\"\\n--- File Content ---\")\n",
    "    print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b67fd8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline (v2) for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solve(\n",
      "    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\n",
      "    first_batch_size: int = 100, # Out of the first 100 balls\n",
      "    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\n",
      "    second_batch_size: int = 75, # Of the next 75 tennis balls\n",
      "    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\n",
      "):\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "    answer = total_not_hit # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "\"\"\"\n",
      "{\n",
      "    \"initial_balls\": 175,\n",
      "    \"first_batch_size\": 100,\n",
      "    \"fraction_hit_first_batch\": 0.4,\n",
      "    \"second_batch_size\": 75,\n",
      "    \"fraction_hit_second_batch\": 0.3333333333333333,\n",
      "    \"fraction_not_hit_first_batch\": 0.6,\n",
      "    \"not_hit_first_batch\": 60.0,\n",
      "    \"fraction_not_hit_second_batch\": 0.6666666666666667,\n",
      "    \"not_hit_second_batch\": 50.00000000000001,\n",
      "    \"total_not_hit\": 110.0,\n",
      "    \"answer\": 110.0\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import libcst as cst\n",
    "from libcst.tool import dump\n",
    "\n",
    "# Assumes previous cells have defined:\n",
    "# - PROJECT_ROOT\n",
    "# - BASE_OUTPUT_DIR\n",
    "# - get_code_lines_dict\n",
    "# - normalize_code_lines\n",
    "\n",
    "FINAL_NORMALIZED_DIR = PROJECT_ROOT / \"data\" / \"code_gen_outputs_final\"\n",
    "\n",
    "# --- ConstantFolder Transformer (now for libcst) ---\n",
    "\n",
    "class CSTConstantFolder(cst.CSTTransformer):\n",
    "    \"\"\"\n",
    "    A libcst transformer that finds and evaluates binary operations on constants.\n",
    "    \"\"\"\n",
    "    def leave_BinaryOperation(\n",
    "        self, original_node: cst.BinaryOperation, updated_node: cst.BinaryOperation\n",
    "    ) -> cst.BaseExpression:\n",
    "        # Check if both left and right are simple numbers (Integer or Float)\n",
    "        if isinstance(updated_node.left, (cst.Integer, cst.Float)) and \\\n",
    "           isinstance(updated_node.right, (cst.Integer, cst.Float)):\n",
    "            \n",
    "            # Use Python's eval on the node's code for safety and simplicity\n",
    "            try:\n",
    "                result = eval(updated_node.code)\n",
    "                if isinstance(result, int):\n",
    "                    return cst.Integer(value=str(result))\n",
    "                elif isinstance(result, float):\n",
    "                    return cst.Float(value=str(result))\n",
    "            except Exception:\n",
    "                return updated_node # Fallback on any eval error\n",
    "        return updated_node\n",
    "\n",
    "# --- Main Orchestrator Function (v2) ---\n",
    "\n",
    "def generate_final_normalized_code_v2(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full normalization pipeline using libcst for lossless\n",
    "    transformation, preserving all comments in the function signature.\n",
    "    \"\"\"\n",
    "    # 1. Get original source code\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines: return\n",
    "    source_code = \"\\n\".join(raw_lines.values())\n",
    "    \n",
    "    # 2. Get normalized step-wise dictionary\n",
    "    normalized_steps = normalize_code_lines(raw_lines)\n",
    "    if not normalized_steps: return\n",
    "\n",
    "    # 3. Parse the original code using libcst to preserve everything\n",
    "    try:\n",
    "        original_tree = cst.parse_module(source_code)\n",
    "    except cst.ParserSyntaxError:\n",
    "        return\n",
    "\n",
    "    # 4. Reconstruct the body from the normalized steps as a string\n",
    "    body_str_parts = []\n",
    "    sorted_steps = sorted([key for key in normalized_steps if key.startswith('L')])\n",
    "    for step in sorted_steps:\n",
    "        body_str_parts.extend(normalized_steps[step])\n",
    "    body_str_parts.extend(normalized_steps.get('answer_line', []))\n",
    "    body_str_parts.extend(normalized_steps.get('return_line', []))\n",
    "    \n",
    "    # Indent each line of the new body correctly\n",
    "    indented_body_str = \"\\n\".join([f\"    {line}\" for line in body_str_parts])\n",
    "\n",
    "    # 5. Parse the new body string into a block of CST nodes\n",
    "    try:\n",
    "        # We parse a whole dummy function to get a valid IndentedBlock\n",
    "        new_body_cst = cst.parse_statement(f\"def dummy():\\n{indented_body_str}\")\n",
    "        new_body_block = new_body_cst.body\n",
    "    except cst.ParserSyntaxError:\n",
    "        return\n",
    "\n",
    "    # 6. Apply AST transformations (Constant Folding) to the new body\n",
    "    folder = CSTConstantFolder()\n",
    "    transformed_body_block = new_body_block.visit(folder)\n",
    "    \n",
    "    # 7. Create a transformer to replace the old body with the new one\n",
    "    class BodyReplacer(cst.CSTTransformer):\n",
    "        def __init__(self, new_body):\n",
    "            self.new_body = new_body\n",
    "        def leave_FunctionDef(\n",
    "            self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef\n",
    "        ) -> cst.FunctionDef:\n",
    "            # If this is the 'solve' function, replace its body\n",
    "            if updated_node.name.value == \"solve\":\n",
    "                return updated_node.with_changes(body=self.new_body)\n",
    "            return updated_node\n",
    "\n",
    "    # 8. Apply the replacement\n",
    "    replacer = BodyReplacer(transformed_body_block)\n",
    "    final_tree = original_tree.visit(replacer)\n",
    "    final_code_str = final_tree.code\n",
    "    \n",
    "    # 9. Execute and get the value_dict (using the same trace method as before)\n",
    "    # The _instrument_and_trace function from the previous answer can be reused here\n",
    "    # as it operates on standard ast, which we can get from the final code string.\n",
    "    try:\n",
    "        final_ast_for_trace = ast.parse(final_code_str)\n",
    "        instrumented_tree, _ = _instrument_and_trace(final_ast_for_trace)\n",
    "        instrumented_code_str = ast.unparse(instrumented_tree)\n",
    "        \n",
    "        namespace = {}\n",
    "        exec(instrumented_code_str, namespace)\n",
    "        value_dict = namespace['solve']()\n",
    "    except Exception as e:\n",
    "        value_dict = {\"__error__\": f\"Execution failed: {e!r}\"}\n",
    "        \n",
    "    # 10. Save the results\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    value_dict_str = json.dumps(value_dict, indent=4)\n",
    "    final_output_content = (\n",
    "        f\"{final_code_str}\\n\\n\"\n",
    "        f'# --- EXECUTION TRACE (value_dict) ---\\n'\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example End-to-End Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "# We need the `_instrument_and_trace` function from the previous response.\n",
    "# Make sure it is defined in your notebook environment.\n",
    "# (I'm assuming it is for this example to be runnable)\n",
    "# For clarity, here it is again:\n",
    "def _instrument_and_trace(tree: ast.AST) -> tuple[ast.AST, str]:\n",
    "    trace_dict_name = \"__execution_trace__\"\n",
    "    class AssignmentTracer(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            trace_init_expr = ast.parse(f\"{trace_dict_name} = {{}}\").body[0]\n",
    "            arg_names = [arg.arg for arg in node.args.args]\n",
    "            arg_trace_expr = ast.parse(\n",
    "                f\"{trace_dict_name}.update({{name: value for name, value in locals().items() if name in {arg_names}}})\"\n",
    "            ).body[0]\n",
    "            new_body = [trace_init_expr, arg_trace_expr]\n",
    "            for stmt in node.body:\n",
    "                new_body.append(stmt)\n",
    "                if isinstance(stmt, ast.Assign):\n",
    "                    var_name = stmt.targets[0].id\n",
    "                    log_expr = ast.parse(f\"{trace_dict_name}['{var_name}'] = {var_name}\").body[0]\n",
    "                    new_body.append(log_expr)\n",
    "            return_trace_expr = ast.parse(f\"return {trace_dict_name}\").body[0]\n",
    "            final_body = [s for s in new_body if not isinstance(s, ast.Return)]\n",
    "            final_body.append(return_trace_expr)\n",
    "            node.body = final_body\n",
    "            return node\n",
    "    tracer = AssignmentTracer()\n",
    "    instrumented_tree = tracer.visit(tree)\n",
    "    return instrumented_tree, trace_dict_name\n",
    "\n",
    "print(f\"--- Running full normalization pipeline (v2) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "generate_final_normalized_code_v2(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "if saved_file.exists():\n",
    "    print(f\"\\n--- Final Normalized File Saved ---\")\n",
    "    print(f\"File created at: {saved_file}\")\n",
    "    print(\"\\n--- File Content ---\")\n",
    "    print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e998400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline (v3) for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solveinitial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\n",
      "    first_batch_size: int = 100, # Out of the first 100 balls\n",
      "    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\n",
      "    second_batch_size: int = 75, # Of the next 75 tennis balls\n",
      "    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\n",
      ":\n",
      "    \"\"\"Index: 25.\n",
      "Returns: the total number of tennis balls Ralph did not hit.\"\"\"\n",
      "    #: L1\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "\n",
      "    #: L2\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "\n",
      "    #: L3\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "\n",
      "    answer = total_not_hit # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "\"\"\"\n",
      "{\n",
      "    \"initial_balls\": 175,\n",
      "    \"first_batch_size\": 100,\n",
      "    \"fraction_hit_first_batch\": 0.4,\n",
      "    \"second_batch_size\": 75,\n",
      "    \"fraction_hit_second_batch\": 0.3333333333333333,\n",
      "    \"fraction_not_hit_first_batch\": 0.6,\n",
      "    \"not_hit_first_batch\": 60.0,\n",
      "    \"fraction_not_hit_second_batch\": 0.6666666666666667,\n",
      "    \"not_hit_second_batch\": 50.00000000000001,\n",
      "    \"total_not_hit\": 110.0,\n",
      "    \"answer\": 110.0\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import libcst as cst\n",
    "from libcst.tool import dump\n",
    "\n",
    "# Ensure all previously defined functions and constants are available in the notebook:\n",
    "# - PROJECT_ROOT, BASE_OUTPUT_DIR, FINAL_NORMALIZED_DIR\n",
    "# - get_code_lines_dict, normalize_code_lines\n",
    "# - The CSTConstantFolder class and the _instrument_and_trace function\n",
    "\n",
    "def generate_final_normalized_code_v3(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full normalization pipeline. (Version 3)\n",
    "    \n",
    "    This version correctly reconstructs the final output file to include\n",
    "    the #: L<n> step marker comments for readability.\n",
    "    \"\"\"\n",
    "    # Steps 1-2: Get raw lines and normalize into step dictionary\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines: return\n",
    "    source_code = \"\\n\".join(raw_lines.values())\n",
    "    \n",
    "    normalized_steps = normalize_code_lines(raw_lines)\n",
    "    if not normalized_steps: return\n",
    "\n",
    "    # Step 3: Parse original code with libcst to get the original function node\n",
    "    try:\n",
    "        original_tree = cst.parse_module(source_code)\n",
    "        original_func_def = next(\n",
    "            (n for n in original_tree.body if isinstance(n, cst.FunctionDef)), None\n",
    "        )\n",
    "        if not original_func_def: return\n",
    "    except cst.ParserSyntaxError:\n",
    "        return\n",
    "\n",
    "    # --- THE LOGIC UP TO THIS POINT IS FOR ANALYSIS AND BODY RECONSTRUCTION ---\n",
    "    # --- WE WILL NOW BUILD TWO THINGS:\n",
    "    # --- 1. A functional but \"flat\" code string for execution tracing.\n",
    "    # --- 2. A formatted, readable code string for saving.\n",
    "\n",
    "    # 4. Reconstruct the FLAT body string for functional processing\n",
    "    flat_body_parts = []\n",
    "    sorted_steps = sorted([key for key in normalized_steps if key.startswith('L')])\n",
    "    for step in sorted_steps:\n",
    "        flat_body_parts.extend(normalized_steps[step])\n",
    "    flat_body_parts.extend(normalized_steps.get('answer_line', []))\n",
    "    flat_body_parts.extend(normalized_steps.get('return_line', []))\n",
    "    flat_body_str = \"\\n\".join(flat_body_parts)\n",
    "\n",
    "    # 5. Create the functional (but unformatted) AST for tracing\n",
    "    try:\n",
    "        # This process is unchanged and produces a correct AST for execution\n",
    "        body_tree = ast.parse(f\"def dummy():\\n    {flat_body_str.replace('\\n', '\\n    ')}\")\n",
    "        body_nodes = body_tree.body[0].body\n",
    "        temp_func_def = ast.parse(source_code).body[0]\n",
    "        temp_func_def.body = body_nodes\n",
    "        final_functional_code_str = ast.unparse(temp_func_def)\n",
    "    except (SyntaxError, IndexError):\n",
    "        return\n",
    "\n",
    "    # 6. Execute trace on the functional version\n",
    "    try:\n",
    "        instrumented_tree, _ = _instrument_and_trace(ast.parse(final_functional_code_str))\n",
    "        instrumented_code_str = ast.unparse(instrumented_tree)\n",
    "        namespace = {}\n",
    "        exec(instrumented_code_str, namespace)\n",
    "        value_dict = namespace['solve']()\n",
    "    except Exception as e:\n",
    "        value_dict = {\"__error__\": f\"Execution failed: {e!r}\"}\n",
    "        \n",
    "    # --- NEW LOGIC FOR FORMATTED OUTPUT ---\n",
    "    \n",
    "    # 7. Reconstruct the FORMATTED code string for saving\n",
    "    # Get the original signature string using libcst's unparser\n",
    "    signature_str = original_tree.code_for_node(original_func_def.params)\n",
    "    signature_docstring_str = f\"def solve{signature_str}:\\n\"\n",
    "    if original_func_def.get_docstring():\n",
    "        signature_docstring_str += f'    \"\"\"{original_func_def.get_docstring()}\"\"\"\\n'\n",
    "\n",
    "    formatted_body_parts = []\n",
    "    for step in sorted_steps:\n",
    "        # Add the step marker comment\n",
    "        formatted_body_parts.append(f\"    #: {step}\")\n",
    "        # Add the code lines for that step\n",
    "        for code_line in normalized_steps[step]:\n",
    "            formatted_body_parts.append(f\"    {code_line}\")\n",
    "        formatted_body_parts.append(\"\") # Add a blank line for readability\n",
    "\n",
    "    # Add the final answer and return lines\n",
    "    if 'answer_line' in normalized_steps:\n",
    "        formatted_body_parts.append(f\"    {normalized_steps['answer_line'][0]}\")\n",
    "    if 'return_line' in normalized_steps:\n",
    "         formatted_body_parts.append(f\"    {normalized_steps['return_line'][0]}\")\n",
    "\n",
    "    formatted_code_str = signature_docstring_str + \"\\n\".join(formatted_body_parts)\n",
    "\n",
    "    # 8. Save the formatted code and the trace\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    value_dict_str = json.dumps(value_dict, indent=4)\n",
    "    final_output_content = (\n",
    "        f\"{formatted_code_str}\\n\\n\"\n",
    "        f'# --- EXECUTION TRACE (value_dict) ---\\n'\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Make sure _instrument_and_trace is defined from the previous cell\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full normalization pipeline (v3) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "generate_final_normalized_code_v3(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "if saved_file.exists():\n",
    "    print(f\"\\n--- Final Normalized File Saved ---\")\n",
    "    print(f\"File created at: {saved_file}\")\n",
    "    print(\"\\n--- File Content ---\")\n",
    "    print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c58f611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline (v3) for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solve(initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\n",
      "    first_batch_size: int = 100, # Out of the first 100 balls\n",
      "    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\n",
      "    second_batch_size: int = 75, # Of the next 75 tennis balls\n",
      "    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\n",
      "):\n",
      "    \"\"\"Index: 25.\n",
      "Returns: the total number of tennis balls Ralph did not hit.\"\"\"\n",
      "    #: L1\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "\n",
      "    #: L2\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "\n",
      "    #: L3\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "\n",
      "    answer = total_not_hit # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "\"\"\"\n",
      "{\n",
      "    \"initial_balls\": \"175/1\",\n",
      "    \"first_batch_size\": \"100/1\",\n",
      "    \"fraction_hit_first_batch\": \"2/5\",\n",
      "    \"second_batch_size\": \"75/1\",\n",
      "    \"fraction_hit_second_batch\": \"1/3\",\n",
      "    \"fraction_not_hit_first_batch\": \"3/5\",\n",
      "    \"not_hit_first_batch\": \"60/1\",\n",
      "    \"fraction_not_hit_second_batch\": \"2/3\",\n",
      "    \"not_hit_second_batch\": \"50/1\",\n",
      "    \"total_not_hit\": \"110/1\",\n",
      "    \"answer\": \"110/1\"\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import libcst as cst\n",
    "from fractions import Fraction # Import the Fraction type\n",
    "\n",
    "# Assumes previous cells have defined:\n",
    "# - PROJECT_ROOT, BASE_OUTPUT_DIR, FINAL_NORMALIZED_DIR\n",
    "# - get_code_lines_dict, normalize_code_lines\n",
    "# - The _instrument_and_trace function\n",
    "\n",
    "# --- NEW: Custom JSON Encoder for Fractions ---\n",
    "class FractionEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Fraction):\n",
    "            # Represent Fraction(2, 5) as the string \"2/5\"\n",
    "            return f\"{obj.numerator}/{obj.denominator}\"\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# --- NEW: AST Transformer to convert numbers to Fractions ---\n",
    "class NumberToFractionTransformer(ast.NodeTransformer):\n",
    "    \"\"\"\n",
    "    Transforms integer and float constants into Fraction objects in an AST.\n",
    "    Example: `2` becomes `Fraction(2, 1)`, `0.5` becomes `Fraction(1, 2)`.\n",
    "    \"\"\"\n",
    "    def visit_Constant(self, node):\n",
    "        if isinstance(node.value, int):\n",
    "            return ast.Call(\n",
    "                func=ast.Name(id='Fraction', ctx=ast.Load()),\n",
    "                args=[ast.Constant(value=node.value), ast.Constant(value=1)],\n",
    "                keywords=[]\n",
    "            )\n",
    "        if isinstance(node.value, float):\n",
    "            # Convert float to fraction to preserve precision\n",
    "            frac = Fraction(node.value).limit_denominator()\n",
    "            return ast.Call(\n",
    "                func=ast.Name(id='Fraction', ctx=ast.Load()),\n",
    "                args=[ast.Constant(value=frac.numerator), ast.Constant(value=frac.denominator)],\n",
    "                keywords=[]\n",
    "            )\n",
    "        return node\n",
    "\n",
    "# --- Final Orchestrator Function (v4) ---\n",
    "def generate_final_normalized_code_v4(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full normalization pipeline. (Version 4)\n",
    "    - Fixes function signature reconstruction.\n",
    "    - Uses the `fractions` module to preserve exact precision.\n",
    "    \"\"\"\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines: return\n",
    "    source_code = \"\\n\".join(raw_lines.values())\n",
    "    normalized_steps = normalize_code_lines(raw_lines)\n",
    "    if not normalized_steps: return\n",
    "\n",
    "    try:\n",
    "        original_tree = cst.parse_module(source_code)\n",
    "        original_func_def = next((n for n in original_tree.body if isinstance(n, cst.FunctionDef)), None)\n",
    "        if not original_func_def: return\n",
    "    except cst.ParserSyntaxError:\n",
    "        return\n",
    "\n",
    "    # --- 1. Reconstruct the signature CORRECTLY ---\n",
    "    params_str = original_tree.code_for_node(original_func_def.params)\n",
    "    signature_header = f\"def solve({params_str}):\"\n",
    "    docstring = original_func_def.get_docstring()\n",
    "    docstring_str = f'\\n    \"\"\"{docstring}\"\"\"' if docstring else \"\"\n",
    "\n",
    "    # --- 2. Reconstruct the FORMATTED body with step markers for saving ---\n",
    "    formatted_body_parts = []\n",
    "    sorted_steps = sorted([key for key in normalized_steps if key.startswith('L')])\n",
    "    for step in sorted_steps:\n",
    "        formatted_body_parts.append(f\"    #: {step}\")\n",
    "        for code_line in normalized_steps[step]:\n",
    "            formatted_body_parts.append(f\"    {code_line}\")\n",
    "        formatted_body_parts.append(\"\")\n",
    "    if 'answer_line' in normalized_steps:\n",
    "        formatted_body_parts.append(f\"    {normalized_steps['answer_line'][0]}\")\n",
    "    if 'return_line' in normalized_steps:\n",
    "         formatted_body_parts.append(f\"    {normalized_steps['return_line'][0]}\")\n",
    "    formatted_code_str = f\"{signature_header}{docstring_str}\\n\" + \"\\n\".join(formatted_body_parts)\n",
    "\n",
    "    # --- 3. Reconstruct the FLAT body for functional processing ---\n",
    "    flat_body_parts = []\n",
    "    for step in sorted_steps:\n",
    "        flat_body_parts.extend(normalized_steps[step])\n",
    "    flat_body_parts.extend(normalized_steps.get('answer_line', []))\n",
    "    flat_body_parts.extend(normalized_steps.get('return_line', []))\n",
    "    flat_body_str = \"\\n\".join(flat_body_parts)\n",
    "\n",
    "    # --- 4. Create the AST to be instrumented, now with Fractions ---\n",
    "    try:\n",
    "        # Create a full functional code string to parse\n",
    "        functional_code_str = f\"{signature_header}{docstring_str}\\n\" + \"\\n\".join([f\"    {line}\" for line in flat_body_parts])\n",
    "        # Add the Fraction import\n",
    "        functional_code_str = \"from fractions import Fraction\\n\" + functional_code_str\n",
    "        \n",
    "        tree_for_instrument = ast.parse(functional_code_str)\n",
    "        \n",
    "        # Transform all numbers to Fraction objects\n",
    "        transformer = NumberToFractionTransformer()\n",
    "        tree_for_instrument = transformer.visit(tree_for_instrument)\n",
    "        \n",
    "        # Instrument the fraction-aware tree to get the execution trace\n",
    "        instrumented_tree, _ = _instrument_and_trace(tree_for_instrument)\n",
    "        instrumented_code_str = ast.unparse(instrumented_tree)\n",
    "        \n",
    "        namespace = {}\n",
    "        exec(instrumented_code_str, namespace)\n",
    "        value_dict = namespace['solve']()\n",
    "    except Exception as e:\n",
    "        value_dict = {\"__error__\": f\"Execution failed: {e!r}\"}\n",
    "        \n",
    "    # --- 5. Save the formatted code and the trace ---\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Use the custom encoder to serialize the value_dict\n",
    "    value_dict_str = json.dumps(value_dict, indent=4, cls=FractionEncoder)\n",
    "    final_output_content = (\n",
    "        f\"{formatted_code_str}\\n\\n\"\n",
    "        f'# --- EXECUTION TRACE (value_dict) ---\\n'\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Make sure _instrument_and_trace is defined from the previous cell\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full normalization pipeline (v3) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "generate_final_normalized_code_v4(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "if saved_file.exists():\n",
    "    print(f\"\\n--- Final Normalized File Saved ---\")\n",
    "    print(f\"File created at: {saved_file}\")\n",
    "    print(\"\\n--- File Content ---\")\n",
    "    print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "456866a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full normalization pipeline (v3) for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Normalized File Saved ---\n",
      "File created at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_final/25/google_gemini-2.0-flash-thinking-exp.py\n",
      "\n",
      "--- File Content ---\n",
      "def solve(initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\n",
      "    first_batch_size: int = 100, # Out of the first 100 balls\n",
      "    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\n",
      "    second_batch_size: int = 75, # Of the next 75 tennis balls\n",
      "    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\n",
      "):\n",
      "    \"\"\"Index: 25.\n",
      "Returns: the total number of tennis balls Ralph did not hit.\"\"\"\n",
      "    #: L1\n",
      "    fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\n",
      "    not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\n",
      "\n",
      "    #: L2\n",
      "    fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\n",
      "    not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\n",
      "\n",
      "    #: L3\n",
      "    total_not_hit = not_hit_first_batch + not_hit_second_batch\n",
      "\n",
      "    answer = total_not_hit # FINAL ANSWER\n",
      "    return answer\n",
      "\n",
      "# --- EXECUTION TRACE (value_dict) ---\n",
      "\"\"\"\n",
      "{\n",
      "    \"initial_balls\": \"175/1\",\n",
      "    \"first_batch_size\": \"100/1\",\n",
      "    \"fraction_hit_first_batch\": \"2/5\",\n",
      "    \"second_batch_size\": \"75/1\",\n",
      "    \"fraction_hit_second_batch\": \"1/3\",\n",
      "    \"fraction_not_hit_first_batch\": \"3/5\",\n",
      "    \"not_hit_first_batch\": \"60/1\",\n",
      "    \"fraction_not_hit_second_batch\": \"2/3\",\n",
      "    \"not_hit_second_batch\": \"50/1\",\n",
      "    \"total_not_hit\": \"110/1\",\n",
      "    \"answer\": \"110/1\"\n",
      "}\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from fractions import Fraction\n",
    "\n",
    "# (Assuming other setup code like PROJECT_ROOT, get_code_lines_dict, etc., is present)\n",
    "\n",
    "# --- NumberToFractionTransformer (Unchanged) ---\n",
    "class NumberToFractionTransformer(ast.NodeTransformer):\n",
    "    def visit_Constant(self, node):\n",
    "        if isinstance(node.value, int):\n",
    "            return ast.Call(func=ast.Name(id='Fraction', ctx=ast.Load()), args=[ast.Constant(value=node.value), ast.Constant(value=1)], keywords=[])\n",
    "        if isinstance(node.value, float):\n",
    "            frac = Fraction(node.value).limit_denominator()\n",
    "            return ast.Call(func=ast.Name(id='Fraction', ctx=ast.Load()), args=[ast.Constant(value=frac.numerator), ast.Constant(value=frac.denominator)], keywords=[])\n",
    "        return node\n",
    "\n",
    "# --- NEW: Helper function for manual serialization ---\n",
    "def _convert_fractions_to_strings(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Recursively iterates through a dictionary and converts all Fraction\n",
    "    objects into their string representation 'numerator/denominator'.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, Fraction):\n",
    "            processed_dict[key] = f\"{value.numerator}/{value.denominator}\"\n",
    "        elif isinstance(value, dict): # For potential future nesting\n",
    "            processed_dict[key] = _convert_fractions_to_strings(value)\n",
    "        else:\n",
    "            processed_dict[key] = value\n",
    "    return processed_dict\n",
    "\n",
    "# --- Final Orchestrator Function (v5 - with manual serialization) ---\n",
    "def generate_final_normalized_code_v5(\n",
    "    problem_index: int,\n",
    "    model_name: str,\n",
    "    base_output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    output_dir: Path = FINAL_NORMALIZED_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full normalization pipeline. (Version 5)\n",
    "    - Uses manual pre-processing of the value_dict to serialize Fractions\n",
    "      without a custom JSONEncoder.\n",
    "    \"\"\"\n",
    "    # (Steps 1-3: Reading files, normalizing steps, parsing CST... are the same)\n",
    "    # ... [Code from v4 for steps 1-3] ...\n",
    "    # This is a simplified placeholder for the existing logic.\n",
    "    raw_lines = get_code_lines_dict(problem_index, model_name)\n",
    "    if not raw_lines: return\n",
    "    source_code = \"\\n\".join(raw_lines.values())\n",
    "    normalized_steps = normalize_code_lines(raw_lines)\n",
    "    original_tree = cst.parse_module(source_code)\n",
    "    original_func_def = next((n for n in original_tree.body if isinstance(n, cst.FunctionDef)), None)\n",
    "    if not original_func_def: return\n",
    "    \n",
    "    params_str = original_tree.code_for_node(original_func_def.params)\n",
    "    signature_header = f\"def solve({params_str}):\"\n",
    "    docstring = original_func_def.get_docstring()\n",
    "    docstring_str = f'\\n    \"\"\"{docstring}\"\"\"' if docstring else \"\"\n",
    "\n",
    "    # (Step 4: Create functional code string... is the same)\n",
    "    # ... [Code from v4 for step 4] ...\n",
    "    flat_body_parts = []\n",
    "    sorted_steps = sorted([key for key in normalized_steps if key.startswith('L')])\n",
    "    for step in sorted_steps: flat_body_parts.extend(normalized_steps[step])\n",
    "    flat_body_parts.extend(normalized_steps.get('answer_line', []))\n",
    "    flat_body_parts.extend(normalized_steps.get('return_line', []))\n",
    "    functional_code_str = f\"from fractions import Fraction\\n{signature_header}{docstring_str}\\n\" + \"\\n\".join([f\"    {line}\" for line in flat_body_parts])\n",
    "\n",
    "    # (Step 5: Instrument and trace... is the same)\n",
    "    # ... [Code from v4 for step 5, leading to `value_dict` with Fraction objects] ...\n",
    "    try:\n",
    "        tree_for_instrument = ast.parse(functional_code_str)\n",
    "        transformer = NumberToFractionTransformer()\n",
    "        tree_for_instrument = transformer.visit(tree_for_instrument)\n",
    "        instrumented_tree, _ = _instrument_and_trace(tree_for_instrument)\n",
    "        instrumented_code_str = ast.unparse(instrumented_tree)\n",
    "        namespace = {}\n",
    "        exec(instrumented_code_str, namespace)\n",
    "        value_dict = namespace['solve']()\n",
    "    except Exception as e:\n",
    "        value_dict = {\"__error__\": f\"Execution failed: {e!r}\"}\n",
    "        \n",
    "    # --- 6. NEW: Manually convert Fractions to strings before JSON dump ---\n",
    "    serializable_value_dict = _convert_fractions_to_strings(value_dict)\n",
    "    value_dict_str = json.dumps(serializable_value_dict, indent=4)\n",
    "\n",
    "    # --- 7. Reconstruct formatted code for saving (Unchanged) ---\n",
    "    formatted_body_parts = []\n",
    "    for step in sorted_steps:\n",
    "        formatted_body_parts.append(f\"    #: {step}\")\n",
    "        for code_line in normalized_steps[step]: formatted_body_parts.append(f\"    {code_line}\")\n",
    "        formatted_body_parts.append(\"\")\n",
    "    if 'answer_line' in normalized_steps: formatted_body_parts.append(f\"    {normalized_steps['answer_line'][0]}\")\n",
    "    if 'return_line' in normalized_steps: formatted_body_parts.append(f\"    {normalized_steps['return_line'][0]}\")\n",
    "    formatted_code_str = f\"{signature_header}{docstring_str}\\n\" + \"\\n\".join(formatted_body_parts)\n",
    "\n",
    "    # --- 8. Save the final output (Unchanged) ---\n",
    "    output_path = output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final_output_content = (\n",
    "        f\"{formatted_code_str}\\n\\n\"\n",
    "        f'# --- EXECUTION TRACE (value_dict) ---\\n'\n",
    "        f'\"\"\"\\n{value_dict_str}\\n\"\"\"\\n'\n",
    "    )\n",
    "    output_path.write_text(final_output_content, encoding=\"utf-8\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Make sure _instrument_and_trace is defined from the previous cell\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full normalization pipeline (v3) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "generate_final_normalized_code_v5(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "saved_file = FINAL_NORMALIZED_DIR / str(TARGET_INDEX) / f\"{TARGET_MODEL}.py\"\n",
    "if saved_file.exists():\n",
    "    print(f\"\\n--- Final Normalized File Saved ---\")\n",
    "    print(f\"File created at: {saved_file}\")\n",
    "    print(\"\\n--- File Content ---\")\n",
    "    print(saved_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d6ccd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1/3'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Fraction(1,3)\n",
    "str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95b2f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca6c8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full TEXT-BASED normalization for Problem 52, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    num_boxes: int = 10, # ten boxes of pencils\\n    pencils_kept: int = 10, # He kept ten pencils\\n    num_friends: int = 5, # shared the remaining pencils equally with his five friends\\n    pencils_per_friend: int = 8 # his friends got eight pencils each\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 52.\\n    Returns: the number of pencils in each box.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"pencils_shared = num_friends * pencils_per_friend\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"total_pencils = pencils_kept + pencils_shared\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"pencils_in_each_box = total_pencils / num_boxes\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = pencils_in_each_box # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    num_boxes: int = 10, # ten boxes of pencils\\n    pencils_kept: int = 10, # He kept ten pencils\\n    num_friends: int = 5, # shared the remaining pencils equally with his five friends\\n    pencils_per_friend: int = 8 # his friends got eight pencils each\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 52.\\n    Returns: the number of pencils in each box.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"pencils_in_each_box = (pencils_kept + (num_friends * pencils_per_friend)) / num_boxes\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = pencils_in_each_box # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "# 0. Utility Function to Get Code Lines\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def get_code_lines_dict(problem_index: int, model_name: str, base_output_dir: Path = BASE_OUTPUT_DIR) -> dict[int, str] | None:\n",
    "    \"\"\"\n",
    "    Returns a dict mapping line numbers (0-based) to the verbatim lines of code\n",
    "    for the given problem index and model name.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    problem_dir = base_output_dir / str(problem_index)\n",
    "    file_path = problem_dir / f\"{model_name}.py\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"[Error] File not found: {file_path}\", file=sys.stderr)\n",
    "        return None\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return {i: line.rstrip(\"\\n\") for i, line in enumerate(lines)}\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. Modified Function to Extract All Parts of the Code\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. Corrected Function to Extract All Parts of the Code\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def extract_code_parts_v2(raw_lines_dict: dict[int, str]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Normalizes raw code lines into a canonical dictionary. (Version 2)\n",
    "    \n",
    "    This version correctly handles multi-line docstrings.\n",
    "    \"\"\"\n",
    "    normalized_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(raw_lines_dict.items())\n",
    "    \n",
    "    # State machine variables\n",
    "    state = \"SEARCHING\"  # SEARCHING, SIGNATURE, BODY\n",
    "    signature_lines = []\n",
    "    body_lines_dict = {}\n",
    "    \n",
    "    # --- First pass: Separate signature from body ---\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        stripped_line = line_content.strip()\n",
    "        \n",
    "        if state == \"SEARCHING\" and stripped_line.startswith(\"def solve(\"):\n",
    "            state = \"SIGNATURE\"\n",
    "        \n",
    "        if state == \"SIGNATURE\":\n",
    "            signature_lines.append(line_content)\n",
    "            if stripped_line.endswith(\"):\"):\n",
    "                state = \"BODY\"\n",
    "                # The rest of the lines belong to the body\n",
    "                body_lines_dict = dict(sorted_lines[i+1:])\n",
    "                break\n",
    "    \n",
    "    normalized_dict['function_signature'] = [\"\\n\".join(signature_lines)]\n",
    "    \n",
    "    # --- Second pass: Separate docstring from the rest of the body ---\n",
    "    docstring_lines = []\n",
    "    rest_of_body_dict = {}\n",
    "    \n",
    "    body_lines_sorted = sorted(body_lines_dict.items())\n",
    "    in_docstring = False\n",
    "    \n",
    "    for i, (line_num, line_content) in enumerate(body_lines_sorted):\n",
    "        stripped_line = line_content.strip()\n",
    "        \n",
    "        if not in_docstring and ('\"\"\"' in stripped_line or \"'''\" in stripped_line):\n",
    "            in_docstring = True\n",
    "            docstring_lines.append(line_content)\n",
    "            if (stripped_line.count('\"\"\"') == 2 or stripped_line.count(\"'''\") == 2) and len(stripped_line) > 3:\n",
    "                # Handle single-line docstring\n",
    "                in_docstring = False\n",
    "                # The rest of the body starts from the next line\n",
    "                rest_of_body_dict = dict(body_lines_sorted[i+1:])\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        if in_docstring:\n",
    "            docstring_lines.append(line_content)\n",
    "            if '\"\"\"' in stripped_line or \"'''\" in stripped_line:\n",
    "                in_docstring = False\n",
    "                # The rest of the body starts from the next line\n",
    "                rest_of_body_dict = dict(body_lines_sorted[i+1:])\n",
    "                break\n",
    "    \n",
    "    if docstring_lines:\n",
    "        normalized_dict['docstring'] = [\"\\n\".join(docstring_lines).strip()]\n",
    "    else:\n",
    "        # No docstring found, the whole body is code\n",
    "        rest_of_body_dict = body_lines_dict\n",
    "\n",
    "    # --- Third pass: Process the remaining body lines ---\n",
    "    body_dict = normalize_code_lines(rest_of_body_dict)\n",
    "    normalized_dict.update(body_dict)\n",
    "            \n",
    "    return dict(normalized_dict)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  2. New Function to Collapse Intermediate Variables\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def collapse_intermediate_vars(normalized_dict: dict[str, list[str]]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Collapses intermediate variables in a normalized code dictionary.\n",
    "    \n",
    "    This uses a simple string search-and-replace logic. It iterates through\n",
    "    the steps and if the variable defined in step L(n) is used in step L(n+1),\n",
    "    it substitutes the expression from L(n) into L(n+1) and removes L(n).\n",
    "    \n",
    "    Args:\n",
    "        normalized_dict: A dictionary from the `extract_code_parts` function.\n",
    "\n",
    "    Returns:\n",
    "        A new dictionary with intermediate steps collapsed.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid modifying the original\n",
    "    collapsed_dict = {k: v[:] for k, v in normalized_dict.items()}\n",
    "    \n",
    "    # Get a sorted list of the logical steps to process in order\n",
    "    step_keys = sorted([key for key in collapsed_dict if key.startswith('L')])\n",
    "    \n",
    "    # A set to keep track of steps that have been collapsed and should be removed\n",
    "    steps_to_remove = set()\n",
    "\n",
    "    for i in range(len(step_keys) - 1):\n",
    "        current_step_key = step_keys[i]\n",
    "        next_step_key = step_keys[i+1]\n",
    "        \n",
    "        # This simple logic assumes one line of code per step after normalization.\n",
    "        # This holds true for the vast majority of our normalized outputs.\n",
    "        if len(collapsed_dict.get(current_step_key, [])) == 1 and \\\n",
    "           len(collapsed_dict.get(next_step_key, [])) == 1:\n",
    "            \n",
    "            current_line = collapsed_dict[current_step_key][0]\n",
    "            next_line = collapsed_dict[next_step_key][0]\n",
    "\n",
    "            # Ensure both lines are assignments\n",
    "            if '=' not in current_line or '=' not in next_line:\n",
    "                continue\n",
    "\n",
    "            # Get the variable (LHS) defined in the current step\n",
    "            current_lhs = current_line.split('=')[0].strip()\n",
    "            # Get the expression (RHS) of the current step\n",
    "            current_rhs = current_line.split('=', 1)[1].strip()\n",
    "\n",
    "            next_line_rhs = next_line.split('=', 1)[1]\n",
    "\n",
    "            # Check if the variable is used in the next step's expression\n",
    "            # Use word boundaries (\\b) to avoid matching substrings (e.g., 'x' in 'x_y')\n",
    "            if re.search(r'\\b' + re.escape(current_lhs) + r'\\b', next_line_rhs):\n",
    "                # Perform the substitution, enclosing the expression in parentheses\n",
    "                substituted_rhs = re.sub(\n",
    "                    r'\\b' + re.escape(current_lhs) + r'\\b',\n",
    "                    f\"({current_rhs})\",\n",
    "                    next_line_rhs\n",
    "                )\n",
    "                \n",
    "                # Update the next line in the dictionary\n",
    "                next_line_lhs = next_line.split('=')[0].strip()\n",
    "                collapsed_dict[next_step_key][0] = f\"{next_line_lhs} = {substituted_rhs.strip()}\"\n",
    "                \n",
    "                # Mark the current step for removal\n",
    "                steps_to_remove.add(current_step_key)\n",
    "\n",
    "    # Remove the collapsed steps from the final dictionary\n",
    "    for step in steps_to_remove:\n",
    "        del collapsed_dict[step]\n",
    "        \n",
    "    return collapsed_dict\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "# Use a model where steps can be collapsed\n",
    "TARGET_INDEX = 52\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Running full TEXT-BASED normalization for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "# 1. Get raw lines\n",
    "raw_lines = get_code_lines_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "if raw_lines:\n",
    "    # 2. Extract parts (Signature, Docstring, Body)\n",
    "    extracted_parts = extract_code_parts_v2(raw_lines)\n",
    "    print(\"\\n--- Step 1: Extracted Code Parts ---\")\n",
    "    print(json.dumps(extracted_parts, indent=4))\n",
    "    \n",
    "    # 3. Collapse intermediate variables\n",
    "    collapsed_code = collapse_intermediate_vars(extracted_parts)\n",
    "    print(\"\\n--- Step 2: Code After Collapsing Intermediate Variables ---\")\n",
    "    print(json.dumps(collapsed_code, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9df3410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_signature': ['def solve(\\n    num_boxes: int = 10, # ten boxes of pencils\\n    pencils_kept: int = 10, # He kept ten pencils\\n    num_friends: int = 5, # shared the remaining pencils equally with his five friends\\n    pencils_per_friend: int = 8 # his friends got eight pencils each\\n):'],\n",
       " 'docstring': ['\"\"\"Index: 52.\\n    Returns: the number of pencils in each box.\\n    \"\"\"'],\n",
       " 'L3': ['pencils_in_each_box = (pencils_kept + (num_friends * pencils_per_friend)) / num_boxes'],\n",
       " 'answer_line': ['answer = pencils_in_each_box # FINAL ANSWER'],\n",
       " 'return_line': ['return answer']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(collapsed_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ed48efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running full TEXT-BASED normalization for Problem 10, Model 'anthropic_claude-3-5-haiku-20241022' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_consumed: int = 847,  # Over three hundred years, it has consumed 847 people\\n    num_periods: int = 3  # Monster rises once every hundred years over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the first ship the monster consumed.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_ship_multiplier = 1 + 2 + 4  # S + 2S + 4S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_ship_multiplier\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_consumed: int = 847,  # Over three hundred years, it has consumed 847 people\\n    num_periods: int = 3  # Monster rises once every hundred years over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the first ship the monster consumed.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / (1 + 2 + 4  # S + 2S + 4S)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "--- Running full TEXT-BASED normalization for Problem 10, Model 'openai_gpt-4.1-mini' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people: int = 847,  # it has consumed 847 people over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_ships_factor = 7  # S + 2S + 4S = 7S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"first_ship_people = total_people / total_ships_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = first_ship_people  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people: int = 847,  # it has consumed 847 people over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"first_ship_people = total_people / (7  # S + 2S + 4S = 7S)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = first_ship_people  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "--- Running full TEXT-BASED normalization for Problem 10, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    every_hundred_years: int = 100, # rises from the waters once every hundred years\\n    over_three_hundred_years: int = 300, # Over three hundred years\\n    total_people_consumed: int = 847, # it has consumed 847 people\\n    twice_as_many: int = 2 # each new ship has twice as many people as the last ship\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"multiplier_third_ship = twice_as_many * twice_as_many\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_factor = 1 + twice_as_many + multiplier_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / total_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    every_hundred_years: int = 100, # rises from the waters once every hundred years\\n    over_three_hundred_years: int = 300, # Over three hundred years\\n    total_people_consumed: int = 847, # it has consumed 847 people\\n    twice_as_many: int = 2 # each new ship has twice as many people as the last ship\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / (1 + twice_as_many + (twice_as_many * twice_as_many))\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "--- Running full TEXT-BASED normalization for Problem 10, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_eaten: int = 847, # Over three hundred years, it has consumed 847 people.\\n    years_between_feasts: int = 100, # once every hundred years\\n    total_years: int = 300 # Over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_proportions = 1 + 2 + 4\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_eaten / total_proportions\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_eaten: int = 847, # Over three hundred years, it has consumed 847 people.\\n    years_between_feasts: int = 100, # once every hundred years\\n    total_years: int = 300 # Over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_eaten / (1 + 2 + 4)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "--- Running full TEXT-BASED normalization for Problem 10, Model 'google_gemini-2.5-flash' ---\n",
      "\n",
      "--- Step 1: Extracted Code Parts ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n        interval_years: int = 100, # once every hundred years\\n        total_period_years: int = 300, # Over three hundred years\\n        total_people_consumed: int = 847, # consumed 847 people\\n        growth_factor: int = 2 # twice as many people as the last ship\\n    ):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"coefficient_first_ship = 1\",\n",
      "        \"coefficient_second_ship = growth_factor\",\n",
      "        \"coefficient_third_ship = growth_factor * growth_factor\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_coefficient_s\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Step 2: Code After Collapsing Intermediate Variables ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n        interval_years: int = 100, # once every hundred years\\n        total_period_years: int = 300, # Over three hundred years\\n        total_people_consumed: int = 847, # consumed 847 people\\n        growth_factor: int = 2 # twice as many people as the last ship\\n    ):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"coefficient_first_ship = 1\",\n",
      "        \"coefficient_second_ship = growth_factor\",\n",
      "        \"coefficient_third_ship = growth_factor * growth_factor\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / (coefficient_first_ship + coefficient_second_ship + coefficient_third_ship)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "# 0. Utility Function to Get Code Lines\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def get_code_lines_dict(problem_index: int, model_name: str, base_output_dir: Path = BASE_OUTPUT_DIR) -> dict[int, str] | None:\n",
    "    \"\"\"\n",
    "    Returns a dict mapping line numbers (0-based) to the verbatim lines of code\n",
    "    for the given problem index and model name.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    problem_dir = base_output_dir / str(problem_index)\n",
    "    file_path = problem_dir / f\"{model_name}.py\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"[Error] File not found: {file_path}\", file=sys.stderr)\n",
    "        return None\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return {i: line.rstrip(\"\\n\") for i, line in enumerate(lines)}\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. Modified Function to Extract All Parts of the Code\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  1. Corrected Function to Extract All Parts of the Code\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def extract_code_parts_v2(raw_lines_dict: dict[int, str]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Normalizes raw code lines into a canonical dictionary. (Version 2)\n",
    "    \n",
    "    This version correctly handles multi-line docstrings.\n",
    "    \"\"\"\n",
    "    normalized_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(raw_lines_dict.items())\n",
    "    \n",
    "    # State machine variables\n",
    "    state = \"SEARCHING\"  # SEARCHING, SIGNATURE, BODY\n",
    "    signature_lines = []\n",
    "    body_lines_dict = {}\n",
    "    \n",
    "    # --- First pass: Separate signature from body ---\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        stripped_line = line_content.strip()\n",
    "        \n",
    "        if state == \"SEARCHING\" and stripped_line.startswith(\"def solve(\"):\n",
    "            state = \"SIGNATURE\"\n",
    "        \n",
    "        if state == \"SIGNATURE\":\n",
    "            signature_lines.append(line_content)\n",
    "            if stripped_line.endswith(\"):\"):\n",
    "                state = \"BODY\"\n",
    "                # The rest of the lines belong to the body\n",
    "                body_lines_dict = dict(sorted_lines[i+1:])\n",
    "                break\n",
    "    \n",
    "    normalized_dict['function_signature'] = [\"\\n\".join(signature_lines)]\n",
    "    \n",
    "    # --- Second pass: Separate docstring from the rest of the body ---\n",
    "    docstring_lines = []\n",
    "    rest_of_body_dict = {}\n",
    "    \n",
    "    body_lines_sorted = sorted(body_lines_dict.items())\n",
    "    in_docstring = False\n",
    "    \n",
    "    for i, (line_num, line_content) in enumerate(body_lines_sorted):\n",
    "        stripped_line = line_content.strip()\n",
    "        \n",
    "        if not in_docstring and ('\"\"\"' in stripped_line or \"'''\" in stripped_line):\n",
    "            in_docstring = True\n",
    "            docstring_lines.append(line_content)\n",
    "            if (stripped_line.count('\"\"\"') == 2 or stripped_line.count(\"'''\") == 2) and len(stripped_line) > 3:\n",
    "                # Handle single-line docstring\n",
    "                in_docstring = False\n",
    "                # The rest of the body starts from the next line\n",
    "                rest_of_body_dict = dict(body_lines_sorted[i+1:])\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        if in_docstring:\n",
    "            docstring_lines.append(line_content)\n",
    "            if '\"\"\"' in stripped_line or \"'''\" in stripped_line:\n",
    "                in_docstring = False\n",
    "                # The rest of the body starts from the next line\n",
    "                rest_of_body_dict = dict(body_lines_sorted[i+1:])\n",
    "                break\n",
    "    \n",
    "    if docstring_lines:\n",
    "        normalized_dict['docstring'] = [\"\\n\".join(docstring_lines).strip()]\n",
    "    else:\n",
    "        # No docstring found, the whole body is code\n",
    "        rest_of_body_dict = body_lines_dict\n",
    "\n",
    "    # --- Third pass: Process the remaining body lines ---\n",
    "    body_dict = normalize_code_lines(rest_of_body_dict)\n",
    "    normalized_dict.update(body_dict)\n",
    "            \n",
    "    return dict(normalized_dict)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  2. New Function to Collapse Intermediate Variables\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "def collapse_intermediate_vars(normalized_dict: dict[str, list[str]]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Collapses intermediate variables in a normalized code dictionary.\n",
    "    \n",
    "    This uses a simple string search-and-replace logic. It iterates through\n",
    "    the steps and if the variable defined in step L(n) is used in step L(n+1),\n",
    "    it substitutes the expression from L(n) into L(n+1) and removes L(n).\n",
    "    \n",
    "    Args:\n",
    "        normalized_dict: A dictionary from the `extract_code_parts` function.\n",
    "\n",
    "    Returns:\n",
    "        A new dictionary with intermediate steps collapsed.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid modifying the original\n",
    "    collapsed_dict = {k: v[:] for k, v in normalized_dict.items()}\n",
    "    \n",
    "    # Get a sorted list of the logical steps to process in order\n",
    "    step_keys = sorted([key for key in collapsed_dict if key.startswith('L')])\n",
    "    \n",
    "    # A set to keep track of steps that have been collapsed and should be removed\n",
    "    steps_to_remove = set()\n",
    "\n",
    "    for i in range(len(step_keys) - 1):\n",
    "        current_step_key = step_keys[i]\n",
    "        next_step_key = step_keys[i+1]\n",
    "        \n",
    "        # This simple logic assumes one line of code per step after normalization.\n",
    "        # This holds true for the vast majority of our normalized outputs.\n",
    "        if len(collapsed_dict.get(current_step_key, [])) == 1 and \\\n",
    "           len(collapsed_dict.get(next_step_key, [])) == 1:\n",
    "            \n",
    "            current_line = collapsed_dict[current_step_key][0]\n",
    "            next_line = collapsed_dict[next_step_key][0]\n",
    "\n",
    "            # Ensure both lines are assignments\n",
    "            if '=' not in current_line or '=' not in next_line:\n",
    "                continue\n",
    "\n",
    "            # Get the variable (LHS) defined in the current step\n",
    "            current_lhs = current_line.split('=')[0].strip()\n",
    "            # Get the expression (RHS) of the current step\n",
    "            current_rhs = current_line.split('=', 1)[1].strip()\n",
    "\n",
    "            next_line_rhs = next_line.split('=', 1)[1]\n",
    "\n",
    "            # Check if the variable is used in the next step's expression\n",
    "            # Use word boundaries (\\b) to avoid matching substrings (e.g., 'x' in 'x_y')\n",
    "            if re.search(r'\\b' + re.escape(current_lhs) + r'\\b', next_line_rhs):\n",
    "                # Perform the substitution, enclosing the expression in parentheses\n",
    "                substituted_rhs = re.sub(\n",
    "                    r'\\b' + re.escape(current_lhs) + r'\\b',\n",
    "                    f\"({current_rhs})\",\n",
    "                    next_line_rhs\n",
    "                )\n",
    "                \n",
    "                # Update the next line in the dictionary\n",
    "                next_line_lhs = next_line.split('=')[0].strip()\n",
    "                collapsed_dict[next_step_key][0] = f\"{next_line_lhs} = {substituted_rhs.strip()}\"\n",
    "                \n",
    "                # Mark the current step for removal\n",
    "                steps_to_remove.add(current_step_key)\n",
    "\n",
    "    # Remove the collapsed steps from the final dictionary\n",
    "    for step in steps_to_remove:\n",
    "        del collapsed_dict[step]\n",
    "        \n",
    "    return collapsed_dict\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "TARGET_INDEX = 10\n",
    "for model in MODELS:\n",
    "    TARGET_MODEL = model\n",
    "    \n",
    "    print(f\"--- Running full TEXT-BASED normalization for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "    # 1. Get raw lines\n",
    "    raw_lines = get_code_lines_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "    if raw_lines:\n",
    "        # 2. Extract parts (Signature, Docstring, Body)\n",
    "        extracted_parts = extract_code_parts_v2(raw_lines)\n",
    "        print(\"\\n--- Step 1: Extracted Code Parts ---\")\n",
    "        print(json.dumps(extracted_parts, indent=4))\n",
    "        \n",
    "        # 3. Collapse intermediate variables\n",
    "        collapsed_code = collapse_intermediate_vars(extracted_parts)\n",
    "        print(\"\\n--- Step 2: Code After Collapsing Intermediate Variables ---\")\n",
    "        print(json.dumps(collapsed_code, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e244d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 10, Model 'anthropic_claude-3-5-haiku-20241022' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_consumed: int = 847,  # Over three hundred years, it has consumed 847 people\\n    num_periods: int = 3  # Monster rises once every hundred years over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the first ship the monster consumed.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / (1 + 2 + 4)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 10, Model 'openai_gpt-4.1-mini' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people: int = 847,  # it has consumed 847 people over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"first_ship_people = total_people / (7)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = first_ship_people  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 10, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    every_hundred_years: int = 100, # rises from the waters once every hundred years\\n    over_three_hundred_years: int = 300, # Over three hundred years\\n    total_people_consumed: int = 847, # it has consumed 847 people\\n    twice_as_many: int = 2 # each new ship has twice as many people as the last ship\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / (1 + twice_as_many + (twice_as_many * twice_as_many))\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 10, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_people_eaten: int = 847, # Over three hundred years, it has consumed 847 people.\\n    years_between_feasts: int = 100, # once every hundred years\\n    total_years: int = 300 # Over three hundred years\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_eaten / (1 + 2 + 4)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 10, Model 'google_gemini-2.5-flash' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n        interval_years: int = 100, # once every hundred years\\n        total_period_years: int = 300, # Over three hundred years\\n        total_people_consumed: int = 847, # consumed 847 people\\n        growth_factor: int = 2 # twice as many people as the last ship\\n    ):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / ((1) + (growth_factor) + (growth_factor * growth_factor))\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# This function is now more robust and will replace the previous version.\n",
    "def collapse_intermediate_vars_v2(normalized_dict: dict[str, list[str]]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Collapses intermediate variables in a normalized code dictionary. (Version 2)\n",
    "    - Handles chained substitutions by running iteratively.\n",
    "    - Handles multi-line steps.\n",
    "    - Strips comments before substitution to prevent syntax errors.\n",
    "    \"\"\"\n",
    "    collapsed_dict = {k: v[:] for k, v in normalized_dict.items()}\n",
    "    \n",
    "    # Use a loop to handle chained substitutions (e.g., L3 -> L4 -> L5)\n",
    "    # The loop continues as long as a substitution was made in the previous pass.\n",
    "    while True:\n",
    "        substitution_made_in_pass = False\n",
    "        steps_to_remove = set()\n",
    "        \n",
    "        # Get a sorted list of the logical steps to process in order\n",
    "        step_keys = sorted([key for key in collapsed_dict if key.startswith('L')])\n",
    "\n",
    "        for i in range(len(step_keys) - 1):\n",
    "            current_step_key = step_keys[i]\n",
    "            next_step_key = step_keys[i+1]\n",
    "\n",
    "            # Get all variables defined in the current step (the LHS of all assignments)\n",
    "            # This now handles multi-line steps correctly.\n",
    "            definitions_in_current_step = {}\n",
    "            for line in collapsed_dict.get(current_step_key, []):\n",
    "                if '=' in line:\n",
    "                    lhs, rhs = line.split('=', 1)\n",
    "                    # --- FIX 1: Strip comments from RHS before storing ---\n",
    "                    rhs_no_comment = rhs.split('#')[0].strip()\n",
    "                    definitions_in_current_step[lhs.strip()] = rhs_no_comment\n",
    "\n",
    "            if not definitions_in_current_step:\n",
    "                continue\n",
    "\n",
    "            # Check for substitutions in the next step\n",
    "            next_step_lines = collapsed_dict.get(next_step_key, [])\n",
    "            updated_next_step_lines = []\n",
    "            made_change_in_next_step = False\n",
    "\n",
    "            for next_line in next_step_lines:\n",
    "                original_next_line = next_line\n",
    "                for var_name, expression in definitions_in_current_step.items():\n",
    "                    # Use word boundaries (\\b) to avoid replacing 'var' in 'other_var'\n",
    "                    pattern = r'\\b' + re.escape(var_name) + r'\\b'\n",
    "                    if re.search(pattern, next_line):\n",
    "                        # Perform substitution\n",
    "                        next_line = re.sub(pattern, f\"({expression})\", next_line)\n",
    "                        made_change_in_next_step = True\n",
    "                \n",
    "                updated_next_step_lines.append(next_line)\n",
    "\n",
    "            if made_change_in_next_step:\n",
    "                # Update the dictionary with the new lines and mark step for removal\n",
    "                collapsed_dict[next_step_key] = updated_next_step_lines\n",
    "                steps_to_remove.add(current_step_key)\n",
    "                substitution_made_in_pass = True\n",
    "        \n",
    "        # Remove the collapsed steps after the pass is complete\n",
    "        for step in steps_to_remove:\n",
    "            if step in collapsed_dict:\n",
    "                del collapsed_dict[step]\n",
    "        \n",
    "        # If a full pass made no substitutions, we are done.\n",
    "        if not substitution_made_in_pass:\n",
    "            break\n",
    "            \n",
    "    return collapsed_dict\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Updated Example Usage (to be run in a new cell)\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Re-run the test for problem 10 using the NEW v2 collapse function.\n",
    "# This requires the `extract_code_parts_v2` function from the previous response.\n",
    "# Make sure it is defined in your notebook environment.\n",
    "\n",
    "TARGET_INDEX = 10\n",
    "for model in MODELS:\n",
    "    TARGET_MODEL = model\n",
    "    \n",
    "    print(f\"\\n--- Running full TEXT-BASED normalization (v2 collapse) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "    raw_lines = get_code_lines_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "    if raw_lines:\n",
    "        extracted_parts = extract_code_parts_v2(raw_lines)\n",
    "        \n",
    "        # Use the new collapse function\n",
    "        collapsed_code = collapse_intermediate_vars_v2(extracted_parts)\n",
    "        \n",
    "        print(\"\\n--- Final Collapsed Code ---\")\n",
    "        print(json.dumps(collapsed_code, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "477cb1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 25, Model 'anthropic_claude-3-5-haiku-20241022' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_tennis_balls: int = 175,  # Ralph loads up the machine with 175 tennis balls\\n    first_batch_balls: int = 100,  # Out of the first 100 balls\\n    first_batch_hit_fraction: float = 2/5,  # he manages to hit 2/5 of them\\n    next_batch_balls: int = 75,  # Of the next 75 tennis balls\\n    next_batch_hit_fraction: float = 1/3  # he manages to hit 1/3 of them\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_missed_balls = ((1 - first_batch_hit_fraction) * first_batch_balls) + ((1 - next_batch_hit_fraction) * next_batch_balls)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_missed_balls  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 25, Model 'openai_gpt-4.1-mini' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    total_balls: int = 175,  # Ralph loads up the machine with 175 tennis balls\\n    first_batch: int = 100,  # Out of the first 100 balls\\n    hit_fraction_first: float = 2/5,  # he manages to hit 2/5 of them\\n    second_batch: int = 75,  # Of the next 75 tennis balls\\n    hit_fraction_second: float = 1/3  # he manages to hit 1/3 of them\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = ((1 - hit_fraction_first) * first_batch) + ((1 - hit_fraction_second) * second_batch)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\\n    first_batch_size: int = 100, # Out of the first 100 balls\\n    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_size: int = 75, # Of the next 75 tennis balls\\n    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = (fraction_not_hit_first_batch * first_batch_size) + (fraction_not_hit_second_batch * second_batch_size)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 25, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    initial_balls: int = 175, # Ralph loads up the machine with 175 tennis balls to start with\\n    first_batch_count: int = 100, # Out of the first 100 balls\\n    first_batch_hit_fraction: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_count: int = 75, # Of the next 75 tennis balls\\n    second_batch_hit_fraction: float = 1/3 # he manages to hit 1/3 of them\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = (first_batch_not_hit_fraction * first_batch_count) + (second_batch_not_hit_fraction * second_batch_count)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Running full TEXT-BASED normalization (v2 collapse) for Problem 25, Model 'google_gemini-2.5-flash' ---\n",
      "\n",
      "--- Final Collapsed Code ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n        initial_load: int = 175, # He loads up the machine with 175 tennis balls to start with\\n        first_set_balls: int = 100, # Out of the first 100 balls\\n        fraction_hit_first_set: float = 2/5, # he manages to hit 2/5 of them\\n        second_set_balls: int = 75, # Of the next 75 tennis balls\\n        fraction_hit_second_set: float = 1/3 # he manages to hit 1/3 of them\\n    ):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_balls_not_hit = (fraction_not_hit_first_set * first_set_balls) + (fraction_not_hit_second_set * second_set_balls)\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_balls_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "TARGET_INDEX = 25\n",
    "for model in MODELS:\n",
    "    TARGET_MODEL = model\n",
    "    \n",
    "    print(f\"\\n--- Running full TEXT-BASED normalization (v2 collapse) for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "    raw_lines = get_code_lines_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "    if raw_lines:\n",
    "        extracted_parts = extract_code_parts_v2(raw_lines)\n",
    "        \n",
    "        # Use the new collapse function\n",
    "        collapsed_code = collapse_intermediate_vars_v2(extracted_parts)\n",
    "        \n",
    "        print(\"\\n--- Final Collapsed Code ---\")\n",
    "        print(json.dumps(collapsed_code, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5df855fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Normalized Dictionary for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "{\n",
      "    \"function_signature\": [\n",
      "        \"def solve(\\n    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\\n    first_batch_size: int = 100, # Out of the first 100 balls\\n    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_size: int = 75, # Of the next 75 tennis balls\\n    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\\n):\"\n",
      "    ],\n",
      "    \"docstring\": [\n",
      "        \"\\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\",\n",
      "        \"not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\",\n",
      "        \"not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = not_hit_first_batch + not_hit_second_batch\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# This block should be self-contained and assumes only BASE_OUTPUT_DIR is defined.\n",
    "# If running in a new notebook, you'll need to define PROJECT_ROOT and BASE_OUTPUT_DIR.\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Module 1: Raw Code -> Normalized Dictionary\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "def get_code_lines_dict(problem_index: int, model_name: str, base_output_dir: Path) -> dict[int, str] | None:\n",
    "    \"\"\"\n",
    "    Returns a dict mapping line numbers (0-based) to the verbatim lines of code\n",
    "    for the given problem index and model name.\n",
    "    \"\"\"\n",
    "    file_path = base_output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    if not file_path.exists():\n",
    "        # Using return None for file not found, no need to print error here.\n",
    "        return None\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return {i: line.rstrip(\"\\n\") for i, line in enumerate(lines)}\n",
    "\n",
    "def _normalize_body_lines(body_lines_dict: dict[int, str]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to process only the body of a function into a\n",
    "    step-wise dictionary.\n",
    "    \"\"\"\n",
    "    normalized_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(body_lines_dict.items())\n",
    "    last_seen_step = None\n",
    "\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        line = line_content.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        step_match = _TRACE_RE.match(line)\n",
    "        if step_match:\n",
    "            last_seen_step = f\"L{step_match.group(1)}\"\n",
    "            continue\n",
    "\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        code_part = line\n",
    "        trailing_match = _TRACE_RE.search(line)\n",
    "        if trailing_match:\n",
    "            last_seen_step = f\"L{trailing_match.group(1)}\"\n",
    "            code_part = line[:trailing_match.start()].strip()\n",
    "        \n",
    "        if 'answer =' in line:\n",
    "            normalized_dict['answer_line'].append(code_part)\n",
    "        elif line.startswith('return'):\n",
    "            normalized_dict['return_line'].append(code_part)\n",
    "        elif last_seen_step:\n",
    "            normalized_dict[last_seen_step].append(code_part)\n",
    "            \n",
    "    return dict(normalized_dict)\n",
    "\n",
    "def get_normalized_code_dict(\n",
    "    problem_index: int, model_name: str, base_output_dir: Path = BASE_OUTPUT_DIR\n",
    ") -> dict[str, list[str]] | None:\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the normalized code dictionary.\n",
    "    \n",
    "    This function reads a raw model output file and produces a dictionary\n",
    "    containing the signature, docstring, and a step-wise breakdown of the\n",
    "    function body.\n",
    "    \"\"\"\n",
    "    raw_lines_dict = get_code_lines_dict(problem_index, model_name, base_output_dir)\n",
    "    if not raw_lines_dict:\n",
    "        return None\n",
    "\n",
    "    final_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(raw_lines_dict.items())\n",
    "    \n",
    "    # --- State machine variables ---\n",
    "    state = \"SEARCHING\"\n",
    "    signature_lines = []\n",
    "    body_lines_dict = {}\n",
    "    \n",
    "    # --- 1. Separate signature from body ---\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        stripped_line = line_content.strip()\n",
    "        if state == \"SEARCHING\" and stripped_line.startswith(\"def solve(\"):\n",
    "            state = \"SIGNATURE\"\n",
    "        \n",
    "        if state == \"SIGNATURE\":\n",
    "            signature_lines.append(line_content)\n",
    "            if stripped_line.endswith(\"):\"):\n",
    "                state = \"BODY\"\n",
    "                body_lines_dict = dict(sorted_lines[i+1:])\n",
    "                break\n",
    "    \n",
    "    final_dict['function_signature'] = [\"\\n\".join(signature_lines)]\n",
    "    \n",
    "    # --- 2. Separate docstring from the rest of the body ---\n",
    "    docstring_lines = []\n",
    "    rest_of_body_dict = {}\n",
    "    body_lines_sorted = sorted(body_lines_dict.items())\n",
    "    in_docstring = False\n",
    "    \n",
    "    docstring_end_index = -1\n",
    "    for i, (line_num, line_content) in enumerate(body_lines_sorted):\n",
    "        stripped_line = line_content.strip()\n",
    "        \n",
    "        if not in_docstring and ('\"\"\"' in stripped_line or \"'''\" in stripped_line):\n",
    "            in_docstring = True\n",
    "        \n",
    "        if in_docstring:\n",
    "            docstring_lines.append(line_content)\n",
    "            # Check if this line closes the docstring\n",
    "            if stripped_line.count('\"\"\"') % 2 == 1 or stripped_line.count(\"'''\") % 2 == 1:\n",
    "                if len(docstring_lines) > 1 or stripped_line.count('\"\"\"') == 2 or stripped_line.count(\"'''\") == 2:\n",
    "                    in_docstring = False\n",
    "                    docstring_end_index = i\n",
    "                    break\n",
    "    \n",
    "    if docstring_lines:\n",
    "        final_dict['docstring'] = [\"\\n\".join(docstring_lines).strip()]\n",
    "        rest_of_body_dict = dict(body_lines_sorted[docstring_end_index+1:])\n",
    "    else:\n",
    "        rest_of_body_dict = body_lines_dict\n",
    "\n",
    "    # --- 3. Process the remaining code body ---\n",
    "    body_dict = _normalize_body_lines(rest_of_body_dict)\n",
    "    final_dict.update(body_dict)\n",
    "            \n",
    "    return dict(final_dict)\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Example Usage\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Assume MODELS list and BASE_OUTPUT_DIR are defined.\n",
    "TARGET_INDEX = 25\n",
    "TARGET_MODEL = \"google_gemini-2.0-flash-thinking-exp\"\n",
    "\n",
    "print(f\"--- Generating Normalized Dictionary for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "normalized_dict = get_normalized_code_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "if normalized_dict:\n",
    "    print(json.dumps(normalized_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79146900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# This block is self-contained.\n",
    "# Assumes BASE_OUTPUT_DIR and MODELS are defined in your environment.\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#  Module 1: Raw Code -> Simplified Normalized Dictionary\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Regex to find the solution step comment, e.g., \"#: L1\"\n",
    "_TRACE_RE = re.compile(r\"#:\\s*L(\\d+)\")\n",
    "\n",
    "def get_code_lines_dict(problem_index: int, model_name: str, base_output_dir: Path) -> dict[int, str] | None:\n",
    "    \"\"\"\n",
    "    Returns a dict mapping line numbers (0-based) to the verbatim lines of code\n",
    "    for the given problem index and model name.\n",
    "    \"\"\"\n",
    "    file_path = base_output_dir / str(problem_index) / f\"{model_name}.py\"\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return {i: line.rstrip(\"\\n\") for i, line in enumerate(lines)}\n",
    "\n",
    "def _normalize_body_lines(body_lines_dict: dict[int, str]) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Helper function to process only the body of a function into a\n",
    "    step-wise dictionary. (This function is reused from before).\n",
    "    \"\"\"\n",
    "    normalized_dict = defaultdict(list)\n",
    "    sorted_lines = sorted(body_lines_dict.items())\n",
    "    last_seen_step = None\n",
    "\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        line = line_content.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        step_match = _TRACE_RE.match(line)\n",
    "        if step_match:\n",
    "            last_seen_step = f\"L{step_match.group(1)}\"\n",
    "            continue\n",
    "\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        code_part = line\n",
    "        trailing_match = _TRACE_RE.search(line)\n",
    "        if trailing_match:\n",
    "            last_seen_step = f\"L{trailing_match.group(1)}\"\n",
    "            code_part = line[:trailing_match.start()].strip()\n",
    "        \n",
    "        if 'answer =' in line:\n",
    "            normalized_dict['answer_line'].append(code_part)\n",
    "        elif line.startswith('return'):\n",
    "            normalized_dict['return_line'].append(code_part)\n",
    "        elif last_seen_step:\n",
    "            normalized_dict[last_seen_step].append(code_part)\n",
    "            \n",
    "    return dict(normalized_dict)\n",
    "\n",
    "\n",
    "def get_simplified_code_dict(\n",
    "    problem_index: int, model_name: str, base_output_dir: Path = BASE_OUTPUT_DIR\n",
    ") -> dict[str, list[str]] | None:\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the normalized code dictionary using a\n",
    "    simplified header/body split.\n",
    "    \n",
    "    The output dict will contain:\n",
    "    - 'function_header': A single string with the signature and docstring.\n",
    "    - Step-wise breakdown of the body (L1, L2, answer_line, etc.).\n",
    "    \"\"\"\n",
    "    raw_lines_dict = get_code_lines_dict(problem_index, model_name, base_output_dir)\n",
    "    if not raw_lines_dict:\n",
    "        return None\n",
    "\n",
    "    sorted_lines = sorted(raw_lines_dict.items())\n",
    "    header_lines = []\n",
    "    split_index = -1\n",
    "\n",
    "    # Generate function signature, including docstring\n",
    "    for i, (line_num, line_content) in enumerate(sorted_lines):\n",
    "        header_lines.append(line_content)\n",
    "        if line_content.endswith('\"\"\"'): # Check for end of docstring\n",
    "            split_index = i\n",
    "            break\n",
    "\n",
    "    # --- Process the two parts ---\n",
    "    header_str = \"\\n\".join(header_lines).strip()\n",
    "    body_lines_dict = dict(sorted_lines[split_index + 1:])\n",
    "    body_dict = _normalize_body_lines(body_lines_dict)\n",
    "    \n",
    "    # --- Combine into the final dictionary ---\n",
    "    final_dict = {\"function_header\": [header_str]}\n",
    "    final_dict.update(body_dict)\n",
    "            \n",
    "    return final_dict\n",
    "\n",
    "for TARGET_INDEX in [10, 25]:\n",
    "    for TARGET_MODEL in MODELS:\n",
    "        print(f\"\\n--- Generating Simplified Dictionary for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "        simplified_dict = get_simplified_code_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "        if simplified_dict:\n",
    "            print(json.dumps(simplified_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e5dae30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Simplified Dictionary for Problem 10, Model 'anthropic_claude-3-5-haiku-20241022' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    total_people_consumed: int = 847,  # Over three hundred years, it has consumed 847 people\\n    num_periods: int = 3  # Monster rises once every hundred years over three hundred years\\n):\\n    \\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the first ship the monster consumed.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_ship_multiplier = 1 + 2 + 4  # S + 2S + 4S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_ship_multiplier\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 10, Model 'openai_gpt-4.1-mini' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    total_people: int = 847,  # it has consumed 847 people over three hundred years\\n):\\n    \\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_ships_factor = 7  # S + 2S + 4S = 7S\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"first_ship_people = total_people / total_ships_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = first_ship_people  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 10, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    every_hundred_years: int = 100, # rises from the waters once every hundred years\\n    over_three_hundred_years: int = 300, # Over three hundred years\\n    total_people_consumed: int = 847, # it has consumed 847 people\\n    twice_as_many: int = 2 # each new ship has twice as many people as the last ship\\n):\\n    \\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"multiplier_third_ship = twice_as_many * twice_as_many\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_factor = 1 + twice_as_many + multiplier_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_first_ship = total_people_consumed / total_factor\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 10, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    total_people_eaten: int = 847, # Over three hundred years, it has consumed 847 people.\\n    years_between_feasts: int = 100, # once every hundred years\\n    total_years: int = 300 # Over three hundred years\\n):\\n    \\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_proportions = 1 + 2 + 4\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_eaten / total_proportions\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 10, Model 'google_gemini-2.5-flash' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n        interval_years: int = 100, # once every hundred years\\n        total_period_years: int = 300, # Over three hundred years\\n        total_people_consumed: int = 847, # consumed 847 people\\n        growth_factor: int = 2 # twice as many people as the last ship\\n    ):\\n    \\\"\\\"\\\"Index: 10.\\n    Returns: the number of people on the ship the monster ate in the first hundred years.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"coefficient_first_ship = 1\",\n",
      "        \"coefficient_second_ship = growth_factor\",\n",
      "        \"coefficient_third_ship = growth_factor * growth_factor\"\n",
      "    ],\n",
      "    \"L4\": [\n",
      "        \"total_coefficient_s = coefficient_first_ship + coefficient_second_ship + coefficient_third_ship\"\n",
      "    ],\n",
      "    \"L5\": [\n",
      "        \"people_on_first_ship = total_people_consumed / total_coefficient_s\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = people_on_first_ship # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 25, Model 'anthropic_claude-3-5-haiku-20241022' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    total_tennis_balls: int = 175,  # Ralph loads up the machine with 175 tennis balls\\n    first_batch_balls: int = 100,  # Out of the first 100 balls\\n    first_batch_hit_fraction: float = 2/5,  # he manages to hit 2/5 of them\\n    next_batch_balls: int = 75,  # Of the next 75 tennis balls\\n    next_batch_hit_fraction: float = 1/3  # he manages to hit 1/3 of them\\n):\\n    \\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"first_batch_missed_balls = (1 - first_batch_hit_fraction) * first_batch_balls\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"next_batch_missed_balls = (1 - next_batch_hit_fraction) * next_batch_balls\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_missed_balls = first_batch_missed_balls + next_batch_missed_balls\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_missed_balls  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 25, Model 'openai_gpt-4.1-mini' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    total_balls: int = 175,  # Ralph loads up the machine with 175 tennis balls\\n    first_batch: int = 100,  # Out of the first 100 balls\\n    hit_fraction_first: float = 2/5,  # he manages to hit 2/5 of them\\n    second_batch: int = 75,  # Of the next 75 tennis balls\\n    hit_fraction_second: float = 1/3  # he manages to hit 1/3 of them\\n):\\n    \\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"not_hit_first = (1 - hit_fraction_first) * first_batch\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"not_hit_second = (1 - hit_fraction_second) * second_batch\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = not_hit_first + not_hit_second\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit  # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 25, Model 'google_gemini-2.0-flash-thinking-exp' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    initial_balls: int = 175, # He loads up the machine with 175 tennis balls to start with\\n    first_batch_size: int = 100, # Out of the first 100 balls\\n    fraction_hit_first_batch: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_size: int = 75, # Of the next 75 tennis balls\\n    fraction_hit_second_batch: float = 1/3 # he manages to hit 1/3 of them\\n):\\n    \\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"fraction_not_hit_first_batch = 1 - fraction_hit_first_batch\",\n",
      "        \"not_hit_first_batch = fraction_not_hit_first_batch * first_batch_size\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"fraction_not_hit_second_batch = 1 - fraction_hit_second_batch\",\n",
      "        \"not_hit_second_batch = fraction_not_hit_second_batch * second_batch_size\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = not_hit_first_batch + not_hit_second_batch\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 25, Model 'google_gemini-2.5-flash-lite-preview-06-17' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n    initial_balls: int = 175, # Ralph loads up the machine with 175 tennis balls to start with\\n    first_batch_count: int = 100, # Out of the first 100 balls\\n    first_batch_hit_fraction: float = 2/5, # he manages to hit 2/5 of them\\n    second_batch_count: int = 75, # Of the next 75 tennis balls\\n    second_batch_hit_fraction: float = 1/3 # he manages to hit 1/3 of them\\n):\\n    \\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"first_batch_not_hit_fraction = 1 - first_batch_hit_fraction\",\n",
      "        \"first_batch_not_hit_count = first_batch_not_hit_fraction * first_batch_count\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"second_batch_not_hit_fraction = 1 - second_batch_hit_fraction\",\n",
      "        \"second_batch_not_hit_count = second_batch_not_hit_fraction * second_batch_count\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_not_hit = first_batch_not_hit_count + second_batch_not_hit_count\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--- Generating Simplified Dictionary for Problem 25, Model 'google_gemini-2.5-flash' ---\n",
      "{\n",
      "    \"function_header\": [\n",
      "        \"def solve(\\n        initial_load: int = 175, # He loads up the machine with 175 tennis balls to start with\\n        first_set_balls: int = 100, # Out of the first 100 balls\\n        fraction_hit_first_set: float = 2/5, # he manages to hit 2/5 of them\\n        second_set_balls: int = 75, # Of the next 75 tennis balls\\n        fraction_hit_second_set: float = 1/3 # he manages to hit 1/3 of them\\n    ):\\n    \\\"\\\"\\\"Index: 25.\\n    Returns: the total number of tennis balls Ralph did not hit.\\n    \\\"\\\"\\\"\"\n",
      "    ],\n",
      "    \"L1\": [\n",
      "        \"fraction_not_hit_first_set = 1 - fraction_hit_first_set\",\n",
      "        \"balls_not_hit_first_set = fraction_not_hit_first_set * first_set_balls\"\n",
      "    ],\n",
      "    \"L2\": [\n",
      "        \"fraction_not_hit_second_set = 1 - fraction_hit_second_set\",\n",
      "        \"balls_not_hit_second_set = fraction_not_hit_second_set * second_set_balls\"\n",
      "    ],\n",
      "    \"L3\": [\n",
      "        \"total_balls_not_hit = balls_not_hit_first_set + balls_not_hit_second_set\"\n",
      "    ],\n",
      "    \"answer_line\": [\n",
      "        \"answer = total_balls_not_hit # FINAL ANSWER\"\n",
      "    ],\n",
      "    \"return_line\": [\n",
      "        \"return answer\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for TARGET_INDEX in [10, 25]:\n",
    "    for TARGET_MODEL in MODELS:\n",
    "        print(f\"\\n--- Generating Simplified Dictionary for Problem {TARGET_INDEX}, Model '{TARGET_MODEL}' ---\")\n",
    "\n",
    "        simplified_dict = get_simplified_code_dict(TARGET_INDEX, TARGET_MODEL)\n",
    "\n",
    "        if simplified_dict:\n",
    "            print(json.dumps(simplified_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f336be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
