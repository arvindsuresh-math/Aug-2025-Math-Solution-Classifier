{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c846c26",
   "metadata": {
    "id": "6c846c26"
   },
   "outputs": [],
   "source": [
    "# Based on june-23-final.ipynb\n",
    "# Handles async processing of Gemini models\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Qaq6ikMhrUOi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qaq6ikMhrUOi",
    "outputId": "3d107374-60bd-483f-a11f-c6306c799fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.55.0)\n",
      "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install anthropic dotenv\n",
    "#!pip install --upgrade datasets fsspec huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7PSP-8ZothzW",
   "metadata": {
    "id": "7PSP-8ZothzW"
   },
   "outputs": [],
   "source": [
    "# REFACTOR: Standard imports + asyncio and nest_asyncio for Jupyter compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import openai\n",
    "from google import genai\n",
    "import anthropic\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI # REFACTOR: Import async client\n",
    "from anthropic import AsyncClient # REFACTOR: Import async client\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm # REFACTOR: Use notebook-friendly tqdm\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio # Alias to avoid name conflict with tqdm.notebook if used elsewhere\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "import textwrap, hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FZlK30RHtlF2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZlK30RHtlF2",
    "outputId": "76364f44-612f-44d6-afa7-dd7dd1dd95c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root identified: content\n"
     ]
    }
   ],
   "source": [
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "# REFACTOR: Apply nest_asyncio to allow asyncio to run in a Jupyter notebook.\n",
    "# This must be done once per kernel.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- 1. Client and Model Configuration ---\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# REFACTOR: Initialize asynchronous clients.\n",
    "# The synchronous clients are no longer needed for the generation script.\n",
    "openai_client_async = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "anthropic_client_async = AsyncClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Initialize the global Gemini client (used for cache operations)\n",
    "# It will pick up the API key configured via genai.configure()\n",
    "gemini_client: Optional[genai.Client] = None\n",
    "# This will store a dictionary of cached_prompt_ids, keyed by model name\n",
    "gemini_model_caches: Dict[str, Optional[str]] = {}\n",
    "# The models used for caching will now be taken directly from the model_dict\n",
    "\n",
    "# Define the project root as a global constant\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"Project root identified: {PROJECT_ROOT}\")\n",
    "BASE_OUTPUT_DIR = PROJECT_ROOT / 'data' / 'code_gen_outputs_raw'\n",
    "\n",
    "# A safe starting point is slightly below the documented RPM.\n",
    "API_CONCURRENCY_LIMITS = {\n",
    "    \"google\":    2,   # 2 parallel calls ≈ 30 RPM  < 10 RPM × 3 models\n",
    "    \"anthropic\": 2,   # 2 × 3 500 tokens × 60/4 ≈ 105 k TPM → still too high\n",
    "    \"openai\":    2,   # unchanged\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "-3ljwabOtmA0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3ljwabOtmA0",
    "outputId": "66843095-959d-46bb-8a4c-6b4d1f0e733a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Guidelines\n",
      "\n",
      "0. **Output wrapping**\n",
      "   Return the code inside a single ```python … ``` block, and nothing else.\n",
      "\n",
      "1.  **Function Naming & Docstring:** The function must be named `solve`. It must begin with a docstring that has exactly two lines:\n",
      "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
      "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
      "\n",
      "2.  **Function Arguments:** The function arguments must be derived from the 'Question' text.\n",
      "    *   Create a distinct argument for every numerical value that is directly stated in the text.\n",
      "    *   The arguments should be created **in the same order in which they appear in the question**.\n",
      "    *   **Note:** Some of these arguments may end up not being used in the function body. This is expected. Do not worry about this and leave the unused arguments in the function signature.\n",
      "\n",
      "3.  **Argument Formatting:** Each argument must include a type-hint (e.g., `int`, `float`) and a default value equal to its value in the 'Question'. You must also add a comment (`#`) next to each argument that quotes or refers to the phrase in the 'Question' it comes from.\n",
      "\n",
      "4.  **Function Body:** The body of the function should follow the logic of the provided 'Solution' dict, which contains the step-by-step solution to the problem. The keys of this dict are strings (e.g. `\"L1\"`, `\"L2\"`) which refer to the line number, and the values of the dict are the corresponding steps in the solution.\n",
      "    * For every relevant line in the 'Solution', you must include a comment in the Python code that indicates the line number (key) from the 'Solution' dict.\n",
      "    * These comments should be formatted as `#: L<n>`, where `<n>` is the line number from the 'Solution' dict.\n",
      "    * Immediately follow the comment with the Python statement that performs the calculation.\n",
      "    * Steps in the solution should result in the creation of new, intermediate variables, which should be named descriptively based on the context of the calculation.\n",
      "    * Wherever possible, in your code try to use only the variables from the function arguments and the intermediate variables you created before, and try to avoid using hard-coded numbers in the calculations.\n",
      "\n",
      "5.  **Calculator Annotations:** Pay close attention to the calculator annotations (e.g., `[[25*8=200]]`) in the 'Solution' as they reveal the precise mathematical operations to implement. **Note**: Some lines in the solution may not contain calculator annotations, but you should still pay attention to the logic and calculations described in those lines.\n",
      "\n",
      "6.  **Final Answer:** Store the final answer in a variable named 'answer', and on the same line, add the comment `# FINAL ANSWER`. In the next line, return the 'answer' variable.\n",
      "\n",
      "7. **No extra output:** Your output should end with the ``` closing the code block. Do not include any additional text, explanations, or comments outside of the code block.\n",
      "\n",
      "--- EXAMPLES ---\n",
      "\n",
      "*Index*:\n",
      "310\n",
      "\n",
      "*Question*:\n",
      "Janet hires six employees. Four of them are warehouse workers who make $15/hour, and the other two are managers who make $20/hour. Janet has to pay 10% of her workers' salaries in FICA taxes. If everyone works 25 days a month and 8 hours a day, how much does Janet owe total for their wages and taxes for one month?\n",
      "\n",
      "*Solution*:\n",
      "{\"L1\": \"First figure out how many hours each worker works per month by multiplying the number of days they work by the number of hours a day they work: 25 days * 8 hours/day = [[25*8=200]]200 hours\", \"L2\": \"Then calculate how much one warehouse worker makes per month by multiplying their hourly rate by the number of hours they work: 200 hours * $15/hour = $[[200*15=3000]]3000\", \"L3\": \"Then multiply that number by 4 to find out how much all the warehouse workers make: $3000/worker * 4 workers = $[[3000*4=12000]]12,000\", \"L4\": \"Now multiply the hours each manager works (also 200) by their hourly wage to find out how much one manager makes per month: 200 hours * $20/hour = $[[200*20=4000]]4,000\", \"L5\": \"Now multiply one manager's wages by the number of managers (2) to find their total wage amount: $4,000/manager * 2 managers = $[[4000*2=8000]]8,000\", \"L6\": \"Now add the wages for the managers and the workers to find the total cost of the wages: $8,000 + $12,000 = $[[8000+12000=20000]]20,000\", \"L7\": \"Now multiply the total wage bill by 10% to find how much the FICA taxes are: $20,000 * .1 = $[[20000*.1=2000]]2,000\", \"L8\": \"Now add the total wage bill to the total tax amount to find the grand total: $2,000 + $20,000 = $[[2000+20000=22000]]22,000\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        num_employees: int = 6, # Janet hires six employees\n",
      "        num_warehouse_workers: int = 4, # Four of them are warehouse workers\n",
      "        num_managers: int = 2, # the other two are managers\n",
      "        hourly_wage_warehouse: int = 15, # warehouse workers make $15/hour\n",
      "        hourly_wage_manager: int = 20, # managers make $20/hour\n",
      "        fica_tax_rate: float = 0.1, # FICA tax rate is 10%\n",
      "        days_per_month: int = 25, # everyone works 25 days a month\n",
      "        hours_per_day: int = 8 # everyone works 8 hours a day\n",
      "    ):\n",
      "    \"\"\"Index: 310.\n",
      "    Returns: the monthly total wage bill, including FICA taxes.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    hours_per_month = days_per_month * hours_per_day\n",
      "\n",
      "    #: L2\n",
      "    monthly_wage_warehouse = hourly_wage_warehouse * hours_per_month\n",
      "\n",
      "    #: L3\n",
      "    total_wage_warehouse = monthly_wage_warehouse * num_warehouse_workers\n",
      "\n",
      "    #: L4\n",
      "    monthly_wage_manager = hourly_wage_manager * hours_per_month\n",
      "\n",
      "    #: L5\n",
      "    total_wage_manager = monthly_wage_manager * num_managers\n",
      "\n",
      "    #: L6\n",
      "    total_wages = total_wage_warehouse + total_wage_manager\n",
      "\n",
      "    #: L7\n",
      "    fica_taxes = total_wages * fica_tax_rate\n",
      "\n",
      "    #: L8\n",
      "    grand_total = total_wages + fica_taxes\n",
      "\n",
      "    answer = grand_total # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "*Index*:\n",
      "3822\n",
      "\n",
      "*Question*:\n",
      "Alec is running for Class President. He thinks that if he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him. Half of the class have already said they will vote for him but out of the remaining students, only 5 have said they are thinking about voting for him. He surveys the students who are thinking about voting for someone else, and changes his flyers to reflect the issues these students are concerned about. This results in a fifth of these students saying they'll vote for him. If Alec's class has 60 students and everyone who said they will vote for him does so, how many more votes does Alec need to reach his goal number of votes?\n",
      "\n",
      "*Solution*:\n",
      "{\"L1\": \"To calculate Alec's goal number of votes, we need to know that 60 students / 4 = [[60/4=15]]15 students is equal to one-quarter of the class students.\", \"L2\": \"Alec's goal is therefore 15 students * 3 quarters = [[15*3=45]]45 votes.\", \"L3\": \"Half of the class said they will vote for him, so there are already 60 students / 2 = [[60/2=30]]30 votes.\", \"L4\": \"Another 5 students are thinking about voting for him which leaves a total so far of 30 + 5 = [[30+5=35]]35 votes.\", \"L5\": \"This means there are 60 students - 35 voting for Alec = [[60-35=25]]25 students not voting for Alec.\", \"L6\": \"A fifth of these decided to vote, so this is a further 25 students / 5 = [[25/5=5]]5 votes.\", \"L7\": \"Alec is therefore receiving a total of 35 + 5 = [[35+5=40]]40 votes.\", \"L8\": \"So he has missed his goal by 45 goal votes - 40 actual votes = [[45-40=5]]5 votes.\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        fraction_needed_to_win: float = 3/4,  # f he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him.\n",
      "        fraction_voting_for_him: float = 1/2,  # Half of the class have already said they will vote for him\n",
      "        students_thinking_about_it: int = 5,  # only 5 have said they are thinking about voting for him\n",
      "        total_students: int = 60  # Alec's class has 60 students\n",
      "):    \n",
      "    \"\"\"Index: 3822.\n",
      "    Returns: the number of votes by which Alec is short of his goal.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    students_per_quarter = total_students / 4\n",
      "\n",
      "    #: L2\n",
      "    votes_needed = students_per_quarter * 3\n",
      "\n",
      "    #: L3\n",
      "    votes_for_him = total_students * fraction_voting_for_him\n",
      "\n",
      "    #: L4\n",
      "    votes_so_far = votes_for_him + students_thinking_about_it\n",
      "\n",
      "    #: L5\n",
      "    students_not_voting_for_him = total_students - votes_so_far\n",
      "\n",
      "    #: L6\n",
      "    new_votes = students_not_voting_for_him / 5\n",
      "\n",
      "    #: L7\n",
      "    total_votes_for_him = votes_so_far + new_votes\n",
      "\n",
      "    #: L8\n",
      "    votes_short_of_goal = votes_needed - total_votes_for_him\n",
      "\n",
      "    answer = votes_short_of_goal  # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "*Index*:\n",
      "7371\n",
      "\n",
      "*Question*:\n",
      "Karen's students are about to take a standardized test. Karen gets a $500 bonus if their average score is above 75, plus an extra $10 bonus for every additional point the average score increases above 75. So far, Karen has graded 8 tests, and the average is 70. Given that each student can have a maximum score of 150, what combined score do the last two tests need to have for Karen to earn a $600 bonus?\n",
      "\n",
      "*Solution*:\n",
      "{\"L1\": \"First subtract $500 from Karen's goal bonus amount to find how much she makes from the extra $10/point bonus: $600 - $500 = $[[600-500=100]]100\", \"L2\": \"Then divide the extra bonus by the extra rate: $100 / $10/point = [[100/10=10]]10 points\", \"L3\": \"Then add the 10 extra points to the baseline 75 point goal to find the students' average test score: 10 points + 75 points = [[10+75=85]]85 points\", \"L4\": \"Then added the 8 graded tests to the 2 ungraded tests to find the total number of tests: 2 tests + 8 tests = [[2+8=10]]10 tests\", \"L5\": \"Then multiply the 85 point average by the number of tests to find the total number of points the students need to earn: 85 points/test * 10 tests = 850 points\", \"L6\": \"Then multiply the current average by the current number of graded tests to find how many points have been earned so far: 70 points/test * 8 tests = [[70*8=560]]560 points\", \"L7\": \"Then subtract the number of points earned from the number of points needed to find the combine score the last two tests need: 850 points - 560 points = [[850-560=290]]290 points\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "    baseline_bonus: int = 500, # Karen gets a $500 bonus\n",
      "    baseline_avg_score: int = 75, # if their average score is above 75\n",
      "    extra_bonus_per_point: int = 10, # plus an extra $10 bonus for every additional point the average score increases above 75\n",
      "    tests_graded_so_far: int = 8, # So far, Karen has graded 8 tests\n",
      "    avg_so_far: int = 70, # and the average is 70\n",
      "    max_score_per_student: int = 150, # each student can have a maximum score of 150\n",
      "    desired_bonus: int = 600 # Karen wants to earn a $600 bonus\n",
      "):\n",
      "    \"\"\"Index: 7371.\n",
      "    Returns: the combined score needed in the last two tests to ensure that Karen earns a $600 bonus.\"\"\"\n",
      "    #: L1\n",
      "    extra_bonus_needed = desired_bonus - baseline_bonus\n",
      "\n",
      "    #: L2\n",
      "    extra_points_needed = extra_bonus_needed / extra_bonus_per_point\n",
      "\n",
      "    #: L3\n",
      "    target_avg_score = baseline_avg_score + extra_points_needed\n",
      "\n",
      "    #: L4\n",
      "    total_tests = tests_graded_so_far + 2\n",
      "\n",
      "    #: L5\n",
      "    total_points_needed = target_avg_score * total_tests\n",
      "\n",
      "    #: L6\n",
      "    points_earned_so_far = avg_so_far * tests_graded_so_far\n",
      "\n",
      "    #: L7\n",
      "    points_needed_last_two_tests = total_points_needed - points_earned_so_far\n",
      "\n",
      "    answer = points_needed_last_two_tests  # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "Static prefix SHA-1: 218e17fcd483e006f66b16712775dfd58a6ae71c\n"
     ]
    }
   ],
   "source": [
    "# --- 2. System Prompt and Helper Functions (no changes needed) ---\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an expert Python programmer specializing in data formalization. Your role is to meticulously convert natural language math problems and their step-by-step solutions into a single, well-structured Python function. You will be presented with examples of the required format followed by a final task to complete.\"\n",
    "\n",
    "\n",
    "PROMPT_GUIDELINES = \"\"\"### Guidelines\n",
    "\n",
    "0. **Output wrapping**\n",
    "   Return the code inside a single ```python … ``` block, and nothing else.\n",
    "\n",
    "1.  **Function Naming & Docstring:** The function must be named `solve`. It must begin with a docstring that has exactly two lines:\n",
    "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
    "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
    "\n",
    "2.  **Function Arguments:** The function arguments must be derived from the 'Question' text.\n",
    "    *   Create a distinct argument for every numerical value that is directly stated in the text.\n",
    "    *   The arguments should be created **in the same order in which they appear in the question**.\n",
    "    *   **Note:** Some of these arguments may end up not being used in the function body. This is expected. Do not worry about this and leave the unused arguments in the function signature.\n",
    "\n",
    "3.  **Argument Formatting:** Each argument must include a type-hint (e.g., `int`, `float`) and a default value equal to its value in the 'Question'. You must also add a comment (`#`) next to each argument that quotes or refers to the phrase in the 'Question' it comes from.\n",
    "\n",
    "4.  **Function Body:** The body of the function should follow the logic of the provided 'Solution' dict, which contains the step-by-step solution to the problem. The keys of this dict are strings (e.g. `\"L1\"`, `\"L2\"`) which refer to the line number, and the values of the dict are the corresponding steps in the solution.\n",
    "    * For every relevant line in the 'Solution', you must include a comment in the Python code that indicates the line number (key) from the 'Solution' dict.\n",
    "    * These comments should be formatted as `#: L<n>`, where `<n>` is the line number from the 'Solution' dict.\n",
    "    * Immediately follow the comment with the Python statement that performs the calculation.\n",
    "    * Steps in the solution should result in the creation of new, intermediate variables, which should be named descriptively based on the context of the calculation.\n",
    "    * Wherever possible, in your code try to use only the variables from the function arguments and the intermediate variables you created before, and try to avoid using hard-coded numbers in the calculations.\n",
    "\n",
    "5.  **Calculator Annotations:** Pay close attention to the calculator annotations (e.g., `[[25*8=200]]`) in the 'Solution' as they reveal the precise mathematical operations to implement. **Note**: Some lines in the solution may not contain calculator annotations, but you should still pay attention to the logic and calculations described in those lines.\n",
    "\n",
    "6.  **Final Answer:** Store the final answer in a variable named 'answer', and on the same line, add the comment `# FINAL ANSWER`. In the next line, return the 'answer' variable.\n",
    "\n",
    "7. **No extra output:** Your output should end with the ``` closing the code block. Do not include any additional text, explanations, or comments outside of the code block.\"\"\"\n",
    "\n",
    "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "def build_solution_mapping(\n",
    "    index: int,\n",
    "    dataset: Any,\n",
    "    convert_brackets: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : int\n",
    "        Position of the sample in the loaded dataset.\n",
    "    dataset : iterable / HuggingFace Dataset\n",
    "    convert_brackets : bool, default ``True``\n",
    "        If ``True`` replace every ``<< … >>`` calculator annotation with\n",
    "        the canonical ``[[ … ]]`` form so downstream code sees a single\n",
    "        bracket style.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Mapping ``{\"L1\": <first non-empty line>, \"L2\": <second>, …}``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Blank lines in ``sample[\"answer\"]`` are ignored.\n",
    "    * The line numbering reflects the *order* in the original solution\n",
    "      string; there is no semantic grouping beyond that.\n",
    "    \"\"\"\n",
    "    # extract & split solution text\n",
    "    solution_text = dataset[index][\"answer\"]\n",
    "    lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "\n",
    "    # Remove the last line if it matches the '####' answer pattern\n",
    "    if lines and re.match(r\"^####\\s*\\d+(\\.\\d+)?$\", lines[-1]):\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    # optional bracket normalisation\n",
    "    if convert_brackets:\n",
    "        angle = re.compile(r\"<<([^>]+)>>\")\n",
    "        lines = [angle.sub(r\"[[\\1]]\", ln) for ln in lines]\n",
    "    # build mapping\n",
    "    return {f\"L{i}\": line for i, line in enumerate(lines, 1)}\n",
    "\n",
    "\n",
    "def format_prompt_query(index: int, code_strings: dict, with_code: bool = False):\n",
    "    sample = gsm8k_train[index]\n",
    "    question = sample[\"question\"]\n",
    "    solution_mapping = build_solution_mapping(index, gsm8k_train)\n",
    "    solution = json.dumps(solution_mapping)\n",
    "    out = f\"\"\"*Index*:\n",
    "{index}\n",
    "\n",
    "*Question*:\n",
    "{question}\n",
    "\n",
    "*Solution*:\n",
    "{solution}\n",
    "\n",
    "*Code*:\"\"\"\n",
    "    if with_code:\n",
    "        out += f\"\"\"\\n```python\n",
    "{code_strings.get(index, \"# Code not found\")}\n",
    "```\"\"\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_code_strings(indices: List[int], savepath: Path = PROJECT_ROOT / 'data' / 'code_examples'):\n",
    "    \"\"\"\n",
    "    Reads code examples directly from .py files instead of importing them.\n",
    "    This is more robust and avoids Python's complex import path mechanics.\n",
    "    \"\"\"\n",
    "    code_strings = {}\n",
    "    for idx in indices:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(savepath, f\"_{idx}.py\")\n",
    "        try:\n",
    "            # Read the entire content of the file\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                code_strings[idx] = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find example file for index {idx} at: {filepath}\")\n",
    "            code_strings[idx] = f\"# Error: Code for example {idx} not found.\"\n",
    "    return code_strings\n",
    "\n",
    "\n",
    "EXAMPLE_INDICES = [310, 3822, 7371]          # keep fixed order\n",
    "_CODE_EXAMPLES  = get_code_strings(EXAMPLE_INDICES)\n",
    "\n",
    "\n",
    "def build_static_prefix() -> str:\n",
    "    \"\"\"Guidelines + few-shot examples rendered once for caching.\"\"\"\n",
    "    examples_block = \"\\n\".join(\n",
    "        format_prompt_query(idx, _CODE_EXAMPLES, with_code=True)\n",
    "        for idx in EXAMPLE_INDICES\n",
    "    )\n",
    "    prefix = \"\\n\".join([\n",
    "        PROMPT_GUIDELINES.strip(),\n",
    "        \"\\n--- EXAMPLES ---\\n\",\n",
    "        examples_block.strip()\n",
    "    ])\n",
    "    # Canonical whitespace – important for cache hits\n",
    "    return textwrap.dedent(prefix).strip()\n",
    "\n",
    "\n",
    "STATIC_PREFIX = build_static_prefix()\n",
    "print(STATIC_PREFIX)\n",
    "print(\"Static prefix SHA-1:\", hashlib.sha1(STATIC_PREFIX.encode()).hexdigest())\n",
    "\n",
    "\n",
    "def build_task_prompt(index: int) -> str:\n",
    "    \"\"\"Problem-specific part that changes every call.\"\"\"\n",
    "    return \"\\n\".join([\n",
    "        \"--- TASK ---\\n\",\n",
    "        format_prompt_query(index=index,\n",
    "                            code_strings={},     # no code in the task part\n",
    "                            with_code=False)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZPraoWC8txg8",
   "metadata": {
    "id": "ZPraoWC8txg8"
   },
   "outputs": [],
   "source": [
    "# --- 3. Asynchronous API Calling Function ---\n",
    "\n",
    "# ── Anthropic token-bucket limiter ────────────────────────────────────\n",
    "\n",
    "# one global bucket: 50 000 input-tokens per rolling minute\n",
    "_anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": time.monotonic() + 60}\n",
    "\n",
    "async def _anthropic_throttle(tokens_needed: int):\n",
    "    \"\"\"\n",
    "    Wait until at least `tokens_needed` tokens are available in the\n",
    "    50 000-tokens-per-minute bucket, then deduct them.\n",
    "    \"\"\"\n",
    "    global _anthropic_bucket\n",
    "\n",
    "    while True:\n",
    "        now = time.monotonic()\n",
    "\n",
    "        # reset bucket if a minute passed\n",
    "        if now >= _anthropic_bucket[\"reset_at\"]:\n",
    "            _anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": now + 60}\n",
    "\n",
    "        if tokens_needed <= _anthropic_bucket[\"tokens\"]:\n",
    "            _anthropic_bucket[\"tokens\"] -= tokens_needed\n",
    "            return                               # we’re clear to send\n",
    "        else:\n",
    "            # need to wait for bucket refill\n",
    "            to_sleep = _anthropic_bucket[\"reset_at\"] - now\n",
    "            await asyncio.sleep(max(to_sleep, 0.01))\n",
    "\n",
    "\n",
    "async def _with_429_retries(send_coroutine_factory, *, max_attempts=3, default_wait=15):\n",
    "    \"\"\"\n",
    "    send_coroutine_factory : lambda  ->  coroutine\n",
    "        Pass a **lambda** that, when called, returns the coroutine you want\n",
    "        to execute (e.g. the gemini.generate_content_async call).\n",
    "\n",
    "    Retries up to `max_attempts` times on 429 errors, using the\n",
    "    `retry_delay` from the API error when available, otherwise `default_wait`.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    wait = default_wait\n",
    "    while True:\n",
    "        try:\n",
    "            return await send_coroutine_factory()\n",
    "        except Exception as e:\n",
    "            # crude but effective 429 detection\n",
    "            if (\"429\" in str(e)) and attempt < max_attempts:\n",
    "                # Google errors include retry_delay.seconds\n",
    "                retry_attr = getattr(e, \"retry_delay\", None)\n",
    "                wait = retry_attr.seconds if retry_attr else wait\n",
    "                attempt += 1\n",
    "                print(f\"🕒 Google 429 — retrying in {wait}s (attempt {attempt}/{max_attempts})\")\n",
    "                await asyncio.sleep(wait)\n",
    "            else:\n",
    "                raise  # re-raise non-429 or maxed-out errors\n",
    "\n",
    "\n",
    "async def call_model_api_async(\n",
    "        provider: str,\n",
    "        model: str,\n",
    "        static_prefix: str,\n",
    "        all_gemini_model_caches: Dict[str, Optional[str]],\n",
    "        task_prompt: str,\n",
    "        semaphore: asyncio.Semaphore):\n",
    "\n",
    "    async with semaphore:\n",
    "        usage = {\"input_tokens\": 0,\n",
    "                 \"output_tokens\": 0,\n",
    "                 \"cached_tokens\": 0}\n",
    "\n",
    "        # ─────────────────── Google (no prompt cache) ───────────────────\n",
    "        if provider == \"google\":\n",
    "            # Retrieve the correct cached_prompt_id for this specific model\n",
    "            gemini_cached_prompt_id = all_gemini_model_caches.get(model)\n",
    "\n",
    "            if gemini_cached_prompt_id is None:\n",
    "                raise ValueError(f\"Gemini cached_prompt_id not found or failed to create for model '{model}'. Cannot proceed with explicit caching.\")\n",
    "\n",
    "            # Removed: gemini_model = GenerativeModel(model_name=model)\n",
    "            # Removed: cfg = genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4000)\n",
    "\n",
    "            def gemini_call_factory():\n",
    "                # FIX: Call generate_content directly on gemini_client.aio.models\n",
    "                # and pass model, contents, and config explicitly.\n",
    "                return gemini_client.aio.models.generate_content(\n",
    "                    model=model, # Pass model name directly as an argument\n",
    "                    contents=[\n",
    "                        genai.types.Content(role=\"user\", parts=[genai.types.Part(text=task_prompt)])\n",
    "                    ],\n",
    "                    # Use 'config' for all optional parameters, including cached_content\n",
    "                    config=genai.types.GenerateContentConfig(\n",
    "                        temperature=0.1, # Move temperature here\n",
    "                        max_output_tokens=4000, # Move max_output_tokens here\n",
    "                        cached_content=gemini_cached_prompt_id\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            response = await _with_429_retries(gemini_call_factory)\n",
    "\n",
    "            text = response.text\n",
    "            if response.usage_metadata:\n",
    "                usage[\"input_tokens\"]  = response.usage_metadata.prompt_token_count\n",
    "                usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "                usage[\"cached_tokens\"] = response.usage_metadata.cached_content_token_count\n",
    "\n",
    "\n",
    "        # ─────────────────── Anthropic (cache_control) ──────────────────\n",
    "        elif provider == \"anthropic\":\n",
    "            system_block = {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": static_prefix,\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "            }\n",
    "\n",
    "            # crude token estimate: words×1.2  (safe over-count)\n",
    "            est_tokens = math.ceil(1.2 * len(static_prefix.split()))\n",
    "            await _anthropic_throttle(est_tokens)\n",
    "\n",
    "            response = await anthropic_client_async.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=4000,\n",
    "                temperature=0.1,\n",
    "                system=[system_block],\n",
    "                messages=[{\"role\": \"user\", \"content\": task_prompt}],\n",
    "            )\n",
    "            text = response.content[0].text\n",
    "            usage[\"input_tokens\"]   = response.usage.input_tokens\n",
    "            usage[\"output_tokens\"]  = response.usage.output_tokens\n",
    "            usage[\"cached_tokens\"]  = response.usage.cache_read_input_tokens\n",
    "\n",
    "        # ─────────────────── OpenAI (auto prompt cache) ─────────────────\n",
    "        elif provider == \"openai\":\n",
    "            msgs = [\n",
    "                {\"role\": \"system\", \"content\": static_prefix},\n",
    "                {\"role\": \"user\",   \"content\": task_prompt},\n",
    "            ]\n",
    "            rsp = await openai_client_async.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=msgs,\n",
    "                temperature=0.1,\n",
    "                max_tokens=4000,\n",
    "            )\n",
    "            text = rsp.choices[0].message.content\n",
    "            usage[\"input_tokens\"]  = rsp.usage.prompt_tokens\n",
    "            usage[\"output_tokens\"] = rsp.usage.completion_tokens\n",
    "            try:\n",
    "                details = rsp.usage.prompt_tokens_details\n",
    "                usage[\"cached_tokens\"] = details.get(\"cached_tokens\", 0)\n",
    "            except AttributeError:\n",
    "                usage[\"cached_tokens\"] = 0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider {provider}\")\n",
    "\n",
    "        return text, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c266cfd",
   "metadata": {
    "id": "4c266cfd"
   },
   "outputs": [],
   "source": [
    "# --- 4. Parallel Orchestration Function ---\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "#  Parallel across PROBLEMS, but seed Anthropic cache with the first\n",
    "#  problem before launching the rest.\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "async def generate_GSM8K_code_parallel(\n",
    "    model_dict: Dict[str, List[str]],\n",
    "    indices_to_generate: List[int],\n",
    "    example_indices: List[int],\n",
    "    output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    max_parallel_problems: int | None = None,      # None = unlimited\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Initialize the global Gemini client\n",
    "    global gemini_client\n",
    "    gemini_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    # Create explicit caches for all specified Google models\n",
    "    global gemini_model_caches\n",
    "    gemini_model_caches = {} # Initialize the dictionary\n",
    "    if \"google\" in model_dict:\n",
    "        for model_name in model_dict[\"google\"]:\n",
    "            print(f\"Attempting to create cache for Google model: {model_name}...\")\n",
    "            try:\n",
    "                cache = await gemini_client.aio.caches.create(\n",
    "                    model=model_name, # Use the specific model name from the list\n",
    "                    config=genai.types.CreateCachedContentConfig(\n",
    "                        display_name=f'GSM8K_StaticPrefixCache_{model_name.replace(\"/\", \"_\")}', # Unique display name\n",
    "                        system_instruction=STATIC_PREFIX,\n",
    "                        ttl=\"86400s\", # Cache for 24 hours\n",
    "                    )\n",
    "                )\n",
    "                gemini_model_caches[model_name] = cache.name\n",
    "                print(f\"Successfully created cache for {model_name}: {cache.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"CRITICAL ERROR: Failed to create Gemini cache for model '{model_name}': {e}\")\n",
    "                gemini_model_caches[model_name] = None # Mark as failed\n",
    "\n",
    "    else:\n",
    "        print(\"No 'google' provider models specified in model_dict. Skipping Gemini cache creation.\")\n",
    "\n",
    "\n",
    "    # choose a unique run-id right at the start  ────────────────────────\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    provider_sems = {\n",
    "        prov: asyncio.Semaphore(limit) for prov, limit in API_CONCURRENCY_LIMITS.items()\n",
    "    }\n",
    "    problem_sem = (\n",
    "        asyncio.Semaphore(max_parallel_problems)\n",
    "        if max_parallel_problems is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    static_prefix = STATIC_PREFIX\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    # ── coroutine for a single GSM8K problem ──────────────────────────\n",
    "    async def _run_one_problem(idx: int):\n",
    "        async def _guard():\n",
    "            nonlocal rows\n",
    "            task_prompt = build_task_prompt(idx)\n",
    "            p_dir = output_dir / str(idx)\n",
    "            p_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # launch model calls (your inner gather)\n",
    "            tasks = []\n",
    "            for prov, models in model_dict.items():\n",
    "                for m in models:\n",
    "                    t = asyncio.create_task(\n",
    "                        call_model_api_async(\n",
    "                            provider=prov,\n",
    "                            model=m,\n",
    "                            static_prefix=static_prefix,\n",
    "                            all_gemini_model_caches=gemini_model_caches, # Pass the dictionary of caches\n",
    "                            task_prompt=task_prompt,\n",
    "                            semaphore=provider_sems[prov],\n",
    "                        )\n",
    "                    )\n",
    "                    t.meta = {\"provider\": prov, \"model\": m, \"index\": idx, \"start\": time.time()}\n",
    "                    tasks.append(t)\n",
    "\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for t, res in zip(tasks, results):\n",
    "                meta = t.meta\n",
    "                elapsed = time.time() - meta[\"start\"]\n",
    "                status = \"Failed\"\n",
    "                text = \"\"\n",
    "                usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "\n",
    "                if not isinstance(res, Exception):\n",
    "                    text, usage = res\n",
    "                    status = \"Success\"\n",
    "                    if text:\n",
    "                        (p_dir / f\"{meta['provider']}_{meta['model']}.txt\").write_text(\n",
    "                            text, encoding=\"utf-8\"\n",
    "                        )\n",
    "                else:\n",
    "                    print(f\"{meta['provider']}_{meta['model']} ❌ {res}\")\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"provider\": meta[\"provider\"],\n",
    "                        \"model\": meta[\"model\"],\n",
    "                        \"index\": meta[\"index\"],\n",
    "                        \"status\": status,\n",
    "                        \"time_s\": round(elapsed, 3),\n",
    "                        \"utc_completed\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "                        \"input_tokens\": usage[\"input_tokens\"],\n",
    "                        \"output_tokens\": usage[\"output_tokens\"],\n",
    "                        \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return await _guard() if problem_sem is None else await problem_sem.__aenter__() or await _guard() or await problem_sem.__aexit__(None, None, None)\n",
    "\n",
    "    # ── 1. Seed cache with the FIRST index (blocking) ─────────────────\n",
    "    first_idx, *rest = indices_to_generate\n",
    "    await _run_one_problem(first_idx)\n",
    "\n",
    "    # ── 2. Fan-out remaining problems concurrently ────────────────────\n",
    "    await asyncio.gather(*[_run_one_problem(i) for i in rest])\n",
    "\n",
    "    # Clean up: Delete all created caches\n",
    "    if gemini_model_caches:\n",
    "        print(\"\\nDeleting Gemini caches...\")\n",
    "        delete_tasks = []\n",
    "        for model_name, cache_id in gemini_model_caches.items():\n",
    "            if cache_id: # Only attempt to delete if a cache ID was successfully created\n",
    "                delete_tasks.append(gemini_client.aio.caches.delete(name=cache_id))\n",
    "\n",
    "        if delete_tasks:\n",
    "            await asyncio.gather(*delete_tasks, return_exceptions=True)\n",
    "            print(\"Gemini caches deletion attempts completed.\")\n",
    "        else:\n",
    "            print(\"No Gemini caches to delete.\")\n",
    "\n",
    "    # ── save CSV ─────────────────────────────────────────────────────\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ All done — log at {csv_path}\")\n",
    "    return df\n",
    "\n",
    "# # Add any problem indices you have generated outputs for.\n",
    "# problem_indices_to_test = sorted([3331, 1647, 636, 399, 4670, 5918, 1531, 7364, 5464, 1205, 3518, 6732, 3779, 4483, 6237, 1202, 2345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dab0fab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dab0fab",
    "outputId": "2b3f68a2-0c31-420d-ef50-7bd811cdd06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create cache for Google model: gemini-2.5-flash-lite-preview-06-17...\n",
      "Successfully created cache for gemini-2.5-flash-lite-preview-06-17: cachedContents/dob8081ne1rfa8tlckjq3z4157bmts276c3h8ots\n",
      "Attempting to create cache for Google model: gemini-2.5-flash...\n",
      "Successfully created cache for gemini-2.5-flash: cachedContents/glikqsospwhoivvp9cgqkk07f1qqv7lxq5l944ao\n",
      "anthropic_claude-3-5-haiku-20241022 ❌ Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (124dd641-0a24-4883-9fd3-6bed401adf9f) of 50,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "anthropic_claude-3-5-haiku-20241022 ❌ Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (124dd641-0a24-4883-9fd3-6bed401adf9f) of 50,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "anthropic_claude-3-5-haiku-20241022 ❌ Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (124dd641-0a24-4883-9fd3-6bed401adf9f) of 50,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "anthropic_claude-3-5-haiku-20241022 ❌ Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (124dd641-0a24-4883-9fd3-6bed401adf9f) of 50,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "\n",
      "Deleting Gemini caches...\n",
      "Gemini caches deletion attempts completed.\n",
      "✓ All done — log at content/data/code_gen_outputs_raw/generation_performance_20250625_064253.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Execution ---\n",
    "\n",
    "# Define your parameters here\n",
    "indices = [310, 3822, 7371] # Use the indices of your few-shot examples\n",
    "indices_to_generate = list(range(100))\n",
    "\n",
    "model_dict = {\n",
    "  \"anthropic\": [\"claude-3-5-haiku-20241022\"],\n",
    "  \"openai\": [\"gpt-4.1-mini\"],\n",
    "  \"google\": [\"gemini-2.5-flash-lite-preview-06-17\",\n",
    "             \"gemini-2.5-flash\"]\n",
    "}\n",
    "\n",
    "# REFACTOR: To run the async function, you must `await` it.\n",
    "# This will execute the entire parallel generation process.\n",
    "# The result (a pandas DataFrame with performance logs) will be stored in `perf_df`.\n",
    "\n",
    "perf_df = await generate_GSM8K_code_parallel(\n",
    "    model_dict=model_dict,\n",
    "    indices_to_generate=indices_to_generate,\n",
    "    example_indices=indices\n",
    ")\n",
    "\n",
    "\n",
    "# print(\"\\nFinal Performance Summary:\")\n",
    "# print(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd9ac540",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "cd9ac540",
    "outputId": "a9705701-84ad-4f94-eaa2-e5f50f474100"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"perf_df\",\n  \"rows\": 400,\n  \"fields\": [\n    {\n      \"column\": \"provider\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"anthropic\",\n          \"openai\",\n          \"google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"gpt-4.1-mini\",\n          \"gemini-2.5-flash\",\n          \"claude-3-5-haiku-20241022\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          84,\n          51,\n          69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Failed\",\n          \"Success\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108.37014680680481,\n        \"min\": 3.698,\n        \"max\": 361.022,\n        \"num_unique_values\": 184,\n        \"samples\": [\n          26.705,\n          55.764\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"utc_completed\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 86,\n        \"samples\": [\n          \"2025-06-25T06:48:04\",\n          \"2025-06-25T06:43:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1450,\n        \"min\": 0,\n        \"max\": 3955,\n        \"num_unique_values\": 246,\n        \"samples\": [\n          3192,\n          198\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 0,\n        \"max\": 626,\n        \"num_unique_values\": 197,\n        \"samples\": [\n          298,\n          360\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cached_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1518,\n        \"min\": 0,\n        \"max\": 3461,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          3461\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "perf_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-00cfa4e9-0b91-4741-80ad-916f222a82f2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>status</th>\n",
       "      <th>time_s</th>\n",
       "      <th>utc_completed</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>cached_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.275</td>\n",
       "      <td>2025-06-25T06:43:00</td>\n",
       "      <td>135</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.276</td>\n",
       "      <td>2025-06-25T06:43:00</td>\n",
       "      <td>3159</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.276</td>\n",
       "      <td>2025-06-25T06:43:00</td>\n",
       "      <td>3586</td>\n",
       "      <td>137</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>7.276</td>\n",
       "      <td>2025-06-25T06:43:00</td>\n",
       "      <td>3586</td>\n",
       "      <td>125</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.698</td>\n",
       "      <td>2025-06-25T06:43:04</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>98</td>\n",
       "      <td>Success</td>\n",
       "      <td>350.538</td>\n",
       "      <td>2025-06-25T06:48:51</td>\n",
       "      <td>3704</td>\n",
       "      <td>209</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>99</td>\n",
       "      <td>Success</td>\n",
       "      <td>361.022</td>\n",
       "      <td>2025-06-25T06:49:02</td>\n",
       "      <td>309</td>\n",
       "      <td>292</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>99</td>\n",
       "      <td>Success</td>\n",
       "      <td>361.022</td>\n",
       "      <td>2025-06-25T06:49:02</td>\n",
       "      <td>3313</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>99</td>\n",
       "      <td>Success</td>\n",
       "      <td>361.022</td>\n",
       "      <td>2025-06-25T06:49:02</td>\n",
       "      <td>3754</td>\n",
       "      <td>370</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>99</td>\n",
       "      <td>Success</td>\n",
       "      <td>361.022</td>\n",
       "      <td>2025-06-25T06:49:02</td>\n",
       "      <td>3754</td>\n",
       "      <td>368</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00cfa4e9-0b91-4741-80ad-916f222a82f2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-00cfa4e9-0b91-4741-80ad-916f222a82f2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-00cfa4e9-0b91-4741-80ad-916f222a82f2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-9eca9331-ee69-4076-a12f-96dd291a44b0\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9eca9331-ee69-4076-a12f-96dd291a44b0')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-9eca9331-ee69-4076-a12f-96dd291a44b0 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_8133e4a1-956e-42bb-bf2f-e52b44f9fb5c\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('perf_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_8133e4a1-956e-42bb-bf2f-e52b44f9fb5c button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('perf_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      provider                                model  index   status   time_s  \\\n",
       "0    anthropic            claude-3-5-haiku-20241022      0  Success    7.275   \n",
       "1       openai                         gpt-4.1-mini      0  Success    7.276   \n",
       "2       google  gemini-2.5-flash-lite-preview-06-17      0  Success    7.276   \n",
       "3       google                     gemini-2.5-flash      0  Success    7.276   \n",
       "4    anthropic            claude-3-5-haiku-20241022      1  Success    3.698   \n",
       "..         ...                                  ...    ...      ...      ...   \n",
       "395     google                     gemini-2.5-flash     98  Success  350.538   \n",
       "396  anthropic            claude-3-5-haiku-20241022     99  Success  361.022   \n",
       "397     openai                         gpt-4.1-mini     99  Success  361.022   \n",
       "398     google  gemini-2.5-flash-lite-preview-06-17     99  Success  361.022   \n",
       "399     google                     gemini-2.5-flash     99  Success  361.022   \n",
       "\n",
       "           utc_completed  input_tokens  output_tokens  cached_tokens  \n",
       "0    2025-06-25T06:43:00           135            175              0  \n",
       "1    2025-06-25T06:43:00          3159            120              0  \n",
       "2    2025-06-25T06:43:00          3586            137           3461  \n",
       "3    2025-06-25T06:43:00          3586            125           3461  \n",
       "4    2025-06-25T06:43:04           139            142           3416  \n",
       "..                   ...           ...            ...            ...  \n",
       "395  2025-06-25T06:48:51          3704            209           3461  \n",
       "396  2025-06-25T06:49:02           309            292           3416  \n",
       "397  2025-06-25T06:49:02          3313            268              0  \n",
       "398  2025-06-25T06:49:02          3754            370           3461  \n",
       "399  2025-06-25T06:49:02          3754            368           3461  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81375b1",
   "metadata": {
    "id": "a81375b1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Matches  ```python … ```  or  ```py … ```  (any EOL style)\n",
    "_CODE_BLOCK_RE = re.compile(\n",
    "    r\"```(?:python|py)\\s*\\r?\\n(.*?)\\r?\\n```\",\n",
    "    re.DOTALL | re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def process_and_clean_outputs(\n",
    "    source_dir: Path | str = BASE_OUTPUT_DIR,\n",
    "    dest_dir:   Path | str = PROJECT_ROOT / \"data\" / \"code_gen_outputs_cleaned\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Traverses `source_dir`, cleans raw .txt model outputs, and writes them as\n",
    "    .py files to `dest_dir`, preserving the relative folder structure.\n",
    "\n",
    "    Steps performed\n",
    "    ---------------\n",
    "    1.  Walk through every sub-directory of `source_dir`.\n",
    "    2.  Detect all files that end with `.txt`.\n",
    "    3.  Build the matching sub-directory path in `dest_dir`.\n",
    "    4.  Read the .txt and extract code inside a markdown fence:\n",
    "            ```python\n",
    "            <code>\n",
    "            ```\n",
    "        – If no fence is found, the whole file is used as is.\n",
    "    5.  Write the cleaned code to `<same_name>.py` in the destination tree.\n",
    "    6.  Leave the original raw files untouched.\n",
    "    \"\"\"\n",
    "    # Resolve to absolute paths\n",
    "    source_dir = Path(source_dir).expanduser().resolve()\n",
    "    dest_dir   = Path(dest_dir).expanduser().resolve()\n",
    "\n",
    "    print(f\"Starting processing from: {source_dir}\")\n",
    "    print(f\"Cleaned files will be saved in: {dest_dir}\\n\")\n",
    "\n",
    "    if not source_dir.is_dir():\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "        return\n",
    "\n",
    "    files_processed = 0\n",
    "\n",
    "    # 1. Walk through every *.txt under source_dir\n",
    "    for txt_path in source_dir.rglob(\"*.txt\"):\n",
    "\n",
    "        try:\n",
    "            # 2. Derive the destination sub-folder (Step 3)\n",
    "            rel_path   = txt_path.parent.relative_to(source_dir)\n",
    "            dest_subdir = dest_dir / rel_path\n",
    "            dest_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 3. Read raw content (Step 4)\n",
    "            raw_content = txt_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "            match = _CODE_BLOCK_RE.search(raw_content.lstrip())\n",
    "            if match:\n",
    "                cleaned_code = match.group(1).strip()\n",
    "                print(f\"✓ extracted code block → {txt_path}\")\n",
    "            else:\n",
    "                cleaned_code = raw_content.strip()\n",
    "                print(f\"⚠ no code block; using full file → {txt_path}\")\n",
    "\n",
    "            # 4. Write to .py in destination (Step 5)\n",
    "            dest_file = dest_subdir / f\"{txt_path.stem}.py\"\n",
    "            dest_file.write_text(cleaned_code, encoding=\"utf-8\")\n",
    "            files_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ error processing {txt_path}: {e}\")\n",
    "\n",
    "    # 5. Summary (Step 6)\n",
    "    print(f\"\\nProcessing complete — {files_processed} files written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde4b19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fde4b19c",
    "outputId": "06f28afc-df7d-41ac-fe9a-37824aed7769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing from: /content/content/data/code_gen_outputs_raw\n",
      "Cleaned files will be saved in: /content/content/data/code_gen_outputs_cleaned\n",
      "\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/18/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/18/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/18/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/18/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/58/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/58/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/58/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/58/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/6/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/6/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/6/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/6/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/63/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/63/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/63/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/63/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/14/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/14/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/14/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/14/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/8/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/8/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/8/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/8/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/81/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/81/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/81/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/81/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/98/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/98/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/98/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/98/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/84/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/84/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/84/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/84/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/7/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/7/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/7/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/7/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/65/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/65/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/65/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/65/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/89/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/89/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/89/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/89/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/67/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/67/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/67/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/67/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/25/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/25/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/25/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/25/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/29/anthropic_claude-3-5-haiku-20241022.txt\n",
      "⚠ no code block; using full file → /content/content/data/code_gen_outputs_raw/29/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/29/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/29/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/32/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/32/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/32/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/32/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/59/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/59/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/59/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/59/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/64/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/64/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/64/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/64/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/5/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/5/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/5/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/5/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/43/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/43/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/43/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/43/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/4/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/4/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/4/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/4/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/56/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/56/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/56/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/56/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/85/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/85/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/85/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/85/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/68/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/68/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/68/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/68/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/61/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/61/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/61/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/61/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/91/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/91/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/91/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/91/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/31/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/31/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/31/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/31/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/93/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/93/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/93/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/93/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/42/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/42/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/42/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/42/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/83/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/83/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/83/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/83/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/80/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/80/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/80/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/80/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/27/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/27/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/27/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/27/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/92/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/92/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/92/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/92/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/39/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/39/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/39/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/39/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/13/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/13/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/13/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/13/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/15/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/15/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/15/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/15/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/44/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/44/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/44/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/44/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/77/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/77/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/77/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/77/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/62/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/62/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/62/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/62/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/36/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/36/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/36/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/36/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/82/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/82/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/82/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/82/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/57/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/57/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/57/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/57/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/75/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/75/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/75/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/75/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/97/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/97/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/97/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/97/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/72/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/72/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/72/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/72/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/33/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/33/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/33/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/33/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/1/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/1/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/1/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/1/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/34/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/34/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/34/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/34/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/23/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/23/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/23/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/23/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/9/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/9/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/9/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/9/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/48/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/48/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/48/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/48/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/30/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/30/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/30/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/30/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/28/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/28/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/28/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/28/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/37/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/37/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/37/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/37/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/51/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/51/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/51/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/51/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/53/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/53/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/53/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/53/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/10/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/10/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/10/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/10/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/21/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/21/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/21/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/21/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/96/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/96/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/96/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/96/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/95/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/95/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/95/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/95/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/46/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/46/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/46/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/46/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/50/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/50/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/50/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/50/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/94/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/94/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/94/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/94/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/41/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/41/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/41/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/41/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/24/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/24/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/24/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/24/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/52/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/52/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/52/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/52/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/54/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/54/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/54/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/54/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/11/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/11/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/11/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/11/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/70/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/70/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/70/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/70/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/12/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/12/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/12/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/12/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/26/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/26/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/26/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/26/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/55/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/55/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/55/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/55/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/99/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/99/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/99/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/99/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/73/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/73/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/73/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/73/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/49/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/49/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/49/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/49/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/79/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/79/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/79/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/79/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/19/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/19/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/19/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/19/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/69/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/69/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/69/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/69/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/2/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/2/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/2/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/2/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/20/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/20/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/20/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/20/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/60/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/60/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/60/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/60/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/22/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/22/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/22/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/22/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/40/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/40/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/40/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/40/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/38/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/38/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/38/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/38/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/35/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/35/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/35/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/35/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/87/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/87/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/87/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/87/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/71/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/71/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/71/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/71/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/16/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/16/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/16/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/16/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/17/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/17/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/17/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/17/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/47/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/47/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/47/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/47/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/66/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/66/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/66/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/66/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/3/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/3/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/3/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/3/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/78/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/78/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/78/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/78/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/74/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/74/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/74/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/74/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/90/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/90/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/90/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/90/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/88/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/88/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/88/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/88/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/45/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/45/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/45/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/45/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/0/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/0/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/0/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/0/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/76/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/76/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/76/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/76/openai_gpt-4.1-mini.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/86/anthropic_claude-3-5-haiku-20241022.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/86/google_gemini-2.5-flash.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/86/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ extracted code block → /content/content/data/code_gen_outputs_raw/86/openai_gpt-4.1-mini.txt\n",
      "\n",
      "Processing complete — 400 files written.\n"
     ]
    }
   ],
   "source": [
    "process_and_clean_outputs()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastai_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
