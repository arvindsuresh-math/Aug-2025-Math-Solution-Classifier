{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7b84b146bd241e7958aaded20fc8db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_603c28998e354b339ea796aa3a9710b9",
              "IPY_MODEL_579fe1ec243d4575928678b13e37b0e1",
              "IPY_MODEL_35bc1b455bd048f6b34b580bf5913953"
            ],
            "layout": "IPY_MODEL_c6f5fa80022b40b7a273d40ebecde875"
          }
        },
        "603c28998e354b339ea796aa3a9710b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81b78d07b4845679ab88d5692110a9a",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb5e768c06c4e57b655ada6ba0b4330",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "579fe1ec243d4575928678b13e37b0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba3992495a945c8a79a417ef316b1df",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39c91311601e47129151276a5b46062b",
            "value": 2
          }
        },
        "35bc1b455bd048f6b34b580bf5913953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53667c35bc3d4bf6868d4eb0ac3c9a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6fbdd51b5bf475bab0f02a709d985d1",
            "value": " 2/2 [00:40&lt;00:00, 19.42s/it]"
          }
        },
        "c6f5fa80022b40b7a273d40ebecde875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81b78d07b4845679ab88d5692110a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb5e768c06c4e57b655ada6ba0b4330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ba3992495a945c8a79a417ef316b1df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c91311601e47129151276a5b46062b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53667c35bc3d4bf6868d4eb0ac3c9a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fbdd51b5bf475bab0f02a709d985d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initial setup"
      ],
      "metadata": {
        "id": "bUm9sufcd3F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Mount Google Drive to access your data ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqNs_uxipzR-",
        "outputId": "428a87f7-3a2e-4c86-b58c-d9595203acee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Install required libraries ---\n",
        "# We install the latest versions for compatibility with new models.\n",
        "!pip install -Uq transformers peft trl accelerate datasets\n",
        "!pip install -Uq bitsandbytes\n",
        "!pip install -q --upgrade fsspec # Resolve dependency conflicts\n",
        "print(\"Required libraries installed and dependencies reconciled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Zp3Udwpz6X",
        "outputId": "219eb170-3b24-48c5-aeab-429592431b4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequired libraries installed and dependencies reconciled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2WcCcRddLsz",
        "outputId": "513b8651-6a4c-4413-b6d2-84a30c6badb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/__MACOSX/._level-1-binary? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Dataset unzipped successfully to '/content/level-1-binary'.\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Unzip your dataset from Google Drive ---\n",
        "# This assumes you uploaded 'level-1-binary.zip' to the root of your Drive.\n",
        "!unzip -q /content/drive/My\\ Drive/level-1-binary.zip -d /content/\n",
        "print(\"Dataset unzipped successfully to '/content/level-1-binary'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "pHSfotbMeD_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # --- Model and Tokenizer ---\n",
        "    MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
        "    # --- Path to save the base model in Google Drive ---\n",
        "    MODEL_SAVE_PATH = \"/content/drive/My Drive/phi-4-mini-base-model\"\n",
        "\n",
        "    # --- Dataset ---\n",
        "    DATASET_PATH = \"/content/level-1-binary\"\n",
        "\n",
        "    # --- Fine-Tuning Output ---\n",
        "    OUTPUT_DIR = \"/content/solution-verifier-level1\"\n",
        "\n",
        "    # LoRA Config\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.05\n",
        "    LORA_TARGET_MODULES = \"all-linear\"\n",
        "\n",
        "    # Training Config\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NUM_EPOCHS = 5\n",
        "    BATCH_SIZE = 8\n",
        "    GRADIENT_ACCUMULATION_STEPS = 1\n",
        "    LOGGING_STEPS = 10\n",
        "    SAVE_STEPS = 500\n",
        "    MAX_SEQ_LENGTH = 1024"
      ],
      "metadata": {
        "id": "YCbN6z7Vp-3o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the dataset"
      ],
      "metadata": {
        "id": "Mcdb6GdieHId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "def format_prompt(example):\n",
        "    \"\"\"Formats a data sample into the required 'text' field for SFTTrainer.\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"\"\"Analyze the following mathematical problem and solution to determine if the solution is correct or flawed.\n",
        "\n",
        "### Problem:\n",
        "{example['question']}\n",
        "\n",
        "### Solution:\n",
        "{example['solution']}\n",
        "\n",
        "### Analysis:\n",
        "{example['label']}\"\"\"\n",
        "    }\n",
        "\n",
        "# Load from disk\n",
        "sft_dataset = load_from_disk(Config.DATASET_PATH)\n",
        "\n",
        "# Apply formatting and remove original columns\n",
        "formatted_dataset = sft_dataset.map(\n",
        "    format_prompt,\n",
        "    remove_columns=sft_dataset['train'].column_names\n",
        ")\n",
        "\n",
        "print(\"--- Dataset formatted and ready for training ---\")\n",
        "print(formatted_dataset)\n",
        "print(\"\\nExample of a formatted training sample:\")\n",
        "print(formatted_dataset['train'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGtXD2oeeKex",
        "outputId": "db5615c5-c961-4cad-8ef4-7c57007e8ed1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dataset formatted and ready for training ---\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3296\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 412\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 412\n",
            "    })\n",
            "})\n",
            "\n",
            "Example of a formatted training sample:\n",
            "Analyze the following mathematical problem and solution to determine if the solution is correct or flawed.\n",
            "\n",
            "### Problem:\n",
            "Sally earned $1000 at work last month. This month, she received a 10% raise. How much money will she make in total for the two months?\n",
            "\n",
            "### Solution:\n",
            "This month she will earn $1000 * (10/100) = $<<1000*(10/100)=100>>100.\n",
            "In total, she will make $1000 + $100 = $<<1000+100=1100>>1100.\n",
            "#### 1100\n",
            "\n",
            "### Analysis:\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Base Model and Tokenizer"
      ],
      "metadata": {
        "id": "hBiRTILXeLMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"\n",
        "    Deletes major model and trainer objects, forces garbage collection,\n",
        "    and empties the PyTorch CUDA cache to free up GPU memory.\n",
        "    \"\"\"\n",
        "    print(\"--- Clearing GPU Memory ---\")\n",
        "\n",
        "    # Check if variables exist in the global scope before trying to delete\n",
        "    variables_to_delete = ['model', 'trainer', 'peft_model', 'eval_model', 'base_model']\n",
        "    for var_name in variables_to_delete:\n",
        "        if var_name in globals():\n",
        "            del globals()[var_name]\n",
        "            print(f\"Deleted '{var_name}'\")\n",
        "\n",
        "    # Force garbage collection\n",
        "    print(\"Collecting garbage...\")\n",
        "    gc.collect()\n",
        "\n",
        "    # Empty the PyTorch CUDA cache\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Emptying CUDA cache...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"--- GPU Memory Cleared ---\")\n",
        "\n",
        "        # Print a summary to confirm\n",
        "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "    else:\n",
        "        print(\"No CUDA device found.\")\n",
        "\n",
        "# --- Example of how to call the function ---\n",
        "# You would run this line in a cell by itself after a run fails.\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieAb-cBPAn8Y",
        "outputId": "fea6434c-5207-46b0-8e08-ef2d23523277"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Clearing GPU Memory ---\n",
            "Collecting garbage...\n",
            "Emptying CUDA cache...\n",
            "--- GPU Memory Cleared ---\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Imports worked.\")\n",
        "\n",
        "# --- FIX: Define the 4-bit quantization configuration for QLoRA ---\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# We do not need to cache the quantized model, as it must be loaded this way each time.\n",
        "print(f\"--- Downloading and loading model in 4-bit (QLoRA)... ---\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    Config.MODEL_ID,\n",
        "    quantization_config=quantization_config, # Apply the 4-bit config\n",
        "    device_map=\"auto\", # device_map is efficient with quantization\n",
        "    trust_remote_code=True,\n",
        "    # attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_disable()\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    Config.MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# --- Configure the tokenizer's padding token ---\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"\\n--- Base model (4-bit) and tokenizer are ready ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "d7b84b146bd241e7958aaded20fc8db2",
            "603c28998e354b339ea796aa3a9710b9",
            "579fe1ec243d4575928678b13e37b0e1",
            "35bc1b455bd048f6b34b580bf5913953",
            "c6f5fa80022b40b7a273d40ebecde875",
            "f81b78d07b4845679ab88d5692110a9a",
            "eeb5e768c06c4e57b655ada6ba0b4330",
            "2ba3992495a945c8a79a417ef316b1df",
            "39c91311601e47129151276a5b46062b",
            "53667c35bc3d4bf6868d4eb0ac3c9a8d",
            "b6fbdd51b5bf475bab0f02a709d985d1"
          ]
        },
        "id": "RQbID9CP79pQ",
        "outputId": "9f51ccf1-85ea-4c36-96a7-f8577ac38a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports worked.\n",
            "--- Downloading and loading model in 4-bit (QLoRA)... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7b84b146bd241e7958aaded20fc8db2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Base model (4-bit) and tokenizer are ready ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from pathlib import Path\n",
        "\n",
        "# # --- Check if the model is already saved in Google Drive ---\n",
        "# model_save_path = Path(Config.MODEL_SAVE_PATH)\n",
        "\n",
        "# if model_save_path.exists():\n",
        "#     print(f\"--- Loading model and tokenizer from Google Drive: {model_save_path} ---\")\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_save_path,\n",
        "#         torch_dtype=torch.bfloat16,\n",
        "#         trust_remote_code=True\n",
        "#         # device_map=\"auto\" is removed\n",
        "#     )\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\n",
        "#         model_save_path,\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "# else:\n",
        "#     print(f\"--- Model not found in Google Drive. Downloading from Hugging Face Hub... ---\")\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#         Config.MODEL_ID,\n",
        "#         torch_dtype=torch.bfloat16,\n",
        "#         trust_remote_code=True\n",
        "#         # device_map=\"auto\" is removed\n",
        "#     )\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\n",
        "#         Config.MODEL_ID,\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "\n",
        "#     print(f\"--- Saving model and tokenizer to Google Drive for future use... ---\")\n",
        "#     model_save_path.mkdir(parents=True, exist_ok=True)\n",
        "#     model.save_pretrained(model_save_path)\n",
        "#     tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# # --- Configure the tokenizer's padding token ---\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "# print(\"\\n--- Base model and tokenizer are ready ---\")"
      ],
      "metadata": {
        "id": "7wLfjVbntRnT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Get the token IDs for the labels \"0\" and \"1\"\n",
        "# We are interested in the actual token ID, not the special start/end tokens.\n",
        "# The exact index [0] or [1] might vary based on tokenizer, but for single chars it's usually the first non-special token.\n",
        "label_0_token_id = tokenizer(\"0\", add_special_tokens=False).input_ids[0]\n",
        "label_1_token_id = tokenizer(\"1\", add_special_tokens=False).input_ids[0]\n",
        "\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    \"\"\"\n",
        "    Accuracy on the **last real token** of each sequence.\n",
        "    Works whether p.predictions / p.label_ids come in as\n",
        "    NumPy arrays (Trainer default) or torch tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- ensure torch tensors on CPU ---\n",
        "    logits = (torch.from_numpy(p.predictions)\n",
        "              if isinstance(p.predictions, np.ndarray) else\n",
        "              p.predictions).cpu()           # (B, L, V)\n",
        "    labels = (torch.from_numpy(p.label_ids)\n",
        "              if isinstance(p.label_ids, np.ndarray) else\n",
        "              p.label_ids).cpu()             # (B, L)\n",
        "\n",
        "    # length of each sequence *before* padding (-100 marks padding)\n",
        "    seq_lens = (labels != -100).sum(dim=1) - 1      # (B,)\n",
        "\n",
        "    # gather logits and labels at that position\n",
        "    batch_idx = torch.arange(logits.size(0))\n",
        "    last_logits = logits[batch_idx, seq_lens]        # (B, V)\n",
        "    gold        = labels[batch_idx, seq_lens]        # (B,)\n",
        "\n",
        "    preds = last_logits.argmax(dim=-1)               # (B,)\n",
        "    accuracy = (preds == gold).float().mean().item()\n",
        "\n",
        "    # free memory early (optional)\n",
        "    del logits, labels, last_logits\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "print(\"Metrics computation function is ready.\")\n",
        "print(f\"Token ID for '0': {label_0_token_id}\")\n",
        "print(f\"Token ID for '1': {label_1_token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFrbZ1_R3uO_",
        "outputId": "6db822dd-a73b-4259-dccb-f7ffee825ad0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics computation function is ready.\n",
            "Token ID for '0': 15\n",
            "Token ID for '1': 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure Fine-Tuning parameters"
      ],
      "metadata": {
        "id": "5-tRQQfdePxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "\n",
        "# --- FIX: Add this crucial step to prepare the quantized model for training ---\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # A smaller rank is fine for QLoRA and saves additional memory\n",
        "    lora_alpha=16, # Typically 2*r\n",
        "    lora_dropout=Config.LORA_DROPOUT,\n",
        "    target_modules=Config.LORA_TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"--- LoRA Configuration defined ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwxYiINYqbFW",
        "outputId": "3ebe02d8-d7bc-41f7-c02d-6a5cc13b530c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LoRA Configuration defined ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# # Define Training Arguments with Evaluation and Early Stopping\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=Config.OUTPUT_DIR,\n",
        "#     num_train_epochs=Config.NUM_EPOCHS,\n",
        "#     per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "#     gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
        "#     learning_rate=Config.LEARNING_RATE,\n",
        "#     logging_steps=Config.LOGGING_STEPS,\n",
        "#     fp16=False,\n",
        "#     bf16=True,\n",
        "#     report_to=\"none\",\n",
        "#     # --- START FIX ---\n",
        "#     # To load the best model, saving and evaluation must happen at the same interval.\n",
        "#     save_steps=200,\n",
        "#     eval_steps=200,\n",
        "#     eval_strategy=\"steps\",   # match save_strategy\n",
        "#     save_strategy=\"steps\",         # both are \"steps\"\n",
        "#     save_total_limit=3,\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"accuracy\", # Specify the metric to monitor\n",
        "#     greater_is_better=True, # Higher accuracy is better\n",
        "#     per_device_eval_batch_size = 4,\n",
        "#     eval_accumulation_steps   = 1\n",
        "#     # --- END FIX ---\n",
        "# )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                 = Config.OUTPUT_DIR,\n",
        "    num_train_epochs           = Config.NUM_EPOCHS,\n",
        "    learning_rate              = Config.LEARNING_RATE,\n",
        "    fp16                       = False,\n",
        "    # logging & evaluation strategies\n",
        "\n",
        "    # --- speed vs memory ---\n",
        "    per_device_train_batch_size= 4,    # was 1\n",
        "    gradient_accumulation_steps= 1,    # net batch = 4\n",
        "    per_device_eval_batch_size = 8,\n",
        "    eval_accumulation_steps    = 2,\n",
        "\n",
        "    # logging / saving\n",
        "    eval_strategy        = \"steps\",\n",
        "    save_strategy              = \"steps\",\n",
        "    eval_steps                 = 200,\n",
        "    save_steps                 = 200,\n",
        "    save_total_limit           = 3,\n",
        "    load_best_model_at_end     = True,\n",
        "    metric_for_best_model      = \"accuracy\",\n",
        "    greater_is_better          = True,\n",
        "    gradient_checkpointing      = False,\n",
        "    include_inputs_for_metrics = True, # ensures\n",
        "\n",
        "    # precision & perf\n",
        "    bf16                       = True,\n",
        "    tf32                       = True,              # needs nightly or torch>=2.3\n",
        "    optim                      = \"paged_adamw_8bit\",\n",
        "    report_to                  = \"none\",\n",
        "    logging_steps              = 50\n",
        ")\n",
        "\n",
        "# After loading the model (4-bit LoRA)\n",
        "model.gradient_checkpointing_disable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"--- training args configured for QLoRA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic8VqHJrqd0H",
        "outputId": "e49557a1-ee41-4c97-8345-34849741dfe4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- training args configured for QLoRA ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize trainer"
      ],
      "metadata": {
        "id": "WZU7F1p1qrfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset[\"train\"],\n",
        "    # --- START MODIFICATION ---\n",
        "    eval_dataset=formatted_dataset[\"validation\"], # Pass the validation set\n",
        "    compute_metrics=compute_metrics, # Pass the metrics function\n",
        "    # --- END MODIFICATION ---\n",
        "    peft_config=peft_config,\n",
        "    # max_seq_length=Config.MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkc9-xyzqsss",
        "outputId": "c63b5671-d540-4577-e4c5-4e845a5f1f1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n",
            "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train it!"
      ],
      "metadata": {
        "id": "O1WfJLy8qzQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process\n",
        "print(\"--- Starting Fine-Tuning with in-training evaluation ---\")\n",
        "trainer.train()\n",
        "print(\"--- Fine-Tuning Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "IHJ1r-rBq0iA",
        "outputId": "3b7d606b-ea97-411a-8fbe-82be5665dab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fine-Tuning with in-training evaluation ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='201' max='4120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 201/4120 01:09 < 22:54, 2.85 it/s, Epoch 0.24/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/52 01:55 < 03:14, 0.16 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Final Adapter Model"
      ],
      "metadata": {
        "id": "c2ZUK_yNq3TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = f\"{Config.OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "print(f\"Final adapter model saved to {final_model_path}\")"
      ],
      "metadata": {
        "id": "Boewd4v1q5qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load trained model for inference"
      ],
      "metadata": {
        "id": "-_VZf7ukq7ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model again (fast, from cache)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    Config.MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "795bqHYiq9QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- Loading fine-tuned adapter from: {final_model_path} ---\")\n",
        "# Load the LoRA adapter\n",
        "peft_model = PeftModel.from_pretrained(base_model, final_model_path)"
      ],
      "metadata": {
        "id": "-IXKBAUarCTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Merging adapter weights into base model ---\")\n",
        "# --- FIX: Merge the adapter weights and unload the PEFT model ---\n",
        "# This returns a standard AutoModelForCausalLM object.\n",
        "eval_model = peft_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "2VkeV75O2l9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Moving final model to GPU for evaluation ---\")\n",
        "# --- FIX: Explicitly move the final, merged model to the GPU ---\n",
        "eval_model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "CgvSren22t6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tokenizer is the same one we loaded earlier\n",
        "eval_tokenizer = tokenizer\n",
        "\n",
        "print(\"--- Trained model ready for evaluation ---\")"
      ],
      "metadata": {
        "id": "OUK45G7MrEDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run evaluation"
      ],
      "metadata": {
        "id": "LQzTPGx_rI7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "eval_dataset = sft_dataset[\"validation\"]\n",
        "correct_predictions = 0\n",
        "total_predictions = len(eval_dataset)\n",
        "\n",
        "print(f\"\\n--- Evaluating model on {total_predictions} validation samples ---\")\n",
        "\n",
        "for i in tqdm(range(total_predictions)):\n",
        "    sample = eval_dataset[i]\n",
        "\n",
        "    prompt_text = f\"\"\"Analyze the following mathematical problem and solution to determine if the solution is correct or flawed.\n",
        "\n",
        "### Problem:\n",
        "{sample['question']}\n",
        "\n",
        "### Solution:\n",
        "{sample['solution']}\n",
        "\n",
        "### Analysis:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = eval_tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = eval_model.generate(\n",
        "            **inputs, max_new_tokens=5, eos_token_id=eval_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_output_text = eval_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_text = full_output_text[len(prompt_text):].strip()\n",
        "\n",
        "    match = re.search(r'\\d', generated_text)\n",
        "    prediction = match.group(0) if match else None\n",
        "\n",
        "    if prediction == str(sample['label']):\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = (correct_predictions / total_predictions) * 100\n",
        "print(f\"\\n--- Evaluation Complete ---\")\n",
        "print(f\"Correct Predictions: {correct_predictions} / {total_predictions}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "6JAVVZhaeSMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3D94xbBU2D_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}