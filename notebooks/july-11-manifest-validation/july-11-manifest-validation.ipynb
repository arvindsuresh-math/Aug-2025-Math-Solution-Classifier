{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1237b250",
   "metadata": {},
   "source": [
    "### Cell 1: Imports and Path Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4d7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Validation Input Directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/tier-manifests-gen-processed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "import inspect\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Callable, Any\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "# --- INPUT: The directory containing the processed manifests to be validated ---\n",
    "PROCESSED_MANIFEST_DIR = DATA_DIR / \"tier-manifests-gen-processed\"\n",
    "\n",
    "# --- Define the list of models to validate ---\n",
    "MODELS = ['openai_gpt-4.1', 'google_gemini-2.5-flash']\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Validation Input Directory: {PROCESSED_MANIFEST_DIR}\")\n",
    "\n",
    "# --- Ensure Input Directory Exists ---\n",
    "if not PROCESSED_MANIFEST_DIR.is_dir():\n",
    "    raise FileNotFoundError(f\"INPUT DIRECTORY NOT FOUND: {PROCESSED_MANIFEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16586a0",
   "metadata": {},
   "source": [
    "### Cell 2: Data Loaders and Ground Truth Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e6e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading utilities are defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Load GSM8K Dataset for ground truth answers ---\n",
    "GSM8K_TRAIN: Dataset = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
    "\n",
    "# --- Utility functions to load manifest components and ground truth data ---\n",
    "\n",
    "def load_function_module(tier: str, index: int, model: str) -> ModuleType | None:\n",
    "    \"\"\"Dynamically loads the 'solve.py' module for a given manifest.\"\"\"\n",
    "    py_file_path = PROCESSED_MANIFEST_DIR / tier / str(index) / f\"{model}.py\"\n",
    "    if not py_file_path.exists(): return None\n",
    "    module_name = f\"manifests.val.t{tier}.i{index}.m_{model.replace('.', '_')}\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, py_file_path)\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    return None\n",
    "\n",
    "def load_logical_steps(tier: str, index: int, model: str) -> list[dict] | None:\n",
    "    \"\"\"Loads the 'logical_steps.json' for a given manifest.\"\"\"\n",
    "    json_file_path = PROCESSED_MANIFEST_DIR / tier / str(index) / f\"{model}.json\"\n",
    "    try:\n",
    "        return json.loads(json_file_path.read_text(encoding='utf-8'))\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return None\n",
    "\n",
    "def get_ground_truth_answer(index: int) -> float | None:\n",
    "    \"\"\"Extracts the final numeric answer from the GSM8K dataset.\"\"\"\n",
    "    try:\n",
    "        answer_text = GSM8K_TRAIN[int(index)]['answer']\n",
    "        # The final answer is always after '####'\n",
    "        final_answer_str = answer_text.split('####')[-1].strip().replace(',', '')\n",
    "        return float(final_answer_str)\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def get_original_solution_lines(index: int) -> dict[str, str] | None:\n",
    "    \"\"\"Extracts the original solution into a line-numbered dictionary.\"\"\"\n",
    "    try:\n",
    "        solution_text = GSM8K_TRAIN[int(index)][\"answer\"]\n",
    "        lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "        if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "            lines.pop(-1)\n",
    "        return {f\"L{i+1}\": line for i, line in enumerate(lines)}\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "print(\"Data loading utilities are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75292a0f",
   "metadata": {},
   "source": [
    "### Cell 3: Core Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18f48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core validation functions are defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Helper for consistent number comparison ---\n",
    "def normalize_value(value):\n",
    "    if isinstance(value, float) and value.is_integer(): return int(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "# --- Validation Function 1: Check Final Answer ---\n",
    "def check_answer(solve_function: Callable, ground_truth_answer: float) -> dict:\n",
    "    \"\"\"\n",
    "    Executes the solve() function and compares its result against the ground truth.\n",
    "    \n",
    "    Returns a dictionary with 'status' ('pass', 'fail', 'error') and details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = solve_function()\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"execution_error\", \"details\": str(e), \"computed_answer\": None}\n",
    "    \n",
    "    # Compare normalized values to handle float/int differences (e.g., 15.0 vs 15)\n",
    "    norm_result = normalize_value(result)\n",
    "    norm_truth = normalize_value(ground_truth_answer)\n",
    "    \n",
    "    if norm_result == norm_truth:\n",
    "        return {\"status\": \"pass\", \"details\": None, \"computed_answer\": result}\n",
    "    else:\n",
    "        details = f\"Expected {ground_truth_answer}, but got {result}.\"\n",
    "        return {\"status\": \"fail\", \"details\": details, \"computed_answer\": result}\n",
    "\n",
    "\n",
    "# --- Validation Function 2: Check Template Reconstruction ---\n",
    "def reconstruct_and_validate_lines(\n",
    "    logical_steps: list[dict],\n",
    "    solve_function: Callable,\n",
    "    original_lines: dict[str, str]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Reconstructs solution lines from templates and compares them to the original.\n",
    "    \n",
    "    As you noted, this currently uses a strict string match. It can be enhanced later\n",
    "    with more sophisticated semantic/syntactic comparison.\n",
    "    \"\"\"\n",
    "    # 1. Get the correct variable values by executing the function\n",
    "    try:\n",
    "        src = inspect.getsource(solve_function)\n",
    "        tree = ast.parse(src)\n",
    "        func_def = tree.body[0]\n",
    "        local_env = {}\n",
    "        # Simple execution trace, assuming no Fraction class needed for this step\n",
    "        for stmt in func_def.body:\n",
    "            if isinstance(stmt, ast.Assign):\n",
    "                module_node = ast.Module([stmt], type_ignores=[])\n",
    "                code_obj = compile(module_node, '<string>', 'exec')\n",
    "                exec(code_obj, {}, local_env)\n",
    "        trace = local_env\n",
    "    except Exception:\n",
    "        return {\"match_score\": 0.0, \"mismatched_lines\": list(original_lines.keys())}\n",
    "\n",
    "    # 2. Reconstruct lines using templates and the trace\n",
    "    reconstructed_lines = {}\n",
    "    for step in logical_steps:\n",
    "        ln = step[\"line_number\"]\n",
    "        template = step[\"solution_line_template\"]\n",
    "        try:\n",
    "            # Use .format_map() for safe formatting with missing keys\n",
    "            reconstructed_lines[ln] = template.format_map(trace)\n",
    "        except (KeyError, ValueError):\n",
    "            reconstructed_lines[ln] = \"FORMATTING_ERROR\"\n",
    "\n",
    "    # 3. Compare and score\n",
    "    match_count = 0\n",
    "    mismatched = []\n",
    "    for ln, original_text in original_lines.items():\n",
    "        if ln in reconstructed_lines and reconstructed_lines[ln] == original_text:\n",
    "            match_count += 1\n",
    "        else:\n",
    "            mismatched.append(ln)\n",
    "            \n",
    "    score = match_count / len(original_lines) if original_lines else 1.0\n",
    "    return {\"match_score\": score, \"mismatched_lines\": mismatched}\n",
    "\n",
    "\n",
    "print(\"Core validation functions are defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8612a05",
   "metadata": {},
   "source": [
    "### Cell 4: Validation orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f014283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation orchestrator is defined.\n"
     ]
    }
   ],
   "source": [
    "def validate_manifest(tier: str, index: int, model: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Runs all validation checks for a single manifest and returns a summary report.\n",
    "    \"\"\"\n",
    "    # 1. Load all necessary components\n",
    "    solve_module = load_function_module(tier, index, model)\n",
    "    logical_steps = load_logical_steps(tier, index, model)\n",
    "    ground_truth_answer = get_ground_truth_answer(index)\n",
    "    original_lines = get_original_solution_lines(index)\n",
    "    \n",
    "    if not all([solve_module, logical_steps, ground_truth_answer, original_lines]):\n",
    "        return None # Incomplete data, cannot validate\n",
    "\n",
    "    solve_function = solve_module.solve\n",
    "    \n",
    "    # 2. Run the validation checks\n",
    "    answer_check_result = check_answer(solve_function, ground_truth_answer)\n",
    "    reconstruction_result = reconstruct_and_validate_lines(logical_steps, solve_function, original_lines)\n",
    "    \n",
    "    # 3. Compile the final report\n",
    "    report = {\n",
    "        \"index\": index,\n",
    "        \"tier\": tier,\n",
    "        \"model\": model,\n",
    "        \"answer_check_status\": answer_check_result[\"status\"],\n",
    "        \"reconstruction_score\": reconstruction_result[\"match_score\"],\n",
    "        \"computed_answer\": answer_check_result[\"computed_answer\"],\n",
    "        \"ground_truth_answer\": ground_truth_answer,\n",
    "        \"mismatched_lines\": reconstruction_result[\"mismatched_lines\"]\n",
    "    }\n",
    "    return report\n",
    "\n",
    "print(\"Validation orchestrator is defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25189f59",
   "metadata": {},
   "source": [
    "### Cell 5: Main driver and report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f8d609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Manifest Validation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6daabc37fb46489e8fda00d4811544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tiers:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a6fc403d04485b104c27ffcb1b556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier1:   0%|          | 0/1138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363725e936ec43a78e302f61c0def718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier2:   0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f1ff7820964c95b11b1c0c13e6bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier3:   0%|          | 0/1262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e32ada471940c9b2e9bf753bcb1d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier4:   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Complete ---\n",
      "Validated 4794 manifests.\n",
      "Report saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/tier-manifests-gen-processed/manifest_validation_report.csv\n",
      "\n",
      "--- Summary Report ---\n",
      "Overall Answer Correctness: 99.10%\n",
      "Perfect Reconstruction Rate: 41.01%\n",
      "\n",
      "Breakdown by Model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>answer_check_status</th>\n",
       "      <th>execution_error</th>\n",
       "      <th>fail</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google_gemini-2.5-flash</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_gpt-4.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "answer_check_status      execution_error   fail   pass\n",
       "model                                                 \n",
       "google_gemini-2.5-flash              0.0  0.006  0.993\n",
       "openai_gpt-4.1                       0.0  0.012  0.988"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_full_validation_pipeline():\n",
    "    \"\"\"\n",
    "    Iterates through all processed manifests, validates them, and saves a report.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Full Manifest Validation ---\")\n",
    "    all_reports = []\n",
    "    \n",
    "    tier_dirs = sorted([d for d in PROCESSED_MANIFEST_DIR.iterdir() if d.is_dir() and d.name.startswith('tier')])\n",
    "\n",
    "    for tier_dir in tqdm(tier_dirs, desc=\"Validating Tiers\"):\n",
    "        index_dirs = sorted([d for d in tier_dir.iterdir() if d.is_dir() and d.name.isdigit()], key=lambda p: int(p.name))\n",
    "        \n",
    "        for index_dir in tqdm(index_dirs, desc=f\"Processing {tier_dir.name}\", leave=False):\n",
    "            for model in MODELS:\n",
    "                report = validate_manifest(tier_dir.name, int(index_dir.name), model)\n",
    "                if report:\n",
    "                    all_reports.append(report)\n",
    "\n",
    "    # --- Save the final validation report ---\n",
    "    if all_reports:\n",
    "        report_df = pd.DataFrame(all_reports)\n",
    "        report_path = PROCESSED_MANIFEST_DIR / \"manifest_validation_report.csv\"\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        \n",
    "        print(\"\\n--- Validation Complete ---\")\n",
    "        print(f\"Validated {len(report_df)} manifests.\")\n",
    "        print(f\"Report saved to: {report_path}\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n--- Summary Report ---\")\n",
    "        answer_pass_rate = (report_df['answer_check_status'] == 'pass').mean()\n",
    "        perfect_recon_rate = (report_df['reconstruction_score'] == 1.0).mean()\n",
    "        \n",
    "        print(f\"Overall Answer Correctness: {answer_pass_rate:.2%}\")\n",
    "        print(f\"Perfect Reconstruction Rate: {perfect_recon_rate:.2%}\")\n",
    "        \n",
    "        print(\"\\nBreakdown by Model:\")\n",
    "        display(report_df.groupby('model')['answer_check_status'].value_counts(normalize=True).unstack(fill_value=0).round(3))\n",
    "        \n",
    "    else:\n",
    "        print(\"No manifests were found to validate.\")\n",
    "        \n",
    "# --- Execute the Validation Pipeline ---\n",
    "run_full_validation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0532e45",
   "metadata": {},
   "source": [
    "### Analyze the reconstruction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f526b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded report from: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/tier-manifests-gen-processed/manifest_validation_report.csv\n",
      "\n",
      "--- Reconstruction Score Value Counts ---\n",
      "This table shows how many manifests achieved each specific reconstruction score.\n",
      "A score of 1.0 indicates a perfect match between original and reconstructed lines.\n",
      "\n",
      "reconstruction_score\n",
      "1.000000    1966\n",
      "0.875000       1\n",
      "0.857143       6\n",
      "0.833333      15\n",
      "0.800000      41\n",
      "0.750000     128\n",
      "0.714286       6\n",
      "0.666667     173\n",
      "0.625000       1\n",
      "0.600000      43\n",
      "0.571429      15\n",
      "0.500000     362\n",
      "0.444444       1\n",
      "0.428571       4\n",
      "0.400000      59\n",
      "0.375000       3\n",
      "0.333333     255\n",
      "0.285714      10\n",
      "0.250000     148\n",
      "0.222222       1\n",
      "0.200000      88\n",
      "0.166667      36\n",
      "0.142857      15\n",
      "0.125000      10\n",
      "0.111111       1\n",
      "0.000000    1406\n",
      "Name: count, dtype: int64\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Reconstruction Score Percentages ---\n",
      "reconstruction_score\n",
      "1.000000    41.01%\n",
      "0.875000     0.02%\n",
      "0.857143     0.13%\n",
      "0.833333     0.31%\n",
      "0.800000     0.86%\n",
      "0.750000     2.67%\n",
      "0.714286     0.13%\n",
      "0.666667     3.61%\n",
      "0.625000     0.02%\n",
      "0.600000      0.9%\n",
      "0.571429     0.31%\n",
      "0.500000     7.55%\n",
      "0.444444     0.02%\n",
      "0.428571     0.08%\n",
      "0.400000     1.23%\n",
      "0.375000     0.06%\n",
      "0.333333     5.32%\n",
      "0.285714     0.21%\n",
      "0.250000     3.09%\n",
      "0.222222     0.02%\n",
      "0.200000     1.84%\n",
      "0.166667     0.75%\n",
      "0.142857     0.31%\n",
      "0.125000     0.21%\n",
      "0.111111     0.02%\n",
      "0.000000    29.33%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the validation report and analyzes the reconstruction scores.\n",
    "\n",
    "# --- 1. Define the path to the report ---\n",
    "REPORT_PATH = PROCESSED_MANIFEST_DIR / \"manifest_validation_report.csv\"\n",
    "\n",
    "# --- 2. Load the data and perform the analysis ---\n",
    "try:\n",
    "    report_df = pd.read_csv(REPORT_PATH)\n",
    "    print(f\"Successfully loaded report from: {REPORT_PATH}\\n\")\n",
    "\n",
    "    # --- 3. Get and display the value counts for the reconstruction score ---\n",
    "    # We sort by the score itself (the index) for better readability.\n",
    "    score_counts = report_df['reconstruction_score'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "    print(\"--- Reconstruction Score Value Counts ---\")\n",
    "    print(\"This table shows how many manifests achieved each specific reconstruction score.\")\n",
    "    print(\"A score of 1.0 indicates a perfect match between original and reconstructed lines.\\n\")\n",
    "    print(score_counts)\n",
    "\n",
    "    # --- 4. Also display the same information as percentages ---\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"\\n--- Reconstruction Score Percentages ---\")\n",
    "    score_percentages = report_df['reconstruction_score'].value_counts(normalize=True).sort_index(ascending=False)\n",
    "    \n",
    "    # Format the output for clarity\n",
    "    formatted_percentages = (score_percentages * 100).round(2).astype(str) + '%'\n",
    "    print(formatted_percentages)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Validation report not found at {REPORT_PATH}\")\n",
    "    print(\"Please ensure you have run the full validation pipeline in the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afcfa2bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set Seaborn style\u001b[39;00m\n\u001b[32m      5\u001b[39m sns.set(style=\u001b[33m\"\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m\"\u001b[39m, palette=\u001b[33m\"\u001b[39m\u001b[33mpastel\u001b[39m\u001b[33m\"\u001b[39m, font_scale=\u001b[32m1.2\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", font_scale=1.2)\n",
    "\n",
    "# Plotting the reconstruction score distribution with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=score_counts.index, y=score_counts.values, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax.set_title('Reconstruction Score Distribution', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Reconstruction Score', fontsize=14)\n",
    "ax.set_ylabel('Number of Manifests', fontsize=14)\n",
    "ax.set_xticks(range(len(score_counts.index)))\n",
    "ax.set_xticklabels(score_counts.index, rotation=0)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80067e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
