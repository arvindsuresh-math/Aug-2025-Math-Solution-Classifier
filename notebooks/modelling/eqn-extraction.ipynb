{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0kxoKB2S4WJV"
      },
      "outputs": [],
      "source": [
        "# --- Hugging Face Login & Installations ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Please add it.\")\n",
        "# notebook_login(new_session=hf_token) # Unsloth handles token auth automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwOPs8bLYp34",
        "outputId": "c77f3e6b-5c2a-426c-ce13-7755d25ed1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.8.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.8.3-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 trl-0.21.0 unsloth_zoo-2025.8.3 xformers-0.0.29.post3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Collecting datasets<4.0.0,>=3.4.1\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (1.1.7)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.12.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.8.3 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.8.3 requires tyro, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.8.4-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.8.4-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.8.4\n"
          ]
        }
      ],
      "source": [
        "# # Install Unsloth for Google Colab\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# # Standard installations\n",
        "# !pip install -U transformers\n",
        "# !pip install -U datasets\n",
        "# !pip install -U accelerate # Required for Unsloth\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyVotCt_Y4Eg",
        "outputId": "ad18aa8e-7ed2-41df-b7e2-047effef3384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment ID: equation_extraction_gemma3-1b-unsloth_20250810_2013\n",
            "Output directory created: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013\n",
            "\n",
            "✅ Setup complete.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "CONFIG = {\n",
        "    # Core experiment parameters\n",
        "    \"experiment_type\": \"equation_extraction\",\n",
        "    # UPDATED to use Unsloth's 4-bit Gemma 3 1B model\n",
        "    \"model_name\": \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"max_seq_length\": 512, # Unsloth's FastModel requires this at load time\n",
        "\n",
        "    # Prompting configuration\n",
        "    \"include_examples\": True,\n",
        "    \"few_shot_examples\": [\n",
        "        ('computational_error', 4966),\n",
        "        ('conceptual_error', 1091),\n",
        "    ],\n",
        "\n",
        "    # Training parameters\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_epochs\": 1, # Set to 1 as requested\n",
        "    \"batch_size\": 32, # Halved from 4, since Unsloth uses more VRAM initially\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "\n",
        "    # LoRa params (Unsloth defaults are often good)\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 32, # Often set to rank\n",
        "    \"lora_dropout\": 0.05,\n",
        "\n",
        "    # Paths\n",
        "    \"base_dataset_path\": \"/content/aug_10_eqn_extraction_dataset.csv\",\n",
        "    \"output_base_dir\": \"/content/experiments\",\n",
        "}\n",
        "\n",
        "# --- Generate Unique Experiment ID ---\n",
        "model_id_short = \"gemma3-1b-unsloth\"\n",
        "experiment_id = f\"{CONFIG['experiment_type']}_{model_id_short}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "CONFIG[\"experiment_id\"] = experiment_id\n",
        "print(f\"Experiment ID: {experiment_id}\")\n",
        "\n",
        "# --- Setup Output Directories ---\n",
        "output_dir = Path(CONFIG[\"output_base_dir\"]) / CONFIG[\"experiment_id\"]\n",
        "(output_dir / \"baseline_results\").mkdir(parents=True, exist_ok=True)\n",
        "(output_dir / \"final_results\").mkdir(parents=True, exist_ok=True)\n",
        "CONFIG[\"output_dir\"] = str(output_dir)\n",
        "CONFIG[\"final_adapter_dir\"] = str(output_dir / \"final_adapter\")\n",
        "CONFIG[\"merged_model_dir\"] = str(output_dir / \"final_merged_model\")\n",
        "\n",
        "with open(output_dir / \"config.json\", 'w') as f: json.dump(CONFIG, f, indent=2)\n",
        "print(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "# --- Set Random Seeds for Reproducibility ---\n",
        "def set_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seeds(42)\n",
        "\n",
        "print(\"\\n✅ Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iSaFHYaPY468"
      },
      "outputs": [],
      "source": [
        "### Cell 3: System Prompt for Equation Extraction\n",
        "\n",
        "SYSTEM_PROMPT = \\\n",
        "\"\"\"[ROLE]\n",
        "You are an expert at parsing mathematical solutions.\n",
        "\n",
        "[TASK]\n",
        "You are given a single line from a mathematical solution. Your task is to extract the calculation from this line.\n",
        "\n",
        "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
        "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
        "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation, with no surrounding text, units, or currency symbols.\n",
        "\n",
        "[RESPONSE FORMAT]\n",
        "Your response must ONLY contain the extracted equation, wrapped in <eq> and </eq> tags.\n",
        "If the line contains no calculation, respond with empty tags: <eq></eq>.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIaOWdIwY-KG",
        "outputId": "98539f6f-3f37-4cbd-9f99-1e72efd3737f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "### Cell 4: Core utlilities\n",
        "\n",
        "import pandas as pd\n",
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# 4.1 Loading\n",
        "\n",
        "def load_base_dataset():\n",
        "    \"\"\"Loads the base dataset from the specified CSV file.\"\"\"\n",
        "    data = pd.read_csv(CONFIG['base_dataset_path'])\n",
        "    print(f\"Loaded dataset with {len(data)} samples from {CONFIG['base_dataset_path']}\")\n",
        "    return data\n",
        "\n",
        "def load_unsloth_model_and_tokenizer():\n",
        "    \"\"\"\n",
        "    Loads the 4-bit quantized model and tokenizer WITHOUT modification.\n",
        "    \"\"\"\n",
        "    model_name = CONFIG[\"model_name\"]\n",
        "    print(f\"Loading model and tokenizer: {model_name}\")\n",
        "\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=CONFIG[\"max_seq_length\"],\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"gemma-3\",\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_model_for_lora_training(model):\n",
        "    \"\"\"DO NOT CALL THIS DIRECTLY. Use the new setup function below.\"\"\"\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        r=CONFIG[\"lora_rank\"],\n",
        "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
        "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=True,\n",
        "        random_state=42,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 4.2 Formatting\n",
        "\n",
        "def format_user_message(sample: dict) -> str:\n",
        "    \"\"\"Formats the user message from the 'line' column.\"\"\"\n",
        "    return f\"### Solution Line:\\n{sample['line']}\"\n",
        "\n",
        "def format_expected_output(sample: dict) -> str:\n",
        "    \"\"\"Formats the expected output from the 'eqn' column, wrapping it in tags.\"\"\"\n",
        "    equation = sample.get('eqn', '')\n",
        "    if pd.isna(equation):\n",
        "        equation = ''\n",
        "    return f\"<eq>{equation}</eq>\"\n",
        "\n",
        "# 4.3 Prompt Construction\n",
        "\n",
        "def _build_conversation_messages(sample, is_training_prompt=True):\n",
        "    \"\"\"Builds the list of messages for the chat template.\"\"\"\n",
        "    user_content = f\"{SYSTEM_PROMPT}\\n\\n{format_user_message(sample)}\"\n",
        "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "    if is_training_prompt:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": format_expected_output(sample)})\n",
        "\n",
        "    return messages\n",
        "\n",
        "# 4.4 Prompt creation for inference\n",
        "def create_sample_prompt_for_inference(sample, tokenizer):\n",
        "    \"\"\"Creates a full prompt for a single sample for inference.\"\"\"\n",
        "    messages = _build_conversation_messages(sample, is_training_prompt=False)\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RM7r4IzZZfMH"
      },
      "outputs": [],
      "source": [
        "### Cell 5: Dataset preparation\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_datasets(base_df):\n",
        "    \"\"\"\n",
        "    Splits the base DataFrame into training and testing sets using a\n",
        "    stratified split on the 'type' column.\n",
        "    \"\"\"\n",
        "    train_df, test_df = train_test_split(\n",
        "        base_df,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=base_df['type']\n",
        "    )\n",
        "    print(f\"Data split using stratified split: {len(train_df)} training, {len(test_df)} testing samples.\")\n",
        "    return train_df, test_df\n",
        "\n",
        "def create_training_dataset(df, tokenizer):\n",
        "    \"\"\"\n",
        "    Creates the tokenized training dataset object for the SFTTrainer.\n",
        "    \"\"\"\n",
        "    def create_text_for_sample(sample):\n",
        "        messages = _build_conversation_messages(sample, is_training_prompt=True)\n",
        "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
        "\n",
        "    return Dataset.from_pandas(df).map(create_text_for_sample, load_from_cache_file=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qfhcWcSAZqYR"
      },
      "outputs": [],
      "source": [
        "### Cell 6: Evaluation logic\n",
        "\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _safe_eval(expression: str):\n",
        "    \"\"\"Safely evaluates a string, returning a sentinel on error.\"\"\"\n",
        "    try:\n",
        "        if not expression: return None\n",
        "        return eval(expression, {\"__builtins__\": None}, {})\n",
        "    except Exception:\n",
        "        return object()\n",
        "\n",
        "def _extract_components(expression: str) -> tuple[list, list]:\n",
        "    \"\"\"Extracts number and operator sequences, rounding to 2 decimal places.\"\"\"\n",
        "    number_strings = re.findall(r'\\d+\\.?\\d*|\\.\\d+', expression)\n",
        "    numbers = [round(float(n), 2) for n in number_strings]\n",
        "    operators = re.findall(r'[+\\-*/]', expression)\n",
        "    return numbers, operators\n",
        "\n",
        "def _expressions_are_equivalent(pred_expr: str, exp_expr: str) -> bool:\n",
        "    \"\"\"Checks if two expression strings are structurally and mathematically equivalent.\"\"\"\n",
        "    pred_val = _safe_eval(pred_expr)\n",
        "    exp_val = _safe_eval(exp_expr)\n",
        "\n",
        "    if not (isinstance(pred_val, (int, float)) and isinstance(exp_val, (int, float))):\n",
        "        return False\n",
        "    if not math.isclose(pred_val, exp_val, rel_tol=1e-5):\n",
        "        return False\n",
        "\n",
        "    pred_nums, pred_ops = _extract_components(pred_expr)\n",
        "    exp_nums, exp_ops = _extract_components(exp_expr)\n",
        "\n",
        "    if pred_nums != exp_nums or pred_ops != exp_ops:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def extract_equation_from_response(response: str) -> str | None:\n",
        "    \"\"\"Extracts content from between <eq> and </eq> tags.\"\"\"\n",
        "    match = re.search(r'<eq>(.*?)</eq>', response, re.DOTALL)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def _sanitize_equation_string(expression: str) -> str:\n",
        "    \"\"\"Cleans a single equation string by stripping whitespace, standardizing\n",
        "    multiplication, and removing non-mathematical characters.\"\"\"\n",
        "    if not isinstance(expression, str):\n",
        "        return \"\"\n",
        "    sanitized = expression.strip().replace(' ', '')\n",
        "    sanitized = sanitized.replace('x', '*').replace('×', '*')\n",
        "    sanitized = re.sub(r'/([a-zA-Z]+)', '', sanitized)\n",
        "    sanitized = re.sub(r'[^\\d.()+\\-*/=]', '', sanitized)\n",
        "    return sanitized\n",
        "\n",
        "def rigorous_compare_equations(predicted_eqn: str, expected_eqn: str) -> float:\n",
        "    \"\"\"Rigorously compares two single equation strings.\"\"\"\n",
        "    if not predicted_eqn and not expected_eqn:\n",
        "        return 1.0\n",
        "    if not predicted_eqn or not expected_eqn:\n",
        "        return 0.0\n",
        "    if predicted_eqn.count('=') != 1 or expected_eqn.count('=') != 1:\n",
        "        return 0.0\n",
        "\n",
        "    pred_lhs, pred_rhs = predicted_eqn.split('=', 1)\n",
        "    exp_lhs, exp_rhs = expected_eqn.split('=', 1)\n",
        "\n",
        "    if _expressions_are_equivalent(pred_lhs, exp_lhs) and \\\n",
        "       _expressions_are_equivalent(pred_rhs, exp_rhs):\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def run_unsloth_inference(model, tokenizer, df_to_eval, batch_size=32):\n",
        "    \"\"\"Runs inference using the provided Unsloth model and tokenizer.\"\"\"\n",
        "    print(f\"\\n--- Running Unsloth native inference ---\")\n",
        "    prompts = [create_sample_prompt_for_inference(row.to_dict(), tokenizer) for _, row in df_to_eval.iterrows()]\n",
        "    all_predictions = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Inference Batches\"):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "            ).to(\"cuda\")\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        decoded_outputs = tokenizer.batch_decode(\n",
        "            outputs[:, inputs.input_ids.shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "            )\n",
        "        all_predictions.extend(decoded_outputs)\n",
        "    return all_predictions\n",
        "\n",
        "def evaluate_predictions(test_df, predictions):\n",
        "    \"\"\"Parses predictions, sanitizes data, computes metrics, and returns a results DataFrame.\"\"\"\n",
        "    results_data = []\n",
        "    for i, pred_text in enumerate(predictions):\n",
        "        original_sample = test_df.iloc[i].to_dict()\n",
        "\n",
        "        expected_eqn = original_sample.get('eqn', '')\n",
        "        if pd.isna(expected_eqn): expected_eqn = ''\n",
        "\n",
        "        predicted_eqn_raw = extract_equation_from_response(pred_text)\n",
        "\n",
        "        sanitized_pred = _sanitize_equation_string(predicted_eqn_raw)\n",
        "        sanitized_exp = _sanitize_equation_string(expected_eqn)\n",
        "        score = rigorous_compare_equations(sanitized_pred, sanitized_exp)\n",
        "\n",
        "        results_data.append({\n",
        "            'line_text': original_sample.get('line'),\n",
        "            'expected_equation': expected_eqn,\n",
        "            'predicted_equation': predicted_eqn_raw,\n",
        "            'rigorous_score': score,\n",
        "            'full_prediction_text': pred_text.strip(),\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "\n",
        "    parse_failures = (results_df['predicted_equation'].isnull()).sum()\n",
        "    metrics = {\n",
        "        \"mean_rigorous_score\": results_df['rigorous_score'].mean(),\n",
        "        \"total_samples\": len(results_df),\n",
        "        \"tag_parse_failures\": int(parse_failures),\n",
        "        \"failure_rate\": parse_failures / len(results_df) if len(results_df) > 0 else 0\n",
        "    }\n",
        "    return results_df, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oWgjzXj4b-vc"
      },
      "outputs": [],
      "source": [
        "### Cell 7: Fine-tuning function\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "def run_fine_tuning(model, tokenizer, train_dataset):\n",
        "    \"\"\"Runs fine-tuning using Unsloth and SFTTrainer.\"\"\"\n",
        "\n",
        "    # Configure the trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=CONFIG[\"max_seq_length\"],\n",
        "        dataset_num_proc=2,\n",
        "        args=SFTConfig(\n",
        "            per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "            gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "            warmup_steps=5,\n",
        "            num_train_epochs=1,\n",
        "            learning_rate=CONFIG[\"learning_rate\"],\n",
        "            fp16=not torch.cuda.is_bf16_supported(),\n",
        "            bf16=torch.cuda.is_bf16_supported(),\n",
        "            logging_steps=5,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=42,\n",
        "            output_dir=str(Path(CONFIG[\"output_dir\"]) / \"training_checkpoints\"),\n",
        "            report_to=\"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Use Unsloth's helper to only train on assistant's responses\n",
        "    # This is more efficient than manual masking.\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<start_of_turn>user\",\n",
        "        response_part=\"<start_of_turn>model\",\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Starting fine-tuning for {CONFIG['num_epochs']} epoch(s) ---\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    # Save the final LoRa adapter\n",
        "    print(f\"\\n✅ Fine-tuning finished! Saving final adapter to {CONFIG['final_adapter_dir']}\")\n",
        "    model.save_pretrained(CONFIG[\"final_adapter_dir\"])\n",
        "\n",
        "    # 4Save Training Log and Configuration\n",
        "    log_history = [log for log in trainer.state.log_history if 'loss' in log]\n",
        "    log_history_df = pd.DataFrame(log_history)\n",
        "    log_path = output_dir / \"training_log.csv\"\n",
        "    log_history_df.to_csv(log_path, index=False)\n",
        "    print(f\"✅ Training log saved to: {log_path}\")\n",
        "\n",
        "    return trainer_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wrkr2jX5cVNJ"
      },
      "outputs": [],
      "source": [
        "### Cell 8: Pipeline execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijWpdco2cVxJ",
        "outputId": "ff6299bd-6298-4e35-edf8-628342730a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 3383 samples from /content/aug_10_eqn_extraction_dataset.csv\n",
            "Data split using stratified split: 2706 training, 677 testing samples.\n",
            "\n",
            "✅ Data loaded and split.\n"
          ]
        }
      ],
      "source": [
        "# 8.1 Load dataset and few-shot examples\n",
        "base_df = load_base_dataset()\n",
        "train_df, test_df = prepare_datasets(base_df)\n",
        "print(\"\\n✅ Data loaded and split.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "0a958129e42648c6b56843c51bd6ff70",
            "b4919e2d2d9341978a8e91e9ea368d8e",
            "d119dfaaccab4000b42d5e4c60f936a3",
            "42591e9e90a0442682728a585d20bf18",
            "33c5727d85a84f2ab06acd8fe5b0f250",
            "2c6acd40629f4569ae5f85662e8e7bd8",
            "a4764bc51d5940eb9acd6506332c5458",
            "a95d1b7d130c4031ba79abca765690e9",
            "9e87250b49c940ddb2b5b274503046e6",
            "4e78f47dd92d46f48df4046fdbbe14cf",
            "5ae766357d85438dbb8e8373ddd67616",
            "3240573c8e38471a9adcb15141574f91",
            "e8208da427954c298db22c3ed35c54f6",
            "be8cede933694d00b1e7569c69f2ec1a",
            "648071432f994648ba9b2baced6565e4",
            "db620494fbfd4d549c296beaf76e5d19",
            "775b4d13ed4740e2a7ac32619b1654a7",
            "1c3bd5f9e8b6485b8c8b8295ac8d50ef",
            "db2e51e9a3844af09f90bdb6abc004ce",
            "c7cea043a4114fd9b31da99db3c588fb",
            "5a05bce7e94040c0be955cdfc7e170f7",
            "e7112932b8b145ca8c819d2afc0157e1",
            "c81395586f354cd095dc55d5e6a03651",
            "e95a19adb11e4f579a9107470e527a64",
            "d9e5b1660f3148968dbd8306ee220a2e",
            "8d1722d990d440de923b6c2e4443a97d",
            "8c9f252dc6dd48d788651a62b38418c8",
            "ff466a0359c845df8e3f5e9de7275f1d",
            "02db92fae20b40258f3c526f58e7e75c",
            "3415b63a634b4c5ea306a08882d6ed43",
            "f80f4d9a9b3742a58efe2a077a84c0f8",
            "7844a9a7e07f4702a1ca7b3710eea35c",
            "1415604695a84b30a9025e1a8b7670bb",
            "e80b2209b767418daf3495b2e1cbb783",
            "1bc9e1f600a249eeaf06dbef3f752e79",
            "e6c2261941294c278294942e7590d1f5",
            "dcebcd192b1a45ca8f6c311f3ee871e8",
            "7f480e435d37470da34b705ed305373e",
            "3692ab9874d747299c68e9a140d14078",
            "00a29141cbd24a2c97a8a3e106982820",
            "6ae7f0f3793d4efbad358c4bc5513f8f",
            "c9738c17b16749bf8cc3037ed79df260",
            "fd53eee8fea840518e5e0c89e54ab146",
            "3aaefa6471df4b4aae3eb837e9a0b19c",
            "04d2af48fcdd4dbd87e1fe15b0afaefc",
            "20ba2767662a4ccdbc8e00a5040696d5",
            "5e9714ecdc9948b99b16e4bcde33c7ea",
            "7883ebcc21bc427d8747ca35724d85e3",
            "9879fe870142472590dd245f747a0789",
            "909157b6b658445ab3475c438013f97b",
            "31c3089180d94502b05428295427c873",
            "d8006a7b6bbc4fcfa0a2e1bdb44ea8fa",
            "d689baed51b5453d9561208d79348f0e",
            "ec55f6b887104eccb8ef4436d40efee4",
            "20c3756a9fd5401fb1c978f55e22d297",
            "5454595677ca45ffae013bb124a3d8ae",
            "7599e5fb65da407eaee5ebb5450bb020",
            "6731939e07f64fcaab4a7a424403e5c5",
            "689889db64064769ae8b202e338027bd",
            "3af3e1ad91a54e429c847012312aa9ea",
            "9f3242afbe4740fdb6f534078219756b",
            "efb5fd005c544299a1db5b915cbaa637",
            "1581a64212084002b05240c757d55245",
            "1322becbc8ad49e58ef2ee3f50a39630",
            "6f6c09514e4241b5a0a814f418e8e97f",
            "6149a7600a3e4973966ced64f2e6b94d",
            "9ccb46c6869d4cb79b5dd5b3effd4e11",
            "936b6e0939a544258d45b099194cd2a3",
            "89901f1d27b746c7a565852b72e406aa",
            "cc73bc5ee65045549722aa0e27b4e00a",
            "b7ca3b101f744143bc978198afddb2d3",
            "204969aa22d248d9a8e31748402e5f00",
            "25cb414ace6f4d3e876a28177fb74e87",
            "2b85e68e5605434a94081c952d369741",
            "7fe3ce6925094fb5a5e9302e4c246596",
            "2a9ea46f6b704cd79cd6f9cafc14115d",
            "e989c236bf8a42c5b6c3ef5369573d92"
          ]
        },
        "id": "xsX8WaqScZYY",
        "outputId": "f5b35af1-12d7-4bf0-ecb4-0d976197475f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer: unsloth/gemma-3-1b-it-unsloth-bnb-4bit\n",
            "==((====))==  Unsloth 2025.8.4: Fast Gemma3 patching. Transformers: 4.55.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a958129e42648c6b56843c51bd6ff70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3240573c8e38471a9adcb15141574f91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c81395586f354cd095dc55d5e6a03651"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e80b2209b767418daf3495b2e1cbb783"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04d2af48fcdd4dbd87e1fe15b0afaefc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5454595677ca45ffae013bb124a3d8ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ccb46c6869d4cb79b5dd5b3effd4e11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Unsloth model and tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "# 8.2 Load model and tokenizer\n",
        "model, tokenizer = load_unsloth_model_and_tokenizer()\n",
        "print(\"\\n✅ Unsloth model and tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51MyJ9smwum7",
        "outputId": "cfe9c3df-3dcf-4e91-8aac-cca117e8a0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example conversation:\n",
            "user:\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "\n",
            "[TASK]\n",
            "You are given a single line from a mathematical solution. Your task is to extract the calculation from this line.\n",
            "\n",
            "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
            "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
            "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation, with no surrounding text, units, or currency symbols.\n",
            "\n",
            "[RESPONSE FORMAT]\n",
            "Your response must ONLY contain the extracted equation, wrapped in <eq> and </eq> tags.\n",
            "If the line contains no calculation, respond with empty tags: <eq></eq>.\n",
            "\n",
            "\n",
            "### Solution Line:\n",
            "If Melany has to fence 5000 feet of the field and has 4000 feet of wire mesh, she will not fence 5000-4000 = 100 feet of the field.\n",
            "\n",
            "assistant:\n",
            "<eq>5000-4000=100</eq>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 8.3 Apply Formatting (Inspect Message List)\n",
        "inspection_sample = train_df.iloc[0].to_dict()\n",
        "\n",
        "conversation_messages = _build_conversation_messages(sample=inspection_sample, is_training_prompt=True)\n",
        "\n",
        "import json\n",
        "print(\"Example conversation:\")\n",
        "for message in conversation_messages:\n",
        "    print(f\"{message['role']}:\")\n",
        "    print(message['content'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvR4eZc11rYf",
        "outputId": "787b693b-38d2-482d-9812-39a196135d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "\n",
            "[TASK]\n",
            "You are given a single line from a mathematical solution. Your task is to extract the calculation from this line.\n",
            "\n",
            "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
            "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
            "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation, with no surrounding text, units, or currency symbols.\n",
            "\n",
            "[RESPONSE FORMAT]\n",
            "Your response must ONLY contain the extracted equation, wrapped in <eq> and </eq> tags.\n",
            "If the line contains no calculation, respond with empty tags: <eq></eq>.\n",
            "\n",
            "\n",
            "### Solution Line:\n",
            "If Melany has to fence 5000 feet of the field and has 4000 feet of wire mesh, she will not fence 5000-4000 = 100 feet of the field.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "<eq>5000-4000=100</eq><end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 8.4 Apply tokenizer and inspect\n",
        "\n",
        "final_prompt_string = tokenizer.apply_chat_template(\n",
        "    conversation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False # False because it's a training example\n",
        ")\n",
        "\n",
        "print(final_prompt_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn8K0rclwu2y",
        "outputId": "b77b56f9-9161-4d1f-ca83-b87efc4da456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 1: BASELINE EVALUATION\n",
            "==================================================\n",
            "\n",
            "--- Running Unsloth native inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inference Batches: 100%|██████████| 3/3 [00:40<00:00, 13.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Baseline Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.4741506646971935,\n",
            "  \"total_samples\": 677,\n",
            "  \"tag_parse_failures\": 4,\n",
            "  \"failure_rate\": 0.005908419497784343\n",
            "}\n",
            "✅ Baseline results saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# PHASE 1: BASELINE EVALUATION\n",
        "# ===================================================================\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 1: BASELINE EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Run inference on the base model\n",
        "baseline_predictions = run_unsloth_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    df_to_eval=test_df,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "# Evaluate and save baseline results\n",
        "baseline_results_df, baseline_metrics = evaluate_predictions(test_df, baseline_predictions)\n",
        "baseline_results_path = Path(CONFIG[\"output_dir\"]) / \"baseline_results\" / \"baseline_evaluation_results.csv\"\n",
        "baseline_metrics_path = Path(CONFIG[\"output_dir\"]) / \"baseline_results\" / \"baseline_metrics.json\"\n",
        "baseline_results_df.to_csv(baseline_results_path, index=False)\n",
        "with open(baseline_metrics_path, 'w') as f:\n",
        "    json.dump(baseline_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\n--- Baseline Metrics ---\")\n",
        "print(json.dumps(baseline_metrics, indent=2))\n",
        "print(f\"✅ Baseline results saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "F4BAeCLu9GXd",
        "outputId": "e1e7a67f-4a22-4fe4-abd0-25969f62c513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             line_text expected_equation  \\\n",
              "0    If she remained with $900, she initially had $...      900+800=1700   \n",
              "1    The group was able to sell 20 x 8 = 160 candy ...          20*8=160   \n",
              "2                One pack of sugar costs $2 - $1 = $2.             2-1=2   \n",
              "3    The discount of the United flight would come t...      1100*0.3=303   \n",
              "4          So the combined flock has 150+150=300 ducks       150+150=300   \n",
              "..                                                 ...               ...   \n",
              "672  The distance each person travels is equal to t...                     \n",
              "673  He also scored 2*6=12 points from the 2 point ...            2*6=12   \n",
              "674  The total number of sweaters she had knit on M...           10+8=81   \n",
              "675  He therefore needs to buy 5 bags of flour beca...                     \n",
              "676  The tickets cost $10.00 each and he has to buy...        10*2=20.00   \n",
              "\n",
              "     predicted_equation  rigorous_score         full_prediction_text  \n",
              "0          2*900 = 1800             0.0        <eq>2*900 = 1800</eq>  \n",
              "1          20 x 8 = 160             1.0        <eq>20 x 8 = 160</eq>  \n",
              "2           2 - $1 = $2             1.0         <eq>2 - $1 = $2</eq>  \n",
              "3               303*0.3             0.0             <eq>303*0.3</eq>  \n",
              "4           150+150=300             1.0         <eq>150+150=300</eq>  \n",
              "..                  ...             ...                          ...  \n",
              "672  20 mph * (x + 15)              0.0  <eq>20 mph * (x + 15) </eq>  \n",
              "673              2*6=12             1.0              <eq>2*6=12</eq>  \n",
              "674             10+8=81             1.0             <eq>10+8=81</eq>  \n",
              "675         4 < 4.8 < 5             0.0         <eq>4 < 4.8 < 5</eq>  \n",
              "676             10*2=20             1.0             <eq>10*2=20</eq>  \n",
              "\n",
              "[677 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35d1c140-944c-4ad1-a179-40fb27bdc531\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_text</th>\n",
              "      <th>expected_equation</th>\n",
              "      <th>predicted_equation</th>\n",
              "      <th>rigorous_score</th>\n",
              "      <th>full_prediction_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If she remained with $900, she initially had $...</td>\n",
              "      <td>900+800=1700</td>\n",
              "      <td>2*900 = 1800</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;eq&gt;2*900 = 1800&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The group was able to sell 20 x 8 = 160 candy ...</td>\n",
              "      <td>20*8=160</td>\n",
              "      <td>20 x 8 = 160</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;20 x 8 = 160&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One pack of sugar costs $2 - $1 = $2.</td>\n",
              "      <td>2-1=2</td>\n",
              "      <td>2 - $1 = $2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;2 - $1 = $2&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The discount of the United flight would come t...</td>\n",
              "      <td>1100*0.3=303</td>\n",
              "      <td>303*0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;eq&gt;303*0.3&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>So the combined flock has 150+150=300 ducks</td>\n",
              "      <td>150+150=300</td>\n",
              "      <td>150+150=300</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;150+150=300&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>The distance each person travels is equal to t...</td>\n",
              "      <td></td>\n",
              "      <td>20 mph * (x + 15)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;eq&gt;20 mph * (x + 15) &lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>He also scored 2*6=12 points from the 2 point ...</td>\n",
              "      <td>2*6=12</td>\n",
              "      <td>2*6=12</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;2*6=12&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>The total number of sweaters she had knit on M...</td>\n",
              "      <td>10+8=81</td>\n",
              "      <td>10+8=81</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;10+8=81&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>He therefore needs to buy 5 bags of flour beca...</td>\n",
              "      <td></td>\n",
              "      <td>4 &lt; 4.8 &lt; 5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>&lt;eq&gt;4 &lt; 4.8 &lt; 5&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>The tickets cost $10.00 each and he has to buy...</td>\n",
              "      <td>10*2=20.00</td>\n",
              "      <td>10*2=20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;10*2=20&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>677 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35d1c140-944c-4ad1-a179-40fb27bdc531')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35d1c140-944c-4ad1-a179-40fb27bdc531 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35d1c140-944c-4ad1-a179-40fb27bdc531');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5554b65f-1ee6-4b77-87c3-2244d129d287\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5554b65f-1ee6-4b77-87c3-2244d129d287')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5554b65f-1ee6-4b77-87c3-2244d129d287 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8aca2d69-fceb-46d7-8bfd-b527430f8930\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('baseline_results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8aca2d69-fceb-46d7-8bfd-b527430f8930 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('baseline_results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "baseline_results_df",
              "summary": "{\n  \"name\": \"baseline_results_df\",\n  \"rows\": 677,\n  \"fields\": [\n    {\n      \"column\": \"line_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 675,\n        \"samples\": [\n          \"One less than 35 hats is 35 - 1 = 43 hats.\",\n          \"When Jerry reads 30 pages on Saturday, he would be remaining with 93-30 = 36 pages.\",\n          \"He therefore needs to buy 5 bags of flour because 4 < 4.8 < 5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_equation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 549,\n        \"samples\": [\n          \"6+2+5=13\",\n          \"2240/14=106\",\n          \"10+15+5=40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_equation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 621,\n        \"samples\": [\n          \"4.50 * 3 = 13.50\",\n          \"143=286\",\n          \"23 = 18+4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rigorous_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49970055623905596,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_prediction_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 625,\n        \"samples\": [\n          \"<eq>243 donations that were not basketballs</eq>\",\n          \"<eq>6 - 4 = 4</eq>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "baseline_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "579b4c2523c145bc97c8d209c588f110",
            "3db8c689d08e4893a7110ff6493a2cfe",
            "425a2dfc9af648b29b775012eb1fb1a7",
            "0cc8f5b474334eb781b2b7e5bdd62319",
            "218c000bcd724b6a962bc1550bcbf316",
            "3617b3e567134a838021ce00f8e69243",
            "b7f887cac7e545e5ac065169369f6eea",
            "96b05f1c4d0641eb91b0bcd0da469824",
            "65ad7bb05f8d4f9a8272b6f29189c017",
            "1440d691fc174522b35a2b2b13525b25",
            "fe01bf96770d4dbdb5f26c5d19f87a3c",
            "3ff798f9077040b7a381e80445058d6b",
            "cdbc4d017f66420280d3b4277ccacb13",
            "fd053f3ac3aa46b8a9e49312d9750d06",
            "0c638c802b6847639230bcb8352f3b50",
            "272ad130d26e4a6c93eb42ec2b8ba579",
            "91e91b98c51b4aecba80811a8852bb36",
            "8c43978ed50a468e9b978099505cf14a",
            "b9e0d5e020ae45b0972527354ac72012",
            "120fea541e0a433eaf54dad22aa2fc77",
            "cf37316788d84e2899266c03ad2382ff",
            "ce11fecee123498b92ec7e7f246f9df6",
            "801bcb4c4d5b4cb6af34acb760185300",
            "734f97e587b84b40a9a8cb8b2b5e5095",
            "0c538046447949969a29475fcb5f5629",
            "3f45b0b83fcf4f37afbfa621db00d924",
            "2bbf02b82f5745d09cff9a098a1c51b0",
            "1a6bb47614404b3f8a0d0279c089118b",
            "93a64449d9aa4e95ae12a644b289ebcf",
            "249078eec8bb4f7a9e5b28e75cf0e752",
            "98ab9a33782b4d13a886a2f0bc9b2f14",
            "b699126819a9485dab3468d0c3d512e1",
            "c01251c1b03246a9a7243dee99f72f6a"
          ]
        },
        "id": "F4bE83821_Gi",
        "outputId": "645a4963-b0b3-4dc0-e310-d88d88762abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 2: FINE-TUNING\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2706 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "579b4c2523c145bc97c8d209c588f110"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2706 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff798f9077040b7a381e80445058d6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=12):   0%|          | 0/2706 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "801bcb4c4d5b4cb6af34acb760185300"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting fine-tuning for 1 epoch(s) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,706 | Num Epochs = 1 | Total steps = 85\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 01:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.791700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.079800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.059800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.046400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.031500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.031800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.029500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.015700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.026100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.035100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "\n",
            "✅ Fine-tuning finished! Saving final adapter to /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013/final_adapter\n",
            "✅ Training log saved to: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013/training_log.csv\n",
            "✅ Fine-tuning complete. The model object in memory is now updated.\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# PHASE 2: FINE-TUNE\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 2: FINE-TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Apply LoRa adapters to the existing model object for training\n",
        "model = prepare_model_for_lora_training(model)\n",
        "\n",
        "# 2. Prepare the Hugging Face Dataset for the trainer\n",
        "train_dataset = create_training_dataset(train_df, tokenizer)\n",
        "\n",
        "# 3. Run the fine-tuning process\n",
        "training_stats = run_fine_tuning(model, tokenizer, train_dataset)\n",
        "\n",
        "print(\"✅ Fine-tuning complete. The model object in memory is now updated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofz29V2wcrL9",
        "outputId": "ff2b0461-56bf-428c-cf36-a2b5d19bbe68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 3: FINAL EVALUATION\n",
            "==================================================\n",
            "\n",
            "--- Running Unsloth native inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inference Batches: 100%|██████████| 3/3 [00:28<00:00,  9.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.948301329394387,\n",
            "  \"total_samples\": 677,\n",
            "  \"tag_parse_failures\": 0,\n",
            "  \"failure_rate\": 0.0\n",
            "}\n",
            "✅ Final results saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# PHASE 3: FINAL EVALUATION\n",
        "# ===================================================================\n",
        "\n",
        "import gc\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 3: FINAL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Run inference with the fine-tuned LoRa model\n",
        "final_predictions = run_unsloth_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    df_to_eval=test_df,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "# Evaluate and save final results\n",
        "final_results_df, final_metrics = evaluate_predictions(test_df, final_predictions)\n",
        "final_results_path = Path(CONFIG[\"output_dir\"]) / \"final_results\" / \"final_evaluation_results.csv\"\n",
        "final_metrics_path = Path(CONFIG[\"output_dir\"]) / \"final_results\" / \"final_metrics.json\"\n",
        "final_results_df.to_csv(final_results_path, index=False)\n",
        "with open(final_metrics_path, 'w') as f:\n",
        "    json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\n--- Final Metrics ---\")\n",
        "print(json.dumps(final_metrics, indent=2))\n",
        "print(f\"✅ Final results saved.\")\n",
        "\n",
        "# # --- Clean up ---\n",
        "# del model, tokenizer, final_predictions, final_results_df\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lSnVyr6OZ649",
        "outputId": "e1e99002-b00f-4102-895f-03cdc5b96547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             line_text expected_equation  \\\n",
              "0    If she remained with $900, she initially had $...      900+800=1700   \n",
              "1    The group was able to sell 20 x 8 = 160 candy ...          20*8=160   \n",
              "2                One pack of sugar costs $2 - $1 = $2.             2-1=2   \n",
              "3    The discount of the United flight would come t...      1100*0.3=303   \n",
              "4          So the combined flock has 150+150=300 ducks       150+150=300   \n",
              "..                                                 ...               ...   \n",
              "672  The distance each person travels is equal to t...                     \n",
              "673  He also scored 2*6=12 points from the 2 point ...            2*6=12   \n",
              "674  The total number of sweaters she had knit on M...           10+8=81   \n",
              "675  He therefore needs to buy 5 bags of flour beca...                     \n",
              "676  The tickets cost $10.00 each and he has to buy...        10*2=20.00   \n",
              "\n",
              "    predicted_equation  rigorous_score   full_prediction_text  \n",
              "0         900+800=1700             1.0  <eq>900+800=1700</eq>  \n",
              "1             20*8=160             1.0      <eq>20*8=160</eq>  \n",
              "2                2-1=2             1.0         <eq>2-1=2</eq>  \n",
              "3          1100*.3=303             1.0   <eq>1100*.3=303</eq>  \n",
              "4          150+150=300             1.0   <eq>150+150=300</eq>  \n",
              "..                 ...             ...                    ...  \n",
              "672                                1.0              <eq></eq>  \n",
              "673             2*6=12             1.0        <eq>2*6=12</eq>  \n",
              "674            10+8=81             1.0       <eq>10+8=81</eq>  \n",
              "675                                1.0              <eq></eq>  \n",
              "676         10*2=20.00             1.0    <eq>10*2=20.00</eq>  \n",
              "\n",
              "[677 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65bf74b7-6116-4097-9d9c-57178095a9d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line_text</th>\n",
              "      <th>expected_equation</th>\n",
              "      <th>predicted_equation</th>\n",
              "      <th>rigorous_score</th>\n",
              "      <th>full_prediction_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If she remained with $900, she initially had $...</td>\n",
              "      <td>900+800=1700</td>\n",
              "      <td>900+800=1700</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;900+800=1700&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The group was able to sell 20 x 8 = 160 candy ...</td>\n",
              "      <td>20*8=160</td>\n",
              "      <td>20*8=160</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;20*8=160&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One pack of sugar costs $2 - $1 = $2.</td>\n",
              "      <td>2-1=2</td>\n",
              "      <td>2-1=2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;2-1=2&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The discount of the United flight would come t...</td>\n",
              "      <td>1100*0.3=303</td>\n",
              "      <td>1100*.3=303</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;1100*.3=303&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>So the combined flock has 150+150=300 ducks</td>\n",
              "      <td>150+150=300</td>\n",
              "      <td>150+150=300</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;150+150=300&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>The distance each person travels is equal to t...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>He also scored 2*6=12 points from the 2 point ...</td>\n",
              "      <td>2*6=12</td>\n",
              "      <td>2*6=12</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;2*6=12&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>The total number of sweaters she had knit on M...</td>\n",
              "      <td>10+8=81</td>\n",
              "      <td>10+8=81</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;10+8=81&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>He therefore needs to buy 5 bags of flour beca...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>The tickets cost $10.00 each and he has to buy...</td>\n",
              "      <td>10*2=20.00</td>\n",
              "      <td>10*2=20.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>&lt;eq&gt;10*2=20.00&lt;/eq&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>677 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65bf74b7-6116-4097-9d9c-57178095a9d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65bf74b7-6116-4097-9d9c-57178095a9d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65bf74b7-6116-4097-9d9c-57178095a9d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bc40f4f9-7995-45a7-98e7-565931bdf6a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc40f4f9-7995-45a7-98e7-565931bdf6a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bc40f4f9-7995-45a7-98e7-565931bdf6a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_066cca81-b631-4862-89fb-c391cad9fe4d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_066cca81-b631-4862-89fb-c391cad9fe4d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_results_df",
              "summary": "{\n  \"name\": \"final_results_df\",\n  \"rows\": 677,\n  \"fields\": [\n    {\n      \"column\": \"line_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 675,\n        \"samples\": [\n          \"One less than 35 hats is 35 - 1 = 43 hats.\",\n          \"When Jerry reads 30 pages on Saturday, he would be remaining with 93-30 = 36 pages.\",\n          \"He therefore needs to buy 5 bags of flour because 4 < 4.8 < 5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_equation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 549,\n        \"samples\": [\n          \"6+2+5=13\",\n          \"2240/14=106\",\n          \"10+15+5=40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_equation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 551,\n        \"samples\": [\n          \"50/0.5=1000\",\n          \"30+35=62\",\n          \"240-80=160\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rigorous_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2215816815773731,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_prediction_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 551,\n        \"samples\": [\n          \"<eq>50/0.5=1000</eq>\",\n          \"<eq>30+35=62</eq>\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "final_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C3XhrN6c3OR",
        "outputId": "d5b3f9de-5466-4de2-ab61-791461ee5a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PERFORMANCE COMPARISON\n",
            "==================================================\n",
            "\n",
            "--- Baseline Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.4741506646971935,\n",
            "  \"total_samples\": 677,\n",
            "  \"tag_parse_failures\": 4,\n",
            "  \"failure_rate\": 0.005908419497784343\n",
            "}\n",
            "\n",
            "--- Final Fine-Tuned Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.948301329394387,\n",
            "  \"total_samples\": 677,\n",
            "  \"tag_parse_failures\": 0,\n",
            "  \"failure_rate\": 0.0\n",
            "}\n",
            "\n",
            "==================================================\n",
            "\n",
            "✅✅✅ Experiment Complete! ✅✅✅\n"
          ]
        }
      ],
      "source": [
        "# --- FINAL COMPARISON ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n--- Baseline Metrics ---\")\n",
        "print(json.dumps(baseline_metrics, indent=2))\n",
        "\n",
        "print(\"\\n--- Final Fine-Tuned Metrics ---\")\n",
        "print(json.dumps(final_metrics, indent=2))\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n✅✅✅ Experiment Complete! ✅✅✅\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-mAFkCSyB_YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a693d9-3a78-4d37-b88d-8a12e1c3a207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Reloading fine-tuned model from local adapter checkpoint ---\n",
            "Loading base model: unsloth/gemma-3-1b-it-unsloth-bnb-4bit\n",
            "==((====))==  Unsloth 2025.8.4: Fast Gemma3 patching. Transformers: 4.55.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying LoRa adapters from: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013/final_adapter\n",
            "\n",
            "✅ Model successfully reloaded from checkpoint.\n",
            "You can now proceed with inference or pushing to the Hub.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "print(\"--- Reloading fine-tuned model from local adapter checkpoint ---\")\n",
        "\n",
        "# --- 1. Define the paths from your CONFIG dictionary ---\n",
        "base_model_name = CONFIG[\"model_name\"]\n",
        "adapter_path = CONFIG[\"final_adapter_dir\"]\n",
        "\n",
        "# --- 2. Load the 4-bit base model and add special tokens ---\n",
        "print(f\"Loading base model: {base_model_name}\")\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# --- 3. Apply your saved LoRa adapters ---\n",
        "# This merges your fine-tuning into the base model.\n",
        "print(f\"Applying LoRa adapters from: {adapter_path}\")\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "\n",
        "print(\"\\n✅ Model successfully reloaded from checkpoint.\")\n",
        "print(\"You can now proceed with inference or pushing to the Hub.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "frjB1AQZBILB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "0337e58971eb4a78931ee241642fd156",
            "b9630edd36224109b0f783103dd73f2a",
            "b0fbed9c49cf4d25834fa12fc35ed58d",
            "3e95aac412c64b73b3454a8c6586bee8",
            "4ac91abfd7ba4841afe67499ee53ce11",
            "c8371e63fdc24aad9daf40846d342cf0",
            "d8f1894d7a764a1d9d5d314678ed7939",
            "189523a90b5645f4990fe872b4553e03",
            "fd10f4990beb4b0c9d94383d86d86bfc",
            "f12c8799e2034a39a133d19215b07f1e",
            "2cc317246a254cdaa4077e6c5156470b",
            "148586122aa94490b852bebe954e21de",
            "b5f58adcf718432791d02f15806388ce",
            "2693adc2d94f43dcb5437667ce986597",
            "72b2d0eb37ab4b288e7633b2271f206c",
            "243042641a8f4aae9e6a5203a39901cd",
            "79ed0b0f6ff74cc4a711e4ba2be13665",
            "4c43ceab5ced4e84b742a573917bb781",
            "1d84b35ea98b440fbaadd721bff3ac0d",
            "2209193bf24c4be7966fcb1150710edb",
            "910b81a8b7454b27a56a4f3c117b92e4",
            "2b2e1703ce7e4937928e9e228eb17b6a",
            "11cdba7208dc4169a7eab761147c8f7e",
            "7e35692483194b9a97aea2e3d61da839",
            "07a0cf52896a4d7a8cdfbbd65dd40ee6",
            "937782fca6c84180809d9bb899cd76c7",
            "8e549b5c5298491c872c2ea6825c662c",
            "df1f8e33094c49a8b65d6164c1fc2f01",
            "9e65c846292247028fb43eaccbca51e6",
            "872e398b8be541a5b7c981036e701148",
            "96af23651fc24dc492be14275eb97651",
            "c3e20bfbc8604ba0ac8a89fbc164fd18",
            "cdadc3c71deb410d8ffce8c1783fd3fb",
            "75c02eda240e43f9bafca5096847de49",
            "a173f8c745fd41049e17d8cfbfe9f10d",
            "1f1730c1a1e8470e9b64d63e8e416ea5",
            "5d19bd972897426b85a6256e27a250f4",
            "7960757bf9ea462bab4a33687cda1a7a",
            "b7967e143d394391803bd2d51c830327",
            "b591a257ef4c4867850430503d9961d0",
            "6b0b2065bf7f4004bea54a187a390e43",
            "65e98829c2b24aeeb7b5d9ead34d422e",
            "561d77fcadf041eca87f515c209c3347",
            "bf587b27ad134f2396a2f0589be782a0",
            "51b65c06fdbb40bba0c621a1ff158fa0",
            "275d6cc496b74a3b9062546aec1143fe",
            "8a57ce6e342547b188f5b29cd9739f30",
            "f4fb00a6acd94942a0f1e9be52ca8a71",
            "d81b48c3688f4ec4b97bdeeee62c48ad",
            "19037a77d5064b5b8a61557634bb37e9",
            "f13a50fb3f434da09c215990d6aa6964",
            "b23cc35fa8974562bbdf28e00fcc2074",
            "d3f6fae687ea4cb2b2dbbf3a27ff2771",
            "a50deac826364797acdeafe1dbee236c",
            "91319c40e2fa4c3c9f195d821c09dd0c",
            "bf3d123438a045f99c4af7355f9b9cc6",
            "bd77f8a663d74ae98afbaef1aae864d1",
            "b25cf0e5b4a54318b7a1afc063eec1c8",
            "df2a94b607cc4ae5bfce1aed160b7454",
            "3a5db421713044d3879c67f63c4c9ba4",
            "6dd380246f724974a0f9e8c41d898bbd",
            "6884c5df2dc5478782cce3d786e79cb9",
            "c4676ce504354ee5a3e6544bd88c73ca",
            "11f029871b874f8d9cc3fcda388d1c70",
            "c3153c49a78b48389e9c175f90ce47c1",
            "fc7aae28b9624cb5b53ef831b1127d31",
            "6680e297d17046ee8ecb780fe9e52fad",
            "1c9b81c7a290452d8971f521630cfc14",
            "c8e867a4a5ef488a85b619a05ad49f12",
            "f32f44dd9ed94ea7bfe0d25c08aff58b",
            "b4ced881026f4a5b9064bf49954c9b1a",
            "4399cc52d0704bdf86b0eb03b6a76bec",
            "f1baeb75488f41d199151cf5603dd813",
            "45364726bd9743368e75f5fdc0089cfc",
            "4c37c0cdb1464b4ba746fa92fda3a33e",
            "59f28de2d2464e089b761af4b67941e7",
            "ce847d1da214437480b63d2fdc467d89",
            "9a91af340bae4821a2605044f09329e9",
            "1ddd371d90af424cb349ec115a897db3",
            "033793cceeff46d2bc79317d607814eb",
            "124b0483bb144a32859277e0ddf7d882",
            "0c03a9c6b6604aeab2c4deed42cb5df2",
            "b6ee38cc3d984686b348e610b7a0797e",
            "8567417634f540be98df03311d9a07a1",
            "afbfbeb9c1984cc1800d6ada459b6f8f",
            "58f3241570064296aaae05545398cd3d",
            "9332ad6e3e7d4ed182da4ebde275c97b",
            "4a35f7b093b04c238887bd8c8995cf05"
          ]
        },
        "outputId": "6513bd1a-8798-4bd1-e58c-a7c517f7410f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pushing LoRa adapters to: arvindsuresh-math/gemma-3-1b-equation-line-extractor-aug-10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:917: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0337e58971eb4a78931ee241642fd156"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "148586122aa94490b852bebe954e21de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11cdba7208dc4169a7eab761147c8f7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...pkv0hvmk7/adapter_model.safetensors:   1%|1         |  561kB / 52.2MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75c02eda240e43f9bafca5096847de49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51b65c06fdbb40bba0c621a1ff158fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf3d123438a045f99c4af7355f9b9cc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  /tmp/tmp6gd_zsuo/tokenizer.json       : 100%|##########| 33.4MB / 33.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6680e297d17046ee8ecb780fe9e52fad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  /tmp/tmp6gd_zsuo/tokenizer.model      : 100%|##########| 4.69MB / 4.69MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a91af340bae4821a2605044f09329e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Adapters successfully pushed to the Hugging Face Hub.\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# CELL 11: PUSH ADAPTERS TO HUGGING FACE HUB\n",
        "# ===================================================================\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with your Hugging Face username and desired repo name\n",
        "hf_username = \"arvindsuresh-math\"\n",
        "hf_repo_name = \"gemma-3-1b-equation-line-extractor-aug-10\"\n",
        "commit_message = \"Fine-tuned with Unsloth on equation extraction dataset\"\n",
        "\n",
        "# --- Login to Hugging Face ---\n",
        "# This uses the token you provided at the start of the notebook\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "# --- Push the LoRa adapters ---\n",
        "# The 'model' object currently in memory is the fine-tuned adapter model\n",
        "print(f\"Pushing LoRa adapters to: {hf_username}/{hf_repo_name}\")\n",
        "model.push_to_hub(f\"{hf_username}/{hf_repo_name}\", use_auth_token=True, commit_message=commit_message)\n",
        "tokenizer.push_to_hub(f\"{hf_username}/{hf_repo_name}\", use_auth_token=True, commit_message=commit_message)\n",
        "\n",
        "print(\"✅ Adapters successfully pushed to the Hugging Face Hub.\")\n",
        "\n",
        "### How to use these adapters in your HF Space `app.py`:\n",
        "\n",
        "# from unsloth import FastModel\n",
        "# from peft import PeftModel\n",
        "# import torch\n",
        "\n",
        "# # Your chosen repo and the original base model\n",
        "# adapter_repo = \"your-hf-username/gemma-3-1b-equation-extractor-lora\"\n",
        "# base_model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "\n",
        "# # 1. Load the 4-bit base model\n",
        "# model, tokenizer = FastModel.from_pretrained(\n",
        "#     model_name = base_model_name,\n",
        "#     max_seq_length = 2048,\n",
        "#     dtype = None,\n",
        "#     load_in_4bit = True,\n",
        "# )\n",
        "\n",
        "# # 2. Apply your fine-tuned adapters\n",
        "# model = PeftModel.from_pretrained(model, adapter_repo)\n",
        "\n",
        "# # Now the 'model' is ready for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fEVICvv4cLPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c78b68-2a0c-4533-c3b5-0f3ecbb444a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPRESSING RESULTS FOR DOWNLOAD\n",
            "==================================================\n",
            "Creating zip archive at: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013/equation_extraction_gemma3-1b-unsloth_20250810_2013_results.zip\n",
            "  - Adding: baseline_results/baseline_evaluation_results.csv\n",
            "  - Adding: baseline_results/baseline_metrics.json\n",
            "  - Adding: final_results/final_evaluation_results.csv\n",
            "  - Adding: final_results/final_metrics.json\n",
            "  - Adding: training_log.csv\n",
            "  - Adding: config.json\n",
            "\n",
            "✅ Successfully created results zip archive at: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250810_2013/equation_extraction_gemma3-1b-unsloth_20250810_2013_results.zip\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPRESSING RESULTS FOR DOWNLOAD\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define paths from the global CONFIG\n",
        "output_dir = Path(CONFIG[\"output_dir\"])\n",
        "experiment_id = CONFIG[\"experiment_id\"]\n",
        "adapter_path = Path(CONFIG[\"final_adapter_dir\"])\n",
        "\n",
        "# Define the name and location of the output zip file\n",
        "zip_path = output_dir / f\"{experiment_id}_results.zip\"\n",
        "\n",
        "# List of files and directories to be included in the zip archive\n",
        "files_to_zip = [\n",
        "    output_dir / \"baseline_results\" / \"baseline_evaluation_results.csv\",\n",
        "    output_dir / \"baseline_results\" / \"baseline_metrics.json\",\n",
        "    output_dir / \"final_results\" / \"final_evaluation_results.csv\",\n",
        "    output_dir / \"final_results\" / \"final_metrics.json\",\n",
        "    output_dir / \"training_log.csv\",\n",
        "    output_dir / \"config.json\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        print(f\"Creating zip archive at: {zip_path}\")\n",
        "        for file_path in files_to_zip:\n",
        "            if file_path.exists():\n",
        "                # The arcname is the path of the file relative to the experiment directory,\n",
        "                # which keeps the folder structure (e.g., 'baseline_results/...') inside the zip.\n",
        "                arcname = file_path.relative_to(output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                print(f\"  - Adding: {arcname}\")\n",
        "            else:\n",
        "                print(f\"  - Skipping (not found): {file_path}\")\n",
        "\n",
        "    print(f\"\\n✅ Successfully created results zip archive at: {zip_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An error occurred while creating the zip file: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}