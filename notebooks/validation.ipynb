{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1237b250",
   "metadata": {},
   "source": [
    "### Cell 1: Imports and Path Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b4d7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Validation Input Directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/template-generated-processed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "import inspect\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Callable, Any\n",
    "from fractions import Fraction as BuiltinFraction\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the git repository.\"\"\"\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_TEMPLATE_DIR = DATA_DIR / \"template-generated-processed\"\n",
    "MODELS = ['openai_gpt-4.1', 'google_gemini-2.5-flash']\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Validation Input Directory: {PROCESSED_TEMPLATE_DIR}\")\n",
    "\n",
    "if not PROCESSED_TEMPLATE_DIR.is_dir():\n",
    "    raise FileNotFoundError(f\"INPUT DIRECTORY NOT FOUND: {PROCESSED_TEMPLATE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16586a0",
   "metadata": {},
   "source": [
    "### Cell 2: Data Loaders and Ground Truth Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e6e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Load GSM8K Dataset for ground truth answers and solution text ---\n",
    "GSM8K_TRAIN: Dataset = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
    "\n",
    "def load_function_module(tier: str, index: int, model: str) -> ModuleType | None:\n",
    "    \"\"\"Dynamically loads the 'solve.py' module for a given template.\"\"\"\n",
    "    py_file_path = PROCESSED_TEMPLATE_DIR / tier / str(index) / f\"{model}.py\"\n",
    "    if not py_file_path.exists(): return None\n",
    "    module_name = f\"templates.val.t{tier}.i{index}.m_{model.replace('.', '_')}\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, py_file_path)\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    return None\n",
    "\n",
    "def load_logical_steps(tier: str, index: int, model: str) -> list[dict] | None:\n",
    "    \"\"\"Loads the 'logical_steps.json' for a given template.\"\"\"\n",
    "    json_file_path = PROCESSED_TEMPLATE_DIR / tier / str(index) / f\"{model}.json\"\n",
    "    try:\n",
    "        return json.loads(json_file_path.read_text(encoding='utf-8'))\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return None\n",
    "\n",
    "def get_ground_truth_answer(index: int) -> float | None:\n",
    "    \"\"\"Extracts the final numeric answer from the GSM8K dataset.\"\"\"\n",
    "    try:\n",
    "        answer_text = GSM8K_TRAIN[int(index)]['answer']\n",
    "        final_answer_str = answer_text.split('####')[-1].strip().replace(',', '')\n",
    "        return float(final_answer_str)\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def get_original_solution_lines(index: int) -> dict[str, str] | None:\n",
    "    \"\"\"Extracts the original solution into a line-numbered dictionary.\"\"\"\n",
    "    try:\n",
    "        solution_text = GSM8K_TRAIN[int(index)][\"answer\"]\n",
    "        lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "        if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "            lines.pop(-1)\n",
    "        return {f\"L{i+1}\": line for i, line in enumerate(lines)}\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "print(\"Data loading utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75292a0f",
   "metadata": {},
   "source": [
    "### Cell 3: Core Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e18f48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core validation functions defined.\n"
     ]
    }
   ],
   "source": [
    "class NonSimplifyingFraction(BuiltinFraction):\n",
    "    \"\"\"\n",
    "    A subclass of fractions.Fraction that does not simplify when converted\n",
    "    to a string, preserving the original representation from the template.\n",
    "    \"\"\"\n",
    "    def __new__(cls, numerator=0, denominator=None):\n",
    "        self = super().__new__(cls, numerator, denominator)\n",
    "        if denominator is not None:\n",
    "            self._original_denominator = denominator\n",
    "            self._original_numerator = numerator\n",
    "        else:\n",
    "            self._original_numerator, self._original_denominator = self.as_integer_ratio()\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self._original_numerator}/{self._original_denominator}\"\n",
    "\n",
    "def normalize_value(value: Any) -> Any:\n",
    "    \"\"\"Normalizes a numeric value by casting integer-like floats to int.\"\"\"\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    return value\n",
    "\n",
    "def normalize_numeric_string(s: str) -> str:\n",
    "    \"\"\"Converts a string representing a number into a canonical form.\"\"\"\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    cleaned = s.strip()\n",
    "    cleaned = re.sub(r'[\\$,]', '', cleaned)\n",
    "    if cleaned.startswith('.'): cleaned = '0' + cleaned\n",
    "    elif cleaned.startswith('-.') or cleaned.startswith('+.'): cleaned = cleaned[0] + '0' + cleaned[1:]\n",
    "    try:\n",
    "        num = float(cleaned)\n",
    "        if num.is_integer(): return str(int(num))\n",
    "        return str(num)\n",
    "    except (ValueError, TypeError):\n",
    "        return cleaned\n",
    "\n",
    "def check_answer(solve_function: Callable, ground_truth_answer: float) -> dict:\n",
    "    \"\"\"\n",
    "    Executes a solve() function and compares its result against the ground truth.\n",
    "    Returns a dictionary with a descriptive status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = solve_function()\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"fail_not_executable\", \"details\": str(e), \"computed_answer\": None}\n",
    "    \n",
    "    norm_result = normalize_value(result) if isinstance(result, (int, float, BuiltinFraction)) else result\n",
    "    norm_truth = normalize_value(ground_truth_answer)\n",
    "    \n",
    "    if norm_result == norm_truth:\n",
    "        return {\"status\": \"pass\", \"details\": None, \"computed_answer\": result}\n",
    "    else:\n",
    "        details = f\"Expected {ground_truth_answer}, but got {result}.\"\n",
    "        return {\"status\": \"fail_wrong_answer\", \"details\": details, \"computed_answer\": result}\n",
    "\n",
    "def validate_structural_integrity(logical_steps: list[dict], original_lines: dict[str, str]) -> dict:\n",
    "    \"\"\"\n",
    "    Validates the structural integrity of the templates line-by-line.\n",
    "    Returns a dictionary containing the score and a list of mismatched line numbers.\n",
    "    \"\"\"\n",
    "    structural_matches, mismatched_lines = 0, []\n",
    "    for step in logical_steps:\n",
    "        template, line_num = step.get('solution_line_template', ''), step.get('line_number')\n",
    "        original_line = original_lines.get(line_num, \"\")\n",
    "        ambient_fragments = re.split(r'\\{[a-zA-Z0-9_]+\\}', template)\n",
    "        current_pos, line_is_ok = 0, True\n",
    "        for fragment in ambient_fragments:\n",
    "            found_pos = original_line.find(fragment, current_pos)\n",
    "            if found_pos == -1:\n",
    "                line_is_ok = False\n",
    "                break\n",
    "            current_pos = found_pos + len(fragment)\n",
    "        if line_is_ok: structural_matches += 1\n",
    "        else: mismatched_lines.append(line_num)\n",
    "            \n",
    "    score = structural_matches / len(logical_steps) if logical_steps else 1.0\n",
    "    return {\"score\": score, \"mismatched_lines\": mismatched_lines}\n",
    "\n",
    "def validate_semantic_integrity(logical_steps: list[dict], correct_trace: dict, original_lines: dict[str, str]) -> dict:\n",
    "    \"\"\"\n",
    "    Validates the semantic (numeric value) integrity of the templates.\n",
    "    ASSUMES the template structure is already a perfect match.\n",
    "    Returns a dictionary containing the score and a list of mismatched variable names.\n",
    "    \"\"\"\n",
    "    total_placeholders, semantic_matches, mismatched_variables = 0, 0, []\n",
    "    for step in logical_steps:\n",
    "        template, line_num = step.get('solution_line_template', ''), step.get('line_number')\n",
    "        original_line = original_lines.get(line_num, \"\")\n",
    "        placeholders = re.findall(r'\\{([a-zA-Z0-9_]+)\\}', template)\n",
    "        ambient_fragments = re.split(r'\\{[a-zA-Z0-9_]+\\}', template)\n",
    "        current_pos = 0\n",
    "        for i, placeholder_var in enumerate(placeholders):\n",
    "            total_placeholders += 1\n",
    "            start_ambient, end_ambient = ambient_fragments[i], ambient_fragments[i+1]\n",
    "            start_val_pos = original_line.find(start_ambient, current_pos) + len(start_ambient)\n",
    "            end_frag_pos = original_line.find(end_ambient, start_val_pos) if end_ambient else len(original_line)\n",
    "            extracted_str = original_line[start_val_pos:end_frag_pos]\n",
    "            ground_truth_val = correct_trace.get(placeholder_var)\n",
    "            if ground_truth_val is not None:\n",
    "                if normalize_numeric_string(extracted_str) == normalize_numeric_string(ground_truth_val):\n",
    "                    semantic_matches += 1\n",
    "                else: mismatched_variables.append(placeholder_var)\n",
    "            else: mismatched_variables.append(placeholder_var)\n",
    "            current_pos = end_frag_pos\n",
    "            \n",
    "    score = semantic_matches / total_placeholders if total_placeholders > 0 else 1.0\n",
    "    return {\"score\": score, \"mismatched_variables\": mismatched_variables}\n",
    "\n",
    "print(\"Core validation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8612a05",
   "metadata": {},
   "source": [
    "### Cell 4: Validation orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f014283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation orchestrator defined.\n"
     ]
    }
   ],
   "source": [
    "def execution_trace(func: Callable[[], Any]) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    Executes a function's source code line by line to build a variable trace,\n",
    "    using NonSimplifyingFraction to preserve fraction representations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src = inspect.getsource(func)\n",
    "        tree = ast.parse(src)\n",
    "        func_def = tree.body[0]\n",
    "        global_namespace = {'Fraction': NonSimplifyingFraction}\n",
    "        local_env = {}\n",
    "        for stmt in func_def.body:\n",
    "            if isinstance(stmt, ast.Assign):\n",
    "                module_node = ast.Module([stmt], type_ignores=[])\n",
    "                code_obj = compile(module_node, '<string>', 'exec')\n",
    "                exec(code_obj, global_namespace, local_env)\n",
    "        return local_env\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def validate_template(tier: str, index: int, model: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Orchestrates the complete validation process for a single template,\n",
    "    calling checks sequentially and returning a summary report dictionary.\n",
    "    \"\"\"\n",
    "    # 1. Load all components; exit if any are missing.\n",
    "    solve_module = load_function_module(tier, index, model)\n",
    "    logical_steps = load_logical_steps(tier, index, model)\n",
    "    ground_truth_answer = get_ground_truth_answer(index)\n",
    "    original_lines = get_original_solution_lines(index)\n",
    "    \n",
    "    if not all([solve_module, logical_steps, ground_truth_answer, original_lines]):\n",
    "        return None\n",
    "\n",
    "    solve_function = solve_module.solve\n",
    "    \n",
    "    # 2. Run the primary validation checks.\n",
    "    answer_check_result = check_answer(solve_function, ground_truth_answer)\n",
    "    structural_results = validate_structural_integrity(logical_steps, original_lines)\n",
    "    \n",
    "    # 3. Semantic check is only performed if structure is perfect and code is executable.\n",
    "    semantic_results = {\"score\": None, \"mismatched_variables\": None}\n",
    "    correct_trace = execution_trace(solve_function)\n",
    "    if structural_results[\"score\"] == 1.0 and correct_trace:\n",
    "        semantic_results = validate_semantic_integrity(logical_steps, correct_trace, original_lines)\n",
    "\n",
    "    # 4. Compile the final report.\n",
    "    report = {\n",
    "        \"index\": index,\n",
    "        \"tier\": tier,\n",
    "        \"model\": model,\n",
    "        \"answer_check_status\": answer_check_result[\"status\"],\n",
    "        \"structural_score\": structural_results[\"score\"],\n",
    "        \"semantic_score\": semantic_results[\"score\"],\n",
    "        \"structural_mismatches\": structural_results[\"mismatched_lines\"],\n",
    "        \"semantic_mismatches\": semantic_results[\"mismatched_variables\"],\n",
    "    }\n",
    "    return report\n",
    "\n",
    "print(\"Validation orchestrator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25189f59",
   "metadata": {},
   "source": [
    "### Cell 5: Main driver and report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67f8d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_validation_pipeline():\n",
    "    \"\"\"\n",
    "    Drives the entire validation pipeline for all processed templates,\n",
    "    compiles the results, and saves a final CSV report with summary statistics.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Full Template Validation ---\")\n",
    "    all_reports = []\n",
    "    \n",
    "    tier_dirs = sorted([d for d in PROCESSED_TEMPLATE_DIR.iterdir() if d.is_dir() and d.name.startswith('tier')])\n",
    "    for tier_dir in tqdm(tier_dirs, desc=\"Validating Tiers\"):\n",
    "        index_dirs = sorted([d for d in tier_dir.iterdir() if d.is_dir() and d.name.isdigit()], key=lambda p: int(p.name))\n",
    "        for index_dir in tqdm(index_dirs, desc=f\"Processing {tier_dir.name}\", leave=False):\n",
    "            for model in MODELS:\n",
    "                report = validate_template(tier_dir.name, int(index_dir.name), model)\n",
    "                if report:\n",
    "                    all_reports.append(report)\n",
    "\n",
    "    if all_reports:\n",
    "        report_df = pd.DataFrame(all_reports)\n",
    "        report_path = PROCESSED_TEMPLATE_DIR / \"template_validation_report.csv\"\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        \n",
    "        print(\"\\n--- Validation Complete ---\")\n",
    "        print(f\"Validated {len(report_df)} templates.\")\n",
    "        print(f\"Report saved to: {report_path}\")\n",
    "        \n",
    "        # Display Final Summary Statistics\n",
    "        print(\"\\n--- Summary Report ---\")\n",
    "        \n",
    "        # Answer Correctness Breakdown\n",
    "        answer_counts = report_df['answer_check_status'].value_counts()\n",
    "        total_templates = len(report_df)\n",
    "        pass_count = answer_counts.get('pass', 0)\n",
    "        wrong_answer_count = answer_counts.get('fail_wrong_answer', 0)\n",
    "        not_executable_count = answer_counts.get('fail_not_executable', 0)\n",
    "        \n",
    "        print(\"Answer Correctness Breakdown:\")\n",
    "        print(f\"  - Pass (Correct Answer): {pass_count:>4} / {total_templates} ({pass_count/total_templates:.2%})\")\n",
    "        print(f\"  - Fail (Wrong Answer):   {wrong_answer_count:>4} / {total_templates} ({wrong_answer_count/total_templates:.2%})\")\n",
    "        print(f\"  - Fail (Not Executable): {not_executable_count:>4} / {total_templates} ({not_executable_count/total_templates:.2%})\")\n",
    "        \n",
    "        # Define the subset eligible for semantic validation\n",
    "        eligible_for_semantic_check = report_df[\n",
    "            (report_df['answer_check_status'] != 'fail_not_executable') &\n",
    "            (report_df['structural_score'] == 1.0)\n",
    "        ]\n",
    "        \n",
    "        # Calculate integrity rates based on the appropriate populations (file-based)\n",
    "        perfect_structure_count = (report_df['structural_score'] == 1.0).sum()\n",
    "        total_eligible = len(eligible_for_semantic_check)\n",
    "        perfect_semantic_count = (eligible_for_semantic_check['semantic_score'] == 1.0).sum() if not eligible_for_semantic_check.empty else 0\n",
    "        \n",
    "        print(f\"\\n--- Integrity Rates (file-based) ---\")\n",
    "        print(f\"Perfect Structural Match: {perfect_structure_count} / {total_templates} ({perfect_structure_count/total_templates:.2%}) of all templates\")\n",
    "        print(f\"Perfect Semantic Match:   {perfect_semantic_count} / {total_eligible} ({perfect_semantic_count/total_eligible:.2%}) of eligible templates\")\n",
    "\n",
    "        # Calculate integrity rates based on the index-based population\n",
    "        structural_pass_df = report_df[report_df['structural_score'] == 1.0].copy()\n",
    "        semantic_pass_df = report_df[report_df['semantic_score'] == 1.0].copy()\n",
    "\n",
    "        structural_pass_indices = list(set(structural_pass_df['index'].tolist()))\n",
    "        semantic_pass_indices = list(set(semantic_pass_df['index'].tolist()))\n",
    "        total_indices = list(set(report_df['index'].tolist()))\n",
    "\n",
    "        print(f\"\\n--- Integrity Rates (index-based) ---\")\n",
    "        print(f\"Perfect Structural Match: {len(structural_pass_indices)} / {len(total_indices)} ({len(structural_pass_indices)/len(total_indices):.2%}) of all indices\")\n",
    "        print(f\"Perfect Semantic Match:   {len(semantic_pass_indices)} / {len(total_indices)} ({len(semantic_pass_indices)/len(total_indices):.2%}) of all indices\")\n",
    "    else:\n",
    "        print(\"\\nNo templates were found to validate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a92da7",
   "metadata": {},
   "source": [
    "### Cell 6: Running the full validation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2cfcb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Template Validation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a153447b1bb482ba5b661b536e9513b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Tiers:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2cf6e7abcf4f21bc6c5d26b4411142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier1:   0%|          | 0/1138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c32f3d61aa4f669ef5d594bd1c70e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier2:   0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d80cc9622db4141a84755d69b4cf44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier3:   0%|          | 0/1262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b65dc66bc0e4854b24884d580c87bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tier4:   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Complete ---\n",
      "Validated 4794 templates.\n",
      "Report saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/template-generated-processed/template_validation_report.csv\n",
      "\n",
      "--- Summary Report ---\n",
      "Answer Correctness Breakdown:\n",
      "  - Pass (Correct Answer): 4751 / 4794 (99.10%)\n",
      "  - Fail (Wrong Answer):     42 / 4794 (0.88%)\n",
      "  - Fail (Not Executable):    1 / 4794 (0.02%)\n",
      "\n",
      "--- Integrity Rates (file-based) ---\n",
      "Perfect Structural Match: 4482 / 4794 (93.49%) of all templates\n",
      "Perfect Semantic Match:   4182 / 4481 (93.33%) of eligible templates\n",
      "\n",
      "--- Integrity Rates (index-based) ---\n",
      "Perfect Structural Match: 2766 / 2916 (94.86%) of all indices\n",
      "Perfect Semantic Match:   2598 / 2916 (89.09%) of all indices\n"
     ]
    }
   ],
   "source": [
    "run_full_validation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccbbaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Diagnostic Tool for Individual Indices\n",
    "\n",
    "def find_tier_for_index(index: int) -> str | None:\n",
    "    \"\"\"Helper function to find the tier for a given problem index.\"\"\"\n",
    "    for tier, indices in TIER_LISTS.items():\n",
    "        if index in indices:\n",
    "            return tier\n",
    "    return None\n",
    "\n",
    "def reconstruct_solution_for_model(tier: str, index: int, model: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reconstructs the solution lines for a single template.\n",
    "    \n",
    "    Returns a dictionary mapping line numbers to the reconstructed text.\n",
    "    \"\"\"\n",
    "    logical_steps = load_logical_steps(tier, index, model)\n",
    "    solve_module = load_function_module(tier, index, model)\n",
    "    \n",
    "    if not (logical_steps and solve_module):\n",
    "        return {}\n",
    "        \n",
    "    correct_trace = execution_trace(solve_module.solve)\n",
    "    if not correct_trace:\n",
    "        return {ln: \"ERROR: Could not execute trace\" for ln in original_lines}\n",
    "\n",
    "    reconstructed_lines = {}\n",
    "    for step in logical_steps:\n",
    "        ln = step[\"line_number\"]\n",
    "        template = step[\"solution_line_template\"]\n",
    "        try:\n",
    "            reconstructed_lines[ln] = template.format_map(correct_trace)\n",
    "        except (KeyError, ValueError):\n",
    "            reconstructed_lines[ln] = \"ERROR: Formatting failed\"\n",
    "            \n",
    "    return reconstructed_lines\n",
    "\n",
    "def normalize_line_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a full line of text by normalizing all numbers found within it.\n",
    "    \"\"\"\n",
    "    # This pattern finds integers, comma-separated numbers, and decimals.\n",
    "    number_pattern = r'[\\d,]+\\.?\\d*'\n",
    "    # Use re.sub with a lambda to apply our string normalizer to each number found.\n",
    "    return re.sub(number_pattern, lambda m: normalize_numeric_string(m.group(0)), text)\n",
    "\n",
    "def diagnose_index(index: int):\n",
    "    \"\"\"\n",
    "    Provides a full diagnostic report for a single problem index, comparing\n",
    "    the original solution against the templates from all models.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"DIAGNOSTIC REPORT FOR INDEX: {index}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Load ground truth data\n",
    "    tier = find_tier_for_index(index)\n",
    "    original_lines = get_original_solution_lines(index)\n",
    "    if not tier or not original_lines:\n",
    "        print(f\"ERROR: Could not find tier or original solution for index {index}.\")\n",
    "        return\n",
    "\n",
    "    # --- Part 1: Print Mismatch Lists ---\n",
    "    print(\"\\n--- Mismatch Analysis ---\")\n",
    "    full_report = report_df[report_df['index'] == index]\n",
    "    for _, row in full_report.iterrows():\n",
    "        model = row['model']\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        \n",
    "        # Safely evaluate the string representation of lists in the DataFrame\n",
    "        structural_mismatches = ast.literal_eval(row['structural_mismatches'])\n",
    "        semantic_mismatches = ast.literal_eval(row['semantic_mismatches'])\n",
    "        \n",
    "        if structural_mismatches:\n",
    "            print(f\"  - Structural Mismatches on Lines: {structural_mismatches}\")\n",
    "        else:\n",
    "            print(\"  - No Structural Mismatches.\")\n",
    "            \n",
    "        if semantic_mismatches:\n",
    "            print(f\"  - Semantic Mismatches for Variables: {semantic_mismatches}\")\n",
    "        else:\n",
    "            print(\"  - No Semantic Mismatches (or not applicable).\")\n",
    "\n",
    "    # --- Part 2: Display Visual Diff DataFrame ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"--- Visual Diff of Reconstructed Lines (with Normalized Values) ---\")\n",
    "    \n",
    "    # Start with the original solution\n",
    "    df = pd.DataFrame.from_dict(original_lines, orient='index', columns=['Original'])\n",
    "    \n",
    "    # Add a column for each model's reconstruction\n",
    "    for model in MODELS:\n",
    "        reconstructed = reconstruct_solution_for_model(tier, index, model)\n",
    "        df[f'Recon_{model}'] = df.index.map(reconstructed)\n",
    "\n",
    "    # Create the final normalized DataFrame by applying the normalization function\n",
    "    normalized_df = df.map(lambda x: normalize_line_text(str(x)) if pd.notna(x) else \"\")\n",
    "    \n",
    "    display(normalized_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
