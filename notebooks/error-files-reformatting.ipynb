{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95f0a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "# Yewei\n",
    "DIR_1001_1500_COMPUTATIONAL = PROJECT_ROOT / 'yewei' / 'gsm8k_data' / 'computational'\n",
    "DIR_1001_1500_CONCEPTUAL = PROJECT_ROOT / 'yewei' / 'gsm8k_data' / 'conceptual'\n",
    "\n",
    "# Ling\n",
    "DIR_1500_1599_CONCEPTUAL = DATA_DIR\n",
    "\n",
    "# Mauro\n",
    "DIR_500_999_MIXED = DATA_DIR / 'manually_gen_incorrect_answers_gsm8k'\n",
    "\n",
    "# Ali\n",
    "DIR_0_400_MIXED = PROJECT_ROOT / 'GSM8_Edited'\n",
    "\n",
    "# Check if all the directories exist\n",
    "for directory in [\n",
    "    DIR_1001_1500_COMPUTATIONAL,\n",
    "    DIR_1001_1500_CONCEPTUAL,\n",
    "    DIR_1500_1599_CONCEPTUAL,\n",
    "    DIR_500_999_MIXED,\n",
    "    DIR_0_400_MIXED\n",
    "]:\n",
    "    if not directory.exists():\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "\n",
    "filepaths = {\n",
    "    # Yewei\n",
    "    \"1001_1500_computational\": DIR_1001_1500_COMPUTATIONAL / \"gsm8k_augmented_1001_1100_computational.json\",\n",
    "    \"1001_1500_conceptual\": DIR_1001_1500_CONCEPTUAL / \"gsm8k_augmented_1001_1100_conceptual.json\",\n",
    "    \"1101_1200_computational\": DIR_1001_1500_COMPUTATIONAL / \"gsm8k_augmented_1101_1200_computational.json\",\n",
    "    \"1101_1200_conceptual\": DIR_1001_1500_CONCEPTUAL / \"gsm8k_augmented_1101_1200_conceptual.json\",\n",
    "    \"1201_1300_computational\": DIR_1001_1500_COMPUTATIONAL / \"gsm8k_augmented_1201_1300_computational.json\",\n",
    "    \"1201_1300_conceptual\": DIR_1001_1500_CONCEPTUAL / \"gsm8k_augmented_1201_1300_conceptual.json\",\n",
    "    \"1301_1400_computational\": DIR_1001_1500_COMPUTATIONAL / \"gsm8k_augmented_1301_1400_computational.json\",\n",
    "    \"1301_1400_conceptual\": DIR_1001_1500_CONCEPTUAL / \"gsm8k_augmented_1301_1400_conceptual.json\",\n",
    "    \"1401_1500_computational\": DIR_1001_1500_COMPUTATIONAL / \"gsm8k_augmented_1401_1500_computational.json\",\n",
    "    \"1401_1500_conceptual\": DIR_1001_1500_CONCEPTUAL / \"gsm8k_augmented_1401_1500_conceptual.json\",\n",
    "\n",
    "    # Ling\n",
    "    \"1500_1599_conceptual\": DIR_1500_1599_CONCEPTUAL / \"1500_1599_conceptual.jsonl\",\n",
    "\n",
    "    # Mauro\n",
    "    \"500_999_mixed\": DIR_500_999_MIXED / \"gsm8k_annotated_500_to_999.jsonl\",\n",
    "\n",
    "    # Ali\n",
    "    \"0_100_mixed\": DIR_0_400_MIXED / \"0-100.json\",\n",
    "    \"101-200_mixed\": DIR_0_400_MIXED / \"101-200.json\",\n",
    "    \"201-300_mixed\": DIR_0_400_MIXED / \"201_to_300.json\",\n",
    "    \"301-400_mixed\": DIR_0_400_MIXED / \"301_to_400.json\"\n",
    "}\n",
    "\n",
    "# Output dir for the merged jsonl file\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "\n",
    "# Load all files into a single dictionary\n",
    "all_data = {}\n",
    "for key, filepath in filepaths.items():\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            if filepath.suffix == \".jsonl\":\n",
    "                # Handle JSONL files\n",
    "                all_data[key] = [json.loads(line) for line in f]\n",
    "            else:\n",
    "                # Handle JSON files\n",
    "                all_data[key] = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "GSM8K_TRAIN = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bae4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Optional\n",
    "from datasets import Dataset\n",
    "\n",
    "def extract_start_index(key: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a key like \"1001_1500_conceptual\" or \"101-200_mixed\",\n",
    "    return the first integer (e.g. 1001 or 101).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d+)\", key)\n",
    "    if not m:\n",
    "        raise ValueError(f\"No integer start index found in key '{key}'\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def build_question_index_map(dataset: Dataset) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a dict mapping each question string to its index in the GSM8K train split.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        sample[\"question\"].strip(): idx\n",
    "        for idx, sample in enumerate(dataset)\n",
    "    }\n",
    "\n",
    "def merge_augmented_data(\n",
    "    all_data: Dict[str, Union[List[dict], dict]],\n",
    "    filepaths: Dict[str, Path],\n",
    "    gsm8k_train: Dataset,\n",
    "    output_dir: Union[str, Path],\n",
    "    project_root: Path,\n",
    "    output_filename: str = \"manually_generated_errors.jsonl\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Merge all of your JSON/JSONL files into a single JSONL, adding:\n",
    "      - \"index\": the example’s index in the GSM8K train split\n",
    "      - \"filepath\": the source filepath, made relative to project_root\n",
    "\n",
    "    Args:\n",
    "        all_data:       mapping from your file-key (e.g. \"0_100_mixed\") to list or dict of records\n",
    "        filepaths:      same keys → Path to each original JSON/JSONL file\n",
    "        gsm8k_train:    the HuggingFace Dataset for \"gsm8k\" train split\n",
    "        output_dir:     directory in which to write the merged JSONL\n",
    "        project_root:   Path to your project root, used to relativize filepaths\n",
    "        output_filename: name of the output JSONL file (default: \"merged_augmented_data.jsonl\")\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = output_dir / output_filename\n",
    "\n",
    "    # build question → index lookup\n",
    "    q2idx = build_question_index_map(gsm8k_train)\n",
    "\n",
    "    total_written = 0\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for key, records in all_data.items():\n",
    "            records_list = records if isinstance(records, list) else [records]\n",
    "            start_idx = extract_start_index(key)\n",
    "            src_path: Optional[Path] = filepaths.get(key)\n",
    "\n",
    "            # compute filepath relative to project_root (or fallback)\n",
    "            if src_path is not None:\n",
    "                try:\n",
    "                    rel_fp = src_path.relative_to(project_root)\n",
    "                except ValueError:\n",
    "                    rel_fp = src_path\n",
    "            else:\n",
    "                rel_fp = Path(key)\n",
    "\n",
    "            for i, rec in enumerate(records_list):\n",
    "                q = rec.get(\"question\", \"\").strip()\n",
    "                idx = q2idx.get(q, start_idx + i)\n",
    "                merged = {\n",
    "                    **rec,\n",
    "                    \"index\": idx,\n",
    "                    \"filepath\": rel_fp.as_posix()\n",
    "                }\n",
    "                fout.write(json.dumps(merged, ensure_ascii=False) + \"\\n\")\n",
    "                total_written += 1\n",
    "\n",
    "    print(f\"Merged {total_written} records → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0db41267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 1963 records → /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/manually_generated_errors.jsonl\n"
     ]
    }
   ],
   "source": [
    "merge_augmented_data(\n",
    "    all_data=all_data,\n",
    "    filepaths=filepaths,\n",
    "    gsm8k_train=GSM8K_TRAIN,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    project_root=PROJECT_ROOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4be88d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def verify_merged_jsonl(\n",
    "    jsonl_path: Path,\n",
    "    gsm8k_train=None,\n",
    "    max_display: int = 10\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Verify that each 'question' in the merged JSONL matches the\n",
    "    GSM8K train split question at the recorded 'index'.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path:    Path to your merged_augmented_data.jsonl\n",
    "        gsm8k_train:   (optional) pre-loaded Dataset; if None, we'll load it for you\n",
    "        max_display:   number of mismatches to print in detail\n",
    "    \"\"\"\n",
    "    # 1) load GSM8K if not provided\n",
    "    if gsm8k_train is None:\n",
    "        gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "    total = 0\n",
    "    mismatches = []\n",
    "\n",
    "    # 2) iterate merged file\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            rec = json.loads(line)\n",
    "            idx = rec.get(\"index\")\n",
    "            merged_q = rec.get(\"question\", \"\").strip()\n",
    "\n",
    "            # 3) fetch original\n",
    "            try:\n",
    "                orig_q = gsm8k_train[idx][\"question\"].strip()\n",
    "            except (IndexError, KeyError, TypeError):\n",
    "                mismatches.append((total, idx, merged_q, None))\n",
    "                continue\n",
    "\n",
    "            # 4) compare\n",
    "            if merged_q != orig_q:\n",
    "                mismatches.append((total, idx, merged_q, orig_q))\n",
    "\n",
    "    # 5) report\n",
    "    print(f\"Checked {total} records.\")\n",
    "    if not mismatches:\n",
    "        print(\"✅ All questions match the GSM8K dataset.\")\n",
    "    else:\n",
    "        print(f\"⚠️  Found {len(mismatches)} mismatches:\")\n",
    "        for record_num, idx, merged_q, orig_q in mismatches[:max_display]:\n",
    "            print(f\"\\n  • Record #{record_num} (index={idx}):\")\n",
    "            print(f\"      Merged : {merged_q!r}\")\n",
    "            if orig_q is None:\n",
    "                print(\"      Original: <index out of range>\")\n",
    "            else:\n",
    "                print(f\"      Original: {orig_q!r}\")\n",
    "        if len(mismatches) > max_display:\n",
    "            print(f\"\\n  ...plus {len(mismatches)-max_display} more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4144602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize and clean up a solution string for consistent line splitting.\n",
    "\n",
    "    Steps:\n",
    "      1. Unicode-normalize to NFC form.\n",
    "      2. Convert all CRLF or CR line endings to LF.\n",
    "      3. Strip trailing whitespace on each line.\n",
    "      4. Remove any zero-width or non-printable characters.\n",
    "      5. Trim leading/trailing blank lines.\n",
    "\n",
    "    Args:\n",
    "        text: raw solution string (may contain weird unicode or mixed line endings)\n",
    "    Returns:\n",
    "        cleaned text with uniform LF endings and no extraneous trailing spaces.\n",
    "    \"\"\"\n",
    "    # 1) Unicode normalize\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # 2) Normalize line endings\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 3) Strip trailing whitespace and remove non-printables\n",
    "    #    (e.g. zero-width spaces, etc.)\n",
    "    def clean_line(line: str) -> str:\n",
    "        # remove zero-width / control chars except newline\n",
    "        line = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", line)\n",
    "        return line.rstrip()\n",
    "\n",
    "    lines = [clean_line(ln) for ln in text.split(\"\\n\")]\n",
    "\n",
    "    # 4) Trim leading/trailing blank lines\n",
    "    while lines and lines[0] == \"\":\n",
    "        lines.pop(0)\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8c3a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "def build_solution_mapping_from_text_nosanitize(\n",
    "    solution_text: str,\n",
    "    exclude_FA: bool = True\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split raw solution text into a line-numbered dict, without any sanitization.\n",
    "\n",
    "    Args:\n",
    "        solution_text: the full multi-line solution string\n",
    "        exclude_FA:    if True, drop the final \"#### {answer}\" line entirely\n",
    "    Returns:\n",
    "        mapping of \"L1\", \"L2\", … to each non-blank line of solution_text\n",
    "        (and optionally \"FA\" if exclude_FA=False)\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "    mapping: Dict[str, str] = {}\n",
    "\n",
    "    # if last line is \"#### {digits}\" treat as final answer\n",
    "    if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "        fa_line = lines.pop(-1).strip()\n",
    "        if not exclude_FA:\n",
    "            mapping[\"FA\"] = fa_line\n",
    "\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        mapping[f\"L{i}\"] = line\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def compute_first_erroneous_line_nosanitize(\n",
    "    input_jsonl: Union[str, Path],\n",
    "    output_jsonl: Optional[Union[str, Path]] = None,\n",
    "    gsm8k_train: Optional[Dataset] = None\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Like compute_first_erroneous_line, but uses the nosanitize mapping.\n",
    "\n",
    "    Returns dict with:\n",
    "      - correct_answer_mismatch\n",
    "      - wrong_answer_full_matches\n",
    "      - wrong_answer_weird\n",
    "\n",
    "    And (if output_jsonl is set) writes an annotated JSONL with\n",
    "    'erroneous_line_number' added where appropriate.\n",
    "    \"\"\"\n",
    "    if gsm8k_train is None:\n",
    "        gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "    cam, wafm, waw = [], [], []\n",
    "    annotated: List[dict] = []\n",
    "\n",
    "    with open(input_jsonl, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for raw in fin:\n",
    "            rec = json.loads(raw)\n",
    "            idx = rec[\"index\"]\n",
    "\n",
    "            ans_map   = build_solution_mapping_from_text_nosanitize(rec[\"answer\"])\n",
    "            wrong_map = build_solution_mapping_from_text_nosanitize(rec[\"wrong_answer\"])\n",
    "            ds_text   = gsm8k_train[idx][\"answer\"]\n",
    "            ds_map    = build_solution_mapping_from_text_nosanitize(ds_text)\n",
    "\n",
    "            # 1) correct-answer mismatch?\n",
    "            if ans_map != ds_map:\n",
    "                cam.append(idx)\n",
    "                annotated.append(rec)\n",
    "                continue\n",
    "\n",
    "            # 2) wrong-answer exactly matches full solution?\n",
    "            if wrong_map == ds_map:\n",
    "                wafm.append(idx)\n",
    "                annotated.append(rec)\n",
    "                continue\n",
    "\n",
    "            # find first line where wrong ≠ dataset\n",
    "            keys = sorted(ds_map.keys(), key=lambda k: int(k[1:]))\n",
    "            first_diff = next((k for k in keys if wrong_map.get(k) != ds_map.get(k)), None)\n",
    "\n",
    "            # 3) weird: any later line matches again?\n",
    "            is_weird = False\n",
    "            if first_diff:\n",
    "                start = int(first_diff[1:])\n",
    "                for k in keys:\n",
    "                    if int(k[1:]) > start and wrong_map.get(k) == ds_map.get(k):\n",
    "                        waw.append(idx)\n",
    "                        is_weird = True\n",
    "                        break\n",
    "\n",
    "            if is_weird:\n",
    "                annotated.append(rec)\n",
    "                continue\n",
    "\n",
    "            # 4) normal: annotate the first differing line\n",
    "            if first_diff:\n",
    "                rec[\"erroneous_line_number\"] = first_diff\n",
    "            annotated.append(rec)\n",
    "\n",
    "    if output_jsonl:\n",
    "        out = Path(output_jsonl)\n",
    "        with out.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "            for rec in annotated:\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Wrote annotated JSONL to {out}\")\n",
    "\n",
    "    return {\n",
    "        \"correct_answer_mismatch\": cam,\n",
    "        \"wrong_answer_full_matches\": wafm,\n",
    "        \"wrong_answer_weird\": waw\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b724d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote annotated JSONL to /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/manually_generated_errors_final.jsonl\n"
     ]
    }
   ],
   "source": [
    "results = compute_first_erroneous_line_nosanitize(\n",
    "    input_jsonl= DATA_DIR / \"manually_generated_errors.jsonl\",\n",
    "    output_jsonl= DATA_DIR / \"manually_generated_errors_final.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afaa47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers from manually gen files that don't match GSM8K answers:\n",
      "[1513, 1533, 1539, 1544, 1556, 1557, 1559, 1560, 3, 12, 20, 44, 105, 108, 109, 137, 167, 168, 198, 200, 269, 317, 378, 399]\n",
      "\n",
      "Wrong answers that fully match GSM8K answers:\n",
      "[881]\n",
      "\n",
      "Wrong answers that are weird (one line differs, but later lines match):\n",
      "[1001, 1019, 1028, 1030, 1031, 1033, 1039, 1042, 1054, 1062, 1066, 1068, 1069, 1072, 1073, 1074, 1075, 1085, 1093, 1001, 1010, 1030, 1031, 1066, 1069, 1102, 1112, 1124, 1125, 1131, 1133, 1136, 1140, 1143, 1149, 1152, 1153, 1154, 1156, 1159, 1160, 1163, 1164, 1169, 1192, 1193, 1196, 1122, 1125, 1146, 1149, 1184, 1189, 1196, 1202, 1205, 1206, 1212, 1213, 1219, 1221, 1222, 1225, 1226, 1227, 1229, 1233, 1242, 1250, 1251, 1253, 1256, 1259, 1264, 1267, 1269, 1271, 1274, 1286, 1289, 1292, 1295, 1205, 1219, 1231, 1250, 1251, 1269, 1289, 1292, 1302, 1354, 1361, 1362, 1363, 1364, 1372, 1375, 1379, 1380, 1385, 1391, 1393, 1302, 1334, 1338, 1375, 1385, 1391, 1393, 1408, 1410, 1412, 1413, 1418, 1422, 1428, 1430, 1435, 1436, 1437, 1440, 1444, 1446, 1447, 1448, 1449, 1455, 1456, 1465, 1472, 1473, 1486, 1489, 1499, 1413, 1418, 1428, 1435, 1440, 1446, 1481, 1489, 1508, 1536, 537, 549, 562, 573, 578, 580, 592, 598, 602, 668, 716, 783, 803, 808, 812, 873, 988, 18, 25, 27, 30, 40, 42, 56, 90, 123, 139, 147, 157, 162, 173, 182, 191, 192, 199, 215, 218, 219, 222, 238, 243, 257, 284, 287, 288, 300, 328, 329, 355, 359, 389]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct answers from manually gen files that don't match GSM8K answers:\")\n",
    "print(results[\"correct_answer_mismatch\"])\n",
    "print()\n",
    "\n",
    "print(\"Wrong answers that fully match GSM8K answers:\")\n",
    "print(results[\"wrong_answer_full_matches\"])\n",
    "print()\n",
    "\n",
    "print(\"Wrong answers that are weird (one line differs, but later lines match):\")\n",
    "print(results[\"wrong_answer_weird\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680a2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
