{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69138661",
   "metadata": {},
   "source": [
    "#### Cell 1: Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a72283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found at: d:\\ErdosDL\\Erdos-DL-June25-Math\n",
      "Data directory found at: d:\\ErdosDL\\Erdos-DL-June25-Math\\data\n",
      "Raw template output directory set to: d:\\ErdosDL\\Erdos-DL-June25-Math\\data\\template-generated-raw\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "# To contain generated template txt files\n",
    "TEMPLATE_OUTPUT_DIR = DATA_DIR / \"template-generated-raw\" \n",
    "\n",
    "# Contains sample template json files, questions answers, and user prompt prefixes\n",
    "TEMPLATE_EXAMPLE_DIR = DATA_DIR / \"template-examples-raw\"\n",
    "\n",
    "TIER_OUTPUT_DIRS = {f\"tier{i}\": TEMPLATE_OUTPUT_DIR / f\"tier{i}\" for i in range(1, 6)}\n",
    "TIER_EXAMPLES_DIRS = {f\"tier{i}\": TEMPLATE_EXAMPLE_DIR / f\"tier{i}\" for i in range(1, 6)}\n",
    "\n",
    "# Make the directory for the tier if it doesn't exist\n",
    "for tier_dir in TIER_OUTPUT_DIRS.values():\n",
    "    tier_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tier_dir in TIER_EXAMPLES_DIRS.values():\n",
    "    tier_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root found at: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory found at: {DATA_DIR}\")\n",
    "print(f\"Raw template output directory set to: {TEMPLATE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789a77f",
   "metadata": {},
   "source": [
    "#### Cell 2: Dataset and tier definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14957c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program_Files\\Miniconda\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 7.94kB [00:00, 7.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier definitions loaded.\n",
      "tier1     : 2767 samples\n",
      "tier2     : 837 samples\n",
      "tier3     : 3113 samples\n",
      "tier4     : 544 samples\n",
      "tier5     : 212 samples\n",
      "Total     : 7473 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset \n",
    "import re\n",
    "\n",
    "# --- Load GSM8K Dataset ---\n",
    "GSM8K_TRAIN: Dataset = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
    "\n",
    "# --- Tier Definition Functions ---\n",
    "def has_computational_division(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'/\\s*\\d')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def has_float(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'(?<!\\d)\\.\\d+|\\d+\\.\\d+')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def is_symbolic(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'^Let [a-zA-Z] ', re.MULTILINE)\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def mutually_disjoint_tiers(dataset: Dataset) -> dict[str, list[int]]:\n",
    "    tiers = {}\n",
    "\n",
    "    symbolic_set = set(idx for idx, sample in enumerate(dataset) if is_symbolic(sample.get(\"answer\", \"\")))\n",
    "\n",
    "    non_symbolic_indices = [idx for idx in range(len(dataset)) if idx not in symbolic_set]\n",
    "\n",
    "    tiers[\"tier1\"] = sorted([idx for idx in non_symbolic_indices if not has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "\n",
    "    tiers[\"tier2\"] = sorted([idx for idx in non_symbolic_indices if has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "\n",
    "    tiers[\"tier3\"] = sorted([idx for idx in non_symbolic_indices if not has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "\n",
    "    tiers[\"tier4\"] = sorted([idx for idx in non_symbolic_indices if has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "    \n",
    "    tiers[\"tier5\"] = sorted(list(symbolic_set))\n",
    "    return tiers\n",
    "\n",
    "TIER_LISTS = mutually_disjoint_tiers(GSM8K_TRAIN)\n",
    "print(\"Tier definitions loaded.\")\n",
    "\n",
    "# Display the number of samples in each tier\n",
    "for tier, indices in TIER_LISTS.items():\n",
    "    print(f\"{tier:<10}: {len(indices)} samples\")\n",
    "print(f\"{'Total':<10}: {len(GSM8K_TRAIN)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31607ee",
   "metadata": {},
   "source": [
    "#### **Selecting the current tier for this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60720481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier for this notebook: tier4\n",
      "Number of samples in tier4: 544\n",
      "Output directory for tier4: d:\\ErdosDL\\Erdos-DL-June25-Math\\data\\template-generated-raw\\tier4\n",
      "Examples directory for tier4: d:\\ErdosDL\\Erdos-DL-June25-Math\\data\\template-examples-raw\\tier4\n"
     ]
    }
   ],
   "source": [
    "### --- Select the tier ONCE for this notebook --- ###\n",
    "CURRENT_TIER = \"tier4\"\n",
    "\n",
    "CURRENT_INDICES = TIER_LISTS[CURRENT_TIER]\n",
    "CURRENT_TIER_OUTPUT_DIR = TIER_OUTPUT_DIRS[CURRENT_TIER]\n",
    "CURRENT_TIER_EXAMPLES_DIR = TIER_EXAMPLES_DIRS[CURRENT_TIER]\n",
    "\n",
    "print(f\"Tier for this notebook: {CURRENT_TIER}\")\n",
    "print(f\"Number of samples in {CURRENT_TIER}: {len(CURRENT_INDICES)}\")\n",
    "print(f\"Output directory for {CURRENT_TIER}: {CURRENT_TIER_OUTPUT_DIR}\")\n",
    "print(f\"Examples directory for {CURRENT_TIER}: {CURRENT_TIER_EXAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a653ef",
   "metadata": {},
   "source": [
    "#### Cell 3: Prompt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0bf0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt for index 1 in tier4 has 22581 characters:\n",
      "--------------------------------------------------------------------------------\n",
      "In the TASK below, you will be given a math problem and its corresponding step-by-step solution. Each step in the solution is numbered (e.g. \"L1\", \"L2\" and so on), and many of the steps include calculator annotations (e.g. \"<<20*0.1=2>>\"). Your goal is to convert this information into a structured JSON object according to the following detialed instructions.\n",
      "\n",
      "# Detailed Field Instructions\n",
      "\n",
      "## 1. \"function_code\"\n",
      "\n",
      "This string must contain a Python function with the following characteristics:\n",
      "\n",
      "*   **1.A. Handling Imports:** The `function_code` should have no imports **unless** your formalization requires the `Fraction` object.\n",
      "    *   If your code uses the `Fraction()` constructor, the very first line of the string MUST be `from fractions import Fraction`.\n",
      "    *   If your code does **not** use the `Fraction()` constructor (e.g., it only uses integers, floats, or standard division `/`), the function definition `def solve():` MUST be the very first line.\n",
      "*   **1.B. Function Naming & Docstring:** The function must be named `solve`, and it should not have any args. It must begin with a docstring that has exactly two lines:\n",
      "    *   **1.B.i.** The first line must be: \"Index: [Index].\" using the index from the task header.\n",
      "    *   **1.B.ii.** The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
      "*   **1.C. Line comments:** For each solution line that is used to compute the final answer, include a comment of the form `# L1`, `# L2` and so on, which references the line number.\n",
      "    *   **1.C.i.** Such a comment must immediately be followed by a code block that precisely formalizes the corresponding solution line. More details about code blocks are provided in 1.D below.\n",
      "    *   **1.C.ii.** If a solution line does not contain any computation relevant to the final answer, then omit it completely from the function code and do NOT add a corresponding line comment.\n",
      "*   **1.D. Code blocks:** Each code block constitutes a complete formalization of its corresponding solution line. It must consist of the following:\n",
      "    *   **1.D.i. Input Variables** First, define any NEW variables needed for the computation, i.e. that will be used for the first time in the solution. Each input variable MUST be followed by a comment (`#`) in the same line. These variables fall into two categories:\n",
      "        *   \"question_inputs\": These are variables whose values are stated in or can be extracted from the question text (only the question text, NOT the answer text). The comment for these variables should quote or refer to the phrase in the question text from which it is extracted.\n",
      "        *   \"WK_inputs\": These are variables drawn from common-sense \"World Knowledge\" (e.g. `minutes_per_hour = 60`, `dozen = 12`). The comment for these variables MUST simply say `# WK`.\n",
      "    *   **1.D.ii. Output Variables** Second, there should be EXACTLY ONE line of code which formalizes the computation in the solution line and assigns the resulting value to a new variable (this is the \"output_variable\" field).\n",
      "*   **1.E. The Direct Substitution Rule:** This is the MOST IMPORTANT RULE, which ensures that the \"solution_line_template\" is purely identical to the original solution line except that numerical values in computations have been replaced with variable placeholders: You MUST define variables in such a way that they can be DIRECTLY SUBSTITUTED into the solution line without changing any operators or surrounding text in the line.\n",
      "*   **1.F. Final Answer:** The line that assigns the final result to the `answer` variable must be immediately preceded by a line containing only the comment `# FA`. The last line of the function must always return the `answer` variable.\n",
      "\n",
      "## 2. \"solution_line_template\"\n",
      "\n",
      "*   **2.A.** The template should be EXACTLY identical to the original solution line, with the ONLY CHANGES being that every NUMERICAL value used in a computation is replaced by its corresponding `{variable_name}` placeholder. This applies to the entire content of the solution line, including the inside and outside of the calculator annotations.\n",
      "*   **2.B.** In particular, EVERY SINGLE numerical value appearing inside the calculator annotation (`<<..>>`) MUST be replaced with a `{variable_name}` placeholder.\n",
      "*   **2.C.** Note: some quantities may appear as words in the solution line (e.g. \"twice as many\"). Do NOT attempt to replace these with variable name placeholders.\n",
      "*   **2.D.** The Direct Substitution Rule will ensure that for correctly defined variables, it will be possible to replace the numerical values with variable name placeholders while leaving all surrounding text, symbols, and operators unchanged. Thus, in a correct \"solution_line_template\", the calculator annotation will not contain any numerical values, and moreover, replacing each `{variable_name}` by its value should exactly recover the original solution line, including the original calculator annotation.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Given below are three examples that illustrate what a perfect formalization will look like. For each example, you are given the following:\n",
      "\n",
      "*   Input: consisting of an index, question, and solution mapping.\n",
      "*   Output: complete output, wrapped inside ```json .. ```\n",
      "\n",
      "In all examples, you will observe the following:\n",
      "\n",
      "*   **A rigid adherence to the Direct Substitution Rule (1.E)**. This is the most important principle. The `solution_line_template` must be an exact copy of the original solution line, with only computational numbers replaced by `{variable}` placeholders. Every other point follows from this rule.\n",
      "\n",
      "*   **How to formalize a rational number depends entirely on how it appears in the solution text.** It is crucial to distinguish between `floats`, `Fractions`, and the division operation (`/`).\n",
      "    *   **Use `float` for decimals:** In **Example 1**, \"half\" is used in the calculation as `0.5`, so it is formalized as `kevin_nice_fraction = 0.5`. In **Example 3**, \"30 percent\" is used as `.30`, formalized as `third_night_stolen_percent = 0.30`.\n",
      "    *   **Use `fractions.Fraction` for fractional quantities:** If \"/\" acts as a separator within a single fractional quantity, use `Fraction`. In **Example 1**, \"three-fourths\" appears as `(3/4)`, so it is formalized as `Fraction(3, 4)`. This requires the `from fractions import Fraction` import.\n",
      "    *   **Use the `/` operator for division operations:** If \"/\" represents the operation of dividing two values, use the standard operator. In **Example 2**, the solution `300/100` is a division operation, formalized using `/`: `total_dollars = total_cents / cents_per_dollar`. This does **not** require an import.\n",
      "\n",
      "*   **A single template can require multiple formalizations.** **Example 1** shows a problem using both `float` (`0.5`) and `Fraction` (`3/4`). **Example 4** shows a mix of percentages (decomposed into floats) and division (`/`). The model must adapt to each line.\n",
      "\n",
      "*   **Decomposition is sometimes needed to follow the rules.** This is a critical and advanced point. In **Example 4**, the solution for L1 is `80% * 20 votes = <<80*0.01*20=16>>16 votes`. To satisfy the Direct Substitution Rule for the calculator annotation, this must be decomposed into two variables: `taotd_discard_percent_num = 80` and `percent_factor = 0.01`.\n",
      "\n",
      "*   **Strict adherence to defining only NEW variables** in each step's `question_inputs` and `WK_inputs` lists. For instance, in **Example 3**, `initial_ducks` is defined in L1 and then simply re-used in the computation for L2 without being listed as an input again.\n",
      "\n",
      "*   Comments for `question_inputs` must cite the question text only, **NEVER** the solution text. Note how `cents_per_dollar = 100` in **Example 2** is correctly labeled `# WK` because that fact constitutes common-sense World Knowledge and is not present in the question text.\n",
      "\n",
      "## Example 1\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "3847\n",
      "\n",
      "**Question:**\n",
      "All people named Barry are nice, while only half of the people named Kevin are nice.  Three-fourths of people named Julie are nice, while 10% of people named Joe are nice.  If a crowd contains 24 people named Barry, 20 people named Kevin, 80 people named Julie, and 50 people named Joe, how many nice people are in the crowd?\n",
      "\n",
      "**Solution mapping:**\n",
      "{\n",
      "  \"L1\": \"If all people named Barry are nice, and the crowd contains 24 people named Barry, then 1*24=<<24*1=24>>24 of these people are nice.\",\n",
      "  \"L2\": \"If only half of people named Kevin are nice, and the crowd contains 20 people named Kevin, then 0.5*20=<<0.5*20=10>>10 of these people are nice.\",\n",
      "  \"L3\": \"If three-fourths of people named Julie are nice, and the crowd contains 80 people named Julie, then (3/4)*80=<<3/4*80=60>>60 of these people are nice.\",\n",
      "  \"L4\": \"If 10% of people named Joe are nice, and the crowd contains 50 people named Joe, then 0.1*50=<<0.1*50=5>>5 of these people are nice.\",\n",
      "  \"L5\": \"In total, the crowd contains 24+10+60+5=<<24+10+60+5=99>>99 people who are nice.\"\n",
      "}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"from fractions import Fraction\\n\\ndef solve():\\n    \\\"\\\"\\\"Index: 3847.\\n    Returns: the number of nice people in the crowd.\\n    \\\"\\\"\\\"\\n    # L1\\n    num_barry = 24 # 24 people named Barry\\n    barry_nice_fraction = 1 # All people named Barry are nice\\n    nice_barry = barry_nice_fraction * num_barry\\n\\n    # L2\\n    num_kevin = 20 # 20 people named Kevin\\n    kevin_nice_fraction = 0.5 # half of the people named Kevin are nice\\n    nice_kevin = kevin_nice_fraction * num_kevin\\n\\n    # L3\\n    num_julie = 80 # 80 people named Julie\\n    julie_nice_fraction = Fraction(3, 4) # Three-fourths of people named Julie are nice\\n    nice_julie = julie_nice_fraction * num_julie\\n\\n    # L4\\n    num_joe = 50 # 50 people named Joe\\n    joe_nice_fraction = 0.1 # 10% of people named Joe are nice\\n    nice_joe = joe_nice_fraction * num_joe\\n\\n    # L5\\n    total_nice_people = nice_barry + nice_kevin + nice_julie + nice_joe\\n\\n    # FA\\n    answer = total_nice_people\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"num_barry\", \"barry_nice_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"nice_barry\",\n",
      "      \"solution_line_template\": \"If all people named Barry are nice, and the crowd contains {num_barry} people named Barry, then {barry_nice_fraction}*{num_barry}=<<{num_barry}*{barry_nice_fraction}={nice_barry}>>{nice_barry} of these people are nice.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [\"num_kevin\", \"kevin_nice_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"nice_kevin\",\n",
      "      \"solution_line_template\": \"If only half of people named Kevin are nice, and the crowd contains {num_kevin} people named Kevin, then {kevin_nice_fraction}*{num_kevin}=<<{kevin_nice_fraction}*{num_kevin}={nice_kevin}>>{nice_kevin} of these people are nice.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"num_julie\", \"julie_nice_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"nice_julie\",\n",
      "      \"solution_line_template\": \"If three-fourths of people named Julie are nice, and the crowd contains {num_julie} people named Julie, then ({julie_nice_fraction})*{num_julie}=<<{julie_nice_fraction}*{num_julie}={nice_julie}>>{nice_julie} of these people are nice.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [\"num_joe\", \"joe_nice_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"nice_joe\",\n",
      "      \"solution_line_template\": \"If 10% of people named Joe are nice, and the crowd contains {num_joe} people named Joe, then {joe_nice_fraction}*{num_joe}=<<{joe_nice_fraction}*{num_joe}={nice_joe}>>{nice_joe} of these people are nice.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L5\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"total_nice_people\",\n",
      "      \"solution_line_template\": \"In total, the crowd contains {nice_barry}+{nice_kevin}+{nice_julie}+{nice_joe}=<<{nice_barry}+{nice_kevin}+{nice_julie}+{nice_joe}={total_nice_people}>>{total_nice_people} people who are nice.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Example 2\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "4847\n",
      "\n",
      "**Question:**\n",
      "Lucy plans to purchase potato chips for a party. Ten people will be at the party, including Lucy. The potato chips cost 25 cents per pound. How much will Lucy pay (in dollars) for the potato chips if she wants each person to get 1.2 pounds?\n",
      "\n",
      "**Solution mapping:**\n",
      "{\n",
      "  \"L1\": \"Lucy needs to purchase 10 x 1.2 = <<10*1.2=12>>12 pounds of potato chips.\",\n",
      "  \"L2\": \"So, Lucy will pay 12 x 25 = <<12*25=300>>300 cents for it.\",\n",
      "  \"L3\": \"Since there are 100 cents in $1, thus, Lucy will pay 300/100 = <<300/100=3>>3 dollars.\"\n",
      "}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"def solve():\\n    \\\"\\\"\\\"Index: 4847.\\n    Returns: the amount Lucy will pay in dollars.\\n    \\\"\\\"\\\"\\n    # L1\\n    num_people = 10 # Ten people will be at the party\\n    chips_per_person = 1.2 # each person to get 1.2 pounds\\n    total_pounds = num_people * chips_per_person\\n\\n    # L2\\n    cents_per_pound = 25 # 25 cents per pound\\n    total_cents = total_pounds * cents_per_pound\\n\\n    # L3\\n    cents_per_dollar = 100 # WK\\n    total_dollars = total_cents / cents_per_dollar\\n\\n    # FA\\n    answer = total_dollars\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"num_people\", \"chips_per_person\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"total_pounds\",\n",
      "      \"solution_line_template\": \"Lucy needs to purchase {num_people} x {chips_per_person} = <<{num_people}*{chips_per_person}={total_pounds}>>{total_pounds} pounds of potato chips.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [\"cents_per_pound\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"total_cents\",\n",
      "      \"solution_line_template\": \"So, Lucy will pay {total_pounds} x {cents_per_pound} = <<{total_pounds}*{cents_per_pound}={total_cents}>>{total_cents} cents for it.\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [\"cents_per_dollar\"],\n",
      "      \"output_variable\": \"total_dollars\",\n",
      "      \"solution_line_template\": \"Since there are 100 cents in $1, thus, Lucy will pay {total_cents}/{cents_per_dollar} = <<{total_cents}/{cents_per_dollar}={total_dollars}>>{total_dollars} dollars.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Example 3\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "5040\n",
      "\n",
      "**Question:**\n",
      "There are 320 ducks in a pond.  On the first night 1/4 of them get eaten by a fox.  On the second night 1/6 of the remaining ducks fly away, and on the third night 30 percent are stolen.  How many ducks remain after the three nights?\n",
      "\n",
      "**Solution mapping:**\n",
      "{\n",
      "  \"L1\": \"First night:320(1/4)=80\",\n",
      "  \"L2\": \"320-80=<<320-80=240>>240\",\n",
      "  \"L3\": \"Second night:240(1/6)=40\",\n",
      "  \"L4\": \"240-40=<<240-40=200>>200\",\n",
      "  \"L5\": \"Third night:200(.30)=60\",\n",
      "  \"L6\": \"200-60=<<200-60=140>>140 ducks remain\"\n",
      "}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"from fractions import Fraction\\n\\ndef solve():\\n    \\\"\\\"\\\"Index: 5040.\\n    Returns: the number of ducks remaining after three nights.\\n    \\\"\\\"\\\"\\n    # L1\\n    initial_ducks = 320 # 320 ducks in a pond\\n    first_night_eaten_fraction = Fraction(1, 4) # 1/4 of them get eaten\\n    first_night_eaten = initial_ducks * first_night_eaten_fraction\\n\\n    # L2\\n    ducks_after_night1 = initial_ducks - first_night_eaten\\n\\n    # L3\\n    second_night_flew_fraction = Fraction(1, 6) # 1/6 of the remaining ducks fly away\\n    second_night_flew = ducks_after_night1 * second_night_flew_fraction\\n\\n    # L4\\n    ducks_after_night2 = ducks_after_night1 - second_night_flew\\n\\n    # L5\\n    third_night_stolen_percent = 0.30 # 30 percent are stolen\\n    third_night_stolen = ducks_after_night2 * third_night_stolen_percent\\n\\n    # L6\\n    ducks_after_night3 = ducks_after_night2 - third_night_stolen\\n\\n    # FA\\n    answer = ducks_after_night3\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"initial_ducks\", \"first_night_eaten_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"first_night_eaten\",\n",
      "      \"solution_line_template\": \"First night:{initial_ducks}({first_night_eaten_fraction})={first_night_eaten}\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"ducks_after_night1\",\n",
      "      \"solution_line_template\": \"{initial_ducks}-{first_night_eaten}=<<{initial_ducks}-{first_night_eaten}={ducks_after_night1}>>{ducks_after_night1}\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"second_night_flew_fraction\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"second_night_flew\",\n",
      "      \"solution_line_template\": \"Second night:{ducks_after_night1}({second_night_flew_fraction})={second_night_flew}\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"ducks_after_night2\",\n",
      "      \"solution_line_template\": \"{ducks_after_night1}-{second_night_flew}=<<{ducks_after_night1}-{second_night_flew}={ducks_after_night2}>>{ducks_after_night2}\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L5\",\n",
      "      \"question_inputs\": [\"third_night_stolen_percent\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"third_night_stolen\",\n",
      "      \"solution_line_template\": \"Third night:{ducks_after_night2}({third_night_stolen_percent})={third_night_stolen}\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L6\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"ducks_after_night3\",\n",
      "      \"solution_line_template\": \"{ducks_after_night2}-{third_night_stolen}=<<{ducks_after_night2}-{third_night_stolen}={ducks_after_night3}>>{ducks_after_night3} ducks remain\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Example 4\n",
      "\n",
      "### Input\n",
      "\n",
      "**Index:**\n",
      "7037\n",
      "\n",
      "**Question:**\n",
      "At a book burning, Fran counts 20 votes for The Art of the Deal, 12 votes for Twilight, and 10 votes for Game of Thrones.  She decides to alter the results by throwing away 80% of the votes for The Art of the Deal and half the votes for Twilight. What percentage of the altered votes are for Game of Thrones?\n",
      "\n",
      "**Solution mapping:**\n",
      "{\n",
      "  \"L1\": \"First find the total number of The Art of the Deal votes Fran throws away: 80% * 20 votes = <<80*0.01*20=16>>16 votes\",\n",
      "  \"L2\": \"Then subtract these votes from the total number of The Art of the Deal votes to find the altered number: 20 votes - 16 votes = <<20-16=4>>4 votes\",\n",
      "  \"L3\": \"Then divide the total number Twilight votes by 2 to find the altered number of votes: 12 votes / 2 = <<12/2=6>>6 votes\",\n",
      "  \"L4\": \"Then add the altered number of votes for each book to find the total altered number of votes: 6 votes + 4 votes + 10 votes = <<6+4+10=20>>20 votes\",\n",
      "  \"L5\": \"Then divide the number of votes for Game of Thrones by the total altered number of votes and multiply by 100% to express the answer as a percentage: 10 votes / 20 votes * 100% = 50%\"\n",
      "}\n",
      "\n",
      "### Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_code\": \"def solve():\\n    \\\"\\\"\\\"Index: 7037.\\n    Returns: the percentage of altered votes for Game of Thrones.\\n    \\\"\\\"\\\"\\n    # L1\\n    taotd_initial_votes = 20 # 20 votes for The Art of the Deal\\n    taotd_discard_percent_num = 80 # throws away 80% of the votes\\n    percent_factor = 0.01 # WK\\n    taotd_discarded_votes = taotd_discard_percent_num * percent_factor * taotd_initial_votes\\n\\n    # L2\\n    taotd_altered_votes = taotd_initial_votes - taotd_discarded_votes\\n\\n    # L3\\n    twilight_initial_votes = 12 # 12 votes for Twilight\\n    twilight_discard_divisor = 2 # half the votes for Twilight\\n    twilight_altered_votes = twilight_initial_votes / twilight_discard_divisor\\n\\n    # L4\\n    got_initial_votes = 10 # 10 votes for Game of Thrones\\n    total_altered_votes = twilight_altered_votes + taotd_altered_votes + got_initial_votes\\n\\n    # L5\\n    percent_multiplier = 100 # WK\\n    got_percentage = got_initial_votes / total_altered_votes * percent_multiplier\\n\\n    # FA\\n    answer = got_percentage\\n    return answer\",\n",
      "  \"logical_steps\": [\n",
      "    {\n",
      "      \"line_number\": \"L1\",\n",
      "      \"question_inputs\": [\"taotd_discard_percent_num\", \"taotd_initial_votes\"],\n",
      "      \"WK_inputs\": [\"percent_factor\"],\n",
      "      \"output_variable\": \"taotd_discarded_votes\",\n",
      "      \"solution_line_template\": \"First find the total number of The Art of the Deal votes Fran throws away: 80% * {taotd_initial_votes} votes = <<{taotd_discard_percent_num}*{percent_factor}*{taotd_initial_votes}={taotd_discarded_votes}>>{taotd_discarded_votes} votes\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L2\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"taotd_altered_votes\",\n",
      "      \"solution_line_template\": \"Then subtract these votes from the total number of The Art of the Deal votes to find the altered number: {taotd_initial_votes} - {taotd_discarded_votes} votes = <<{taotd_initial_votes}-{taotd_discarded_votes}={taotd_altered_votes}>>{taotd_altered_votes} votes\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L3\",\n",
      "      \"question_inputs\": [\"twilight_initial_votes\", \"twilight_discard_divisor\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"twilight_altered_votes\",\n",
      "      \"solution_line_template\": \"Then divide the total number Twilight votes by {twilight_discard_divisor} to find the altered number of votes: {twilight_initial_votes} votes / {twilight_discard_divisor} = <<{twilight_initial_votes}/{twilight_discard_divisor}={twilight_altered_votes}>>{twilight_altered_votes} votes\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L4\",\n",
      "      \"question_inputs\": [\"got_initial_votes\"],\n",
      "      \"WK_inputs\": [],\n",
      "      \"output_variable\": \"total_altered_votes\",\n",
      "      \"solution_line_template\": \"Then add the altered number of votes for each book to find the total altered number of votes: {twilight_altered_votes} votes + {taotd_altered_votes} votes + {got_initial_votes} votes = <<{twilight_altered_votes}+{taotd_altered_votes}+{got_initial_votes}={total_altered_votes}>>{total_altered_votes} votes\"\n",
      "    },\n",
      "    {\n",
      "      \"line_number\": \"L5\",\n",
      "      \"question_inputs\": [],\n",
      "      \"WK_inputs\": [\"percent_multiplier\"],\n",
      "      \"output_variable\": \"got_percentage\",\n",
      "      \"solution_line_template\": \"Then divide the number of votes for Game of Thrones by the total altered number of votes and multiply by {percent_multiplier}% to express the answer as a percentage: {got_initial_votes} votes / {total_altered_votes} votes * {percent_multiplier}% = {got_percentage}%\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "## Input\n",
      "\n",
      "**Index:**:\n",
      "1\n",
      "\n",
      "**Question:**:\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "**Solution mapping:**:\n",
      "{\n",
      "  \"L1\": \"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\",\n",
      "  \"L2\": \"Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\"\n",
      "}\n",
      "\n",
      "## Output\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def sanitize_text(text: str):\n",
    "    \"\"\"\n",
    "    Replaces a comprehensive set of problematic Unicode characters with their\n",
    "    ASCII equivalents to prevent model generation and string parsing errors.\n",
    "    \"\"\"\n",
    "    replacements = {\n",
    "        # Mathematical Operators\n",
    "        \"\\u2212\": \"-\",  # Minus Sign\n",
    "        \"\\u00d7\": \"*\",  # Multiplication Sign\n",
    "        \"\\u00f7\": \"/\",  # Division Sign\n",
    "        \"\\u22c5\": \"*\",  # Dot Operator\n",
    "        \n",
    "        # Typographic Quotes\n",
    "        \"\\u201c\": '\"',  # Left Double Quotation Mark\n",
    "        \"\\u201d\": '\"',  # Right Double Quotation Mark\n",
    "        \"\\u2018\": \"'\",  # Left Single Quotation Mark\n",
    "        \"\\u2019\": \"'\",  # Right Single Quotation Mark\n",
    "        \n",
    "        # Typographic Dashes\n",
    "        \"\\u2014\": \"-\",  # Em Dash\n",
    "        \"\\u2013\": \"-\",  # En Dash\n",
    "        \n",
    "        # Other\n",
    "        \"\\u2026\": \"...\", # Horizontal Ellipsis\n",
    "        \"\\u00a0\": \" \",  # Non-breaking Space\n",
    "    }\n",
    "    for uni, ascii_char in replacements.items():\n",
    "        text = text.replace(uni, ascii_char)\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_solution_mapping(\n",
    "        index: int, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "        exclude_FA: bool = True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Extracts the natural language solution, sanitizes it, and structures\n",
    "    it into a line-numbered dictionary.\n",
    "    \"\"\"\n",
    "    solution_mapping = {}\n",
    "    solution_text = dataset[index][\"answer\"]\n",
    "    sanitized_solution_text = sanitize_text(solution_text)\n",
    "    \n",
    "    # --- CORRECTION: Use the sanitized variable ---\n",
    "    lines = [ln.strip() for ln in sanitized_solution_text.splitlines() if ln.strip()]\n",
    "\n",
    "    if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "        solution_mapping[\"FA\"] = lines.pop(-1).strip()\n",
    "\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        solution_mapping[f\"L{i}\"] = line\n",
    "\n",
    "    if exclude_FA and \"FA\" in solution_mapping:\n",
    "        del solution_mapping[\"FA\"]\n",
    "\n",
    "    return solution_mapping\n",
    "\n",
    "\n",
    "BASE_TEMPLATE_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"function_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A single string containing a complete, self-contained Python function that constitutes an end-to-end formalization of the solution.\"\n",
    "        },\n",
    "        \"logical_steps\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"line_number\": {\"type\": \"string\"},\n",
    "                    \"question_inputs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"WK_inputs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"output_variable\": {\"type\": \"string\"},\n",
    "                    \"solution_line_template\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"line_number\", \"question_inputs\", \"WK_inputs\", \"output_variable\", \"solution_line_template\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"function_code\", \"logical_steps\"]\n",
    "}\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a data formalization expert who excels in mathematical reasoning and writing python code. You will be presented with a math word problem accompanied by a step-by-step natural language solution. You goal is to carefully and meticulously analyze the given question and solution, and formalize it by converting it into a structured json object that deconstructs the logic of the solution.\n",
    "\n",
    "You MUST follow all rules and formatting instructions provided in the user prompt without deviation. Your entire output MUST be a single JSON object wrapped in ```json ... ```. Do not include any text or explanation before or after the JSON object.\"\"\"\n",
    "\n",
    "STATIC_PREFIXES = {}\n",
    "for tier in TIER_LISTS.keys():\n",
    "    prefix_file = TIER_EXAMPLES_DIRS[tier] / f\"{tier}_user_prompt_prefix.txt\"\n",
    "    with open(prefix_file, 'r', encoding='utf-8') as f:\n",
    "        STATIC_PREFIXES[tier] = f.read()\n",
    "\n",
    "\n",
    "def append_sample_to_user_prompt(\n",
    "        tier: str, \n",
    "        index: int, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Appends a chosen sample from the GSM8K dataset to the user prompt for a specific tier. Returns the complete user prompt, ready to be sent to the LLM for template generation.\n",
    "    \"\"\"\n",
    "    if tier not in TIER_LISTS:\n",
    "        raise ValueError(f\"Invalid tier: {tier}. Must be one of {list(TIER_LISTS.keys())}.\")\n",
    "\n",
    "    sample = dataset[index]\n",
    "    question = sample['question']\n",
    "    answer = build_solution_mapping(index, dataset)\n",
    "\n",
    "    task_block = f\"\"\"## Input\n",
    "\n",
    "**Index:**:\n",
    "{index}\n",
    "\n",
    "**Question:**:\n",
    "{question}\n",
    "\n",
    "**Solution mapping:**:\n",
    "{json.dumps(answer, indent=2)}\n",
    "\n",
    "## Output\n",
    "\n",
    "\"\"\"\n",
    "    return STATIC_PREFIXES[tier] + task_block\n",
    "\n",
    "\n",
    "# Example usage\n",
    "idx = CURRENT_INDICES[0]\n",
    "user_prompt = append_sample_to_user_prompt(CURRENT_TIER, idx, GSM8K_TRAIN)\n",
    "print(f\"User prompt for index {idx} in {CURRENT_TIER} has {len(user_prompt)} characters:\")\n",
    "print(\"-\"*80)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d0856",
   "metadata": {},
   "source": [
    "#### Cell 4: API clients, concurrency limits, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API clients initialized successfully.\n",
      "API concurrency limits set to: {'google': 30, 'anthropic': 2, 'openai': 2}\n",
      "Available models: ['google_gemini-2.5-flash']\n"
     ]
    }
   ],
   "source": [
    "# Imports for API clients and related functionality\n",
    "import os\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# This must be done once per kernel to allow asyncio to run in a Jupyter notebook..\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load API Keys from .env file\n",
    "load_dotenv()\n",
    "print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "# Initialize Asynchronous API Clients\n",
    "try:\n",
    "    #openai_client_async = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    #anthropic_client_async = AsyncClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    print(\"API clients initialized successfully.\")\n",
    "except TypeError:\n",
    "    print(\"API key not found for one or more services. Please check your .env file.\")\n",
    "    # Assign None to prevent errors in subsequent cells\n",
    "    openai_client_async = None\n",
    "    anthropic_client_async = None\n",
    "\n",
    "# Define API Concurrency Limits to prevent 429 \"Too Many Requests\" errors.\n",
    "API_CONCURRENCY_LIMITS = {\n",
    "    \"google\": 30,    \n",
    "    \"anthropic\": 2, \n",
    "    \"openai\": 2,    \n",
    "}\n",
    "print(f\"API concurrency limits set to: {API_CONCURRENCY_LIMITS}\")\n",
    "\n",
    "MODEL_DICT = {\n",
    "  #\"openai\": \"gpt-4.1\",\n",
    "  \"google\": \"gemini-2.5-flash\"\n",
    "}\n",
    "\n",
    "MODELS = [f\"{provider}_{model}\" for provider, model in MODEL_DICT.items()]\n",
    "print(f\"Available models: {MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2d695",
   "metadata": {},
   "source": [
    "#### Cell 5: Helper functions to avoid rate limits in API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb47255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "_log_lock = threading.Lock()\n",
    "def log_event(\n",
    "        level: str, \n",
    "        index: int, \n",
    "        model: str, \n",
    "        message: str\n",
    "    ):\n",
    "    \"\"\"A thread-safe logger for concurrent operations.\"\"\"\n",
    "    with _log_lock:\n",
    "        ts = datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"milliseconds\")\n",
    "        task_id = asyncio.current_task().get_name()\n",
    "        print(f\"{ts} [{level:^7s}] [Idx {index:<4}] [Mdl: {model:<15}] [Task {task_id:<8}] {message}\")\n",
    " \n",
    "\n",
    "async def with_api_retries(\n",
    "        send_coroutine_factory,\n",
    "        *,\n",
    "        model_info: str,  # For informative logging\n",
    "        max_attempts: int = 10,\n",
    "        base_wait_seconds: int = 10\n",
    "    ):\n",
    "    \"\"\"A wrapper to handle API retries with exponential backoff.\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return await send_coroutine_factory()\n",
    "        except (openai.RateLimitError, anthropic.RateLimitError, Exception) as e:\n",
    "            # Check for specific rate limit error types or a 429 status code in the error string\n",
    "            if isinstance(e, (openai.RateLimitError, anthropic.RateLimitError)) or \"429\" in str(e):\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(f\"❌ Final attempt failed for {model_info}. Giving up.\")\n",
    "                    raise\n",
    "                \n",
    "                # Exponential backoff with jitter\n",
    "                wait_time = base_wait_seconds * (2 ** attempt) + random.uniform(0, 1)\n",
    "                \n",
    "                # More informative error message\n",
    "                print(f\"🕒 Rate limit on {model_info}. Retrying in {wait_time:.2f}s... (Attempt {attempt + 1}/{max_attempts})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                # If it's not a rate limit error, re-raise immediately\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "\n",
    "class RateLimitCoordinator:\n",
    "    def __init__(self, refill_rate_per_sec: float, max_tokens: int):\n",
    "        self.refill_rate_per_sec = refill_rate_per_sec\n",
    "        self.max_tokens = float(max_tokens)\n",
    "        self.tokens = self.max_tokens\n",
    "        self._lock = asyncio.Lock()\n",
    "        self._refill_task = None\n",
    "        \n",
    "    async def _refill(self):\n",
    "        \"\"\"The background task that refills the token bucket at 1-second intervals.\"\"\"\n",
    "        while True:\n",
    "            await asyncio.sleep(1)\n",
    "            async with self._lock:\n",
    "                self.tokens = min(self.max_tokens, self.tokens + self.refill_rate_per_sec)\n",
    "                if int(time.time()) % 10 == 0:\n",
    "                    log_event(\"REFILL\", -1, \"Coordinator\", f\"Bucket budget at {int(self.tokens)} / {int(self.max_tokens)}\")\n",
    "\n",
    "    async def start(self):\n",
    "        if self._refill_task is None:\n",
    "            self._refill_task = asyncio.create_task(self._refill())\n",
    "\n",
    "    async def stop(self):\n",
    "        if self._refill_task:\n",
    "            self._refill_task.cancel()\n",
    "            self._refill_task = None\n",
    "\n",
    "    async def get_tokens(self, index: int, model: str, tokens_needed: int):\n",
    "        while True:\n",
    "            async with self._lock:\n",
    "                if self.tokens >= tokens_needed:\n",
    "                    self.tokens -= tokens_needed\n",
    "                    log_event(\"GRANT\", index, model, f\"Permission granted. Budget: {int(self.tokens + tokens_needed)} -> {int(self.tokens)}\")\n",
    "                    return\n",
    "            \n",
    "            deficit = tokens_needed - self.tokens\n",
    "            wait_time = max(0.1, deficit / self.refill_rate_per_sec) # Ensure at least a small wait\n",
    "            \n",
    "            log_event(\"WAIT\", index, model, f\"Budget low. Needed: {tokens_needed}, Have: {int(self.tokens)}. Waiting ~{wait_time:.2f}s...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "\n",
    "    async def return_tokens(self, index: int, model: str, tokens_returned: int):\n",
    "        async with self._lock:\n",
    "            self.tokens += tokens_returned\n",
    "            log_event(\"RETURN\", index, model, f\"Tokens returned on failure. Budget restored to {int(self.tokens)}.\")\n",
    "\n",
    "    async def refund_tokens(self, index: int, model: str, tokens_refunded: int):\n",
    "        \"\"\"Refunds tokens to the budget after a successful call, correcting our estimate.\"\"\"\n",
    "        async with self._lock:\n",
    "            self.tokens += tokens_refunded\n",
    "            log_event(\"REFUND\", index, model, f\"Correcting estimate. Budget restored by {tokens_refunded} -> {int(self.tokens)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c463c",
   "metadata": {},
   "source": [
    "#### Cell 6: Provider-specific API calling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd962c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "async def call_openai_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int,\n",
    "        json_schema: dict,\n",
    "        openai_max_tokens: int \n",
    "    ) -> tuple[str, dict, dict]:\n",
    "    \"\"\"\n",
    "    Prepares a provider-specific schema and handles an API call to OpenAI.\n",
    "    \"\"\"\n",
    "    \n",
    "    openai_schema = copy.deepcopy(json_schema)\n",
    "\n",
    "    def add_additional_properties(schema_part):\n",
    "        if isinstance(schema_part, dict):\n",
    "            if schema_part.get(\"type\") == \"object\":\n",
    "                schema_part[\"additionalProperties\"] = False\n",
    "            for value in schema_part.values():\n",
    "                add_additional_properties(value)\n",
    "        elif isinstance(schema_part, list):\n",
    "            for item in schema_part:\n",
    "                add_additional_properties(item)\n",
    "\n",
    "    add_additional_properties(openai_schema)\n",
    "    openai_schema_wrapper = {\n",
    "        \"name\": \"template_formalization\", \n",
    "        \"strict\": True, \n",
    "        \"schema\": openai_schema\n",
    "    }\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    \n",
    "    model_info = f\"{model} (Index {index})\"\n",
    "    \n",
    "    response_with_headers = await with_api_retries(\n",
    "        lambda: openai_client_async.chat.completions.with_raw_response.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            temperature=0, \n",
    "            max_tokens=openai_max_tokens, \n",
    "            response_format={\"type\": \"json_schema\", \"json_schema\": openai_schema_wrapper} # type: ignore\n",
    "        ),\n",
    "        model_info=model_info\n",
    "    )\n",
    "    \n",
    "    response = response_with_headers.parse()\n",
    "    text_response = response.choices[0].message.content\n",
    "    \n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.prompt_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.completion_tokens\n",
    "        if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details:\n",
    "             usage[\"cached_tokens\"] = response.usage.prompt_tokens_details.get(\"cached_tokens\", 0)\n",
    "\n",
    "    headers = response_with_headers.headers\n",
    "    rate_limit_info = {\n",
    "        \"limit_requests\": headers.get(\"x-ratelimit-limit-requests\"),\n",
    "        \"limit_tokens\": headers.get(\"x-ratelimit-limit-tokens\"),\n",
    "        \"remaining_requests\": headers.get(\"x-ratelimit-remaining-requests\"),\n",
    "        \"remaining_tokens\": headers.get(\"x-ratelimit-remaining-tokens\"),\n",
    "        \"reset_requests\": headers.get(\"x-ratelimit-reset-requests\"),\n",
    "        \"reset_tokens\": headers.get(\"x-ratelimit-reset-tokens\"),\n",
    "    }\n",
    "             \n",
    "    return text_response, usage, rate_limit_info\n",
    "\n",
    "\n",
    "async def call_google_async(\n",
    "        model: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        index: int,\n",
    "        json_schema: dict\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Handles a Google API call with schema enforcement and a safe\n",
    "    upper bound on output tokens to prevent runaway generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    safety_settings = {\n",
    "        \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_NONE\",\n",
    "        \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_NONE\",\n",
    "        \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": \"BLOCK_NONE\",\n",
    "        \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"BLOCK_NONE\",\n",
    "    }\n",
    "    \n",
    "    gemini = genai.GenerativeModel(\n",
    "        model_name=model,\n",
    "        system_instruction=system_prompt,\n",
    "        safety_settings=safety_settings\n",
    "    )\n",
    "    \n",
    "    cfg = genai.types.GenerationConfig(\n",
    "        temperature=0, \n",
    "        max_output_tokens=8192, # Safe upper bound\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=json_schema\n",
    "    )\n",
    "    \n",
    "    model_info = f\"{model} (Index {index})\"\n",
    "\n",
    "    response = await with_api_retries(\n",
    "        lambda: gemini.generate_content_async(user_prompt, generation_config=cfg),\n",
    "        model_info=model_info\n",
    "    )\n",
    "\n",
    "    if not response.parts:\n",
    "        raise ValueError(f\"Google API returned an empty response for Index {index}.\")\n",
    "\n",
    "    text_response = response.text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage_metadata:\n",
    "        usage[\"input_tokens\"] = response.usage_metadata.prompt_token_count\n",
    "        usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "        \n",
    "    return text_response, usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61236607",
   "metadata": {},
   "source": [
    "#### Cell 7: Single API call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdba26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "async def run_single_api_call(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    index: int,\n",
    "    tier: str,\n",
    "    dataset: 'datasets.Dataset',\n",
    "    system_prompt: str,\n",
    "    output_dir: Path,\n",
    "    provider_sem: asyncio.Semaphore,\n",
    "    json_schema: dict,\n",
    "    coordinator: RateLimitCoordinator,\n",
    "    openai_max_tokens: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs a single API call, but first checks if the output file already\n",
    "    exists to avoid re-generating completed work.\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = output_dir / str(index) / f\"{provider}_{model}.txt\"\n",
    "    if output_path.exists() and output_path.stat().st_size > 0:\n",
    "        log_event(\"SKIPPED\", index, model, \"Output file already exists.\")\n",
    "        return {\n",
    "            \"provider\": provider, \"model\": model, \"index\": index,\n",
    "            \"status\": \"Skipped\", \"time_s\": 0,\n",
    "            \"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0,\n",
    "            \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "        }\n",
    "        \n",
    "    async with provider_sem:\n",
    "        user_prompt = append_sample_to_user_prompt(tier=tier, index=index, dataset=dataset)\n",
    "\n",
    "        tokens_needed = 0\n",
    "        if provider == \"openai\":\n",
    "            estimated_prompt_tokens = len(user_prompt.split()) * 1.25\n",
    "            tokens_needed = int(estimated_prompt_tokens + openai_max_tokens)\n",
    "            await coordinator.get_tokens(index, model, tokens_needed)\n",
    "\n",
    "        start_time = time.time()\n",
    "        status = \"Failed\"\n",
    "        usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "\n",
    "        try:\n",
    "            if provider == \"openai\":\n",
    "                text_response, usage, _ = await call_openai_async(\n",
    "                    model, system_prompt, user_prompt, index, json_schema, openai_max_tokens\n",
    "                )\n",
    "                log_event(\"SUCCESS\", index, model, f\"Call OK. Usage: {json.dumps(usage)}\")\n",
    "                \n",
    "                true_cost = usage['input_tokens'] + usage['output_tokens']\n",
    "                refund_amount = tokens_needed - true_cost\n",
    "                if refund_amount > 0:\n",
    "                    await coordinator.refund_tokens(index, model, refund_amount)\n",
    "                    \n",
    "            elif provider == \"google\":\n",
    "                text_response, usage = await call_google_async(\n",
    "                    model, system_prompt, user_prompt, index, json_schema\n",
    "                )\n",
    "                log_event(\"SUCCESS\", index, model, f\"Call OK. Usage: {json.dumps(usage)}\")\n",
    "            \n",
    "            status = \"Success\"\n",
    "\n",
    "        except Exception as e:\n",
    "            log_event(\"ERROR\", index, model, f\"{type(e).__name__}: {str(e).splitlines()[0]}\")\n",
    "            text_response = f\"--- ERROR ---\\nIndex: {index}, Model: {model}\\n{type(e).__name__}: {e}\"\n",
    "            \n",
    "        finally:\n",
    "            if status == \"Failed\" and provider == \"openai\" and tokens_needed > 0:\n",
    "                await coordinator.return_tokens(index, model, tokens_needed)\n",
    "\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        output_path.write_text(text_response, encoding='utf-8')\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"provider\": provider, \"model\": model, \"index\": index,\n",
    "            \"status\": status, \"time_s\": round(elapsed, 2),\n",
    "            \"input_tokens\": usage[\"input_tokens\"], \"output_tokens\": usage[\"output_tokens\"],\n",
    "            \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "            \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9593e58",
   "metadata": {},
   "source": [
    "#### Cell 8: Main template generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "async def generate_templates_parallel_fixed(\n",
    "    indices_to_generate: list[int],\n",
    "    tier: str = CURRENT_TIER,\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "    model_dict: dict[str, str] = MODEL_DICT,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    concurrency_limits: dict[str, int] = API_CONCURRENCY_LIMITS,\n",
    "    json_schema: dict = BASE_TEMPLATE_SCHEMA,\n",
    "    openai_max_tokens: int = 4000\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fully parallel version with a robust token bucket coordinator for OpenAI.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Template Generation (Token Bucket) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    openai_coordinator = RateLimitCoordinator(refill_rate_per_sec=800, max_tokens=30000)\n",
    "    await openai_coordinator.start()\n",
    "    \n",
    "    provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "    output_dir = TIER_OUTPUT_DIRS[tier]\n",
    "    \n",
    "    all_tasks = []\n",
    "    for index in indices_to_generate:\n",
    "        for provider, model in model_dict.items():\n",
    "            task = asyncio.create_task(\n",
    "                run_single_api_call(\n",
    "                    provider=provider, model=model, index=index,\n",
    "                    tier=tier, dataset=dataset, system_prompt=system_prompt,\n",
    "                    output_dir=output_dir, provider_sem=provider_semaphores[provider],\n",
    "                    json_schema=json_schema,\n",
    "                    coordinator=openai_coordinator,\n",
    "                    openai_max_tokens=openai_max_tokens\n",
    "                )\n",
    "            )\n",
    "            all_tasks.append(task)\n",
    "    \n",
    "    print(f\"Created {len(all_tasks)} total API call tasks\")\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        with tqdm(total=len(all_tasks), desc=\"API Calls\") as pbar:\n",
    "            for task in asyncio.as_completed(all_tasks):\n",
    "                result = await task\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "    finally:\n",
    "        await openai_coordinator.stop()\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Template Generation Complete ---\")\n",
    "    print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "    success_count = len(df[df['status'] == 'Success'])\n",
    "    total_calls = len(df)\n",
    "    if total_calls > 0:\n",
    "      print(f\"Success rate: {success_count}/{total_calls} ({100*success_count/total_calls:.1f}%)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219062ef",
   "metadata": {},
   "source": [
    "#### Cell 9: Running the main Template generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation for 359 indices in tier4 between 3001 and 7473.\n",
      "--- Starting Template Generation (Token Bucket) ---\n",
      "Created 359 total API call tasks\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m INDICES_TO_GENERATE = [idx \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m CURRENT_INDICES \u001b[38;5;28;01mif\u001b[39;00m idx <= UPPER_LIMIT \u001b[38;5;129;01mand\u001b[39;00m idx >= LOWER_LIMIT]\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting generation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(INDICES_TO_GENERATE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m indices in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURRENT_TIER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLOWER_LIMIT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUPPER_LIMIT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m perf_df = \u001b[38;5;28;01mawait\u001b[39;00m generate_templates_parallel_fixed(indices_to_generate=INDICES_TO_GENERATE)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Generation Performance Summary ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m perf_df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mgenerate_templates_parallel_fixed\u001b[39m\u001b[34m(indices_to_generate, tier, dataset, model_dict, system_prompt, concurrency_limits, json_schema, openai_max_tokens)\u001b[39m\n\u001b[32m     43\u001b[39m results = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAPI Calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m asyncio.as_completed(all_tasks):\n\u001b[32m     47\u001b[39m             result = \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files\\Miniconda\\envs\\genai\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program_Files\\Miniconda\\envs\\genai\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-18T20:38:21.541+00:00 [SUCCESS] [Idx 3150] [Mdl: gemini-2.5-flash] [Task Task-13 ] Call OK. Usage: {\"input_tokens\": 7170, \"output_tokens\": 454, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:22.196+00:00 [SUCCESS] [Idx 3002] [Mdl: gemini-2.5-flash] [Task Task-3  ] Call OK. Usage: {\"input_tokens\": 7218, \"output_tokens\": 627, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:22.527+00:00 [SUCCESS] [Idx 3035] [Mdl: gemini-2.5-flash] [Task Task-6  ] Call OK. Usage: {\"input_tokens\": 7181, \"output_tokens\": 478, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:22.672+00:00 [SUCCESS] [Idx 3169] [Mdl: gemini-2.5-flash] [Task Task-14 ] Call OK. Usage: {\"input_tokens\": 7181, \"output_tokens\": 725, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:22.795+00:00 [SUCCESS] [Idx 3048] [Mdl: gemini-2.5-flash] [Task Task-8  ] Call OK. Usage: {\"input_tokens\": 7186, \"output_tokens\": 656, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:22.848+00:00 [SUCCESS] [Idx 3422] [Mdl: gemini-2.5-flash] [Task Task-29 ] Call OK. Usage: {\"input_tokens\": 7163, \"output_tokens\": 629, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:23.060+00:00 [SUCCESS] [Idx 3217] [Mdl: gemini-2.5-flash] [Task Task-16 ] Call OK. Usage: {\"input_tokens\": 7199, \"output_tokens\": 674, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:23.623+00:00 [SUCCESS] [Idx 3430] [Mdl: gemini-2.5-flash] [Task Task-30 ] Call OK. Usage: {\"input_tokens\": 7214, \"output_tokens\": 938, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:24.134+00:00 [SUCCESS] [Idx 3440] [Mdl: gemini-2.5-flash] [Task Task-32 ] Call OK. Usage: {\"input_tokens\": 7251, \"output_tokens\": 1028, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:24.680+00:00 [SUCCESS] [Idx 3431] [Mdl: gemini-2.5-flash] [Task Task-31 ] Call OK. Usage: {\"input_tokens\": 7243, \"output_tokens\": 695, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:24.918+00:00 [SUCCESS] [Idx 3298] [Mdl: gemini-2.5-flash] [Task Task-22 ] Call OK. Usage: {\"input_tokens\": 7325, \"output_tokens\": 943, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:25.137+00:00 [SUCCESS] [Idx 3233] [Mdl: gemini-2.5-flash] [Task Task-19 ] Call OK. Usage: {\"input_tokens\": 7333, \"output_tokens\": 1152, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:25.539+00:00 [SUCCESS] [Idx 3118] [Mdl: gemini-2.5-flash] [Task Task-10 ] Call OK. Usage: {\"input_tokens\": 7325, \"output_tokens\": 759, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:25.698+00:00 [SUCCESS] [Idx 3412] [Mdl: gemini-2.5-flash] [Task Task-28 ] Call OK. Usage: {\"input_tokens\": 7248, \"output_tokens\": 815, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:25.730+00:00 [SUCCESS] [Idx 3224] [Mdl: gemini-2.5-flash] [Task Task-18 ] Call OK. Usage: {\"input_tokens\": 7279, \"output_tokens\": 918, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:26.167+00:00 [SUCCESS] [Idx 3006] [Mdl: gemini-2.5-flash] [Task Task-4  ] Call OK. Usage: {\"input_tokens\": 7264, \"output_tokens\": 891, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:26.702+00:00 [SUCCESS] [Idx 3036] [Mdl: gemini-2.5-flash] [Task Task-7  ] Call OK. Usage: {\"input_tokens\": 7252, \"output_tokens\": 1033, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:27.230+00:00 [SUCCESS] [Idx 3125] [Mdl: gemini-2.5-flash] [Task Task-11 ] Call OK. Usage: {\"input_tokens\": 7314, \"output_tokens\": 1303, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:27.400+00:00 [SUCCESS] [Idx 3273] [Mdl: gemini-2.5-flash] [Task Task-21 ] Call OK. Usage: {\"input_tokens\": 7215, \"output_tokens\": 750, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:27.622+00:00 [SUCCESS] [Idx 3500] [Mdl: gemini-2.5-flash] [Task Task-38 ] Call OK. Usage: {\"input_tokens\": 7109, \"output_tokens\": 396, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:27.866+00:00 [SUCCESS] [Idx 3220] [Mdl: gemini-2.5-flash] [Task Task-17 ] Call OK. Usage: {\"input_tokens\": 7309, \"output_tokens\": 1367, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:27.999+00:00 [SUCCESS] [Idx 3126] [Mdl: gemini-2.5-flash] [Task Task-12 ] Call OK. Usage: {\"input_tokens\": 7415, \"output_tokens\": 1247, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:28.046+00:00 [SUCCESS] [Idx 3408] [Mdl: gemini-2.5-flash] [Task Task-26 ] Call OK. Usage: {\"input_tokens\": 7355, \"output_tokens\": 1425, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:28.100+00:00 [SUCCESS] [Idx 3022] [Mdl: gemini-2.5-flash] [Task Task-5  ] Call OK. Usage: {\"input_tokens\": 7237, \"output_tokens\": 691, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:28.241+00:00 [SUCCESS] [Idx 3478] [Mdl: gemini-2.5-flash] [Task Task-37 ] Call OK. Usage: {\"input_tokens\": 7168, \"output_tokens\": 595, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:28.272+00:00 [SUCCESS] [Idx 3388] [Mdl: gemini-2.5-flash] [Task Task-23 ] Call OK. Usage: {\"input_tokens\": 7373, \"output_tokens\": 1502, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:29.470+00:00 [SUCCESS] [Idx 3508] [Mdl: gemini-2.5-flash] [Task Task-40 ] Call OK. Usage: {\"input_tokens\": 7170, \"output_tokens\": 445, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:29.961+00:00 [SUCCESS] [Idx 3501] [Mdl: gemini-2.5-flash] [Task Task-39 ] Call OK. Usage: {\"input_tokens\": 7146, \"output_tokens\": 568, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:30.247+00:00 [SUCCESS] [Idx 3395] [Mdl: gemini-2.5-flash] [Task Task-25 ] Call OK. Usage: {\"input_tokens\": 7100, \"output_tokens\": 438, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:31.466+00:00 [SUCCESS] [Idx 3519] [Mdl: gemini-2.5-flash] [Task Task-43 ] Call OK. Usage: {\"input_tokens\": 7192, \"output_tokens\": 739, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:32.006+00:00 [SUCCESS] [Idx 3679] [Mdl: gemini-2.5-flash] [Task Task-49 ] Call OK. Usage: {\"input_tokens\": 7138, \"output_tokens\": 396, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:32.153+00:00 [SUCCESS] [Idx 3444] [Mdl: gemini-2.5-flash] [Task Task-34 ] Call OK. Usage: {\"input_tokens\": 7392, \"output_tokens\": 1104, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:33.127+00:00 [SUCCESS] [Idx 3737] [Mdl: gemini-2.5-flash] [Task Task-53 ] Call OK. Usage: {\"input_tokens\": 7147, \"output_tokens\": 446, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:33.791+00:00 [SUCCESS] [Idx 3790] [Mdl: gemini-2.5-flash] [Task Task-58 ] Call OK. Usage: {\"input_tokens\": 7154, \"output_tokens\": 552, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:34.125+00:00 [SUCCESS] [Idx 3741] [Mdl: gemini-2.5-flash] [Task Task-54 ] Call OK. Usage: {\"input_tokens\": 7158, \"output_tokens\": 478, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:34.431+00:00 [SUCCESS] [Idx 3460] [Mdl: gemini-2.5-flash] [Task Task-36 ] Call OK. Usage: {\"input_tokens\": 7324, \"output_tokens\": 1263, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:34.671+00:00 [SUCCESS] [Idx 3557] [Mdl: gemini-2.5-flash] [Task Task-45 ] Call OK. Usage: {\"input_tokens\": 7351, \"output_tokens\": 921, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:35.086+00:00 [SUCCESS] [Idx 3510] [Mdl: gemini-2.5-flash] [Task Task-41 ] Call OK. Usage: {\"input_tokens\": 7211, \"output_tokens\": 512, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:35.380+00:00 [SUCCESS] [Idx 3706] [Mdl: gemini-2.5-flash] [Task Task-51 ] Call OK. Usage: {\"input_tokens\": 7270, \"output_tokens\": 729, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:35.819+00:00 [SUCCESS] [Idx 3742] [Mdl: gemini-2.5-flash] [Task Task-55 ] Call OK. Usage: {\"input_tokens\": 7233, \"output_tokens\": 595, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:35.918+00:00 [SUCCESS] [Idx 3718] [Mdl: gemini-2.5-flash] [Task Task-52 ] Call OK. Usage: {\"input_tokens\": 7250, \"output_tokens\": 711, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:36.218+00:00 [SUCCESS] [Idx 3857] [Mdl: gemini-2.5-flash] [Task Task-63 ] Call OK. Usage: {\"input_tokens\": 7113, \"output_tokens\": 425, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:36.354+00:00 [SUCCESS] [Idx 3182] [Mdl: gemini-2.5-flash] [Task Task-15 ] Call OK. Usage: {\"input_tokens\": 7396, \"output_tokens\": 1287, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:36.494+00:00 [SUCCESS] [Idx 3255] [Mdl: gemini-2.5-flash] [Task Task-20 ] Call OK. Usage: {\"input_tokens\": 7262, \"output_tokens\": 925, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:36.591+00:00 [SUCCESS] [Idx 3844] [Mdl: gemini-2.5-flash] [Task Task-60 ] Call OK. Usage: {\"input_tokens\": 7219, \"output_tokens\": 610, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:36.617+00:00 [SUCCESS] [Idx 3454] [Mdl: gemini-2.5-flash] [Task Task-35 ] Call OK. Usage: {\"input_tokens\": 7389, \"output_tokens\": 1611, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:37.154+00:00 [SUCCESS] [Idx 3775] [Mdl: gemini-2.5-flash] [Task Task-57 ] Call OK. Usage: {\"input_tokens\": 7140, \"output_tokens\": 482, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:37.329+00:00 [SUCCESS] [Idx 3409] [Mdl: gemini-2.5-flash] [Task Task-27 ] Call OK. Usage: {\"input_tokens\": 7101, \"output_tokens\": 395, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:37.346+00:00 [SUCCESS] [Idx 3873] [Mdl: gemini-2.5-flash] [Task Task-64 ] Call OK. Usage: {\"input_tokens\": 7161, \"output_tokens\": 524, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:37.881+00:00 [SUCCESS] [Idx 3749] [Mdl: gemini-2.5-flash] [Task Task-56 ] Call OK. Usage: {\"input_tokens\": 7291, \"output_tokens\": 964, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:38.122+00:00 [SUCCESS] [Idx 3577] [Mdl: gemini-2.5-flash] [Task Task-46 ] Call OK. Usage: {\"input_tokens\": 7238, \"output_tokens\": 778, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:38.193+00:00 [SUCCESS] [Idx 3877] [Mdl: gemini-2.5-flash] [Task Task-65 ] Call OK. Usage: {\"input_tokens\": 7130, \"output_tokens\": 433, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:38.672+00:00 [SUCCESS] [Idx 3640] [Mdl: gemini-2.5-flash] [Task Task-47 ] Call OK. Usage: {\"input_tokens\": 7215, \"output_tokens\": 740, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:39.241+00:00 [SUCCESS] [Idx 3645] [Mdl: gemini-2.5-flash] [Task Task-48 ] Call OK. Usage: {\"input_tokens\": 7418, \"output_tokens\": 1499, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:40.311+00:00 [SUCCESS] [Idx 3808] [Mdl: gemini-2.5-flash] [Task Task-59 ] Call OK. Usage: {\"input_tokens\": 7328, \"output_tokens\": 1118, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:40.584+00:00 [SUCCESS] [Idx 3517] [Mdl: gemini-2.5-flash] [Task Task-42 ] Call OK. Usage: {\"input_tokens\": 7248, \"output_tokens\": 745, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:41.175+00:00 [SUCCESS] [Idx 3927] [Mdl: gemini-2.5-flash] [Task Task-69 ] Call OK. Usage: {\"input_tokens\": 7188, \"output_tokens\": 646, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:41.259+00:00 [SUCCESS] [Idx 3934] [Mdl: gemini-2.5-flash] [Task Task-70 ] Call OK. Usage: {\"input_tokens\": 7171, \"output_tokens\": 595, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:41.291+00:00 [SUCCESS] [Idx 3853] [Mdl: gemini-2.5-flash] [Task Task-62 ] Call OK. Usage: {\"input_tokens\": 7159, \"output_tokens\": 529, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:41.316+00:00 [SUCCESS] [Idx 3520] [Mdl: gemini-2.5-flash] [Task Task-44 ] Call OK. Usage: {\"input_tokens\": 7322, \"output_tokens\": 943, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:41.775+00:00 [SUCCESS] [Idx 3989] [Mdl: gemini-2.5-flash] [Task Task-72 ] Call OK. Usage: {\"input_tokens\": 7156, \"output_tokens\": 698, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:42.007+00:00 [SUCCESS] [Idx 3847] [Mdl: gemini-2.5-flash] [Task Task-61 ] Call OK. Usage: {\"input_tokens\": 7353, \"output_tokens\": 1098, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:42.112+00:00 [SUCCESS] [Idx 3924] [Mdl: gemini-2.5-flash] [Task Task-68 ] Call OK. Usage: {\"input_tokens\": 7196, \"output_tokens\": 721, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:42.546+00:00 [SUCCESS] [Idx 4032] [Mdl: gemini-2.5-flash] [Task Task-76 ] Call OK. Usage: {\"input_tokens\": 7190, \"output_tokens\": 532, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:43.200+00:00 [SUCCESS] [Idx 4022] [Mdl: gemini-2.5-flash] [Task Task-75 ] Call OK. Usage: {\"input_tokens\": 7111, \"output_tokens\": 562, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:43.276+00:00 [SUCCESS] [Idx 3697] [Mdl: gemini-2.5-flash] [Task Task-50 ] Call OK. Usage: {\"input_tokens\": 7376, \"output_tokens\": 1131, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:43.456+00:00 [SUCCESS] [Idx 4019] [Mdl: gemini-2.5-flash] [Task Task-74 ] Call OK. Usage: {\"input_tokens\": 7182, \"output_tokens\": 723, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.533+00:00 [SUCCESS] [Idx 3958] [Mdl: gemini-2.5-flash] [Task Task-71 ] Call OK. Usage: {\"input_tokens\": 7287, \"output_tokens\": 786, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.630+00:00 [SUCCESS] [Idx 3921] [Mdl: gemini-2.5-flash] [Task Task-66 ] Call OK. Usage: {\"input_tokens\": 7362, \"output_tokens\": 1209, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.636+00:00 [SUCCESS] [Idx 4118] [Mdl: gemini-2.5-flash] [Task Task-80 ] Call OK. Usage: {\"input_tokens\": 7161, \"output_tokens\": 607, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.658+00:00 [SUCCESS] [Idx 4180] [Mdl: gemini-2.5-flash] [Task Task-86 ] Call OK. Usage: {\"input_tokens\": 7148, \"output_tokens\": 448, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.697+00:00 [SUCCESS] [Idx 4045] [Mdl: gemini-2.5-flash] [Task Task-77 ] Call OK. Usage: {\"input_tokens\": 7288, \"output_tokens\": 1047, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:44.708+00:00 [SUCCESS] [Idx 4145] [Mdl: gemini-2.5-flash] [Task Task-82 ] Call OK. Usage: {\"input_tokens\": 7197, \"output_tokens\": 627, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:45.034+00:00 [SUCCESS] [Idx 3442] [Mdl: gemini-2.5-flash] [Task Task-33 ] Call OK. Usage: {\"input_tokens\": 7237, \"output_tokens\": 824, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:45.470+00:00 [SUCCESS] [Idx 4157] [Mdl: gemini-2.5-flash] [Task Task-84 ] Call OK. Usage: {\"input_tokens\": 7211, \"output_tokens\": 661, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:45.861+00:00 [SUCCESS] [Idx 4132] [Mdl: gemini-2.5-flash] [Task Task-81 ] Call OK. Usage: {\"input_tokens\": 7321, \"output_tokens\": 1020, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:45.978+00:00 [SUCCESS] [Idx 4186] [Mdl: gemini-2.5-flash] [Task Task-87 ] Call OK. Usage: {\"input_tokens\": 7142, \"output_tokens\": 521, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:46.135+00:00 [SUCCESS] [Idx 4047] [Mdl: gemini-2.5-flash] [Task Task-78 ] Call OK. Usage: {\"input_tokens\": 7392, \"output_tokens\": 1050, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:46.944+00:00 [SUCCESS] [Idx 4051] [Mdl: gemini-2.5-flash] [Task Task-79 ] Call OK. Usage: {\"input_tokens\": 7278, \"output_tokens\": 1060, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:47.015+00:00 [SUCCESS] [Idx 4210] [Mdl: gemini-2.5-flash] [Task Task-90 ] Call OK. Usage: {\"input_tokens\": 7218, \"output_tokens\": 609, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:47.065+00:00 [SUCCESS] [Idx 4233] [Mdl: gemini-2.5-flash] [Task Task-92 ] Call OK. Usage: {\"input_tokens\": 7146, \"output_tokens\": 563, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:47.171+00:00 [SUCCESS] [Idx 4272] [Mdl: gemini-2.5-flash] [Task Task-95 ] Call OK. Usage: {\"input_tokens\": 7143, \"output_tokens\": 518, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:47.891+00:00 [SUCCESS] [Idx 4236] [Mdl: gemini-2.5-flash] [Task Task-93 ] Call OK. Usage: {\"input_tokens\": 7132, \"output_tokens\": 557, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:47.981+00:00 [SUCCESS] [Idx 3085] [Mdl: gemini-2.5-flash] [Task Task-9  ] Call OK. Usage: {\"input_tokens\": 7373, \"output_tokens\": 1149, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:48.096+00:00 [SUCCESS] [Idx 4306] [Mdl: gemini-2.5-flash] [Task Task-97 ] Call OK. Usage: {\"input_tokens\": 7142, \"output_tokens\": 537, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:48.751+00:00 [SUCCESS] [Idx 4231] [Mdl: gemini-2.5-flash] [Task Task-91 ] Call OK. Usage: {\"input_tokens\": 7198, \"output_tokens\": 626, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:48.810+00:00 [SUCCESS] [Idx 4196] [Mdl: gemini-2.5-flash] [Task Task-88 ] Call OK. Usage: {\"input_tokens\": 7138, \"output_tokens\": 662, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:49.506+00:00 [SUCCESS] [Idx 4178] [Mdl: gemini-2.5-flash] [Task Task-85 ] Call OK. Usage: {\"input_tokens\": 7254, \"output_tokens\": 740, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:49.588+00:00 [SUCCESS] [Idx 4156] [Mdl: gemini-2.5-flash] [Task Task-83 ] Call OK. Usage: {\"input_tokens\": 7259, \"output_tokens\": 1006, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:50.167+00:00 [SUCCESS] [Idx 3390] [Mdl: gemini-2.5-flash] [Task Task-24 ] Call OK. Usage: {\"input_tokens\": 7365, \"output_tokens\": 565, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:50.244+00:00 [SUCCESS] [Idx 4340] [Mdl: gemini-2.5-flash] [Task Task-102] Call OK. Usage: {\"input_tokens\": 7204, \"output_tokens\": 591, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:51.311+00:00 [SUCCESS] [Idx 4320] [Mdl: gemini-2.5-flash] [Task Task-99 ] Call OK. Usage: {\"input_tokens\": 7212, \"output_tokens\": 888, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:51.893+00:00 [SUCCESS] [Idx 4429] [Mdl: gemini-2.5-flash] [Task Task-111] Call OK. Usage: {\"input_tokens\": 7106, \"output_tokens\": 416, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:52.054+00:00 [SUCCESS] [Idx 4297] [Mdl: gemini-2.5-flash] [Task Task-96 ] Call OK. Usage: {\"input_tokens\": 7294, \"output_tokens\": 942, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:52.267+00:00 [SUCCESS] [Idx 4334] [Mdl: gemini-2.5-flash] [Task Task-101] Call OK. Usage: {\"input_tokens\": 7258, \"output_tokens\": 897, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:52.767+00:00 [SUCCESS] [Idx 4326] [Mdl: gemini-2.5-flash] [Task Task-100] Call OK. Usage: {\"input_tokens\": 7233, \"output_tokens\": 832, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:54.449+00:00 [SUCCESS] [Idx 4540] [Mdl: gemini-2.5-flash] [Task Task-121] Call OK. Usage: {\"input_tokens\": 7156, \"output_tokens\": 672, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:54.604+00:00 [SUCCESS] [Idx 4459] [Mdl: gemini-2.5-flash] [Task Task-115] Call OK. Usage: {\"input_tokens\": 7231, \"output_tokens\": 813, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:54.883+00:00 [SUCCESS] [Idx 4442] [Mdl: gemini-2.5-flash] [Task Task-113] Call OK. Usage: {\"input_tokens\": 7255, \"output_tokens\": 853, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:55.212+00:00 [SUCCESS] [Idx 4309] [Mdl: gemini-2.5-flash] [Task Task-98 ] Call OK. Usage: {\"input_tokens\": 7366, \"output_tokens\": 973, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:55.469+00:00 [SUCCESS] [Idx 4513] [Mdl: gemini-2.5-flash] [Task Task-118] Call OK. Usage: {\"input_tokens\": 7260, \"output_tokens\": 707, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:55.511+00:00 [SUCCESS] [Idx 4408] [Mdl: gemini-2.5-flash] [Task Task-107] Call OK. Usage: {\"input_tokens\": 7372, \"output_tokens\": 1125, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:55.664+00:00 [SUCCESS] [Idx 4479] [Mdl: gemini-2.5-flash] [Task Task-116] Call OK. Usage: {\"input_tokens\": 7333, \"output_tokens\": 955, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:56.155+00:00 [SUCCESS] [Idx 4521] [Mdl: gemini-2.5-flash] [Task Task-120] Call OK. Usage: {\"input_tokens\": 7167, \"output_tokens\": 744, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:56.211+00:00 [SUCCESS] [Idx 3923] [Mdl: gemini-2.5-flash] [Task Task-67 ] Call OK. Usage: {\"input_tokens\": 7243, \"output_tokens\": 610, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:56.837+00:00 [SUCCESS] [Idx 4418] [Mdl: gemini-2.5-flash] [Task Task-108] Call OK. Usage: {\"input_tokens\": 7213, \"output_tokens\": 646, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:57.151+00:00 [SUCCESS] [Idx 4378] [Mdl: gemini-2.5-flash] [Task Task-103] Call OK. Usage: {\"input_tokens\": 7206, \"output_tokens\": 808, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:58.261+00:00 [SUCCESS] [Idx 4607] [Mdl: gemini-2.5-flash] [Task Task-127] Call OK. Usage: {\"input_tokens\": 7202, \"output_tokens\": 809, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:58.306+00:00 [SUCCESS] [Idx 4548] [Mdl: gemini-2.5-flash] [Task Task-122] Call OK. Usage: {\"input_tokens\": 7143, \"output_tokens\": 612, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:58.507+00:00 [SUCCESS] [Idx 4493] [Mdl: gemini-2.5-flash] [Task Task-117] Call OK. Usage: {\"input_tokens\": 7231, \"output_tokens\": 683, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:58.520+00:00 [SUCCESS] [Idx 4435] [Mdl: gemini-2.5-flash] [Task Task-112] Call OK. Usage: {\"input_tokens\": 7305, \"output_tokens\": 1339, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:59.065+00:00 [SUCCESS] [Idx 4249] [Mdl: gemini-2.5-flash] [Task Task-94 ] Call OK. Usage: {\"input_tokens\": 7199, \"output_tokens\": 742, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:59.368+00:00 [SUCCESS] [Idx 4629] [Mdl: gemini-2.5-flash] [Task Task-129] Call OK. Usage: {\"input_tokens\": 7116, \"output_tokens\": 472, \"cached_tokens\": 0}\n",
      "2025-07-18T20:38:59.776+00:00 [SUCCESS] [Idx 4014] [Mdl: gemini-2.5-flash] [Task Task-73 ] Call OK. Usage: {\"input_tokens\": 7319, \"output_tokens\": 1153, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4840). Retrying in 10.96s... (Attempt 1/10)\n",
      "2025-07-18T20:39:00.776+00:00 [SUCCESS] [Idx 4649] [Mdl: gemini-2.5-flash] [Task Task-133] Call OK. Usage: {\"input_tokens\": 7134, \"output_tokens\": 454, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:00.979+00:00 [SUCCESS] [Idx 4565] [Mdl: gemini-2.5-flash] [Task Task-123] Call OK. Usage: {\"input_tokens\": 7217, \"output_tokens\": 718, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4861). Retrying in 10.30s... (Attempt 1/10)\n",
      "2025-07-18T20:39:01.252+00:00 [SUCCESS] [Idx 4603] [Mdl: gemini-2.5-flash] [Task Task-126] Call OK. Usage: {\"input_tokens\": 7276, \"output_tokens\": 960, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4867). Retrying in 10.48s... (Attempt 1/10)\n",
      "2025-07-18T20:39:01.420+00:00 [SUCCESS] [Idx 4632] [Mdl: gemini-2.5-flash] [Task Task-131] Call OK. Usage: {\"input_tokens\": 7212, \"output_tokens\": 656, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:01.440+00:00 [SUCCESS] [Idx 4425] [Mdl: gemini-2.5-flash] [Task Task-110] Call OK. Usage: {\"input_tokens\": 7157, \"output_tokens\": 567, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4873). Retrying in 10.22s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4883). Retrying in 10.40s... (Attempt 1/10)\n",
      "2025-07-18T20:39:01.898+00:00 [SUCCESS] [Idx 4455] [Mdl: gemini-2.5-flash] [Task Task-114] Call OK. Usage: {\"input_tokens\": 7321, \"output_tokens\": 1197, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4917). Retrying in 10.51s... (Attempt 1/10)\n",
      "2025-07-18T20:39:02.126+00:00 [SUCCESS] [Idx 4631] [Mdl: gemini-2.5-flash] [Task Task-130] Call OK. Usage: {\"input_tokens\": 7318, \"output_tokens\": 1014, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4922). Retrying in 10.69s... (Attempt 1/10)\n",
      "2025-07-18T20:39:02.796+00:00 [SUCCESS] [Idx 4379] [Mdl: gemini-2.5-flash] [Task Task-104] Call OK. Usage: {\"input_tokens\": 7474, \"output_tokens\": 1402, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4925). Retrying in 10.74s... (Attempt 1/10)\n",
      "2025-07-18T20:39:03.151+00:00 [SUCCESS] [Idx 4710] [Mdl: gemini-2.5-flash] [Task Task-135] Call OK. Usage: {\"input_tokens\": 7276, \"output_tokens\": 655, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4944). Retrying in 10.12s... (Attempt 1/10)\n",
      "2025-07-18T20:39:03.251+00:00 [SUCCESS] [Idx 4810] [Mdl: gemini-2.5-flash] [Task Task-143] Call OK. Usage: {\"input_tokens\": 7135, \"output_tokens\": 415, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4966). Retrying in 10.54s... (Attempt 1/10)\n",
      "2025-07-18T20:39:03.428+00:00 [SUCCESS] [Idx 4419] [Mdl: gemini-2.5-flash] [Task Task-109] Call OK. Usage: {\"input_tokens\": 7350, \"output_tokens\": 1324, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4978). Retrying in 10.65s... (Attempt 1/10)\n",
      "2025-07-18T20:39:03.707+00:00 [SUCCESS] [Idx 4803] [Mdl: gemini-2.5-flash] [Task Task-142] Call OK. Usage: {\"input_tokens\": 7210, \"output_tokens\": 681, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4983). Retrying in 10.67s... (Attempt 1/10)\n",
      "2025-07-18T20:39:04.115+00:00 [SUCCESS] [Idx 4839] [Mdl: gemini-2.5-flash] [Task Task-145] Call OK. Usage: {\"input_tokens\": 7102, \"output_tokens\": 387, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4986). Retrying in 10.75s... (Attempt 1/10)\n",
      "2025-07-18T20:39:04.239+00:00 [SUCCESS] [Idx 4711] [Mdl: gemini-2.5-flash] [Task Task-136] Call OK. Usage: {\"input_tokens\": 7268, \"output_tokens\": 764, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4991). Retrying in 10.54s... (Attempt 1/10)\n",
      "2025-07-18T20:39:04.356+00:00 [SUCCESS] [Idx 4625] [Mdl: gemini-2.5-flash] [Task Task-128] Call OK. Usage: {\"input_tokens\": 7269, \"output_tokens\": 624, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4995). Retrying in 10.57s... (Attempt 1/10)\n",
      "2025-07-18T20:39:05.451+00:00 [SUCCESS] [Idx 4399] [Mdl: gemini-2.5-flash] [Task Task-105] Call OK. Usage: {\"input_tokens\": 7309, \"output_tokens\": 1268, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5010). Retrying in 10.69s... (Attempt 1/10)\n",
      "2025-07-18T20:39:06.390+00:00 [SUCCESS] [Idx 4400] [Mdl: gemini-2.5-flash] [Task Task-106] Call OK. Usage: {\"input_tokens\": 7336, \"output_tokens\": 1306, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5027). Retrying in 10.64s... (Attempt 1/10)\n",
      "2025-07-18T20:39:06.697+00:00 [SUCCESS] [Idx 4572] [Mdl: gemini-2.5-flash] [Task Task-125] Call OK. Usage: {\"input_tokens\": 7285, \"output_tokens\": 1310, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5040). Retrying in 10.59s... (Attempt 1/10)\n",
      "2025-07-18T20:39:07.181+00:00 [SUCCESS] [Idx 4847] [Mdl: gemini-2.5-flash] [Task Task-147] Call OK. Usage: {\"input_tokens\": 7184, \"output_tokens\": 554, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5060). Retrying in 10.70s... (Attempt 1/10)\n",
      "2025-07-18T20:39:07.443+00:00 [SUCCESS] [Idx 4518] [Mdl: gemini-2.5-flash] [Task Task-119] Call OK. Usage: {\"input_tokens\": 7436, \"output_tokens\": 1264, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:07.466+00:00 [SUCCESS] [Idx 4738] [Mdl: gemini-2.5-flash] [Task Task-138] Call OK. Usage: {\"input_tokens\": 7332, \"output_tokens\": 1043, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:07.492+00:00 [SUCCESS] [Idx 4799] [Mdl: gemini-2.5-flash] [Task Task-141] Call OK. Usage: {\"input_tokens\": 7303, \"output_tokens\": 904, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5091). Retrying in 10.93s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5115). Retrying in 10.13s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5126). Retrying in 10.69s... (Attempt 1/10)\n",
      "2025-07-18T20:39:07.766+00:00 [SUCCESS] [Idx 4633] [Mdl: gemini-2.5-flash] [Task Task-132] Call OK. Usage: {\"input_tokens\": 7286, \"output_tokens\": 1014, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:07.779+00:00 [SUCCESS] [Idx 4714] [Mdl: gemini-2.5-flash] [Task Task-137] Call OK. Usage: {\"input_tokens\": 7234, \"output_tokens\": 745, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5127). Retrying in 10.57s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5135). Retrying in 10.90s... (Attempt 1/10)\n",
      "2025-07-18T20:39:07.947+00:00 [SUCCESS] [Idx 4787] [Mdl: gemini-2.5-flash] [Task Task-140] Call OK. Usage: {\"input_tokens\": 7351, \"output_tokens\": 1180, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5146). Retrying in 10.12s... (Attempt 1/10)\n",
      "2025-07-18T20:39:08.600+00:00 [SUCCESS] [Idx 4676] [Mdl: gemini-2.5-flash] [Task Task-134] Call OK. Usage: {\"input_tokens\": 7349, \"output_tokens\": 982, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:08.603+00:00 [SUCCESS] [Idx 4568] [Mdl: gemini-2.5-flash] [Task Task-124] Call OK. Usage: {\"input_tokens\": 7161, \"output_tokens\": 483, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5178). Retrying in 10.80s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5152). Retrying in 10.58s... (Attempt 1/10)\n",
      "2025-07-18T20:39:08.760+00:00 [SUCCESS] [Idx 4834] [Mdl: gemini-2.5-flash] [Task Task-144] Call OK. Usage: {\"input_tokens\": 7190, \"output_tokens\": 726, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5191). Retrying in 10.10s... (Attempt 1/10)\n",
      "2025-07-18T20:39:09.404+00:00 [SUCCESS] [Idx 4758] [Mdl: gemini-2.5-flash] [Task Task-139] Call OK. Usage: {\"input_tokens\": 7178, \"output_tokens\": 636, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5205). Retrying in 10.54s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4840). Retrying in 20.85s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4861). Retrying in 20.21s... (Attempt 2/10)\n",
      "2025-07-18T20:39:11.756+00:00 [SUCCESS] [Idx 4206] [Mdl: gemini-2.5-flash] [Task Task-89 ] Call OK. Usage: {\"input_tokens\": 7374, \"output_tokens\": 1081, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4873). Retrying in 20.70s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 5213). Retrying in 10.95s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4867). Retrying in 20.43s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4883). Retrying in 20.52s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4917). Retrying in 20.96s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4922). Retrying in 20.03s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4944). Retrying in 20.86s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4925). Retrying in 20.10s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4966). Retrying in 20.63s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4978). Retrying in 20.55s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4983). Retrying in 20.31s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4991). Retrying in 20.27s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4986). Retrying in 20.14s... (Attempt 2/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 4995). Retrying in 20.30s... (Attempt 2/10)\n",
      "2025-07-18T20:39:21.348+00:00 [SUCCESS] [Idx 5010] [Mdl: gemini-2.5-flash] [Task Task-162] Call OK. Usage: {\"input_tokens\": 7185, \"output_tokens\": 469, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:23.189+00:00 [SUCCESS] [Idx 5127] [Mdl: gemini-2.5-flash] [Task Task-169] Call OK. Usage: {\"input_tokens\": 7172, \"output_tokens\": 455, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:23.388+00:00 [SUCCESS] [Idx 5126] [Mdl: gemini-2.5-flash] [Task Task-168] Call OK. Usage: {\"input_tokens\": 7136, \"output_tokens\": 427, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:23.405+00:00 [SUCCESS] [Idx 5135] [Mdl: gemini-2.5-flash] [Task Task-170] Call OK. Usage: {\"input_tokens\": 7152, \"output_tokens\": 546, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:25.313+00:00 [SUCCESS] [Idx 5191] [Mdl: gemini-2.5-flash] [Task Task-174] Call OK. Usage: {\"input_tokens\": 7166, \"output_tokens\": 578, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:25.940+00:00 [SUCCESS] [Idx 5091] [Mdl: gemini-2.5-flash] [Task Task-166] Call OK. Usage: {\"input_tokens\": 7288, \"output_tokens\": 1030, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:26.638+00:00 [SUCCESS] [Idx 5146] [Mdl: gemini-2.5-flash] [Task Task-171] Call OK. Usage: {\"input_tokens\": 7181, \"output_tokens\": 774, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:26.640+00:00 [SUCCESS] [Idx 5060] [Mdl: gemini-2.5-flash] [Task Task-165] Call OK. Usage: {\"input_tokens\": 7234, \"output_tokens\": 789, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:26.737+00:00 [SUCCESS] [Idx 5205] [Mdl: gemini-2.5-flash] [Task Task-175] Call OK. Usage: {\"input_tokens\": 7169, \"output_tokens\": 745, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:27.512+00:00 [SUCCESS] [Idx 5115] [Mdl: gemini-2.5-flash] [Task Task-167] Call OK. Usage: {\"input_tokens\": 7316, \"output_tokens\": 1126, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:27.766+00:00 [SUCCESS] [Idx 5027] [Mdl: gemini-2.5-flash] [Task Task-163] Call OK. Usage: {\"input_tokens\": 7333, \"output_tokens\": 891, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:27.814+00:00 [SUCCESS] [Idx 5040] [Mdl: gemini-2.5-flash] [Task Task-164] Call OK. Usage: {\"input_tokens\": 7223, \"output_tokens\": 1018, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:29.245+00:00 [SUCCESS] [Idx 5213] [Mdl: gemini-2.5-flash] [Task Task-176] Call OK. Usage: {\"input_tokens\": 7177, \"output_tokens\": 630, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:29.565+00:00 [SUCCESS] [Idx 5245] [Mdl: gemini-2.5-flash] [Task Task-179] Call OK. Usage: {\"input_tokens\": 7193, \"output_tokens\": 712, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:30.483+00:00 [SUCCESS] [Idx 5256] [Mdl: gemini-2.5-flash] [Task Task-180] Call OK. Usage: {\"input_tokens\": 7237, \"output_tokens\": 719, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:31.133+00:00 [SUCCESS] [Idx 5356] [Mdl: gemini-2.5-flash] [Task Task-184] Call OK. Usage: {\"input_tokens\": 7167, \"output_tokens\": 547, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:31.369+00:00 [SUCCESS] [Idx 5283] [Mdl: gemini-2.5-flash] [Task Task-182] Call OK. Usage: {\"input_tokens\": 7178, \"output_tokens\": 599, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:31.572+00:00 [SUCCESS] [Idx 5238] [Mdl: gemini-2.5-flash] [Task Task-178] Call OK. Usage: {\"input_tokens\": 7250, \"output_tokens\": 1001, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:32.931+00:00 [SUCCESS] [Idx 5489] [Mdl: gemini-2.5-flash] [Task Task-188] Call OK. Usage: {\"input_tokens\": 7168, \"output_tokens\": 501, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:33.107+00:00 [SUCCESS] [Idx 5355] [Mdl: gemini-2.5-flash] [Task Task-183] Call OK. Usage: {\"input_tokens\": 7157, \"output_tokens\": 656, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:33.529+00:00 [SUCCESS] [Idx 5220] [Mdl: gemini-2.5-flash] [Task Task-177] Call OK. Usage: {\"input_tokens\": 7379, \"output_tokens\": 1072, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:34.836+00:00 [SUCCESS] [Idx 5370] [Mdl: gemini-2.5-flash] [Task Task-185] Call OK. Usage: {\"input_tokens\": 7160, \"output_tokens\": 551, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:35.597+00:00 [SUCCESS] [Idx 5543] [Mdl: gemini-2.5-flash] [Task Task-191] Call OK. Usage: {\"input_tokens\": 7096, \"output_tokens\": 413, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:35.931+00:00 [SUCCESS] [Idx 5565] [Mdl: gemini-2.5-flash] [Task Task-193] Call OK. Usage: {\"input_tokens\": 7109, \"output_tokens\": 384, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:36.682+00:00 [SUCCESS] [Idx 4861] [Mdl: gemini-2.5-flash] [Task Task-148] Call OK. Usage: {\"input_tokens\": 7101, \"output_tokens\": 435, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:37.203+00:00 [SUCCESS] [Idx 5276] [Mdl: gemini-2.5-flash] [Task Task-181] Call OK. Usage: {\"input_tokens\": 7192, \"output_tokens\": 706, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:37.674+00:00 [SUCCESS] [Idx 5450] [Mdl: gemini-2.5-flash] [Task Task-187] Call OK. Usage: {\"input_tokens\": 7372, \"output_tokens\": 1034, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:38.124+00:00 [SUCCESS] [Idx 5152] [Mdl: gemini-2.5-flash] [Task Task-172] Call OK. Usage: {\"input_tokens\": 7325, \"output_tokens\": 1109, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:38.224+00:00 [SUCCESS] [Idx 4917] [Mdl: gemini-2.5-flash] [Task Task-152] Call OK. Usage: {\"input_tokens\": 7131, \"output_tokens\": 430, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:38.519+00:00 [SUCCESS] [Idx 4840] [Mdl: gemini-2.5-flash] [Task Task-146] Call OK. Usage: {\"input_tokens\": 7147, \"output_tokens\": 629, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:38.562+00:00 [SUCCESS] [Idx 4867] [Mdl: gemini-2.5-flash] [Task Task-149] Call OK. Usage: {\"input_tokens\": 7209, \"output_tokens\": 874, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:39.235+00:00 [SUCCESS] [Idx 5437] [Mdl: gemini-2.5-flash] [Task Task-186] Call OK. Usage: {\"input_tokens\": 7226, \"output_tokens\": 637, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:39.656+00:00 [SUCCESS] [Idx 4883] [Mdl: gemini-2.5-flash] [Task Task-151] Call OK. Usage: {\"input_tokens\": 7235, \"output_tokens\": 755, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:39.866+00:00 [SUCCESS] [Idx 5586] [Mdl: gemini-2.5-flash] [Task Task-196] Call OK. Usage: {\"input_tokens\": 7141, \"output_tokens\": 596, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:40.136+00:00 [SUCCESS] [Idx 4925] [Mdl: gemini-2.5-flash] [Task Task-154] Call OK. Usage: {\"input_tokens\": 7215, \"output_tokens\": 794, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.189+00:00 [SUCCESS] [Idx 4966] [Mdl: gemini-2.5-flash] [Task Task-156] Call OK. Usage: {\"input_tokens\": 7247, \"output_tokens\": 773, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.222+00:00 [SUCCESS] [Idx 5580] [Mdl: gemini-2.5-flash] [Task Task-195] Call OK. Usage: {\"input_tokens\": 7261, \"output_tokens\": 939, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.269+00:00 [SUCCESS] [Idx 4995] [Mdl: gemini-2.5-flash] [Task Task-161] Call OK. Usage: {\"input_tokens\": 7195, \"output_tokens\": 750, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.303+00:00 [SUCCESS] [Idx 4983] [Mdl: gemini-2.5-flash] [Task Task-158] Call OK. Usage: {\"input_tokens\": 7230, \"output_tokens\": 764, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.371+00:00 [SUCCESS] [Idx 5569] [Mdl: gemini-2.5-flash] [Task Task-194] Call OK. Usage: {\"input_tokens\": 7365, \"output_tokens\": 1086, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.442+00:00 [SUCCESS] [Idx 4944] [Mdl: gemini-2.5-flash] [Task Task-155] Call OK. Usage: {\"input_tokens\": 7236, \"output_tokens\": 656, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:41.587+00:00 [SUCCESS] [Idx 5614] [Mdl: gemini-2.5-flash] [Task Task-198] Call OK. Usage: {\"input_tokens\": 7254, \"output_tokens\": 828, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.018+00:00 [SUCCESS] [Idx 4978] [Mdl: gemini-2.5-flash] [Task Task-157] Call OK. Usage: {\"input_tokens\": 7144, \"output_tokens\": 459, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.151+00:00 [SUCCESS] [Idx 5178] [Mdl: gemini-2.5-flash] [Task Task-173] Call OK. Usage: {\"input_tokens\": 7270, \"output_tokens\": 853, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.161+00:00 [SUCCESS] [Idx 5609] [Mdl: gemini-2.5-flash] [Task Task-197] Call OK. Usage: {\"input_tokens\": 7223, \"output_tokens\": 872, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.256+00:00 [SUCCESS] [Idx 5564] [Mdl: gemini-2.5-flash] [Task Task-192] Call OK. Usage: {\"input_tokens\": 7414, \"output_tokens\": 1268, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.503+00:00 [SUCCESS] [Idx 5621] [Mdl: gemini-2.5-flash] [Task Task-199] Call OK. Usage: {\"input_tokens\": 7176, \"output_tokens\": 905, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:42.798+00:00 [SUCCESS] [Idx 4986] [Mdl: gemini-2.5-flash] [Task Task-159] Call OK. Usage: {\"input_tokens\": 7274, \"output_tokens\": 830, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:45.478+00:00 [SUCCESS] [Idx 5751] [Mdl: gemini-2.5-flash] [Task Task-211] Call OK. Usage: {\"input_tokens\": 7198, \"output_tokens\": 702, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:45.540+00:00 [SUCCESS] [Idx 5707] [Mdl: gemini-2.5-flash] [Task Task-206] Call OK. Usage: {\"input_tokens\": 7291, \"output_tokens\": 980, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.061+00:00 [SUCCESS] [Idx 5731] [Mdl: gemini-2.5-flash] [Task Task-210] Call OK. Usage: {\"input_tokens\": 7266, \"output_tokens\": 619, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.284+00:00 [SUCCESS] [Idx 4922] [Mdl: gemini-2.5-flash] [Task Task-153] Call OK. Usage: {\"input_tokens\": 7313, \"output_tokens\": 952, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.501+00:00 [SUCCESS] [Idx 5728] [Mdl: gemini-2.5-flash] [Task Task-208] Call OK. Usage: {\"input_tokens\": 7178, \"output_tokens\": 600, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.664+00:00 [SUCCESS] [Idx 5650] [Mdl: gemini-2.5-flash] [Task Task-202] Call OK. Usage: {\"input_tokens\": 7247, \"output_tokens\": 976, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.687+00:00 [SUCCESS] [Idx 5671] [Mdl: gemini-2.5-flash] [Task Task-203] Call OK. Usage: {\"input_tokens\": 7271, \"output_tokens\": 1171, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:46.766+00:00 [SUCCESS] [Idx 5729] [Mdl: gemini-2.5-flash] [Task Task-209] Call OK. Usage: {\"input_tokens\": 7233, \"output_tokens\": 697, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:47.180+00:00 [SUCCESS] [Idx 5776] [Mdl: gemini-2.5-flash] [Task Task-213] Call OK. Usage: {\"input_tokens\": 7209, \"output_tokens\": 745, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:48.265+00:00 [SUCCESS] [Idx 5714] [Mdl: gemini-2.5-flash] [Task Task-207] Call OK. Usage: {\"input_tokens\": 7271, \"output_tokens\": 847, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:48.814+00:00 [SUCCESS] [Idx 5634] [Mdl: gemini-2.5-flash] [Task Task-200] Call OK. Usage: {\"input_tokens\": 7332, \"output_tokens\": 1119, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.074+00:00 [SUCCESS] [Idx 5867] [Mdl: gemini-2.5-flash] [Task Task-219] Call OK. Usage: {\"input_tokens\": 7238, \"output_tokens\": 733, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.317+00:00 [SUCCESS] [Idx 5791] [Mdl: gemini-2.5-flash] [Task Task-215] Call OK. Usage: {\"input_tokens\": 7205, \"output_tokens\": 747, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.392+00:00 [SUCCESS] [Idx 5788] [Mdl: gemini-2.5-flash] [Task Task-214] Call OK. Usage: {\"input_tokens\": 7216, \"output_tokens\": 795, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.420+00:00 [SUCCESS] [Idx 4991] [Mdl: gemini-2.5-flash] [Task Task-160] Call OK. Usage: {\"input_tokens\": 7396, \"output_tokens\": 1383, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.615+00:00 [SUCCESS] [Idx 5824] [Mdl: gemini-2.5-flash] [Task Task-217] Call OK. Usage: {\"input_tokens\": 7180, \"output_tokens\": 648, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:49.634+00:00 [SUCCESS] [Idx 5837] [Mdl: gemini-2.5-flash] [Task Task-218] Call OK. Usage: {\"input_tokens\": 7247, \"output_tokens\": 855, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:51.464+00:00 [SUCCESS] [Idx 5691] [Mdl: gemini-2.5-flash] [Task Task-205] Call OK. Usage: {\"input_tokens\": 7285, \"output_tokens\": 894, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:51.913+00:00 [SUCCESS] [Idx 5977] [Mdl: gemini-2.5-flash] [Task Task-227] Call OK. Usage: {\"input_tokens\": 7204, \"output_tokens\": 510, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:52.122+00:00 [SUCCESS] [Idx 5878] [Mdl: gemini-2.5-flash] [Task Task-220] Call OK. Usage: {\"input_tokens\": 7242, \"output_tokens\": 859, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:52.438+00:00 [SUCCESS] [Idx 5935] [Mdl: gemini-2.5-flash] [Task Task-225] Call OK. Usage: {\"input_tokens\": 7266, \"output_tokens\": 996, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:52.543+00:00 [SUCCESS] [Idx 5914] [Mdl: gemini-2.5-flash] [Task Task-222] Call OK. Usage: {\"input_tokens\": 7238, \"output_tokens\": 1078, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:52.688+00:00 [SUCCESS] [Idx 5932] [Mdl: gemini-2.5-flash] [Task Task-224] Call OK. Usage: {\"input_tokens\": 7267, \"output_tokens\": 1025, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:53.432+00:00 [SUCCESS] [Idx 6081] [Mdl: gemini-2.5-flash] [Task Task-234] Call OK. Usage: {\"input_tokens\": 7120, \"output_tokens\": 466, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:53.565+00:00 [SUCCESS] [Idx 5765] [Mdl: gemini-2.5-flash] [Task Task-212] Call OK. Usage: {\"input_tokens\": 7382, \"output_tokens\": 1493, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:53.788+00:00 [SUCCESS] [Idx 5931] [Mdl: gemini-2.5-flash] [Task Task-223] Call OK. Usage: {\"input_tokens\": 7342, \"output_tokens\": 1169, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:53.837+00:00 [SUCCESS] [Idx 5688] [Mdl: gemini-2.5-flash] [Task Task-204] Call OK. Usage: {\"input_tokens\": 7205, \"output_tokens\": 786, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:53.839+00:00 [SUCCESS] [Idx 5956] [Mdl: gemini-2.5-flash] [Task Task-226] Call OK. Usage: {\"input_tokens\": 7314, \"output_tokens\": 957, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:54.035+00:00 [SUCCESS] [Idx 6211] [Mdl: gemini-2.5-flash] [Task Task-240] Call OK. Usage: {\"input_tokens\": 7141, \"output_tokens\": 431, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:54.154+00:00 [SUCCESS] [Idx 6151] [Mdl: gemini-2.5-flash] [Task Task-238] Call OK. Usage: {\"input_tokens\": 7099, \"output_tokens\": 470, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:54.395+00:00 [SUCCESS] [Idx 6143] [Mdl: gemini-2.5-flash] [Task Task-237] Call OK. Usage: {\"input_tokens\": 7162, \"output_tokens\": 657, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:54.514+00:00 [SUCCESS] [Idx 5793] [Mdl: gemini-2.5-flash] [Task Task-216] Call OK. Usage: {\"input_tokens\": 7229, \"output_tokens\": 650, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:54.662+00:00 [SUCCESS] [Idx 6068] [Mdl: gemini-2.5-flash] [Task Task-233] Call OK. Usage: {\"input_tokens\": 7204, \"output_tokens\": 709, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:55.035+00:00 [SUCCESS] [Idx 6058] [Mdl: gemini-2.5-flash] [Task Task-232] Call OK. Usage: {\"input_tokens\": 7290, \"output_tokens\": 1138, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:55.560+00:00 [SUCCESS] [Idx 6003] [Mdl: gemini-2.5-flash] [Task Task-229] Call OK. Usage: {\"input_tokens\": 7317, \"output_tokens\": 1047, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:55.753+00:00 [SUCCESS] [Idx 6216] [Mdl: gemini-2.5-flash] [Task Task-241] Call OK. Usage: {\"input_tokens\": 7212, \"output_tokens\": 727, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:56.312+00:00 [SUCCESS] [Idx 6097] [Mdl: gemini-2.5-flash] [Task Task-235] Call OK. Usage: {\"input_tokens\": 7262, \"output_tokens\": 957, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:56.736+00:00 [SUCCESS] [Idx 5513] [Mdl: gemini-2.5-flash] [Task Task-189] Call OK. Usage: {\"input_tokens\": 7341, \"output_tokens\": 1039, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:57.105+00:00 [SUCCESS] [Idx 5882] [Mdl: gemini-2.5-flash] [Task Task-221] Call OK. Usage: {\"input_tokens\": 7256, \"output_tokens\": 923, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:58.136+00:00 [SUCCESS] [Idx 6316] [Mdl: gemini-2.5-flash] [Task Task-247] Call OK. Usage: {\"input_tokens\": 7169, \"output_tokens\": 436, \"cached_tokens\": 0}\n",
      "2025-07-18T20:39:59.052+00:00 [SUCCESS] [Idx 6115] [Mdl: gemini-2.5-flash] [Task Task-236] Call OK. Usage: {\"input_tokens\": 7336, \"output_tokens\": 1161, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:00.062+00:00 [SUCCESS] [Idx 6241] [Mdl: gemini-2.5-flash] [Task Task-244] Call OK. Usage: {\"input_tokens\": 7177, \"output_tokens\": 726, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:00.082+00:00 [SUCCESS] [Idx 6205] [Mdl: gemini-2.5-flash] [Task Task-239] Call OK. Usage: {\"input_tokens\": 7201, \"output_tokens\": 724, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:00.398+00:00 [SUCCESS] [Idx 6324] [Mdl: gemini-2.5-flash] [Task Task-249] Call OK. Usage: {\"input_tokens\": 7222, \"output_tokens\": 623, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:00.462+00:00 [SUCCESS] [Idx 6043] [Mdl: gemini-2.5-flash] [Task Task-230] Call OK. Usage: {\"input_tokens\": 7198, \"output_tokens\": 688, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.133+00:00 [SUCCESS] [Idx 6238] [Mdl: gemini-2.5-flash] [Task Task-243] Call OK. Usage: {\"input_tokens\": 7238, \"output_tokens\": 976, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.539+00:00 [SUCCESS] [Idx 6400] [Mdl: gemini-2.5-flash] [Task Task-253] Call OK. Usage: {\"input_tokens\": 7255, \"output_tokens\": 837, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.541+00:00 [SUCCESS] [Idx 6380] [Mdl: gemini-2.5-flash] [Task Task-252] Call OK. Usage: {\"input_tokens\": 7155, \"output_tokens\": 670, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.733+00:00 [SUCCESS] [Idx 6434] [Mdl: gemini-2.5-flash] [Task Task-260] Call OK. Usage: {\"input_tokens\": 7150, \"output_tokens\": 537, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.735+00:00 [SUCCESS] [Idx 6049] [Mdl: gemini-2.5-flash] [Task Task-231] Call OK. Usage: {\"input_tokens\": 7374, \"output_tokens\": 1329, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:01.974+00:00 [SUCCESS] [Idx 6267] [Mdl: gemini-2.5-flash] [Task Task-245] Call OK. Usage: {\"input_tokens\": 7207, \"output_tokens\": 698, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:02.522+00:00 [SUCCESS] [Idx 6404] [Mdl: gemini-2.5-flash] [Task Task-255] Call OK. Usage: {\"input_tokens\": 7262, \"output_tokens\": 937, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:02.630+00:00 [SUCCESS] [Idx 6338] [Mdl: gemini-2.5-flash] [Task Task-250] Call OK. Usage: {\"input_tokens\": 7288, \"output_tokens\": 792, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:02.756+00:00 [SUCCESS] [Idx 5542] [Mdl: gemini-2.5-flash] [Task Task-190] Call OK. Usage: {\"input_tokens\": 7380, \"output_tokens\": 781, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:02.883+00:00 [SUCCESS] [Idx 6419] [Mdl: gemini-2.5-flash] [Task Task-257] Call OK. Usage: {\"input_tokens\": 7245, \"output_tokens\": 878, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:03.070+00:00 [SUCCESS] [Idx 6424] [Mdl: gemini-2.5-flash] [Task Task-258] Call OK. Usage: {\"input_tokens\": 7151, \"output_tokens\": 666, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:03.092+00:00 [SUCCESS] [Idx 6470] [Mdl: gemini-2.5-flash] [Task Task-262] Call OK. Usage: {\"input_tokens\": 7187, \"output_tokens\": 578, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:03.464+00:00 [SUCCESS] [Idx 6237] [Mdl: gemini-2.5-flash] [Task Task-242] Call OK. Usage: {\"input_tokens\": 7489, \"output_tokens\": 1410, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:04.095+00:00 [SUCCESS] [Idx 4873] [Mdl: gemini-2.5-flash] [Task Task-150] Call OK. Usage: {\"input_tokens\": 7200, \"output_tokens\": 590, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:04.288+00:00 [SUCCESS] [Idx 6483] [Mdl: gemini-2.5-flash] [Task Task-265] Call OK. Usage: {\"input_tokens\": 7171, \"output_tokens\": 475, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:04.677+00:00 [SUCCESS] [Idx 6321] [Mdl: gemini-2.5-flash] [Task Task-248] Call OK. Usage: {\"input_tokens\": 7324, \"output_tokens\": 1192, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:04.887+00:00 [SUCCESS] [Idx 6402] [Mdl: gemini-2.5-flash] [Task Task-254] Call OK. Usage: {\"input_tokens\": 7206, \"output_tokens\": 672, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.489+00:00 [SUCCESS] [Idx 6558] [Mdl: gemini-2.5-flash] [Task Task-271] Call OK. Usage: {\"input_tokens\": 7104, \"output_tokens\": 454, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.544+00:00 [SUCCESS] [Idx 6430] [Mdl: gemini-2.5-flash] [Task Task-259] Call OK. Usage: {\"input_tokens\": 7328, \"output_tokens\": 1066, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.635+00:00 [SUCCESS] [Idx 6460] [Mdl: gemini-2.5-flash] [Task Task-261] Call OK. Usage: {\"input_tokens\": 7290, \"output_tokens\": 714, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.734+00:00 [SUCCESS] [Idx 6533] [Mdl: gemini-2.5-flash] [Task Task-269] Call OK. Usage: {\"input_tokens\": 7169, \"output_tokens\": 618, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6747). Retrying in 10.38s... (Attempt 1/10)\n",
      "2025-07-18T20:40:06.836+00:00 [SUCCESS] [Idx 6477] [Mdl: gemini-2.5-flash] [Task Task-263] Call OK. Usage: {\"input_tokens\": 7375, \"output_tokens\": 1033, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.966+00:00 [SUCCESS] [Idx 6509] [Mdl: gemini-2.5-flash] [Task Task-267] Call OK. Usage: {\"input_tokens\": 7162, \"output_tokens\": 635, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:06.968+00:00 [SUCCESS] [Idx 6498] [Mdl: gemini-2.5-flash] [Task Task-266] Call OK. Usage: {\"input_tokens\": 7250, \"output_tokens\": 731, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6783). Retrying in 10.41s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6786). Retrying in 10.64s... (Attempt 1/10)\n",
      "2025-07-18T20:40:08.516+00:00 [SUCCESS] [Idx 6510] [Mdl: gemini-2.5-flash] [Task Task-268] Call OK. Usage: {\"input_tokens\": 7263, \"output_tokens\": 955, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6801). Retrying in 10.13s... (Attempt 1/10)\n",
      "2025-07-18T20:40:09.294+00:00 [SUCCESS] [Idx 6555] [Mdl: gemini-2.5-flash] [Task Task-270] Call OK. Usage: {\"input_tokens\": 7234, \"output_tokens\": 743, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6818). Retrying in 10.47s... (Attempt 1/10)\n",
      "2025-07-18T20:40:09.552+00:00 [SUCCESS] [Idx 6648] [Mdl: gemini-2.5-flash] [Task Task-280] Call OK. Usage: {\"input_tokens\": 7190, \"output_tokens\": 689, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6845). Retrying in 10.41s... (Attempt 1/10)\n",
      "2025-07-18T20:40:10.222+00:00 [SUCCESS] [Idx 6641] [Mdl: gemini-2.5-flash] [Task Task-279] Call OK. Usage: {\"input_tokens\": 7160, \"output_tokens\": 598, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6849). Retrying in 10.22s... (Attempt 1/10)\n",
      "2025-07-18T20:40:10.700+00:00 [SUCCESS] [Idx 6621] [Mdl: gemini-2.5-flash] [Task Task-278] Call OK. Usage: {\"input_tokens\": 7256, \"output_tokens\": 737, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6872). Retrying in 10.81s... (Attempt 1/10)\n",
      "2025-07-18T20:40:11.148+00:00 [SUCCESS] [Idx 6723] [Mdl: gemini-2.5-flash] [Task Task-289] Call OK. Usage: {\"input_tokens\": 7167, \"output_tokens\": 563, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6892). Retrying in 10.80s... (Attempt 1/10)\n",
      "2025-07-18T20:40:11.438+00:00 [SUCCESS] [Idx 6580] [Mdl: gemini-2.5-flash] [Task Task-273] Call OK. Usage: {\"input_tokens\": 7189, \"output_tokens\": 709, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6897). Retrying in 10.56s... (Attempt 1/10)\n",
      "2025-07-18T20:40:11.878+00:00 [SUCCESS] [Idx 6695] [Mdl: gemini-2.5-flash] [Task Task-284] Call OK. Usage: {\"input_tokens\": 7244, \"output_tokens\": 821, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:11.902+00:00 [SUCCESS] [Idx 6368] [Mdl: gemini-2.5-flash] [Task Task-251] Call OK. Usage: {\"input_tokens\": 7413, \"output_tokens\": 868, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6898). Retrying in 10.23s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6907). Retrying in 10.81s... (Attempt 1/10)\n",
      "2025-07-18T20:40:12.257+00:00 [SUCCESS] [Idx 6664] [Mdl: gemini-2.5-flash] [Task Task-282] Call OK. Usage: {\"input_tokens\": 7216, \"output_tokens\": 974, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6908). Retrying in 10.86s... (Attempt 1/10)\n",
      "2025-07-18T20:40:12.923+00:00 [SUCCESS] [Idx 6693] [Mdl: gemini-2.5-flash] [Task Task-283] Call OK. Usage: {\"input_tokens\": 7247, \"output_tokens\": 1063, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6909). Retrying in 10.49s... (Attempt 1/10)\n",
      "2025-07-18T20:40:13.014+00:00 [ ERROR ] [Idx 5648] [Mdl: gemini-2.5-flash] [Task Task-201] ValueError: Google API returned an empty response for Index 5648.\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6912). Retrying in 10.73s... (Attempt 1/10)\n",
      "2025-07-18T20:40:13.434+00:00 [SUCCESS] [Idx 6275] [Mdl: gemini-2.5-flash] [Task Task-246] Call OK. Usage: {\"input_tokens\": 7336, \"output_tokens\": 1254, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6916). Retrying in 10.47s... (Attempt 1/10)\n",
      "2025-07-18T20:40:13.536+00:00 [SUCCESS] [Idx 6589] [Mdl: gemini-2.5-flash] [Task Task-275] Call OK. Usage: {\"input_tokens\": 7247, \"output_tokens\": 958, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:13.580+00:00 [SUCCESS] [Idx 6482] [Mdl: gemini-2.5-flash] [Task Task-264] Call OK. Usage: {\"input_tokens\": 7261, \"output_tokens\": 1016, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6917). Retrying in 10.48s... (Attempt 1/10)\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6935). Retrying in 11.00s... (Attempt 1/10)\n",
      "2025-07-18T20:40:14.211+00:00 [SUCCESS] [Idx 6620] [Mdl: gemini-2.5-flash] [Task Task-277] Call OK. Usage: {\"input_tokens\": 7425, \"output_tokens\": 1240, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6937). Retrying in 10.16s... (Attempt 1/10)\n",
      "2025-07-18T20:40:14.953+00:00 [SUCCESS] [Idx 6660] [Mdl: gemini-2.5-flash] [Task Task-281] Call OK. Usage: {\"input_tokens\": 7473, \"output_tokens\": 1528, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6947). Retrying in 10.93s... (Attempt 1/10)\n",
      "2025-07-18T20:40:15.261+00:00 [SUCCESS] [Idx 6702] [Mdl: gemini-2.5-flash] [Task Task-286] Call OK. Usage: {\"input_tokens\": 7226, \"output_tokens\": 693, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6953). Retrying in 10.32s... (Attempt 1/10)\n",
      "2025-07-18T20:40:16.423+00:00 [SUCCESS] [Idx 6705] [Mdl: gemini-2.5-flash] [Task Task-287] Call OK. Usage: {\"input_tokens\": 7221, \"output_tokens\": 921, \"cached_tokens\": 0}\n",
      "🕒 Rate limit on gemini-2.5-flash (Index 6954). Retrying in 10.34s... (Attempt 1/10)\n",
      "2025-07-18T20:40:20.016+00:00 [SUCCESS] [Idx 6760] [Mdl: gemini-2.5-flash] [Task Task-291] Call OK. Usage: {\"input_tokens\": 7210, \"output_tokens\": 792, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:21.362+00:00 [SUCCESS] [Idx 6601] [Mdl: gemini-2.5-flash] [Task Task-276] Call OK. Usage: {\"input_tokens\": 7190, \"output_tokens\": 680, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:21.997+00:00 [SUCCESS] [Idx 6001] [Mdl: gemini-2.5-flash] [Task Task-228] Call OK. Usage: {\"input_tokens\": 7201, \"output_tokens\": 678, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:23.473+00:00 [SUCCESS] [Idx 6801] [Mdl: gemini-2.5-flash] [Task Task-294] Call OK. Usage: {\"input_tokens\": 7121, \"output_tokens\": 431, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:24.256+00:00 [SUCCESS] [Idx 6786] [Mdl: gemini-2.5-flash] [Task Task-293] Call OK. Usage: {\"input_tokens\": 7241, \"output_tokens\": 779, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:25.106+00:00 [SUCCESS] [Idx 6783] [Mdl: gemini-2.5-flash] [Task Task-292] Call OK. Usage: {\"input_tokens\": 7203, \"output_tokens\": 659, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:25.879+00:00 [SUCCESS] [Idx 6892] [Mdl: gemini-2.5-flash] [Task Task-299] Call OK. Usage: {\"input_tokens\": 7096, \"output_tokens\": 372, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:26.385+00:00 [SUCCESS] [Idx 6818] [Mdl: gemini-2.5-flash] [Task Task-295] Call OK. Usage: {\"input_tokens\": 7242, \"output_tokens\": 702, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:26.804+00:00 [SUCCESS] [Idx 6979] [Mdl: gemini-2.5-flash] [Task Task-313] Call OK. Usage: {\"input_tokens\": 7219, \"output_tokens\": 677, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:26.815+00:00 [SUCCESS] [Idx 6872] [Mdl: gemini-2.5-flash] [Task Task-298] Call OK. Usage: {\"input_tokens\": 7180, \"output_tokens\": 477, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:27.500+00:00 [SUCCESS] [Idx 6982] [Mdl: gemini-2.5-flash] [Task Task-314] Call OK. Usage: {\"input_tokens\": 7110, \"output_tokens\": 390, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:27.592+00:00 [SUCCESS] [Idx 6747] [Mdl: gemini-2.5-flash] [Task Task-290] Call OK. Usage: {\"input_tokens\": 7372, \"output_tokens\": 1206, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:27.720+00:00 [SUCCESS] [Idx 6587] [Mdl: gemini-2.5-flash] [Task Task-274] Call OK. Usage: {\"input_tokens\": 7279, \"output_tokens\": 936, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:28.231+00:00 [SUCCESS] [Idx 6909] [Mdl: gemini-2.5-flash] [Task Task-304] Call OK. Usage: {\"input_tokens\": 7131, \"output_tokens\": 407, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:28.330+00:00 [SUCCESS] [Idx 6996] [Mdl: gemini-2.5-flash] [Task Task-315] Call OK. Usage: {\"input_tokens\": 7208, \"output_tokens\": 627, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:28.931+00:00 [SUCCESS] [Idx 6897] [Mdl: gemini-2.5-flash] [Task Task-300] Call OK. Usage: {\"input_tokens\": 7195, \"output_tokens\": 703, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:29.822+00:00 [SUCCESS] [Idx 6898] [Mdl: gemini-2.5-flash] [Task Task-301] Call OK. Usage: {\"input_tokens\": 7202, \"output_tokens\": 717, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:29.838+00:00 [SUCCESS] [Idx 6908] [Mdl: gemini-2.5-flash] [Task Task-303] Call OK. Usage: {\"input_tokens\": 7228, \"output_tokens\": 776, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:30.831+00:00 [SUCCESS] [Idx 6917] [Mdl: gemini-2.5-flash] [Task Task-307] Call OK. Usage: {\"input_tokens\": 7192, \"output_tokens\": 548, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:31.051+00:00 [SUCCESS] [Idx 7010] [Mdl: gemini-2.5-flash] [Task Task-317] Call OK. Usage: {\"input_tokens\": 7257, \"output_tokens\": 726, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:31.399+00:00 [SUCCESS] [Idx 6417] [Mdl: gemini-2.5-flash] [Task Task-256] Call OK. Usage: {\"input_tokens\": 7394, \"output_tokens\": 1103, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:31.469+00:00 [SUCCESS] [Idx 6935] [Mdl: gemini-2.5-flash] [Task Task-308] Call OK. Usage: {\"input_tokens\": 7167, \"output_tokens\": 620, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:32.524+00:00 [SUCCESS] [Idx 7009] [Mdl: gemini-2.5-flash] [Task Task-316] Call OK. Usage: {\"input_tokens\": 7185, \"output_tokens\": 581, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:32.692+00:00 [SUCCESS] [Idx 7022] [Mdl: gemini-2.5-flash] [Task Task-318] Call OK. Usage: {\"input_tokens\": 7208, \"output_tokens\": 766, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:32.803+00:00 [SUCCESS] [Idx 6937] [Mdl: gemini-2.5-flash] [Task Task-309] Call OK. Usage: {\"input_tokens\": 7236, \"output_tokens\": 980, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:33.755+00:00 [SUCCESS] [Idx 7027] [Mdl: gemini-2.5-flash] [Task Task-319] Call OK. Usage: {\"input_tokens\": 7200, \"output_tokens\": 682, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:34.700+00:00 [SUCCESS] [Idx 7074] [Mdl: gemini-2.5-flash] [Task Task-324] Call OK. Usage: {\"input_tokens\": 7122, \"output_tokens\": 428, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:35.020+00:00 [SUCCESS] [Idx 7112] [Mdl: gemini-2.5-flash] [Task Task-326] Call OK. Usage: {\"input_tokens\": 7125, \"output_tokens\": 413, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:35.246+00:00 [SUCCESS] [Idx 7039] [Mdl: gemini-2.5-flash] [Task Task-322] Call OK. Usage: {\"input_tokens\": 7230, \"output_tokens\": 726, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:35.511+00:00 [SUCCESS] [Idx 6912] [Mdl: gemini-2.5-flash] [Task Task-305] Call OK. Usage: {\"input_tokens\": 7253, \"output_tokens\": 825, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:35.568+00:00 [SUCCESS] [Idx 6916] [Mdl: gemini-2.5-flash] [Task Task-306] Call OK. Usage: {\"input_tokens\": 7321, \"output_tokens\": 988, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:35.737+00:00 [SUCCESS] [Idx 7123] [Mdl: gemini-2.5-flash] [Task Task-328] Call OK. Usage: {\"input_tokens\": 7278, \"output_tokens\": 838, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:36.409+00:00 [SUCCESS] [Idx 7183] [Mdl: gemini-2.5-flash] [Task Task-334] Call OK. Usage: {\"input_tokens\": 7205, \"output_tokens\": 497, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:36.525+00:00 [SUCCESS] [Idx 7120] [Mdl: gemini-2.5-flash] [Task Task-327] Call OK. Usage: {\"input_tokens\": 7208, \"output_tokens\": 555, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:36.526+00:00 [SUCCESS] [Idx 7168] [Mdl: gemini-2.5-flash] [Task Task-331] Call OK. Usage: {\"input_tokens\": 7112, \"output_tokens\": 424, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:36.654+00:00 [SUCCESS] [Idx 7037] [Mdl: gemini-2.5-flash] [Task Task-321] Call OK. Usage: {\"input_tokens\": 7354, \"output_tokens\": 1123, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:36.871+00:00 [SUCCESS] [Idx 7041] [Mdl: gemini-2.5-flash] [Task Task-323] Call OK. Usage: {\"input_tokens\": 7216, \"output_tokens\": 790, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:37.027+00:00 [SUCCESS] [Idx 6947] [Mdl: gemini-2.5-flash] [Task Task-310] Call OK. Usage: {\"input_tokens\": 7239, \"output_tokens\": 1030, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:38.058+00:00 [SUCCESS] [Idx 7077] [Mdl: gemini-2.5-flash] [Task Task-325] Call OK. Usage: {\"input_tokens\": 7348, \"output_tokens\": 1214, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:38.271+00:00 [SUCCESS] [Idx 7180] [Mdl: gemini-2.5-flash] [Task Task-333] Call OK. Usage: {\"input_tokens\": 7240, \"output_tokens\": 667, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:38.550+00:00 [ ERROR ] [Idx 6575] [Mdl: gemini-2.5-flash] [Task Task-272] ValueError: Google API returned an empty response for Index 6575.\n",
      "2025-07-18T20:40:39.385+00:00 [SUCCESS] [Idx 7195] [Mdl: gemini-2.5-flash] [Task Task-339] Call OK. Usage: {\"input_tokens\": 7138, \"output_tokens\": 444, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:40.195+00:00 [SUCCESS] [Idx 6953] [Mdl: gemini-2.5-flash] [Task Task-311] Call OK. Usage: {\"input_tokens\": 7376, \"output_tokens\": 1690, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:40.695+00:00 [SUCCESS] [Idx 7184] [Mdl: gemini-2.5-flash] [Task Task-335] Call OK. Usage: {\"input_tokens\": 7226, \"output_tokens\": 698, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:40.856+00:00 [SUCCESS] [Idx 7237] [Mdl: gemini-2.5-flash] [Task Task-344] Call OK. Usage: {\"input_tokens\": 7184, \"output_tokens\": 650, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:40.986+00:00 [SUCCESS] [Idx 7192] [Mdl: gemini-2.5-flash] [Task Task-337] Call OK. Usage: {\"input_tokens\": 7207, \"output_tokens\": 706, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.066+00:00 [SUCCESS] [Idx 7191] [Mdl: gemini-2.5-flash] [Task Task-336] Call OK. Usage: {\"input_tokens\": 7161, \"output_tokens\": 553, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.268+00:00 [SUCCESS] [Idx 6845] [Mdl: gemini-2.5-flash] [Task Task-296] Call OK. Usage: {\"input_tokens\": 7370, \"output_tokens\": 1549, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.373+00:00 [SUCCESS] [Idx 7212] [Mdl: gemini-2.5-flash] [Task Task-341] Call OK. Usage: {\"input_tokens\": 7195, \"output_tokens\": 672, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.588+00:00 [SUCCESS] [Idx 7201] [Mdl: gemini-2.5-flash] [Task Task-340] Call OK. Usage: {\"input_tokens\": 7203, \"output_tokens\": 666, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.890+00:00 [SUCCESS] [Idx 7227] [Mdl: gemini-2.5-flash] [Task Task-342] Call OK. Usage: {\"input_tokens\": 7141, \"output_tokens\": 552, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:41.989+00:00 [SUCCESS] [Idx 6954] [Mdl: gemini-2.5-flash] [Task Task-312] Call OK. Usage: {\"input_tokens\": 7171, \"output_tokens\": 482, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:42.958+00:00 [ ERROR ] [Idx 6713] [Mdl: gemini-2.5-flash] [Task Task-288] ValueError: Google API returned an empty response for Index 6713.\n",
      "2025-07-18T20:40:43.105+00:00 [SUCCESS] [Idx 7257] [Mdl: gemini-2.5-flash] [Task Task-347] Call OK. Usage: {\"input_tokens\": 7181, \"output_tokens\": 673, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:43.603+00:00 [SUCCESS] [Idx 7124] [Mdl: gemini-2.5-flash] [Task Task-329] Call OK. Usage: {\"input_tokens\": 7391, \"output_tokens\": 1260, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:43.604+00:00 [ ERROR ] [Idx 6700] [Mdl: gemini-2.5-flash] [Task Task-285] ValueError: Google API returned an empty response for Index 6700.\n",
      "2025-07-18T20:40:43.657+00:00 [SUCCESS] [Idx 7332] [Mdl: gemini-2.5-flash] [Task Task-350] Call OK. Usage: {\"input_tokens\": 7237, \"output_tokens\": 751, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:43.718+00:00 [SUCCESS] [Idx 6907] [Mdl: gemini-2.5-flash] [Task Task-302] Call OK. Usage: {\"input_tokens\": 7345, \"output_tokens\": 1579, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:44.091+00:00 [SUCCESS] [Idx 7146] [Mdl: gemini-2.5-flash] [Task Task-330] Call OK. Usage: {\"input_tokens\": 7304, \"output_tokens\": 1039, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:44.114+00:00 [SUCCESS] [Idx 7350] [Mdl: gemini-2.5-flash] [Task Task-352] Call OK. Usage: {\"input_tokens\": 7259, \"output_tokens\": 656, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:44.141+00:00 [SUCCESS] [Idx 7288] [Mdl: gemini-2.5-flash] [Task Task-348] Call OK. Usage: {\"input_tokens\": 7205, \"output_tokens\": 754, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:44.334+00:00 [SUCCESS] [Idx 7241] [Mdl: gemini-2.5-flash] [Task Task-346] Call OK. Usage: {\"input_tokens\": 7235, \"output_tokens\": 768, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:44.726+00:00 [SUCCESS] [Idx 7238] [Mdl: gemini-2.5-flash] [Task Task-345] Call OK. Usage: {\"input_tokens\": 7285, \"output_tokens\": 876, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:46.367+00:00 [SUCCESS] [Idx 7411] [Mdl: gemini-2.5-flash] [Task Task-355] Call OK. Usage: {\"input_tokens\": 7215, \"output_tokens\": 680, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:46.382+00:00 [SUCCESS] [Idx 7335] [Mdl: gemini-2.5-flash] [Task Task-351] Call OK. Usage: {\"input_tokens\": 7267, \"output_tokens\": 810, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:46.861+00:00 [SUCCESS] [Idx 7307] [Mdl: gemini-2.5-flash] [Task Task-349] Call OK. Usage: {\"input_tokens\": 7234, \"output_tokens\": 969, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:48.373+00:00 [SUCCESS] [Idx 7193] [Mdl: gemini-2.5-flash] [Task Task-338] Call OK. Usage: {\"input_tokens\": 7322, \"output_tokens\": 989, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:48.430+00:00 [SUCCESS] [Idx 7228] [Mdl: gemini-2.5-flash] [Task Task-343] Call OK. Usage: {\"input_tokens\": 7281, \"output_tokens\": 946, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:48.901+00:00 [SUCCESS] [Idx 7465] [Mdl: gemini-2.5-flash] [Task Task-361] Call OK. Usage: {\"input_tokens\": 7181, \"output_tokens\": 680, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:50.017+00:00 [SUCCESS] [Idx 7450] [Mdl: gemini-2.5-flash] [Task Task-358] Call OK. Usage: {\"input_tokens\": 7269, \"output_tokens\": 970, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:50.649+00:00 [SUCCESS] [Idx 7420] [Mdl: gemini-2.5-flash] [Task Task-357] Call OK. Usage: {\"input_tokens\": 7312, \"output_tokens\": 1005, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:52.717+00:00 [SUCCESS] [Idx 7030] [Mdl: gemini-2.5-flash] [Task Task-320] Call OK. Usage: {\"input_tokens\": 7316, \"output_tokens\": 802, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:53.712+00:00 [SUCCESS] [Idx 7458] [Mdl: gemini-2.5-flash] [Task Task-359] Call OK. Usage: {\"input_tokens\": 7195, \"output_tokens\": 634, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:55.689+00:00 [SUCCESS] [Idx 6849] [Mdl: gemini-2.5-flash] [Task Task-297] Call OK. Usage: {\"input_tokens\": 7363, \"output_tokens\": 1026, \"cached_tokens\": 0}\n",
      "2025-07-18T20:40:59.138+00:00 [SUCCESS] [Idx 7413] [Mdl: gemini-2.5-flash] [Task Task-356] Call OK. Usage: {\"input_tokens\": 7214, \"output_tokens\": 675, \"cached_tokens\": 0}\n",
      "2025-07-18T20:41:00.645+00:00 [SUCCESS] [Idx 7364] [Mdl: gemini-2.5-flash] [Task Task-353] Call OK. Usage: {\"input_tokens\": 7559, \"output_tokens\": 1980, \"cached_tokens\": 0}\n",
      "2025-07-18T20:41:01.116+00:00 [SUCCESS] [Idx 7408] [Mdl: gemini-2.5-flash] [Task Task-354] Call OK. Usage: {\"input_tokens\": 7218, \"output_tokens\": 884, \"cached_tokens\": 0}\n",
      "2025-07-18T20:41:05.356+00:00 [SUCCESS] [Idx 7178] [Mdl: gemini-2.5-flash] [Task Task-332] Call OK. Usage: {\"input_tokens\": 7258, \"output_tokens\": 996, \"cached_tokens\": 0}\n",
      "2025-07-18T20:41:09.465+00:00 [SUCCESS] [Idx 7463] [Mdl: gemini-2.5-flash] [Task Task-360] Call OK. Usage: {\"input_tokens\": 7280, \"output_tokens\": 1001, \"cached_tokens\": 0}\n"
     ]
    }
   ],
   "source": [
    "LOWER_LIMIT = 3001\n",
    "UPPER_LIMIT = 7473\n",
    "INDICES_TO_GENERATE = [idx for idx in CURRENT_INDICES if idx <= UPPER_LIMIT and idx >= LOWER_LIMIT]\n",
    "\n",
    "print(f\"Starting generation for {len(INDICES_TO_GENERATE)} indices in {CURRENT_TIER} between {LOWER_LIMIT} and {UPPER_LIMIT}.\")\n",
    "\n",
    "perf_df = await generate_templates_parallel_fixed(indices_to_generate=INDICES_TO_GENERATE)\n",
    "\n",
    "print(\"\\n--- Generation Performance Summary ---\")\n",
    "perf_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
