{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50cf620",
   "metadata": {},
   "source": [
    "Of course. Here is a high-level, cell-by-cell outline for the Level 2 dataset creation notebook, designed to implement your \"triplet\" strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Notebook Outline: Level 2 Dataset Preparation**\n",
    "\n",
    "**Part 1: Setup and Configuration**\n",
    "\n",
    "*   **Cell 1.1: Imports and Global Setup.**\n",
    "    *   Import necessary libraries (`json`, `pandas`, `pathlib`, `datasets`, etc.).\n",
    "    *   Define project root and I/O directories, identical to the Level 1 script.\n",
    "*   **Cell 1.2: Define Label Mapping.**\n",
    "    *   Explicitly define the class-to-integer mapping in a dictionary for clarity.\n",
    "    *   `LABEL_MAP = {\"correct\": 0, \"conceptual\": 1, \"computational\": 2}`.\n",
    "\n",
    "**Part 2: Data Loading**\n",
    "\n",
    "*   **Cell 2.1: Load Raw Data Sources.**\n",
    "    *   Load the manual errors CSV (`manually_generated_errors_final.csv`).\n",
    "    *   Load the programmatic computational error catalog (`computational_error_catalog.csv`).\n",
    "    *   Load the original GSM8K dataset to retrieve correct answers.\n",
    "    *   Print counts for each source.\n",
    "\n",
    "**Part 3: Data Preparation Pipeline**\n",
    "\n",
    "*   **Cell 3.1: Build Master Index and Data Lookups.**\n",
    "    *   **Goal:** Create the set of problem indices that will form the basis of our dataset.\n",
    "    *   **Action:** Scan the manual errors CSV and identify all unique problem indices that have at least one `conceptual` error. This set, `anchor_indices`, will be our master list (`N â‰ˆ 1000`).\n",
    "    *   **Action:** Create a `gsm8k_lookup` dictionary for easy access to the original \"correct\" question/answer pairs.\n",
    "*   **Cell 3.2: Prepare \"Correct\" and \"Conceptual\" Samples.**\n",
    "    *   **Goal:** Create the first two-thirds of the dataset.\n",
    "    *   **Action:** Iterate through each `idx` in `anchor_indices`.\n",
    "    *   For each `idx`, add the \"Correct\" sample (Label 0) from `gsm8k_lookup`.\n",
    "    *   For each `idx`, find the corresponding manual `conceptual` error and add it as a sample (Label 1). Handle cases where an index might have multiple conceptual errors by randomly selecting one.\n",
    "*   **Cell 3.3: Prepare \"Computational\" Samples (Hierarchical Sourcing).**\n",
    "    *   **Goal:** Create the final third of the dataset, ensuring every anchor index gets one computational error.\n",
    "    *   **Action:** Iterate through each `idx` in `anchor_indices`.\n",
    "    *   **First, check for a manual computational error:** Look in the manual errors CSV for a `computational` error with that `idx`. If found, add it as a sample (Label 2) and move to the next index.\n",
    "    *   **If not found, source from the programmatic catalog:** Query the programmatic catalog for a valid computational error file for that `idx`. If found, load the `flawed_nl_solution` from the JSON file, create the sample (Label 2), and add it.\n",
    "    *   **Handle missing data:** Keep track of any indices for which no computational error could be found.\n",
    "*   **Cell 3.4: Final Assembly and Verification.**\n",
    "    *   **Goal:** Combine all generated samples and verify the class balance.\n",
    "    *   **Action:** Merge the lists of \"Correct\", \"Conceptual\", and \"Computational\" samples into a single list (`final_dataset_list`).\n",
    "    *   **Action:** Print the final counts for each label (0, 1, 2) to confirm the 1:1:1 balance.\n",
    "    *   **Action:** Save a metadata catalog CSV, similar to the Level 1 script, but with the new label scheme.\n",
    "\n",
    "**Part 4: Dataset Splitting and Saving**\n",
    "\n",
    "*   **Cell 4.1: Split and Save the Dataset.**\n",
    "    *   **Goal:** Split the data into train/validation/test sets based on `index` to prevent leakage.\n",
    "    *   **Action:** Use the `split_dataset_by_index` function from the Level 1 script on `final_dataset_list`.\n",
    "    *   **Action:** Save the final `DatasetDict` to disk.\n",
    "    *   **Action:** Print the final structure of the saved dataset.\n",
    "\n",
    "**Part 5: Verification**\n",
    "\n",
    "*   **Cell 5.1: Inspect Random Triplets.**\n",
    "    *   **Goal:** Visually inspect the generated data for correctness.\n",
    "    *   **Action:** Create a `display_triplet_samples` function that selects a random `index`, finds the corresponding \"Correct\", \"Conceptual\", and \"Computational\" samples from `final_dataset_list`, and prints all three for comparison. Run this for a few random indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56756c7",
   "metadata": {},
   "source": [
    "# Level 2 SFT Dataset Preparation\n",
    "\n",
    "This notebook prepares the dataset for the Level 2 fine-tuning task. The goal is to create a 3-class classification dataset with the following labels:\n",
    "-   `0`: Correct\n",
    "-   `1`: Conceptual Error\n",
    "-   `2`: Computational Error\n",
    "\n",
    "The script will generate a perfectly balanced dataset with a 1:1:1 ratio across the three classes, using a \"triplet\" structure where each problem index is represented by one sample from each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327c283",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "### 1.1 Imports and Global Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92322015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Dataset output directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/level-2-three-class\n"
     ]
    }
   ],
   "source": [
    "# --- 1.1: Imports and Global Paths ---\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = DATA_DIR / \"sft-datasets/level-2-three-class\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Dataset output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89005e32",
   "metadata": {},
   "source": [
    "### 1.2 Define Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0011629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping defined:\n",
      "{'correct': 0, 'conceptual': 1, 'computational': 2}\n"
     ]
    }
   ],
   "source": [
    "# --- 1.2: Define Label Mapping ---\n",
    "LABEL_MAP = {\n",
    "    \"correct\": 0,\n",
    "    \"conceptual\": 1,\n",
    "    \"computational\": 2,\n",
    "}\n",
    "\n",
    "print(\"Label mapping defined:\")\n",
    "print(LABEL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dad1b",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bd56dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,963 records from manual error CSV.\n",
      "Loaded 16,506 records from programmatic computational catalog.\n",
      "Loaded 7,473 samples from gsm8k/main train split.\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1: Load Raw Data Sources ---\n",
    "\n",
    "# Load the comprehensive CSV of all manually generated/annotated errors\n",
    "MANUAL_ERRORS_CSV_PATH = DATA_DIR / \"manually_generated_errors_final.csv\"\n",
    "manual_errors_df = pd.read_csv(MANUAL_ERRORS_CSV_PATH)\n",
    "\n",
    "# Load the catalog of all programmatically generated computational errors\n",
    "PROGRAMMATIC_COMP_CATALOG_PATH = DATA_DIR / \"computational-errors-generated/computational_error_catalog.csv\"\n",
    "programmatic_catalog_df = pd.read_csv(PROGRAMMATIC_COMP_CATALOG_PATH)\n",
    "\n",
    "# Load the original GSM8K training set for correct answers\n",
    "GSM8K_TRAIN_DATASET: Dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "print(f\"Loaded {len(manual_errors_df):,} records from manual error CSV.\")\n",
    "print(f\"Loaded {len(programmatic_catalog_df):,} records from programmatic computational catalog.\")\n",
    "print(f\"Loaded {len(GSM8K_TRAIN_DATASET):,} samples from gsm8k/main train split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78951",
   "metadata": {},
   "source": [
    "## 3. Data Preparation Pipeline\n",
    "\n",
    "### 3.1 Define Core Data Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2456ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core data pipeline functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1: Define Core Data Pipeline Functions ---\n",
    "\n",
    "def sanitize_commas(text: str) -> str:\n",
    "    \"\"\"Removes comma separators from numbers to prevent model artifacts.\"\"\"\n",
    "    return re.sub(r'(\\d),(\\d)', r'\\1\\2', text)\n",
    "\n",
    "def prepare_conceptual_and_correct_samples(\n",
    "    anchor_indices: list[int],\n",
    "    manual_errors_df: pd.DataFrame,\n",
    "    gsm8k_lookup: dict,\n",
    "    label_map: dict,\n",
    "    seed: int = 42\n",
    ") -> tuple[list[dict], list[dict]]:\n",
    "    \"\"\"\n",
    "    For each anchor index, prepares a 'correct' sample and a 'conceptual' error sample.\n",
    "    \"\"\"\n",
    "    samples, metadata = [], []\n",
    "    rng = random.Random(seed)\n",
    "    \n",
    "    conceptual_df = manual_errors_df[\n",
    "        (manual_errors_df['error_type'] == 'conceptual') &\n",
    "        (manual_errors_df['index'].isin(anchor_indices))\n",
    "    ]\n",
    "\n",
    "    for idx in anchor_indices:\n",
    "        # --- START OF FIX ---\n",
    "        py_idx = int(idx) # Convert numpy.int64 to standard Python int\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        # 1. Add the \"Correct\" sample\n",
    "        original_problem = gsm8k_lookup[py_idx]\n",
    "        samples.append({\n",
    "            \"index\": py_idx, \"question\": original_problem['question'],\n",
    "            \"solution\": sanitize_commas(original_problem['answer']),\n",
    "            \"label\": label_map['correct']\n",
    "        })\n",
    "        metadata.append({\n",
    "            \"index\": py_idx, \"label\": label_map['correct'], \"error_type\": \"correct\",\n",
    "            \"source_file\": \"gsm8k.main.train\"\n",
    "        })\n",
    "\n",
    "        # 2. Add the \"Conceptual\" sample\n",
    "        candidate_rows = conceptual_df[conceptual_df['index'] == py_idx]\n",
    "        chosen_row = candidate_rows.sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "        samples.append({\n",
    "            \"index\": py_idx, \"question\": chosen_row['question'],\n",
    "            \"solution\": sanitize_commas(chosen_row['wrong_answer']),\n",
    "            \"label\": label_map['conceptual']\n",
    "        })\n",
    "        metadata.append({\n",
    "            \"index\": py_idx, \"label\": label_map['conceptual'], \"error_type\": 'conceptual',\n",
    "            \"source_file\": chosen_row['filepath']\n",
    "        })\n",
    "        \n",
    "    return samples, metadata\n",
    "\n",
    "print(\"Core data pipeline functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd598e7b",
   "metadata": {},
   "source": [
    "### 3.2 Build Master Index and Data Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae59ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 1030 anchor indices with manual conceptual errors.\n"
     ]
    }
   ],
   "source": [
    "# --- 3.2: Build Master Index and Data Lookups ---\n",
    "\n",
    "# Identify all unique problem indices that have a manual conceptual error.\n",
    "# These indices will form the foundation of our \"triplet\" dataset.\n",
    "anchor_indices = sorted(list(\n",
    "    manual_errors_df[manual_errors_df['error_type'] == 'conceptual']['index'].unique()\n",
    "))\n",
    "\n",
    "# Create a lookup dictionary for original GSM8K problems for fast access.\n",
    "gsm8k_lookup = {\n",
    "    i: {\"question\": q, \"answer\": a}\n",
    "    for i, (q, a) in enumerate(zip(GSM8K_TRAIN_DATASET[\"question\"], GSM8K_TRAIN_DATASET[\"answer\"]))\n",
    "}\n",
    "\n",
    "print(f\"Identified {len(anchor_indices)} anchor indices with manual conceptual errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045f0fa",
   "metadata": {},
   "source": [
    "### 3.3 Prepare \"Correct\" and \"Conceptual\" Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54842b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2060 samples for 'Correct' and 'Conceptual' classes.\n",
      "Total unique problems: 1030\n"
     ]
    }
   ],
   "source": [
    "# --- 3.3: Prepare \"Correct\" and \"Conceptual\" Samples ---\n",
    "\n",
    "# Use the utility function to generate the first two legs of our triplets.\n",
    "base_samples, base_metadata = prepare_conceptual_and_correct_samples(\n",
    "    anchor_indices=anchor_indices,\n",
    "    manual_errors_df=manual_errors_df,\n",
    "    gsm8k_lookup=gsm8k_lookup,\n",
    "    label_map=LABEL_MAP\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(base_samples)} samples for 'Correct' and 'Conceptual' classes.\")\n",
    "print(f\"Total unique problems: {len(base_samples) // 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897dd048",
   "metadata": {},
   "source": [
    "### 3.4 Define \"Computational\" Sample Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc22009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational sample preparation function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3.4: Define \"Computational\" Sample Preparation Function ---\n",
    "\n",
    "def prepare_computational_samples(\n",
    "    anchor_indices: list[int],\n",
    "    manual_errors_df: pd.DataFrame,\n",
    "    programmatic_catalog_df: pd.DataFrame,\n",
    "    gsm8k_lookup: dict,\n",
    "    label_map: dict,\n",
    "    project_root: Path\n",
    ") -> tuple[list[dict], list[dict], list[int]]:\n",
    "    \"\"\"\n",
    "    For each anchor index, sources one 'computational' error sample.\n",
    "    It prioritizes manually generated errors, then falls back to programmatic ones.\n",
    "    \"\"\"\n",
    "    samples, metadata = [], []\n",
    "    unfound_indices = []\n",
    "    \n",
    "    manual_comp_df = manual_errors_df[manual_errors_df['error_type'] == 'computational'].set_index('index')\n",
    "    \n",
    "    prog_comp_lookup = {\n",
    "        row['index']: row['filepath']\n",
    "        for _, row in programmatic_catalog_df.iterrows()\n",
    "        if pd.notna(row['filepath'])\n",
    "    }\n",
    "    \n",
    "    print(\"Sourcing computational errors...\")\n",
    "    for idx in anchor_indices:\n",
    "        # --- START OF FIX ---\n",
    "        py_idx = int(idx) # Convert numpy.int64 to standard Python int\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        found = False\n",
    "        if py_idx in manual_comp_df.index:\n",
    "            result = manual_comp_df.loc[py_idx]\n",
    "            if isinstance(result, pd.DataFrame):\n",
    "                chosen_row = result.iloc[0]\n",
    "            else:\n",
    "                chosen_row = result\n",
    "                \n",
    "            question = chosen_row['question']\n",
    "            flawed_solution = chosen_row['wrong_answer']\n",
    "            source_file = chosen_row['filepath']\n",
    "            found = True\n",
    "        elif py_idx in prog_comp_lookup:\n",
    "            try:\n",
    "                filepath = project_root / prog_comp_lookup[py_idx]\n",
    "                with open(filepath, 'r') as f:\n",
    "                    flawed_solution = json.load(f)[\"flawed_nl_solution\"]\n",
    "                question = gsm8k_lookup[py_idx]['question']\n",
    "                source_file = prog_comp_lookup[py_idx]\n",
    "                found = True\n",
    "            except (FileNotFoundError, KeyError, json.JSONDecodeError):\n",
    "                pass\n",
    "\n",
    "        if found:\n",
    "            samples.append({\n",
    "                \"index\": py_idx, \"question\": question,\n",
    "                \"solution\": sanitize_commas(flawed_solution),\n",
    "                \"label\": label_map['computational']\n",
    "            })\n",
    "            metadata.append({\n",
    "                \"index\": py_idx, \"label\": label_map['computational'], \"error_type\": 'computational',\n",
    "                \"source_file\": source_file\n",
    "            })\n",
    "        else:\n",
    "            unfound_indices.append(py_idx)\n",
    "            \n",
    "    return samples, metadata, unfound_indices\n",
    "\n",
    "print(\"Computational sample preparation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb8888",
   "metadata": {},
   "source": [
    "### 3.5 Prepare \"Computational\" Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46034632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sourcing computational errors...\n",
      "\n",
      "Generated 973 'Computational' error samples.\n",
      "Warning: Could not find a computational error for 57 indices.\n",
      "Sample of unfound indices: [10, 11, 29, 37, 145]\n"
     ]
    }
   ],
   "source": [
    "# --- 3.5: Prepare \"Computational\" Samples ---\n",
    "\n",
    "# Use the utility function to generate the final leg of our triplets.\n",
    "computational_samples, computational_metadata, unfound_indices = prepare_computational_samples(\n",
    "    anchor_indices=anchor_indices,\n",
    "    manual_errors_df=manual_errors_df,\n",
    "    programmatic_catalog_df=programmatic_catalog_df,\n",
    "    gsm8k_lookup=gsm8k_lookup,\n",
    "    label_map=LABEL_MAP,\n",
    "    project_root=PROJECT_ROOT\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(computational_samples)} 'Computational' error samples.\")\n",
    "if unfound_indices:\n",
    "    print(f\"Warning: Could not find a computational error for {len(unfound_indices)} indices.\")\n",
    "    print(f\"Sample of unfound indices: {unfound_indices[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3273e6",
   "metadata": {},
   "source": [
    "### 3.6 Final Assembly and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35b9beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Dataset Class Distribution ---\n",
      "label\n",
      "0    1030\n",
      "1    1030\n",
      "2     973\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mapping:\n",
      "  0: correct\n",
      "  1: conceptual\n",
      "  2: computational\n",
      "\n",
      "Metadata catalog with 3033 records saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/level-2-three-class/sft_level2_catalog.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 3.6: Final Assembly and Verification ---\n",
    "\n",
    "# Combine the samples from all three classes\n",
    "final_dataset_list = base_samples + computational_samples\n",
    "final_metadata_list = base_metadata + computational_metadata\n",
    "\n",
    "# Verify class balance\n",
    "final_df = pd.DataFrame(final_dataset_list)\n",
    "print(\"--- Final Dataset Class Distribution ---\")\n",
    "print(final_df['label'].value_counts().sort_index())\n",
    "print(\"\\nMapping:\")\n",
    "for class_name, label_int in LABEL_MAP.items():\n",
    "    print(f\"  {label_int}: {class_name}\")\n",
    "\n",
    "# Save the combined metadata catalog\n",
    "metadata_df = pd.DataFrame(final_metadata_list)\n",
    "CATALOG_PATH = OUTPUT_DIR / \"sft_level2_catalog.csv\"\n",
    "metadata_df.to_csv(CATALOG_PATH, index=False)\n",
    "print(f\"\\nMetadata catalog with {len(metadata_df)} records saved to: {CATALOG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9fb48",
   "metadata": {},
   "source": [
    "## 4. Dataset Splitting and Saving\n",
    "\n",
    "### 4.1 Split and Save the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2aaaaa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting final dataset and saving to disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8864e6a0d7942618d29cc9ab2198ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af2dd87e0b946fcbf9125c401d795bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3af38a41f464583b356b8760751c278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Dataset splits successfully saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/level-2-three-class\n",
      "\n",
      "--- Final Dataset Structure ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'question', 'solution', 'label'],\n",
      "        num_rows: 2426\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'question', 'solution', 'label'],\n",
      "        num_rows: 302\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'question', 'solution', 'label'],\n",
      "        num_rows: 305\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# --- 4.1: Split and Save the Dataset ---\n",
    "\n",
    "def split_dataset_by_index(dataset_list: list[dict], train_size: float = 0.8, seed: int = 42) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Splits a list of samples into train, validation, and test sets based on\n",
    "    the problem 'index' to prevent data leakage between splits.\n",
    "    All samples (correct, conceptual, computational) for a given index will\n",
    "    be placed in the same split.\n",
    "    \"\"\"\n",
    "    # Get a unique list of all problem indices in the dataset\n",
    "    all_indices = sorted(list(set(item['index'] for item in dataset_list)))\n",
    "    random.Random(seed).shuffle(all_indices)\n",
    "    \n",
    "    # Calculate the split points\n",
    "    train_end = int(len(all_indices) * train_size)\n",
    "    val_end = train_end + int((len(all_indices) - train_end) / 2)\n",
    "    \n",
    "    # Create sets of indices for each split\n",
    "    train_indices = set(all_indices[:train_end])\n",
    "    val_indices = set(all_indices[train_end:val_end])\n",
    "    test_indices = set(all_indices[val_end:])\n",
    "    \n",
    "    # Partition the full dataset list based on the index sets\n",
    "    train_data = [item for item in dataset_list if item['index'] in train_indices]\n",
    "    val_data = [item for item in dataset_list if item['index'] in val_indices]\n",
    "    test_data = [item for item in dataset_list if item['index'] in test_indices]\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_list(train_data),\n",
    "        \"validation\": Dataset.from_list(val_data),\n",
    "        \"test\": Dataset.from_list(test_data)\n",
    "    })\n",
    "\n",
    "# Shuffle the master list before splitting to ensure randomness within each epoch.\n",
    "# Note: The split_dataset_by_index function handles the split randomness itself.\n",
    "# This shuffle is for the benefit of the Trainer's data loader.\n",
    "random.Random(42).shuffle(final_dataset_list)\n",
    "\n",
    "# Perform the split\n",
    "print(\"Splitting final dataset and saving to disk...\")\n",
    "sft_dataset = split_dataset_by_index(final_dataset_list, train_size=0.8)\n",
    "\n",
    "# Save the final DatasetDict to disk\n",
    "sft_dataset.save_to_disk(OUTPUT_DIR)\n",
    "print(f\"-> Dataset splits successfully saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n--- Final Dataset Structure ---\")\n",
    "print(sft_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ef2bf",
   "metadata": {},
   "source": [
    "## 5. Inspect Random Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4775e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1: Inspect Random Triplets ---\n",
    "\n",
    "def display_triplet_samples(dataset_list: list[dict], n: int = 3, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Selects n random problem indices and displays the corresponding correct,\n",
    "    conceptual, and computational samples for verification.\n",
    "    \"\"\"\n",
    "    if not dataset_list:\n",
    "        print(\"Error: Dataset list is empty.\")\n",
    "        return\n",
    "\n",
    "    unique_indices = sorted(list(set(item['index'] for item in dataset_list)))\n",
    "    if not unique_indices:\n",
    "        print(\"Dataset contains no valid indices.\")\n",
    "        return\n",
    "        \n",
    "    n = min(n, len(unique_indices))\n",
    "    random.Random(seed).shuffle(unique_indices)\n",
    "    random_indices_to_display = unique_indices[:n]\n",
    "    \n",
    "    print(f\"--- Displaying {n} random sample triplets for inspection ---\")\n",
    "    \n",
    "    for i, problem_index in enumerate(random_indices_to_display):\n",
    "        print(f\"\\n{'='*25} Triplet {i + 1}/{n} (Problem Index: {problem_index}) {'='*25}\")\n",
    "        \n",
    "        # Find all samples for the current index\n",
    "        triplet = [item for item in dataset_list if item.get('index') == problem_index]\n",
    "        triplet.sort(key=lambda x: x['label']) # Sort by label: 0, 1, 2\n",
    "        \n",
    "        if len(triplet) != 3:\n",
    "            print(f\"Warning: Expected 3 samples for index {problem_index}, but found {len(triplet)}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\n--- Correct (Label 0) ---\")\n",
    "        print(json.dumps(triplet[0], indent=2, ensure_ascii=False))\n",
    "        \n",
    "        print(\"\\n--- Conceptual (Label 1) ---\")\n",
    "        print(json.dumps(triplet[1], indent=2, ensure_ascii=False))\n",
    "\n",
    "        print(\"\\n--- Computational (Label 2) ---\")\n",
    "        print(json.dumps(triplet[2], indent=2, ensure_ascii=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61767bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Displaying 3 random sample triplets for inspection ---\n",
      "\n",
      "========================= Triplet 1/3 (Problem Index: 1280) =========================\n",
      "\n",
      "--- Correct (Label 0) ---\n",
      "{\n",
      "  \"index\": 1280,\n",
      "  \"question\": \"Honey earned $80 a day. Every day, she spent part of her pay and saved the rest. After 20 days of work, she spent $1360. How much did Honey save in 20 days?\",\n",
      "  \"solution\": \"Honey earned 20 x $80 = $<<20*80=1600>>1600 for 20 days of work.\\nTherefore, Honey saved $1600 - $1360 = $<<1600-1360=240>>240 in 20 days.\\n#### 240\",\n",
      "  \"label\": 0\n",
      "}\n",
      "\n",
      "--- Conceptual (Label 1) ---\n",
      "{\n",
      "  \"index\": 1280,\n",
      "  \"question\": \"Honey earned $80 a day. Every day, she spent part of her pay and saved the rest. After 20 days of work, she spent $1360. How much did Honey save in 20 days?\",\n",
      "  \"solution\": \"Honey earned 20 x $80 = $<<20*80=1600>>1600 for 20 days of work.\\nTherefore, Honey saved $1600 + $1360 = $<<1600+1360=2960>>2960 in 20 days.\\n#### 2960\",\n",
      "  \"label\": 1\n",
      "}\n",
      "\n",
      "--- Computational (Label 2) ---\n",
      "{\n",
      "  \"index\": 1280,\n",
      "  \"question\": \"Honey earned $80 a day. Every day, she spent part of her pay and saved the rest. After 20 days of work, she spent $1360. How much did Honey save in 20 days?\",\n",
      "  \"solution\": \"Honey earned 20 x $80 = $<<20*80=1500>>1500 for 20 days of work.\\nTherefore, Honey saved $1500 - $1360 = $<<1500-1360=140>>140 in 20 days.\\n#### 140\",\n",
      "  \"label\": 2\n",
      "}\n",
      "\n",
      "========================= Triplet 2/3 (Problem Index: 788) =========================\n",
      "\n",
      "--- Correct (Label 0) ---\n",
      "{\n",
      "  \"index\": 788,\n",
      "  \"question\": \"Karen has 32 quarters in her piggy bank.  Her older brother Christopher has 64 quarters in his piggy bank.  How much more money does Christopher have?\",\n",
      "  \"solution\": \"Christopher has 64 quarters so he has 64*.25 = $<<64*.25=16.00>>16.00\\nKaren has 32 quarters so she has 32*.25 = $<<32*.25=8.00>>8.00\\nIf Christopher has $16.00 and Karen has $8.00 then Christopher has 16-8 = $<<16-8=8.00>>8.00 more than Karen\\n#### 8\",\n",
      "  \"label\": 0\n",
      "}\n",
      "\n",
      "--- Conceptual (Label 1) ---\n",
      "{\n",
      "  \"index\": 788,\n",
      "  \"question\": \"Karen has 32 quarters in her piggy bank.  Her older brother Christopher has 64 quarters in his piggy bank.  How much more money does Christopher have?\",\n",
      "  \"solution\": \"Christopher has 64 quarters so he has 64*.25 = $<<64*.25=16.00>>16.00\\nKaren has 32 quarters so she has 32*.25 = $<<32*.25=8.00>>8.00\\nIf Christopher has $16.00 and Karen has $8.00 then Christopher has 16+8 = $<<16+8=24.00>>24.00 more than Karen\\n#### 24\",\n",
      "  \"label\": 1\n",
      "}\n",
      "\n",
      "--- Computational (Label 2) ---\n",
      "{\n",
      "  \"index\": 788,\n",
      "  \"question\": \"Karen has 32 quarters in her piggy bank.  Her older brother Christopher has 64 quarters in his piggy bank.  How much more money does Christopher have?\",\n",
      "  \"solution\": \"Christopher has 64 quarters so he has 64*0.25 = $<<64*0.25=16>>16\\nKaren has 32 quarters so she has 32*0.25 = $<<32*0.25=8>>8\\nIf Christopher has $16 and Karen has $8 then Christopher has 16-8 = $<<16-8=10>>10 more than Karen\\n#### 10\",\n",
      "  \"label\": 2\n",
      "}\n",
      "\n",
      "========================= Triplet 3/3 (Problem Index: 1500) =========================\n",
      "\n",
      "--- Correct (Label 0) ---\n",
      "{\n",
      "  \"index\": 1500,\n",
      "  \"question\": \"The Arevalo family went out to dinner. The smoky salmon costs $40, the black burger costs $15, and the chicken katsu costs $25. If the bill includes a 10% service charge and 5% tip, how much change will Mr. Arevalo receive from his $100?\",\n",
      "  \"solution\": \"The total amount spent on food is $40 + $15 + $25 = $<<40+15+25=80>>80.\\nThe service charge is $80 x 10/100 = $<<80*10/100=8>>8.\\nThe tip is $80 x 5/100 = $<<80*5/100=4>>4.\\nSo, Mr. Arevalo had to pay $80 + $8 + $4 = $<<80+8+4=92>>92 in all.\\nTherefore, his change will only be $100 - $92 = $<<100-92=8>>8.\\n#### 8\",\n",
      "  \"label\": 0\n",
      "}\n",
      "\n",
      "--- Conceptual (Label 1) ---\n",
      "{\n",
      "  \"index\": 1500,\n",
      "  \"question\": \"The Arevalo family went out to dinner. The smoky salmon costs $40, the black burger costs $15, and the chicken katsu costs $25. If the bill includes a 10% service charge and 5% tip, how much change will Mr. Arevalo receive from his $100?\",\n",
      "  \"solution\": \"The total amount spent on food is $40 + $15 + $25 = $<<40+15+25=80>>80.\\nThe service charge is $80 x 10/100 = $<<80*10/100=8>>8.\\nThe tip is $(80+8) x 5/100 = $<<88*5/100=4.4>>4.4.\\nSo, Mr. Arevalo had to pay $80 + $8 + $4.4 = $<<80+8+4.4=92.4>>92.4 in all.\\nTherefore, his change will only be $100 - $92.4 = $<<100-92.4=7.6>>7.6.\\n#### 7.6\",\n",
      "  \"label\": 1\n",
      "}\n",
      "\n",
      "--- Computational (Label 2) ---\n",
      "{\n",
      "  \"index\": 1500,\n",
      "  \"question\": \"The Arevalo family went out to dinner. The smoky salmon costs $40, the black burger costs $15, and the chicken katsu costs $25. If the bill includes a 10% service charge and 5% tip, how much change will Mr. Arevalo receive from his $100?\",\n",
      "  \"solution\": \"The total amount spent on food is $40 + $15 + $25 = $<<40+15+25=80>>80.\\nThe service charge is $80 x 10/100 = $<<80*10/100=8>>8.\\nThe tip is $80 x 5/100 = $<<80*5/100=1600/1>>1600/1.\\nSo, Mr. Arevalo had to pay $80 + $8 + $1600/1 = $<<80+8+1600/1=1688>>1688 in all.\\nTherefore, his change will only be $100 - $1688 = $<<100-1688=-1588>>-1588.\\n#### -1588\",\n",
      "  \"label\": 2\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect 3 random triplets from the full, unsplit dataset list\n",
    "display_triplet_samples(final_dataset_list, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a37646f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2426.4"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4/5 * 3033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
