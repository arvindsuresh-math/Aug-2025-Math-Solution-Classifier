{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa10bfe",
   "metadata": {},
   "source": [
    "Cell 1: Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",  # \"discriminative\" or \"generative\"\n",
    "    \"classification_type\": \"binary\",   # \"binary\" or \"ternary\"\n",
    "    \"dataset_strategy\": \"4N\",          # \"4N\" or \"3N\" (generative only)\n",
    "    \"include_explanation\": False,      # True or False (generative only)\n",
    "    \"include_eln\": True,              # True or False (generative only)\n",
    "    \"solution_format\": \"dict\",        # \"dict\" or \"nl\" (generative only)\n",
    "    \"model_name\": \"microsoft/Phi-4-mini-instruct\",  # or \"Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # Prompting configuration\n",
    "    \"system_prompt\": None,  # Will auto-generate if None, or use custom string\n",
    "    \"include_examples\": True,\n",
    "    \"num_examples\": 3,\n",
    "    \"example_strategy\": \"balanced\",  # \"balanced\", \"error_focused\", \"custom\"\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": 1024,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \n",
    "    # Infrastructure\n",
    "    \"use_lora\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Paths and tokens\n",
    "    \"base_dataset_dir\": \"/content/drive/MyDrive/sft_datasets\",\n",
    "    \"output_base_dir\": \"/content/drive/MyDrive/sft_experiments\",\n",
    "    \"hf_token\": \"your_huggingface_token_here\",\n",
    "    \"wandb_project\": \"math_error_classification\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "    \"use_wandb\": True\n",
    "}\n",
    "\n",
    "# Generate experiment ID\n",
    "import datetime\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],  # \"gene\" or \"disc\"\n",
    "    CONFIG[\"classification_type\"][:3],  # \"bin\" or \"ter\"\n",
    "    CONFIG[\"dataset_strategy\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"qwen\" if \"Qwen\" in CONFIG[\"model_name\"] else \"phi4\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766c573",
   "metadata": {},
   "source": [
    "Cell 2: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# Logging and tracking\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Google Drive mounting (for Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - skipping Drive mount\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"Dependencies imported and seeds set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd140f7",
   "metadata": {},
   "source": [
    "Cell 3: System Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_prompt(config):\n",
    "    \"\"\"Auto-generates appropriate system prompt based on config\"\"\"\n",
    "    \n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        return \"You are a mathematics tutor. Classify the given solution.\"\n",
    "    \n",
    "    # Generative prompts\n",
    "    base_prompt = \"You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format.\"\n",
    "    \n",
    "    # Add classification instructions\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        base_prompt += \" Determine if the solution is 'correct' or 'flawed'.\"\n",
    "    else:\n",
    "        base_prompt += \" Classify as 'correct', 'conceptual_error', or 'computational_error'.\"\n",
    "    \n",
    "    # Add field instructions\n",
    "    fields = []\n",
    "    if config[\"include_eln\"]:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            fields.append(\"identify the erroneous line number (e.g., 'L1', 'FA')\")\n",
    "        else:\n",
    "            fields.append(\"quote the full erroneous line text\")\n",
    "    \n",
    "    if config[\"include_explanation\"]:\n",
    "        fields.append(\"provide a brief explanation of any error\")\n",
    "    \n",
    "    if fields:\n",
    "        base_prompt += f\" Also {', and '.join(fields)}.\"\n",
    "    \n",
    "    base_prompt += \" Respond only with valid JSON.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Auto-generate system prompt if not provided\n",
    "if CONFIG[\"system_prompt\"] is None:\n",
    "    CONFIG[\"system_prompt\"] = generate_system_prompt(CONFIG)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(CONFIG[\"system_prompt\"])\n",
    "print()\n",
    "\n",
    "# Allow manual override\n",
    "print(\"To customize the system prompt, run:\")\n",
    "print('CONFIG[\"system_prompt\"] = \"Your custom prompt here\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246c873",
   "metadata": {},
   "source": [
    "Cell 4: Dataset Loading and Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_dataset(config):\n",
    "    \"\"\"Loads the appropriate base dataset\"\"\"\n",
    "    dataset_strategy = config[\"dataset_strategy\"]\n",
    "    base_dir = Path(config[\"base_dataset_dir\"])\n",
    "    \n",
    "    dataset_file = base_dir / f\"base_{dataset_strategy}_dataset.json\"\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        raise FileNotFoundError(f\"Base dataset not found: {dataset_file}\")\n",
    "    \n",
    "    with open(dataset_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded base {dataset_strategy} dataset with {len(data['samples'])} samples\")\n",
    "    return data['samples']\n",
    "\n",
    "def format_solution(sample, config):\n",
    "    \"\"\"Formats solution according to config\"\"\"\n",
    "    if config[\"solution_format\"] == \"dict\":\n",
    "        solution = {}\n",
    "        for i, line in enumerate(sample[\"solution_lines\"][:-1]):\n",
    "            solution[f\"L{i+1}\"] = line\n",
    "        solution[\"FA\"] = sample[\"solution_lines\"][-1]\n",
    "        return json.dumps(solution, indent=2)\n",
    "    else:\n",
    "        return \"\\n\".join(sample[\"solution_lines\"])\n",
    "\n",
    "def format_expected_output(sample, config):\n",
    "    \"\"\"Creates the expected JSON output for a sample\"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    # Verdict\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        output[\"verdict\"] = \"correct\" if sample[\"error_type\"] == \"correct\" else \"flawed\"\n",
    "    else:\n",
    "        output[\"verdict\"] = sample[\"error_type\"]\n",
    "    \n",
    "    # ELN\n",
    "    if config[\"include_eln\"]:\n",
    "        if sample[\"error_type\"] != \"correct\":\n",
    "            if config[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = sample.get(\"erroneous_line_number\", None)\n",
    "            else:\n",
    "                # Get the actual line text\n",
    "                if sample.get(\"erroneous_line_number\"):\n",
    "                    line_num = int(sample[\"erroneous_line_number\"][1:]) - 1\n",
    "                    if line_num < len(sample[\"solution_lines\"]):\n",
    "                        output[\"erroneous_line\"] = sample[\"solution_lines\"][line_num]\n",
    "                    else:\n",
    "                        output[\"erroneous_line\"] = sample[\"solution_lines\"][-1]  # Final answer\n",
    "                else:\n",
    "                    output[\"erroneous_line\"] = None\n",
    "        else:\n",
    "            key = \"erroneous_line_number\" if config[\"solution_format\"] == \"dict\" else \"erroneous_line\"\n",
    "            output[key] = None\n",
    "    \n",
    "    # Explanation\n",
    "    if config[\"include_explanation\"]:\n",
    "        output[\"explanation\"] = sample.get(\"explanation\", None) if sample[\"error_type\"] != \"correct\" else None\n",
    "    \n",
    "    return json.dumps(output)\n",
    "\n",
    "print(\"Dataset formatting functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1f56e",
   "metadata": {},
   "source": [
    "Cell 5: Example Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset, config):\n",
    "        self.samples = base_dataset\n",
    "        self.config = config\n",
    "        self._prepare_examples_by_type()\n",
    "    \n",
    "    def _prepare_examples_by_type(self):\n",
    "        \"\"\"Organizes samples by error type for easy sampling\"\"\"\n",
    "        self.examples_by_type = {\n",
    "            \"correct\": [],\n",
    "            \"conceptual_error\": [],\n",
    "            \"computational_error\": []\n",
    "        }\n",
    "        \n",
    "        for sample in self.samples:\n",
    "            error_type = sample[\"error_type\"]\n",
    "            self.examples_by_type[error_type].append(sample)\n",
    "        \n",
    "        print(f\"Examples by type: {[(k, len(v)) for k, v in self.examples_by_type.items()]}\")\n",
    "    \n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns appropriate few-shot examples\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        \n",
    "        strategy = self.config[\"example_strategy\"]\n",
    "        num_examples = self.config[\"num_examples\"]\n",
    "        \n",
    "        if strategy == \"balanced\":\n",
    "            return self._get_balanced_examples(num_examples)\n",
    "        elif strategy == \"error_focused\":\n",
    "            return self._get_error_focused_examples(num_examples)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def _get_balanced_examples(self, n):\n",
    "        \"\"\"Gets balanced representation across error types\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        if self.config[\"classification_type\"] == \"binary\":\n",
    "            # Get n//2 correct, n//2 flawed\n",
    "            correct_needed = n // 2\n",
    "            flawed_needed = n - correct_needed\n",
    "            \n",
    "            examples.extend(random.sample(self.examples_by_type[\"correct\"], \n",
    "                                        min(correct_needed, len(self.examples_by_type[\"correct\"]))))\n",
    "            \n",
    "            flawed_samples = (self.examples_by_type[\"conceptual_error\"] + \n",
    "                            self.examples_by_type[\"computational_error\"])\n",
    "            examples.extend(random.sample(flawed_samples, \n",
    "                                        min(flawed_needed, len(flawed_samples))))\n",
    "        else:\n",
    "            # Get roughly equal conceptual, computational, correct\n",
    "            per_type = n // 3\n",
    "            remainder = n % 3\n",
    "            \n",
    "            for i, error_type in enumerate([\"correct\", \"conceptual_error\", \"computational_error\"]):\n",
    "                needed = per_type + (1 if i < remainder else 0)\n",
    "                available = self.examples_by_type[error_type]\n",
    "                examples.extend(random.sample(available, min(needed, len(available))))\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def _get_error_focused_examples(self, n):\n",
    "        \"\"\"Gets examples focused on error types\"\"\"\n",
    "        # Prioritize error samples over correct ones\n",
    "        error_samples = (self.examples_by_type[\"conceptual_error\"] + \n",
    "                        self.examples_by_type[\"computational_error\"])\n",
    "        \n",
    "        error_count = min(n * 2 // 3, len(error_samples))\n",
    "        correct_count = n - error_count\n",
    "        \n",
    "        examples = random.sample(error_samples, error_count)\n",
    "        examples.extend(random.sample(self.examples_by_type[\"correct\"], \n",
    "                                    min(correct_count, len(self.examples_by_type[\"correct\"]))))\n",
    "        \n",
    "        return examples\n",
    "\n",
    "print(\"Example manager class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455adbd",
   "metadata": {},
   "source": [
    "Cell 6: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(config):\n",
    "    \"\"\"Loads and formats complete dataset for training/evaluation\"\"\"\n",
    "    \n",
    "    # Load base dataset\n",
    "    base_samples = load_base_dataset(config)\n",
    "    \n",
    "    # Initialize example manager\n",
    "    example_manager = ExampleManager(base_samples, config)\n",
    "    examples = example_manager.get_examples()\n",
    "    \n",
    "    print(f\"Using {len(examples)} few-shot examples\")\n",
    "    \n",
    "    # Format all samples\n",
    "    formatted_data = []\n",
    "    \n",
    "    for sample in tqdm(base_samples, desc=\"Formatting samples\"):\n",
    "        # Create input messages\n",
    "        messages = []\n",
    "        \n",
    "        # System message\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": config[\"system_prompt\"]\n",
    "        })\n",
    "        \n",
    "        # Few-shot examples\n",
    "        for example in examples:\n",
    "            user_content = f\"Problem: {example['question']}\\nSolution: {format_solution(example, config)}\"\n",
    "            assistant_content = format_expected_output(example, config)\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "        \n",
    "        # Actual sample\n",
    "        user_content = f\"Problem: {sample['question']}\\nSolution: {format_solution(sample, config)}\"\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        \n",
    "        # Expected output\n",
    "        expected_output = format_expected_output(sample, config)\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"id\": sample.get(\"id\", f\"sample_{len(formatted_data)}\"),\n",
    "            \"messages\": messages,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"metadata\": {\n",
    "                \"error_type\": sample[\"error_type\"],\n",
    "                \"tier\": sample.get(\"tier\", \"unknown\"),\n",
    "                \"source\": sample.get(\"source\", \"unknown\")\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Split into train/eval\n",
    "    split_point = int(0.8 * len(formatted_data))\n",
    "    train_data = formatted_data[:split_point]\n",
    "    eval_data = formatted_data[split_point:]\n",
    "    \n",
    "    print(f\"Dataset prepared: {len(train_data)} training, {len(eval_data)} evaluation samples\")\n",
    "    \n",
    "    return train_data, eval_data, examples\n",
    "\n",
    "# Prepare the dataset\n",
    "train_data, eval_data, few_shot_examples = prepare_dataset(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3af4e",
   "metadata": {},
   "source": [
    "Cell 7: Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"Loads model and tokenizer with appropriate configuration\"\"\"\n",
    "    \n",
    "    model_name = config[\"model_name\"]\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Configure quantization for LoRA\n",
    "    if config[\"use_lora\"]:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        # Load model with quantization\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            # For discriminative, we need a classification model\n",
    "            num_labels = 2 if config[\"classification_type\"] == \"binary\" else 3\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=num_labels,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        else:\n",
    "            # For generative, use causal LM\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        # Prepare model for LoRA\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Configure LoRA\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            task_type = TaskType.SEQ_CLS\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        else:\n",
    "            task_type = TaskType.CAUSAL_LM\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            r=config[\"lora_rank\"],\n",
    "            lora_alpha=config[\"lora_alpha\"],\n",
    "            lora_dropout=config[\"lora_dropout\"],\n",
    "            target_modules=target_modules,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "    else:\n",
    "        # Load full model\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            num_labels = 2 if config[\"classification_type\"] == \"binary\" else 3\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=num_labels,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d9eb1",
   "metadata": {},
   "source": [
    "Cell 8: Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a089c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output_directory(config):\n",
    "    \"\"\"Creates organized output directory structure\"\"\"\n",
    "    \n",
    "    output_dir = Path(config[\"output_base_dir\"]) / config[\"experiment_id\"]\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"baseline\", \"training\", \"final\", \"checkpoints\"]\n",
    "    for subdir in subdirs:\n",
    "        (output_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = output_dir / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = setup_output_directory(CONFIG)\n",
    "CONFIG[\"output_dir\"] = str(output_dir)\n",
    "\n",
    "# Initialize wandb if enabled\n",
    "if CONFIG[\"use_wandb\"]:\n",
    "    wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=CONFIG[\"experiment_id\"],\n",
    "        config=CONFIG\n",
    "    )\n",
    "    print(\"Wandb initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f1ffe",
   "metadata": {},
   "source": [
    "Cell 9: Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, data, config, stage=\"baseline\"):\n",
    "    \"\"\"Runs inference on dataset with comprehensive logging\"\"\"\n",
    "    \n",
    "    print(f\"Running {stage} inference on {len(data)} samples...\")\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    metadata = {\n",
    "        \"response_times\": [],\n",
    "        \"input_token_counts\": [],\n",
    "        \"output_token_counts\": [],\n",
    "        \"memory_usage\": []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(tqdm(data, desc=f\"{stage} inference\")):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prepare input\n",
    "            if config[\"experiment_type\"] == \"discriminative\":\n",
    "                # For discriminative, we need to format differently\n",
    "                # This is a simplified version - you'll need to adapt based on your needs\n",
    "                input_text = sample[\"messages\"][-1][\"content\"]  # Last user message\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=config[\"max_length\"])\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get classification logits\n",
    "                outputs = model(**inputs)\n",
    "                predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                \n",
    "                response = str(predicted_class)\n",
    "                output_tokens = 1  # Single token output\n",
    "                \n",
    "            else:\n",
    "                # For generative, format as chat\n",
    "                input_text = tokenizer.apply_chat_template(\n",
    "                    sample[\"messages\"], \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=config[\"max_length\"])\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Generate response\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=200,\n",
    "                        do_sample=False,\n",
    "                        temperature=0.1,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                # Decode response\n",
    "                response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "                output_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Log metrics\n",
    "            metadata[\"response_times\"].append(end_time - start_time)\n",
    "            metadata[\"input_token_counts\"].append(inputs[\"input_ids\"].shape[1])\n",
    "            metadata[\"output_token_counts\"].append(output_tokens)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                metadata[\"memory_usage\"].append(torch.cuda.memory_allocated() / 1024**3)  # GB\n",
    "            \n",
    "            results.append({\n",
    "                \"sample_id\": sample[\"id\"],\n",
    "                \"input\": sample[\"messages\"][-1][\"content\"],\n",
    "                \"expected_output\": sample[\"expected_output\"],\n",
    "                \"model_output\": response.strip(),\n",
    "                \"metadata\": sample[\"metadata\"],\n",
    "                \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                \"response_time\": end_time - start_time\n",
    "            })\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if i % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"{stage} inference completed!\")\n",
    "    print(f\"Average response time: {np.mean(metadata['response_times']):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean(metadata['input_token_counts']):.1f}\")\n",
    "    print(f\"Average output tokens: {np.mean(metadata['output_token_counts']):.1f}\")\n",
    "    \n",
    "    return results, metadata\n",
    "\n",
    "def save_results(results, metadata, stage, config):\n",
    "    \"\"\"Saves results and metadata to appropriate locations\"\"\"\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(config[\"output_dir\"]) / stage\n",
    "    \n",
    "    # Save results\n",
    "    results_path = output_dir / f\"results_{timestamp}.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = output_dir / f\"metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # Log to wandb if enabled\n",
    "    if config[\"use_wandb\"]:\n",
    "        wandb.log({\n",
    "            f\"{stage}_avg_response_time\": np.mean(metadata[\"response_times\"]),\n",
    "            f\"{stage}_avg_input_tokens\": np.mean(metadata[\"input_token_counts\"]),\n",
    "            f\"{stage}_avg_output_tokens\": np.mean(metadata[\"output_token_counts\"]),\n",
    "            f\"{stage}_total_samples\": len(results)\n",
    "        })\n",
    "    \n",
    "    return results_path, metadata_path\n",
    "\n",
    "print(\"Inference functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b884d",
   "metadata": {},
   "source": [
    "Cell 10: Baseline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace74c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline inference\n",
    "print(\"Starting baseline inference...\")\n",
    "baseline_results, baseline_metadata = run_inference(model, tokenizer, eval_data, CONFIG, \"baseline\")\n",
    "\n",
    "# Save baseline results\n",
    "baseline_results_path, baseline_metadata_path = save_results(\n",
    "    baseline_results, baseline_metadata, \"baseline\", CONFIG\n",
    ")\n",
    "\n",
    "print(f\"Baseline inference completed and saved!\")\n",
    "print(f\"Baseline results: {len(baseline_results)} samples processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
