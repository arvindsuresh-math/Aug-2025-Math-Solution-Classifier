{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "44518f5b90cb4c9fb8bb7221e16ba9ea",
      "fc6bb53ce4884b1a931e22229aa3da07",
      "4bcb9fbd5c7f4b38976b2e5e96bcb30b",
      "01c47777c57b48eaa4e7912bdf86b2f9",
      "50e7769e59204a44ba7bc56bb92be843",
      "9e721664efa649368424e41c3d32f5df",
      "4b8c08b743f44fdba08e4dc42aac5922",
      "5a10e22b444a4e99b0a8b12c920168e6",
      "046f3f9ce3894b0785a0316650a3d971",
      "f2094ef9defb4ebb9c44391d346945d6",
      "fb5f496f1de64ff592ce1af36eaa9ed7",
      "f2bce4310d6d496d9ae13bcad425fff3",
      "e1aaf7c13efa4c34b563e6aac893c2e9",
      "39a03fa910cf4a389cd5b6de3a866489",
      "a718d01710b349c390e563ec457cb8b1",
      "e7e367bb8a1444bf87f7befb6e1ee639",
      "6bd4c7ff4c5347db93a23a48bfe6c63d"
     ]
    },
    "id": "svFisQlfe6g1",
    "outputId": "96716416-d393-4b44-da80-c8e11dced7db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44518f5b90cb4c9fb8bb7221e16ba9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vllm==0.7.3 in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (5.9.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (1.26.4)\n",
      "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.60.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (4.53.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.21.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (5.29.5)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.116.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (3.12.14)\n",
      "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (1.97.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.22.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.10.12)\n",
      "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.1.11)\n",
      "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.1.11)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (4.14.1)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (3.18.0)\n",
      "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (26.2.1)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.19.0)\n",
      "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (8.7.0)\n",
      "Requirement already satisfied: mistral_common>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.7.3) (1.8.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (6.0.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.9.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.9.1)\n",
      "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (3.1.1)\n",
      "Requirement already satisfied: ray==2.40.0 in /usr/local/lib/python3.11/dist-packages (from ray[adag]==2.40.0->vllm==0.7.3) (2.40.0)\n",
      "Collecting torch==2.5.1 (from vllm==0.7.3)\n",
      "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (2.5.1)\n",
      "Collecting torchvision==0.20.1 (from vllm==0.7.3)\n",
      "  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.11/dist-packages (from vllm==0.7.3) (0.0.28.post3)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm==0.7.3) (0.8.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm==0.7.3) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.60.0->vllm==0.7.3) (0.43.0)\n",
      "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (1.6.0)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (5.6.3)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (4.25.0)\n",
      "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (20250706)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.7.3) (0.1.26)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3) (8.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3) (25.0)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3) (1.4.0)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.40.0->ray[adag]==2.40.0->vllm==0.7.3) (1.7.0)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[adag]==2.40.0->vllm==0.7.3) (13.3.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (12.4.127)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->vllm==0.7.3)\n",
      "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.7.3) (1.13.1)\n",
      "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.11->vllm==0.7.3) (3.0.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.11->vllm==0.7.3) (8.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.7.3) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.47.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.0.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.35.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.5.0->mistral_common[opencv]>=1.5.0->vllm==0.7.3) (2.10.5)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.7.3) (4.11.0.86)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm==0.7.3) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm==0.7.3) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm==0.7.3) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm==0.7.3) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.7.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.7.3) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.7.3) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.7.3) (2025.7.14)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm==0.7.3) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm==0.7.3) (0.34.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.48.2->vllm==0.7.3) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.7.3) (1.20.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm==0.7.3) (3.23.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.16.0)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.14.9)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.1.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.7.3) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm==0.7.3) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.7.3) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.7.3) (0.26.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[adag]==2.40.0->vllm==0.7.3) (0.8.3)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar==0.1.11->vllm==0.7.3) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar==0.1.11->vllm==0.7.3) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar==0.1.11->vllm==0.7.3) (2.19.2)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.6.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (2.33.2)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.7.3) (0.1.2)\n",
      "Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Installing collected packages: triton, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Successfully installed torch-2.5.1 torchvision-0.20.1 triton-3.1.0\n",
      "Requirement already satisfied: transformers==4.53.2 in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2025.7.14)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.53.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.34.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
      "Requirement already satisfied: trl==0.20.0 in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.20.0) (1.9.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.20.0) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.53.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.20.0) (4.53.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (0.34.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.20.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl==0.20.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->trl==0.20.0) (0.21.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.20.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.20.0) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.20.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.20.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.20.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.20.0) (2025.7.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.20.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.20.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.20.0) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.20.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.20.0) (3.0.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Looking in indexes: https://download.pytorch.org/whl/test/cu124\n",
      "Collecting torch==2.6.0\n",
      "  Using cached https://download.pytorch.org/whl/test/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Using cached https://download.pytorch.org/whl/test/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/test/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/test/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/test/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/test/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "Installing collected packages: triton, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1 requires torch==2.5.1, but you have torch 2.6.0+cu124 which is incompatible.\n",
      "vllm 0.7.3 requires torch==2.5.1, but you have torch 2.6.0+cu124 which is incompatible.\n",
      "vllm 0.7.3 requires torchvision==0.20.1, but you have torchvision 0.21.0+cu124 which is incompatible.\n",
      "xformers 0.0.28.post3 requires torch==2.5.1, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: flash-attn==2.7.4.post1 in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1) (2.6.0+cu124)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.7.4.post1) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.7.4.post1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.4.post1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 1. SETUP & CONFIGURATION\n",
    "########################################\n",
    "\n",
    "# --- Hugging Face Login ---\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Please complete the prerequisite steps.\")\n",
    "notebook_login(new_session=hf_token)\n",
    "\n",
    "!pip install vllm==0.7.3\n",
    "!pip install -U transformers==4.53.2\n",
    "!pip install -U peft\n",
    "!pip install -U trl==0.20.0\n",
    "!pip install -U accelerate\n",
    "!pip install -U datasets\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -q sympy\n",
    "!pip install torch==2.6.0 torchvision --index-url https://download.pytorch.org/whl/test/cu124\n",
    "\n",
    "# Install Flash Attention 2\n",
    "!pip install flash-attn==2.7.4.post1 \\\n",
    "  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "  --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9xaG2KLm2Fd",
    "outputId": "227728b7-f3a9-4f82-cb0b-95b73771b2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: gene_ter_4N_exp_eln_nl_phi4_20250806_001250\n",
      "Output directory created: /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250\n",
      "âœ… Setup complete. Dependencies installed and seeds set.\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment Configuration ---\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",\n",
    "    \"classification_type\": \"ternary\",\n",
    "    \"dataset_strategy\": \"4N\",\n",
    "    \"include_explanation\": True,\n",
    "    \"include_eln\": True,\n",
    "    \"solution_format\": \"nl\",\n",
    "    \"model_name\": \"microsoft/phi-4-mini-instruct\",\n",
    "\n",
    "    # Prompting configuration\n",
    "    \"include_examples\": True,\n",
    "    \"num_examples\": 1,\n",
    "    \"example_strategy\": \"balanced\",\n",
    "\n",
    "    # Training parameters\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"inference_batch_size\": 8, # Adjusted for few-shot prompt length\n",
    "    \"max_length\": 2048, # Increased to accommodate few-shot examples\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "\n",
    "    # LoRa params\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "\n",
    "    # Paths and tokens\n",
    "    \"base_dataset_dir\": \"/content\",\n",
    "    \"output_base_dir\": \"/content\",\n",
    "\n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "}\n",
    "\n",
    "# --- Generate Unique Experiment ID ---\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],\n",
    "    CONFIG[\"classification_type\"][:3],\n",
    "    CONFIG[\"dataset_strategy\"],\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"],\n",
    "    \"phi4\" if \"phi\" in CONFIG[\"model_name\"].lower() else \"qwen\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "# --- Setup Output Directories ---\n",
    "def setup_output_directory():\n",
    "    output_dir = Path(CONFIG[\"output_base_dir\"]) / CONFIG[\"experiment_id\"]\n",
    "    # Create the directory structure from the screenshot\n",
    "    (output_dir / \"baseline\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"training\").mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / \"final\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_path = output_dir / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2, default=str)\n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "CONFIG[\"output_dir\"] = setup_output_directory()\n",
    "\n",
    "# --- Set Random Seeds for Reproducibility ---\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"âœ… Setup complete. Dependencies installed and seeds set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emLgFW1cGG7C"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \\\n",
    "\"\"\"[ROLE]\n",
    "You are a mathematics tutor.\n",
    "\n",
    "[TASK]\n",
    "You are given a math word problem and a solution written by a student.\n",
    "Analyze the solution carefully, line-by-line, and classify it into one of the following categories:\n",
    "- Correct: All logic is correct, and all calculations are correct\n",
    "- Conceptual Error: There is an error in reasoning or logic somewhere in the solution\n",
    "- Computational Error: All logic and reasoning is correct, but the result of some calculation is incorrect\n",
    "\n",
    "[RESPONSE FORMAT]\n",
    "Your response must contain exactly two parts, placed one after another with no additional text:\n",
    "\n",
    "1. A thinking trace where you examine the solution line-by-line, ending with either \"I found no errors!\" or \"Aha! I see the error.\"\n",
    "This part must begin with \"<think>\" and end with \"</think>\".\n",
    "\n",
    "2. A valid JSON object that follows this exact schema:\n",
    "```json\n",
    "{\n",
    "  \"verdict\": \"must be one of 'correct', 'conceptual_error', or 'computational_error'\",\n",
    "  \"erroneous_line\": \"the exact, verbatim text of the VERY FIRST incorrect line, or null if the verdict is 'correct'\",\n",
    "  \"explanation\": \"a brief, one-sentence explanation of the error\"\n",
    "}\n",
    "```\n",
    "\n",
    "Do NOT add any text before, after, or between these two parts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJbapdELfTiu"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2. CORE FUNCTION DEFINITIONS\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "def load_base_dataset():\n",
    "    \"\"\"Loads the base dataset from the specified CSV file.\"\"\"\n",
    "    base_dir = Path(CONFIG[\"base_dataset_dir\"])\n",
    "    dataset_file = base_dir / f\"error_detection_dataset_with_traces.csv\"\n",
    "    # dataset_file = \"../data/aug-5-dataset/error_detection_dataset_with_traces.csv\"\n",
    "    data = pd.read_csv(dataset_file)\n",
    "    print(f\"Loaded base {CONFIG['dataset_strategy']} dataset with {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the quantized model.\"\"\"\n",
    "    model_name = CONFIG[\"model_name\"]\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=CONFIG[\"lora_rank\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        target_modules=\"all-linear\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.generation_config = GenerationConfig(\n",
    "        max_new_tokens=300,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_tokenizer():\n",
    "    \"\"\"Loads the tokenizer.\"\"\"\n",
    "    model_name = CONFIG[\"model_name\"]\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nb7FA0HmfuOg",
    "outputId": "7e8b2207-4818-4b41-bbaa-8578b6d05c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data preparation functions (with ExampleManager) loaded.\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 3. DATA PREPARATION (WITH FEW-SHOT EXAMPLES)\n",
    "##################################################\n",
    "\n",
    "from datasets import Dataset\n",
    "import math\n",
    "\n",
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset):\n",
    "        self.samples = base_dataset.to_dict('records')\n",
    "        self.config = CONFIG\n",
    "        self._prepare_examples_by_problem()\n",
    "\n",
    "    def _prepare_examples_by_problem(self):\n",
    "        self.problems_by_type = {\"correct\": {}, \"conceptual_error\": {}, \"computational_error\": {}}\n",
    "        for sample in self.samples:\n",
    "            problem_index = sample[\"index\"]\n",
    "            error_type = sample[\"error_type\"]\n",
    "            if problem_index not in self.problems_by_type[error_type]:\n",
    "                self.problems_by_type[error_type][problem_index] = []\n",
    "            self.problems_by_type[error_type][problem_index].append(sample)\n",
    "\n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns examples based on dataset strategy\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        num_examples = self.config[\"num_examples\"]\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        examples = []\n",
    "\n",
    "        import random\n",
    "        if dataset_strategy == \"3N\":\n",
    "            # Choose num_examples distinct problem indices that have all 3 versions\n",
    "            available_problems = set(self.problems_by_type[\"correct\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"conceptual_error\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            # Sample problem indices\n",
    "            selected_problems = random.sample(list(available_problems), num_examples)\n",
    "            for problem_index in selected_problems:\n",
    "                # Add all 3 versions: correct, conceptual_error, computational_error\n",
    "                examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "\n",
    "            return examples\n",
    "\n",
    "        elif dataset_strategy == \"4N\":\n",
    "            import math\n",
    "            # Get problems that have conceptual errors (with correct versions)\n",
    "            conceptual_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) &\n",
    "                set(self.problems_by_type[\"conceptual_error\"].keys())\n",
    "            )\n",
    "            # Get problems that have computational errors (with correct versions)\n",
    "            computational_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) &\n",
    "                set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            )\n",
    "            # Calculate splits: floor(n/2) conceptual, ceil(n/2) computational\n",
    "            n_conceptual = num_examples // 2  # This is floor(n/2)\n",
    "            n_computational = math.ceil(num_examples / 2)\n",
    "\n",
    "            # Sample conceptual problems\n",
    "            if conceptual_problems and n_conceptual > 0:\n",
    "                selected_conceptual = random.sample(conceptual_problems,n_conceptual)\n",
    "                for problem_index in selected_conceptual:\n",
    "                    # Add correct + conceptual_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "\n",
    "            # Sample computational problems\n",
    "            if computational_problems and n_computational > 0:\n",
    "                selected_computational = random.sample(computational_problems,n_computational)\n",
    "                for problem_index in selected_computational:\n",
    "                    # Add correct + computational_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "\n",
    "            return examples\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Unknown dataset strategy '{dataset_strategy}'\")\n",
    "            return []\n",
    "\n",
    "def format_user_message(sample):\n",
    "    \"\"\"Formats a sample into a user message string.\"\"\"\n",
    "    question = sample.get('question', '') or ''\n",
    "    solution = sample.get('correct_answer' if sample['error_type'] == 'correct' else 'wrong_answer', '').strip()\n",
    "    return f\"### Question:\\n{question}\\n\\n### Answer:\\n{solution}\"\n",
    "\n",
    "def format_expected_output(sample):\n",
    "    \"\"\"Creates the expected output string for a sample.\"\"\"\n",
    "    json_output_string = sample.get('json_output_string', '').strip()\n",
    "\n",
    "    # Concatenate thinking trace with JSON output\n",
    "    return thinking_trace + '\\n' + \"```json\\n\" + json_output_string + \"\\n```\"\n",
    "\n",
    "def create_sample_messages(sample, examples):\n",
    "    \"\"\"Creates the full message list for a sample, including few-shot examples.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for example in examples:\n",
    "        messages.append({\"role\": \"user\", \"content\": format_user_message(example)})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": format_expected_output(example)})\n",
    "    messages.append({\"role\": \"user\", \"content\": format_user_message(sample)})\n",
    "    return messages\n",
    "\n",
    "def prepare_data_for_run(tokenizer, base_df):\n",
    "    \"\"\"Prepares data, generates examples, and returns datasets.\"\"\"\n",
    "    test_df = base_df.sample(frac=0.2, random_state=42)\n",
    "    train_df = base_df.drop(test_df.index)\n",
    "    print(f\"Data split: {len(train_df)} training samples, {len(test_df)} test samples.\")\n",
    "\n",
    "    example_manager = ExampleManager(base_df)\n",
    "    examples = example_manager.get_examples()\n",
    "    print(f\"Generated {len(examples)} few-shot examples for this run.\")\n",
    "\n",
    "    def create_training_text(sample):\n",
    "        # Re-use the message creation logic, now including examples\n",
    "        messages = create_sample_messages(sample, examples)\n",
    "        # Add the final assistant response for the training label\n",
    "        messages.append({\"role\": \"assistant\", \"content\": format_expected_output(sample)})\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    train_dataset_formatted = train_dataset.map(create_training_text, load_from_cache_file=False)\n",
    "    train_dataset_tokenized = train_dataset_formatted.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_length\"],\n",
    "            padding=False),\n",
    "        remove_columns=['text'] + list(train_df.columns)\n",
    "    )\n",
    "    return train_dataset_tokenized, test_df, examples\n",
    "\n",
    "print(\"âœ… Data preparation functions (with ExampleManager) loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZzXTpxSfykp",
    "outputId": "539f0c2a-0e13-45c3-ebea-38fecd0d0fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference and evaluation functions loaded.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 4. INFERENCE & EVALUATION FUNCTIONS\n",
    "########################################\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import math\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def run_vllm_inference_on_dataframe(model_name_or_path, tokenizer, df, examples):\n",
    "    \"\"\"Runs highly optimized inference using the vLLM engine.\"\"\"\n",
    "    print(f\"\\n--- Initializing vLLM engine for model: {model_name_or_path} ---\")\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=2048,\n",
    "        gpu_memory_utilization=0.90\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0, max_tokens=300)\n",
    "\n",
    "    print(\"--- Preparing prompts for vLLM ---\")\n",
    "    all_prompts = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Formatting Prompts\"):\n",
    "        messages = create_sample_messages(row.to_dict(), examples)\n",
    "        prompt_string = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        all_prompts.append(prompt_string)\n",
    "\n",
    "    print(f\"\\n--- Running vLLM inference on {len(all_prompts)} prompts ---\")\n",
    "    vllm_outputs = llm.generate(all_prompts, sampling_params)\n",
    "\n",
    "    predictions = [output.outputs[0].text for output in vllm_outputs]\n",
    "    print(\"--- vLLM Inference complete ---\")\n",
    "    return predictions\n",
    "\n",
    "def extract_thinking_from_response(response):\n",
    "    \"\"\"Extracts the thinking trace from a model's text response.\"\"\"\n",
    "    match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"Extracts a JSON object from a model's text response.\"\"\"\n",
    "    match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    return json.loads(match.group(0)) if match else {}\n",
    "\n",
    "def create_detailed_results(test_df, predictions):\n",
    "    \"\"\"Merges model predictions with original test data.\"\"\"\n",
    "    results = []\n",
    "    for i, pred_text in enumerate(predictions):\n",
    "        original_sample = test_df.iloc[i].to_dict()\n",
    "        parsed_json = extract_json_from_response(pred_text)\n",
    "        predicted_thinking = extract_thinking_from_response(pred_text)\n",
    "\n",
    "        # Get expected values\n",
    "        expected_thinking = original_sample.get('thinking_trace', '').replace('<think>', '').replace('</think>', '').strip()\n",
    "        try:\n",
    "            expected_json = json.loads(original_sample.get('json_output_string', '{}'))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            expected_json = {}\n",
    "\n",
    "        result_row = {\n",
    "            'problem_index': original_sample.get('index'),\n",
    "            'expected_verdict': original_sample.get('error_type'),\n",
    "            'predicted_verdict': parsed_json.get('verdict'),\n",
    "            'verdict_correct': original_sample.get('error_type') == parsed_json.get('verdict'),\n",
    "            'expected_erroneous_line': expected_json.get('erroneous_line'),\n",
    "            'predicted_erroneous_line': parsed_json.get('erroneous_line'),\n",
    "            'expected_thinking': expected_thinking,\n",
    "            'predicted_thinking': predicted_thinking,\n",
    "            'full_prediction_text': pred_text.strip(),\n",
    "        }\n",
    "        results.append(result_row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalizes text by removing all whitespace and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'\\s+', '', str(text).lower().strip()) if text else \"\"\n",
    "\n",
    "def compute_metrics_from_results(results_df):\n",
    "    \"\"\"Calculates verdict and erroneous line accuracy from the results DataFrame.\"\"\"\n",
    "    y_true_verdict = results_df['expected_verdict']\n",
    "    y_pred_verdict = results_df['predicted_verdict'].fillna('parse_failure')\n",
    "    accuracy = accuracy_score(y_true_verdict, y_pred_verdict)\n",
    "    labels = list(y_true_verdict.unique()) + ['parse_failure']\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_verdict,\n",
    "        y_pred_verdict,\n",
    "        average='macro',\n",
    "        zero_division=0,\n",
    "        labels=labels)\n",
    "\n",
    "    error_rows = results_df[results_df['expected_verdict'].isin(['conceptual_error', 'computational_error'])].copy()\n",
    "    eln_accuracy = accuracy_score(\n",
    "        error_rows['expected_erroneous_line'].apply(normalize_text),\n",
    "        error_rows['predicted_erroneous_line'].apply(normalize_text)\n",
    "        ) if not error_rows.empty else 1.0\n",
    "\n",
    "    # Count different types of parse failures\n",
    "    json_parse_failures = int(results_df['predicted_verdict'].isnull().sum())\n",
    "\n",
    "    # Count thinking parse failures (empty thinking traces)\n",
    "    thinking_parse_failures = 0\n",
    "    if 'predicted_thinking' in results_df.columns:\n",
    "        thinking_parse_failures = int(results_df['predicted_thinking'].isnull().sum()) + \\\n",
    "                                 int((results_df['predicted_thinking'] == '').sum())\n",
    "\n",
    "    return {\n",
    "        \"overall_accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"eln_accuracy_on_errors\": eln_accuracy,\n",
    "        \"total_samples\": len(results_df),\n",
    "        \"error_samples_for_eln\": len(error_rows),\n",
    "        \"json_parse_failures\": json_parse_failures,\n",
    "        \"thinking_parse_failures\": thinking_parse_failures\n",
    "    }\n",
    "\n",
    "print(\"âœ… Inference and evaluation functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRV0a2QsgBTD",
    "outputId": "3beb7bec-bb31-4a5e-e235-2ca136e43f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuning function loaded.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 5. FINE-TUNING LOOP\n",
    "########################################\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "def run_fine_tuning(model, tokenizer, train_dataset):\n",
    "    \"\"\"\n",
    "    Runs a focused fine-tuning process without intermediate evaluation.\n",
    "    \"\"\"\n",
    "    output_dir = CONFIG[\"output_dir\"]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir / \"training_run\"),\n",
    "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\", # Save adapter at the end of each epoch\n",
    "        save_total_limit=1, # Only keep the best/last checkpoint\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Starting fine-tuning for {CONFIG['num_epochs']} epochs ---\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the final adapter to a dedicated directory\n",
    "    final_adapter_dir = output_dir / \"final_adapter\"\n",
    "    trainer.save_model(str(final_adapter_dir))\n",
    "    print(f\"\\nâœ…âœ…âœ… Fine-tuning finished! Final adapter saved to {final_adapter_dir} âœ…âœ…âœ…\")\n",
    "\n",
    "print(\"âœ… Fine-tuning function loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# NEW: MATH EVALUATION TOOL\n",
    "##########\n",
    "\n",
    "import sympy\n",
    "import re\n",
    "\n",
    "def evaluate_equation(equation_str: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Safely evaluates a mathematical equation string.\n",
    "\n",
    "    Args:\n",
    "        equation_str: A string like \"5 * 2 = 12\" or \"10 + 5 = 15\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - bool: True if the equation is mathematically correct, False otherwise.\n",
    "        - str: The actual computed result of the left-hand side.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Clean and split the equation\n",
    "        # Handles potential floating point issues and extra spaces\n",
    "        if '=' not in equation_str:\n",
    "            return False, \"Malformed equation\"\n",
    "        \n",
    "        lhs_str, rhs_str = equation_str.split('=', 1)\n",
    "        \n",
    "        # Remove any text that is not part of the math expression\n",
    "        lhs_str = re.sub(r'[^\\d\\s\\.\\+\\-\\*\\/(\\)]', '', lhs_str).strip()\n",
    "        rhs_str = re.sub(r'[^\\d\\s\\.\\+\\-\\*\\/(\\)]', '', rhs_str).strip()\n",
    "\n",
    "        # 2. Use sympy to safely parse the expressions\n",
    "        lhs_expr = sympy.parse_expr(lhs_str, evaluate=True)\n",
    "        rhs_expr = sympy.parse_expr(rhs_str, evaluate=True)\n",
    "\n",
    "        # 3. Compare the results\n",
    "        # Use a small tolerance for floating point comparisons\n",
    "        is_correct = abs(lhs_expr - rhs_expr) < 1e-9\n",
    "        \n",
    "        return is_correct, str(lhs_expr)\n",
    "    except (sympy.SympifyError, IndexError, TypeError, SyntaxError):\n",
    "        # If parsing fails, it's not a valid or evaluatable equation\n",
    "        return False, \"Invalid expression\"\n",
    "\n",
    "print(\"âœ… Math evaluation tool loaded.\")\n",
    "# Test cases\n",
    "print(f\"'10 + 5 = 15' -> {evaluate_equation('10 + 5 = 15')}\")\n",
    "print(f\"'5 * 2 = 12'   -> {evaluate_equation('5 * 2 = 12')}\")\n",
    "print(f\"'10/2 = 5.0' -> {evaluate_equation('10/2 = 5.0')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "2b502370f284428bbf6e3b768aa6abed",
      "5ade1936cec742b4820251605e0bf921",
      "b392907e0ab24948b35d61f56db9147f",
      "e9124e9cb5bf4f0c9773f98deec27642",
      "20c729b12689453586ac8fffd65139aa",
      "640144e2c3664faeb700dd2e33728cee",
      "8380557d10b44372ac9bfef345e1b53d",
      "3414656528764ee48254d96b041fd374",
      "fc7db57e7df745848875ea4d98ab2a63",
      "0253880c85034329894feb5e5f6386cb",
      "86bf45b291b34f12ab1689a0aa51a3be",
      "8f539cd24ac64ea3afeef97f80fbc765",
      "9e165127269f4a4db890d70703e6e66d",
      "439f660a85304a89a83c7be867be05ee",
      "f2b1a9ed13694e03bf7fd886ad6225d3",
      "d6bb839ce52341d090a63892e8b00698",
      "fac7f364701e4b88a556d6e88d6001f3",
      "9de8bdc1527d42a6bce57ec58b3def61",
      "ba65d45010984a30b80c1d5185d14b27",
      "b087538776314773912eacebff185db9",
      "64a10d793961488cb1ff4d70aa43a20c",
      "bf256c07d14047388aa862f5e65564f5"
     ]
    },
    "id": "fg4Zt5LegFXA",
    "outputId": "72374df8-c1e7-4edd-d812-a92f5e09f3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: microsoft/phi-4-mini-instruct\n",
      "Loaded base 4N dataset with 6067 samples\n",
      "Data split: 4854 training samples, 1213 test samples.\n",
      "Generated 2 few-shot examples for this run.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b502370f284428bbf6e3b768aa6abed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f539cd24ac64ea3afeef97f80fbc765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ########################################\n",
    "# # PIPELINE STEP 1: INITIALIZATION\n",
    "# ########################################\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "base_df = load_base_dataset()\n",
    "train_dataset, test_df, examples = prepare_data_for_run(tokenizer, base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cjdo6bUJawyA",
    "outputId": "008fa20a-8d8d-481e-c72f-324e9ef1c12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Displaying a complete sample prompt for verification ---\n",
      "\n",
      "--- Using Sample from Problem Index: 7385 ---\n",
      "--- Expected Verdict: computational_error ---\n",
      "\n",
      "--- FULL PROMPT AS SEEN BY THE MODEL ---\n",
      "<|system|>[ROLE]\n",
      "You are a mathematics tutor.\n",
      "\n",
      "[TASK]\n",
      "You are given a math word problem and a solution written by a student.\n",
      "Analyze the solution carefully, line-by-line, and classify it into one of the following categories:\n",
      "- Correct: All logic is correct, and all calculations are correct\n",
      "- Conceptual Error: There is an error in reasoning or logic somewhere in the solution\n",
      "- Computational Error: All logic and reasoning is correct, but the result of some calculation is incorrect\n",
      "\n",
      "[RESPONSE FORMAT]\n",
      "Your response must contain exactly two parts, placed one after another with no additional text:\n",
      "\n",
      "1. A thinking trace where you examine the solution line-by-line, ending with either \"I found no errors!\" or \"Aha! I see the error.\"\n",
      "This part must begin with \"<think>\" and end with \"</think>\".\n",
      "\n",
      "2. A valid JSON object that follows this exact schema:\n",
      "```json\n",
      "{\n",
      "  \"verdict\": \"must be one of 'correct', 'conceptual_error', or 'computational_error'\",\n",
      "  \"erroneous_line\": \"the exact, verbatim text of the VERY FIRST incorrect line, or null if the verdict is 'correct'\",\n",
      "  \"explanation\": \"a brief, one-sentence explanation of the error\"\n",
      "}\n",
      "```\n",
      "\n",
      "Do NOT add any text before, after, or between these two parts.\n",
      "<|end|><|user|>### Question:\n",
      "At the zoo, there are 5 different types of animals. Seeing each animal type takes around 6 minutes. How much time would it take to see each animal type if the zoo decided to import 4 new species?\n",
      "\n",
      "### Answer:\n",
      "If the zoo would decide to import 4 new species, there would be 5 + 4 = 9 species in total.\n",
      "This would mean, that seeing each type of animal would take 9 * 6 = 54 minutes.\n",
      "FINAL ANSWER: 54<|end|><|assistant|><think>\n",
      "There are 3 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "</think>\n",
      "```json\n",
      "{\n",
      "  \"verdict\": \"correct\",\n",
      "  \"erroneous_line\": null,\n",
      "  \"explanation\": \"All calculations and reasoning steps are correct.\"\n",
      "}\n",
      "```<|end|><|user|>### Question:\n",
      "At the zoo, there are 5 different types of animals. Seeing each animal type takes around 6 minutes. How much time would it take to see each animal type if the zoo decided to import 4 new species?\n",
      "\n",
      "### Answer:\n",
      "If the zoo would decide to import 4 new species, there would be 5 + 4 = 9 species in total.\n",
      "This would mean, that seeing each type of animal would take 9 * 6 = 45 minutes.\n",
      "FINAL ANSWER: 45<|end|><|assistant|><think>\n",
      "There are 3 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: Aha! I see the error. I will now prepare the output json.\n",
      "</think>\n",
      "```json\n",
      "{\n",
      "  \"verdict\": \"flawed\",\n",
      "  \"erroneous_line\": \"This would mean, that seeing each type of animal would take 9 * 6 = 45 minutes.\",\n",
      "  \"explanation\": \"The result of this computation should be 54, not 45. It appears two adjacent digits were swapped.\"\n",
      "}\n",
      "```<|end|><|user|>### Question:\n",
      "Bob has a cube of silver that measures 3 inches on each side.  A cubic inch of silver weighs 6 ounces.  Each ounce of silver sells for $25.  He sells it for 110% of its silver value.  How much does he sell it for?\n",
      "\n",
      "### Answer:\n",
      "The silver has a volume of 3*3*3=27 cubic inches\n",
      "So it weighs 27*6=162 ounces\n",
      "So its value is 162*25=$4050\n",
      "That means he sold it for 4050*1.1=$4545\n",
      "FINAL ANSWER: 4545<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# VIEW A SAMPLE PROMPT\n",
    "##########\n",
    "\n",
    "print(\"--- Displaying a complete sample prompt for verification ---\\n\")\n",
    "\n",
    "# Make sure the necessary variables are available from previous cells\n",
    "if 'test_df' not in locals() or 'examples' not in locals() or 'tokenizer' not in locals():\n",
    "    print(\"Please run the 'PIPELINE STEP 1: INITIALIZATION' cell first to load data.\")\n",
    "else:\n",
    "    # Select the first sample from the test set to use for this example\n",
    "    sample_to_view = test_df.iloc[0].to_dict()\n",
    "    print(f\"--- Using Sample from Problem Index: {sample_to_view.get('index')} ---\")\n",
    "    print(f\"--- Expected Verdict: {sample_to_view.get('error_type')} ---\\n\")\n",
    "\n",
    "    # Use the existing helper function to construct the full message list,\n",
    "    # including the system prompt and the few-shot examples.\n",
    "    sample_messages = create_sample_messages(sample_to_view, examples)\n",
    "\n",
    "    # Use the tokenizer's chat template to render the final string.\n",
    "    # - tokenize=False returns the formatted string.\n",
    "    # - add_generation_prompt=True adds the final special tokens that signal\n",
    "    #   to the model that it is its turn to generate a response.\n",
    "    final_prompt_string = tokenizer.apply_chat_template(\n",
    "        sample_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Print the final, complete prompt\n",
    "    print(\"--- FULL PROMPT AS SEEN BY THE MODEL ---\")\n",
    "    print(final_prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTzBNGh1PXMk"
   },
   "outputs": [],
   "source": [
    "# ##########\n",
    "# # TOKEN COUNT STATISTICS\n",
    "# ##########\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# def compute_token_statistics(tokenizer, base_df, examples):\n",
    "#     \"\"\"\n",
    "#     Compute token count statistics for inputs and expected outputs to help set max_length.\n",
    "#     \"\"\"\n",
    "#     print(\"ðŸ”¢ Computing token count statistics...\")\n",
    "\n",
    "#     input_token_counts = []\n",
    "#     output_token_counts = []\n",
    "#     total_token_counts = []\n",
    "\n",
    "#     # Sample a subset for faster computation if dataset is large\n",
    "#     sample_size = len(base_df)\n",
    "#     sample_df = base_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "#     print(f\"Analyzing {sample_size} samples...\")\n",
    "\n",
    "#     for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Computing token counts\"):\n",
    "#         sample_dict = row.to_dict()\n",
    "\n",
    "#         # Create input messages (system + examples + user query)\n",
    "#         input_messages = create_sample_messages(sample_dict, examples)\n",
    "#         input_text = tokenizer.apply_chat_template(\n",
    "#             input_messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True\n",
    "#         )\n",
    "\n",
    "#         # Get expected output\n",
    "#         expected_output = format_expected_output(sample_dict)\n",
    "\n",
    "#         # Tokenize and count\n",
    "#         input_tokens = tokenizer.encode(input_text, add_special_tokens=False)\n",
    "#         output_tokens = tokenizer.encode(expected_output, add_special_tokens=False)\n",
    "\n",
    "#         input_token_counts.append(len(input_tokens))\n",
    "#         output_token_counts.append(len(output_tokens))\n",
    "#         total_token_counts.append(len(input_tokens) + len(output_tokens))\n",
    "\n",
    "#     # Convert to numpy arrays for statistics\n",
    "#     import numpy as np\n",
    "#     input_counts = np.array(input_token_counts)\n",
    "#     output_counts = np.array(output_token_counts)\n",
    "#     total_counts = np.array(total_token_counts)\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"ðŸ“Š TOKEN COUNT STATISTICS\")\n",
    "#     print(\"=\"*60)\n",
    "\n",
    "#     print(f\"\\nðŸ”¤ INPUT TOKEN COUNTS (System + Examples + User Query):\")\n",
    "#     print(f\"  Mean: {input_counts.mean():.1f}\")\n",
    "#     print(f\"  Median: {np.median(input_counts):.1f}\")\n",
    "#     print(f\"  Min: {input_counts.min()}\")\n",
    "#     print(f\"  Max: {input_counts.max()}\")\n",
    "#     print(f\"  95th percentile: {np.percentile(input_counts, 95):.1f}\")\n",
    "#     print(f\"  99th percentile: {np.percentile(input_counts, 99):.1f}\")\n",
    "\n",
    "#     print(f\"\\nðŸ“ OUTPUT TOKEN COUNTS (Expected Response):\")\n",
    "#     print(f\"  Mean: {output_counts.mean():.1f}\")\n",
    "#     print(f\"  Median: {np.median(output_counts):.1f}\")\n",
    "#     print(f\"  Min: {output_counts.min()}\")\n",
    "#     print(f\"  Max: {output_counts.max()}\")\n",
    "#     print(f\"  95th percentile: {np.percentile(output_counts, 95):.1f}\")\n",
    "#     print(f\"  99th percentile: {np.percentile(output_counts, 99):.1f}\")\n",
    "\n",
    "#     print(f\"\\nðŸ”— TOTAL TOKEN COUNTS (Input + Output):\")\n",
    "#     print(f\"  Mean: {total_counts.mean():.1f}\")\n",
    "#     print(f\"  Median: {np.median(total_counts):.1f}\")\n",
    "#     print(f\"  Min: {total_counts.min()}\")\n",
    "#     print(f\"  Max: {total_counts.max()}\")\n",
    "#     print(f\"  95th percentile: {np.percentile(total_counts, 95):.1f}\")\n",
    "#     print(f\"  99th percentile: {np.percentile(total_counts, 99):.1f}\")\n",
    "\n",
    "#     print(f\"\\nðŸ“ˆ RECOMMENDED MAX_LENGTH SETTINGS:\")\n",
    "#     print(f\"  Conservative (99th percentile): {int(np.percentile(total_counts, 99))}\")\n",
    "#     print(f\"  Balanced (95th percentile): {int(np.percentile(total_counts, 95))}\")\n",
    "#     print(f\"  Aggressive (90th percentile): {int(np.percentile(total_counts, 90))}\")\n",
    "\n",
    "#     print(f\"\\nðŸŽ¯ TRUNCATION IMPACT ANALYSIS:\")\n",
    "#     for percentile in [90, 95, 99]:\n",
    "#         threshold = np.percentile(total_counts, percentile)\n",
    "#         truncated = (total_counts > threshold).sum()\n",
    "#         percentage = (truncated / len(total_counts)) * 100\n",
    "#         print(f\"  Max length {int(threshold)}: {truncated}/{len(total_counts)} samples truncated ({percentage:.1f}%)\")\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "#     # Return the statistics for further analysis\n",
    "#     return {\n",
    "#         'input_stats': {\n",
    "#             'mean': float(input_counts.mean()),\n",
    "#             'median': float(np.median(input_counts)),\n",
    "#             'min': int(input_counts.min()),\n",
    "#             'max': int(input_counts.max()),\n",
    "#             'p95': float(np.percentile(input_counts, 95)),\n",
    "#             'p99': float(np.percentile(input_counts, 99))\n",
    "#         },\n",
    "#         'output_stats': {\n",
    "#             'mean': float(output_counts.mean()),\n",
    "#             'median': float(np.median(output_counts)),\n",
    "#             'min': int(output_counts.min()),\n",
    "#             'max': int(output_counts.max()),\n",
    "#             'p95': float(np.percentile(output_counts, 95)),\n",
    "#             'p99': float(np.percentile(output_counts, 99))\n",
    "#         },\n",
    "#         'total_stats': {\n",
    "#             'mean': float(total_counts.mean()),\n",
    "#             'median': float(np.median(total_counts)),\n",
    "#             'min': int(total_counts.min()),\n",
    "#             'max': int(total_counts.max()),\n",
    "#             'p95': float(np.percentile(total_counts, 95)),\n",
    "#             'p99': float(np.percentile(total_counts, 99))\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "# # Run the analysis\n",
    "# token_stats = compute_token_statistics(tokenizer, base_df, examples)\n",
    "\n",
    "# # Optional: Show distribution histogram\n",
    "# def plot_token_distribution(tokenizer, base_df, examples):\n",
    "#     \"\"\"Plot token count distribution\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # Compute token counts for a sample\n",
    "#     sample_size = min(500, len(base_df))\n",
    "#     sample_df = base_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "#     total_counts = []\n",
    "#     for _, row in sample_df.iterrows():\n",
    "#         sample_dict = row.to_dict()\n",
    "#         input_messages = create_sample_messages(sample_dict, examples)\n",
    "#         input_text = tokenizer.apply_chat_template(input_messages, tokenize=False, add_generation_prompt=True)\n",
    "#         expected_output = format_expected_output(sample_dict)\n",
    "\n",
    "#         input_tokens = len(tokenizer.encode(input_text, add_special_tokens=False))\n",
    "#         output_tokens = len(tokenizer.encode(expected_output, add_special_tokens=False))\n",
    "#         total_counts.append(input_tokens + output_tokens)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(total_counts, bins=30, alpha=0.7, edgecolor='black')\n",
    "#     plt.axvline(np.percentile(total_counts, 95), color='red', linestyle='--', label='95th percentile')\n",
    "#     plt.axvline(np.percentile(total_counts, 99), color='orange', linestyle='--', label='99th percentile')\n",
    "#     plt.xlabel('Total Token Count')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Distribution of Total Token Counts')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.show()\n",
    "\n",
    "# # Uncomment to see the distribution plot\n",
    "# # plot_token_distribution(tokenizer, base_df, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjtYR7Tn1l65"
   },
   "outputs": [],
   "source": [
    "# ##########\n",
    "# # PIPELINE STEP 2: BASELINE EVALUATION (with vLLM)\n",
    "# ##########\n",
    "\n",
    "# import gc\n",
    "\n",
    "# baseline_predictions = run_vllm_inference_on_dataframe(CONFIG[\"model_name\"], tokenizer, test_df, examples)\n",
    "# baseline_results_df = create_detailed_results(test_df, baseline_predictions)\n",
    "# baseline_metrics = compute_metrics_from_results(baseline_results_df)\n",
    "\n",
    "# print(\"\\n--- Baseline Metrics ---\")\n",
    "# print(json.dumps(baseline_metrics, indent=2))\n",
    "\n",
    "# # Save results\n",
    "# baseline_dir = CONFIG[\"output_dir\"] / \"baseline\"\n",
    "# baseline_results_df.to_csv(baseline_dir / \"results_baseline.csv\", index=False)\n",
    "# with open(baseline_dir / \"metrics_baseline.json\", 'w') as f:\n",
    "#     json.dump(baseline_metrics, f, indent=2)\n",
    "# print(f\"Baseline results saved to {baseline_dir}\")\n",
    "\n",
    "# # --- CRUCIAL: Release vLLM resources ---\n",
    "# # We must delete the llm object created inside the function, which is now out of scope.\n",
    "# # Triggering garbage collection and emptying the cache ensures the GPU is free.\n",
    "# print(\"\\nReleasing vLLM engine from GPU memory...\")\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"âœ… GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5314f982278b485c96bc5df4bf66dad5",
      "2d1ba5b1915d483f8745418fd3d964dc",
      "2237626406bd4c84abd5970bffd3e71e",
      "66b6be3998f14136849ed5a8e580858e",
      "987375aa842b4cd4a7a2caae2ff1cdea",
      "9e0ccc766a8a42c9bedb712315c79033",
      "1fa291069e044cb8bebc80cf4064367e",
      "1a29f410f9a84e46b4703bcda436b2cb",
      "610eed07f648491db731465b7ad509d0",
      "82340180de0a429fba53806470591919",
      "34e6e1f476b1420eb55c3e3466450906"
     ]
    },
    "id": "MyyCu_96gOwA",
    "outputId": "66a32137-fce6-41e2-9a4f-673faa51915a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: microsoft/phi-4-mini-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314f982278b485c96bc5df4bf66dad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "WARNING:transformers_modules.microsoft.phi-4-mini-instruct.5a149550068a1eb93398160d8953f5f56c3603e9.modeling_phi3:`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting fine-tuning for 3 epochs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 1:14:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ…âœ…âœ… Fine-tuning finished! Final adapter saved to /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_adapter âœ…âœ…âœ…\n",
      "\n",
      "Releasing training model from GPU memory...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-541378230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nReleasing training model from GPU memory...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… GPU memory cleared.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# PIPELINE STEP 3: RUN FINE-TUNING\n",
    "##########\n",
    "\n",
    "# Load a fresh model specifically for training\n",
    "training_model = load_model()\n",
    "\n",
    "run_fine_tuning(training_model, tokenizer, train_dataset)\n",
    "\n",
    "# --- CRUCIAL: Release training resources ---\n",
    "print(\"\\nReleasing training model from GPU memory...\")\n",
    "del training_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELAO9GewB1Xj"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865,
     "referenced_widgets": [
      "eaf9e495ce8a49818d78861b4cbb2666",
      "637db87c41834c63a5c88bb44ae28e94",
      "70f52d7d51324b478aa57b40d7fc69f5",
      "3c4cc3e568224b79a7e7db54f6c4739a",
      "306d64d76b5f468ea13008c17fc6d501",
      "a0c26355951b480ab6528bcab53a1819",
      "a6b186afdd4a416d81ec07e40b4eceae",
      "effc2f44c1224c088d1bc47886670216",
      "6cfd6730536e4d9ab2d88dd91df9c372",
      "88d8434a9a62446aad89a76ea726707a",
      "614f892dda344f58a84d913f1b5a0f0d",
      "871c6edc326646ffa3535da76ff40a3b",
      "fea407fa5743459998c90c72dbace8f3",
      "50d511fb23d84631b3d0531c0cc2c32c",
      "808d1b7742bb497fa34e821b7352b8fc",
      "396744d287d145a98f92bf3b085489a2",
      "0765e95d66e249b88b88f572ea71ffdb",
      "1fc00db0c7f4489993af04f6be62023c",
      "053424666f6b4a14ba88654a81dcd641",
      "257f3a5412f0493288b1bf4a113f9ce6",
      "afbcf8cc4a9d4782a96fe22e61d76e33",
      "b38cc56427644294a469069eaa22e111",
      "09532f2dec4b4ffd8a642ce14be9d7e5",
      "186fd047946a4267a21b872036152631",
      "f0f3b1f9d0804c8d8134ae83faacef60",
      "d6f77a106ee74661b286355aaabe8bfe",
      "925536a372844f38a919a213df02112a",
      "cf11d1c4664c4fa6b57bf244405fe02c",
      "3c861e10a8bb43b9a6f904f0a01e0b37",
      "a5f689966e594936975f1bdb2879a51b",
      "3f1aaf3225c74e15b32c331f0bda4dec",
      "58ef7327d8cf40fa8d75d02ab8c00821",
      "e27f8bdfc8fd4aa8b4593f40c2d71d98"
     ]
    },
    "id": "wlN2fVeIgRoZ",
    "outputId": "bb554283-54dc-48ca-8688-0f4c43c4bc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging fine-tuned adapter with the base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf9e495ce8a49818d78861b4cbb2666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model\n",
      "\n",
      "--- Initializing vLLM engine for model: /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model ---\n",
      "INFO 08-06 01:39:04 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 08-06 01:39:04 config.py:208] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 08-06 01:39:16 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-06 01:39:16 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model', speculative_config=None, tokenizer='/content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 08-06 01:39:18 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 08-06 01:39:18 model_runner.py:1110] Starting to load model /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final_merged_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871c6edc326646ffa3535da76ff40a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 01:39:21 model_runner.py:1115] Loading model weights took 7.1694 GB\n",
      "INFO 08-06 01:39:22 worker.py:267] Memory profiling takes 0.75 seconds\n",
      "INFO 08-06 01:39:22 worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
      "INFO 08-06 01:39:22 worker.py:267] model weights take 7.17GiB; non_torch_memory takes 0.01GiB; PyTorch activation peak memory takes 1.83GiB; the rest of the memory reserved for KV Cache is 26.60GiB.\n",
      "INFO 08-06 01:39:23 executor_base.py:111] # cuda blocks: 13618, # CPU blocks: 2048\n",
      "INFO 08-06 01:39:23 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 106.39x\n",
      "INFO 08-06 01:39:25 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:33<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 01:39:59 model_runner.py:1562] Graph capturing finished in 34 secs, took 0.41 GiB\n",
      "INFO 08-06 01:39:59 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 37.47 seconds\n",
      "--- Preparing prompts for vLLM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09532f2dec4b4ffd8a642ce14be9d7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting Prompts:   0%|          | 0/1213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running vLLM inference on 1213 prompts ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessed prompts:   0%|          | 0/1213 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-06 01:40:11 scheduler.py:1754] Sequence group 242 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1213/1213 [01:41<00:00, 11.92it/s, est. speed input: 10468.96 toks/s, output: 1399.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- vLLM Inference complete ---\n",
      "\n",
      "--- Final Metrics (Best Model) ---\n",
      "{\n",
      "  \"overall_accuracy\": 0.2943116240725474,\n",
      "  \"precision_macro\": 0.07940391459074733,\n",
      "  \"recall_macro\": 0.24586776859504134,\n",
      "  \"f1_macro\": 0.12004034969737727,\n",
      "  \"eln_accuracy_on_errors\": 0.04941176470588235,\n",
      "  \"total_samples\": 1213,\n",
      "  \"error_samples_for_eln\": 850,\n",
      "  \"json_parse_failures\": 0,\n",
      "  \"thinking_parse_failures\": 0\n",
      "}\n",
      "Final results saved to /content/gene_ter_4N_exp_eln_nl_phi4_20250806_001250/final\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# PIPELINE STEP 4: FINAL EVALUATION ON BEST MODEL (with vLLM)\n",
    "##########\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Merge the adapter with the base model ---\n",
    "print(\"\\nMerging fine-tuned adapter with the base model...\")\n",
    "adapter_dir = CONFIG[\"output_dir\"] / \"final_adapter\"\n",
    "merged_model_dir = CONFIG[\"output_dir\"] / \"final_merged_model\"\n",
    "\n",
    "# Load base model on CPU to avoid using GPU memory during merge\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, str(adapter_dir))\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "merged_model.save_pretrained(str(merged_model_dir))\n",
    "tokenizer.save_pretrained(str(merged_model_dir))\n",
    "print(f\"Merged model saved to {merged_model_dir}\")\n",
    "\n",
    "# --- Clear memory before loading vLLM ---\n",
    "del base_model, model_with_adapter, merged_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run Final Inference on Merged Model ---\n",
    "final_predictions = run_vllm_inference_on_dataframe(str(merged_model_dir), tokenizer, test_df, examples)\n",
    "final_results_df = create_detailed_results(test_df, final_predictions)\n",
    "final_metrics = compute_metrics_from_results(final_results_df)\n",
    "\n",
    "print(\"\\n--- Final Metrics (Best Model) ---\")\n",
    "print(json.dumps(final_metrics, indent=2))\n",
    "\n",
    "final_dir = CONFIG[\"output_dir\"] / \"final\"\n",
    "final_results_df.to_csv(final_dir / \"results_final_best.csv\", index=False)\n",
    "with open(final_dir / \"metrics_final_best.json\", 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print(f\"Final results saved to {final_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYGtu-FrSwpJ"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# PIPELINE STEP 6: COMPRESS RESULTS FOR DOWNLOAD\n",
    "##########\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Compressing experiment results...\")\n",
    "\n",
    "# Define the root directory of your experiment\n",
    "root_dir = Path(CONFIG[\"output_dir\"])\n",
    "zip_path = root_dir / f\"{CONFIG['experiment_id']}_results.zip\"\n",
    "\n",
    "# Define the folder to exclude\n",
    "exclude_dir = root_dir / \"final_merged_model\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Walk through all the files and folders in the root directory\n",
    "    for file_path in root_dir.rglob('*'):\n",
    "        # Check if the current file_path is inside the excluded directory\n",
    "        if exclude_dir in file_path.parents or file_path == exclude_dir:\n",
    "            continue # Skip this file or folder\n",
    "\n",
    "        # Check if the path is the zip file itself to avoid recursion\n",
    "        if file_path == zip_path:\n",
    "            continue\n",
    "\n",
    "        # Write the file to the zip archive with a relative path\n",
    "        arcname = file_path.relative_to(root_dir)\n",
    "        zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"\\nâœ… Successfully created zip archive at: {zip_path}\")\n",
    "print(\"You can now download this file from the file browser on the left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3kGHEQYUQpG"
   },
   "outputs": [],
   "source": [
    "final_results_df.sort_values(by='problem_index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "QatIO8kkUEjc",
    "outputId": "2b552048-e719-487a-feb2-598c6a76d776"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"final_results_df\",\n  \"rows\": 1213,\n  \"fields\": [\n    {\n      \"column\": \"problem_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2315,\n        \"min\": 11,\n        \"max\": 7471,\n        \"num_unique_values\": 1057,\n        \"samples\": [\n          804,\n          2474,\n          6181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_verdict\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"computational_error\",\n          \"conceptual_error\",\n          \"correct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_verdict\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"correct\",\n          \"flawed\",\n          \"incorrect\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verdict_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_erroneous_line\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 829,\n        \"samples\": [\n          \"Therefore, Joseph can save 10 - 4 = 8 hours if he takes route B both ways.\",\n          \"Together, Sam, Isabella, and Giselle has $255+$90=$354\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_erroneous_line\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 86,\n        \"samples\": [\n          \"Then add the weight-based charge to the flat fee to find the total cost: $4 * $5 = $20\",\n          \"In two minutes, Carrie would perform 90*2-180 sit-ups.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_thinking\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          \"There are 8 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: Aha! I see the error. I will now prepare the output json.\",\n          \"There are 4 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: Aha! I see the error. I will now prepare the output json.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_thinking\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"There are 6 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: no errors\\nLine 4: no errors\\nLine 5: no errors\\nFinal Answer: Aha! I see the error. I will now prepare the output json.\",\n          \"There are 9 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: no errors\\nLine 4: no errors\\nLine 5: no errors\\nLine 6: no errors\\nLine 7: no errors\\nLine 8: no errors\\nLine 9: no errors\\nI found no errors! I will now prepare the output json.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_prediction_text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 104,\n        \"samples\": [\n          \"<think>\\nThere are 6 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: no errors\\nLine 4: no errors\\nLine 5: no errors\\nFinal Answer: no errors\\nI found no errors! I will now prepare the output json.\\n</think>\\n```json\\n{\\n  \\\"verdict\\\": \\\"flawed\\\",\\n  \\\"erroneous_line\\\": \\\"Thus, Silvia would save $995 - $900 = $95 when buying from Sweetwater.\\\",\\n  \\\"explanation\\\": \\\"The result of this computation should be 95, not 5. It appears two adjacent digits were swapped.\\\"\\n}\\n```\",\n          \"<think>\\nThere are 5 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\\nLine 1: no errors\\nLine 2: no errors\\nLine 3: no errors\\nFinal Answer: no errors\\nI found no errors! I will now prepare the output json.\\n</think>\\n```json\\n{\\n  \\\"verdict\\\": \\\"flawed\\\",\\n  \\\"erroneous_line\\\": \\\"To make 200 of the second type of shirt, they will need 200+5=205 buttons.\\\",\\n  \\\"explanation\\\": \\\"The operation should be multiplication, not addition. The correct calculation is 200*5=1000.\\\"\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "final_results_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-254bcbd7-7351-488a-8a1a-2663e5400386\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_index</th>\n",
       "      <th>expected_verdict</th>\n",
       "      <th>predicted_verdict</th>\n",
       "      <th>verdict_correct</th>\n",
       "      <th>expected_erroneous_line</th>\n",
       "      <th>predicted_erroneous_line</th>\n",
       "      <th>expected_thinking</th>\n",
       "      <th>predicted_thinking</th>\n",
       "      <th>full_prediction_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7385</td>\n",
       "      <td>computational_error</td>\n",
       "      <td>correct</td>\n",
       "      <td>False</td>\n",
       "      <td>That means he sold it for 4050*1.1=$4545</td>\n",
       "      <td>None</td>\n",
       "      <td>There are 5 lines in the solution, including t...</td>\n",
       "      <td>There are 6 lines in the solution, including t...</td>\n",
       "      <td>&lt;think&gt;\\nThere are 6 lines in the solution, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2668</td>\n",
       "      <td>computational_error</td>\n",
       "      <td>correct</td>\n",
       "      <td>False</td>\n",
       "      <td>So, Hannah's family has 20 - 3 = 20 cows.</td>\n",
       "      <td>None</td>\n",
       "      <td>There are 5 lines in the solution, including t...</td>\n",
       "      <td>There are 5 lines in the solution, including t...</td>\n",
       "      <td>&lt;think&gt;\\nThere are 5 lines in the solution, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>858</td>\n",
       "      <td>conceptual_error</td>\n",
       "      <td>correct</td>\n",
       "      <td>False</td>\n",
       "      <td>This means he took in 300+125 = 425 calories f...</td>\n",
       "      <td>None</td>\n",
       "      <td>There are 5 lines in the solution, including t...</td>\n",
       "      <td>There are 6 lines in the solution, including t...</td>\n",
       "      <td>&lt;think&gt;\\nThere are 6 lines in the solution, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3796</td>\n",
       "      <td>computational_error</td>\n",
       "      <td>correct</td>\n",
       "      <td>False</td>\n",
       "      <td>He has 70 - 9 = $16 left after buying the fris...</td>\n",
       "      <td>None</td>\n",
       "      <td>There are 3 lines in the solution, including t...</td>\n",
       "      <td>There are 4 lines in the solution, including t...</td>\n",
       "      <td>&lt;think&gt;\\nThere are 4 lines in the solution, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2243</td>\n",
       "      <td>correct</td>\n",
       "      <td>correct</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>There are 4 lines in the solution, including t...</td>\n",
       "      <td>There are 4 lines in the solution, including t...</td>\n",
       "      <td>&lt;think&gt;\\nThere are 4 lines in the solution, in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-254bcbd7-7351-488a-8a1a-2663e5400386')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-254bcbd7-7351-488a-8a1a-2663e5400386 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-254bcbd7-7351-488a-8a1a-2663e5400386');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ae692cc2-fa8a-4ecb-830d-993b5cdf59b7\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae692cc2-fa8a-4ecb-830d-993b5cdf59b7')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ae692cc2-fa8a-4ecb-830d-993b5cdf59b7 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   problem_index     expected_verdict predicted_verdict  verdict_correct  \\\n",
       "0           7385  computational_error           correct            False   \n",
       "1           2668  computational_error           correct            False   \n",
       "2            858     conceptual_error           correct            False   \n",
       "3           3796  computational_error           correct            False   \n",
       "4           2243              correct           correct             True   \n",
       "\n",
       "                             expected_erroneous_line predicted_erroneous_line  \\\n",
       "0           That means he sold it for 4050*1.1=$4545                     None   \n",
       "1          So, Hannah's family has 20 - 3 = 20 cows.                     None   \n",
       "2  This means he took in 300+125 = 425 calories f...                     None   \n",
       "3  He has 70 - 9 = $16 left after buying the fris...                     None   \n",
       "4                                               None                     None   \n",
       "\n",
       "                                   expected_thinking  \\\n",
       "0  There are 5 lines in the solution, including t...   \n",
       "1  There are 5 lines in the solution, including t...   \n",
       "2  There are 5 lines in the solution, including t...   \n",
       "3  There are 3 lines in the solution, including t...   \n",
       "4  There are 4 lines in the solution, including t...   \n",
       "\n",
       "                                  predicted_thinking  \\\n",
       "0  There are 6 lines in the solution, including t...   \n",
       "1  There are 5 lines in the solution, including t...   \n",
       "2  There are 6 lines in the solution, including t...   \n",
       "3  There are 4 lines in the solution, including t...   \n",
       "4  There are 4 lines in the solution, including t...   \n",
       "\n",
       "                                full_prediction_text  \n",
       "0  <think>\\nThere are 6 lines in the solution, in...  \n",
       "1  <think>\\nThere are 5 lines in the solution, in...  \n",
       "2  <think>\\nThere are 6 lines in the solution, in...  \n",
       "3  <think>\\nThere are 4 lines in the solution, in...  \n",
       "4  <think>\\nThere are 4 lines in the solution, in...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vx27g-LMDWSV",
    "outputId": "3cb17a47-a32c-4a4c-f02f-ed0662150027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7385\n",
      "computational_error\n",
      "correct\n",
      "There are 6 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Line 4: no errors\n",
      "Line 5: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "There are 5 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Line 4: Aha! I see the error. I will now prepare the output json.\n",
      "2668\n",
      "computational_error\n",
      "correct\n",
      "There are 5 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Line 4: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "There are 5 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: Aha! I see the error. I will now prepare the output json.\n",
      "858\n",
      "conceptual_error\n",
      "correct\n",
      "There are 6 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Line 4: no errors\n",
      "Line 5: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "There are 5 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: Aha! I see the error. I will now prepare the output json.\n",
      "3796\n",
      "computational_error\n",
      "correct\n",
      "There are 4 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "There are 3 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: Aha! I see the error. I will now prepare the output json.\n",
      "2243\n",
      "correct\n",
      "correct\n",
      "There are 4 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n",
      "There are 4 lines in the solution, including the final answer line. Let's examine the lines one-by-one.\n",
      "Line 1: no errors\n",
      "Line 2: no errors\n",
      "Line 3: no errors\n",
      "Final Answer: no errors\n",
      "I found no errors! I will now prepare the output json.\n"
     ]
    }
   ],
   "source": [
    "temp = final_results_df.head()\n",
    "for index, row in temp.iterrows():\n",
    "    print(row['problem_index'])\n",
    "    print(row['expected_verdict'])\n",
    "    print(row['predicted_verdict'])\n",
    "    print(row['predicted_thinking'])\n",
    "    print(row['expected_thinking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoYbqMQiWPg9"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# CONFUSION MATRIX ANALYSIS\n",
    "##########\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- Generating Confusion Matrix for Final Model Verdicts ---\")\n",
    "\n",
    "# Ensure the final results DataFrame is available\n",
    "if 'final_results_df' not in locals():\n",
    "    print(\"Error: `final_results_df` not found. Please run the final evaluation cell first.\")\n",
    "else:\n",
    "    # Define the order of labels for a consistent matrix layout\n",
    "    labels = ['correct', 'conceptual_error', 'computational_error', 'parse_failure']\n",
    "\n",
    "    # Extract the true and predicted values, filling any parse failures\n",
    "    y_true = final_results_df['expected_verdict']\n",
    "    y_pred = final_results_df['predicted_verdict'].fillna('parse_failure')\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # Create a DataFrame for better labeling with seaborn\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues',\n",
    "                annot_kws={\"size\": 14}) # Increase annotation font size\n",
    "\n",
    "    plt.title('Verdict Confusion Matrix (Final Model)', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdD7_eYzgbiq"
   },
   "outputs": [],
   "source": [
    "# ##########\n",
    "# # PIPELINE STEP 5: PUSH TO HUB\n",
    "# ##########\n",
    "\n",
    "# if CONFIG[\"save_to_hf\"]:\n",
    "#     print(\"\\nPushing the final MERGED model to the Hugging Face Hub...\")\n",
    "#     # The merged model can be pushed directly using its save directory\n",
    "#     hf_repo_id = CONFIG[\"experiment_id\"]\n",
    "#     merged_model_dir = str(CONFIG[\"output_dir\"] / \"final_merged_model\")\n",
    "\n",
    "#     # Use the API to upload the folder\n",
    "#     from huggingface_hub import HfApi\n",
    "#     api = HfApi()\n",
    "#     api.upload_folder(\n",
    "#         folder_path=merged_model_dir,\n",
    "#         repo_id=hf_repo_id,\n",
    "#         repo_type=\"model\"\n",
    "#     )\n",
    "#     print(f\"âœ… Final merged model pushed to Hub repo: {hf_repo_id}\")\n",
    "# else:\n",
    "#     print(\"\\nSkipping Hugging Face Hub upload as per configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_Oh0iZoTxui"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 2. UNZIP YOUR SAVED RESULTS\n",
    "##########\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# IMPORTANT: Replace this with the actual name of your uploaded zip file\n",
    "zip_filename = \"gene_ter_3N_exp_eln_nl_phi4_20250803_174501_results.zip\" #<-- CHANGE THIS\n",
    "\n",
    "zip_path = Path(\"/content/\") / zip_filename\n",
    "\n",
    "if not zip_path.exists():\n",
    "    raise FileNotFoundError(f\"'{zip_filename}' not found. Please upload the file first.\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/content/\")\n",
    "\n",
    "# The unzipped folder name is the experiment ID\n",
    "experiment_id = zip_filename.replace(\"_results.zip\", \"\")\n",
    "experiment_dir = Path(\"/content/\") / experiment_id\n",
    "print(f\"âœ… Successfully unzipped results to: {experiment_dir}\")\n",
    "\n",
    "##########\n",
    "# 3. LOAD, MERGE, AND PUSH\n",
    "##########\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Configuration ---\n",
    "# We can get these from your unzipped folder name and a config file\n",
    "# or define them manually.\n",
    "BASE_MODEL_NAME = \"microsoft/phi-4-mini-instruct\" # Or whatever you used\n",
    "ADAPTER_DIR = experiment_dir / \"final_best_model\" # Or final_adapter\n",
    "HF_REPO_ID = experiment_id # Use the experiment ID as the repo name\n",
    "\n",
    "# --- Load Base Model and Adapter ---\n",
    "print(\"Loading base model for merging...\")\n",
    "# Load on the CPU to be memory-efficient\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading adapter from: {ADAPTER_DIR}\")\n",
    "# Apply the saved LoRA adapter to the base model\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, str(ADAPTER_DIR))\n",
    "\n",
    "# --- Merge the Weights ---\n",
    "print(\"Merging adapter weights into the base model...\")\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "print(\"Merge complete.\")\n",
    "\n",
    "# --- Load the Tokenizer ---\n",
    "# The tokenizer was saved alongside the adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
    "\n",
    "# --- Push to Hugging Face Hub ---\n",
    "print(f\"Pushing final merged model and tokenizer to Hub repo: {HF_REPO_ID}\")\n",
    "merged_model.push_to_hub(HF_REPO_ID)\n",
    "tokenizer.push_to_hub(HF_REPO_ID)\n",
    "\n",
    "print(f\"\\nâœ…âœ…âœ… Successfully uploaded model to: https://huggingface.co/{HF_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c47777c57b48eaa4e7912bdf86b2f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_f2bce4310d6d496d9ae13bcad425fff3",
      "style": "IPY_MODEL_e1aaf7c13efa4c34b563e6aac893c2e9",
      "value": true
     }
    },
    "0253880c85034329894feb5e5f6386cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "046f3f9ce3894b0785a0316650a3d971": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "053424666f6b4a14ba88654a81dcd641": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0765e95d66e249b88b88f572ea71ffdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09532f2dec4b4ffd8a642ce14be9d7e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_186fd047946a4267a21b872036152631",
       "IPY_MODEL_f0f3b1f9d0804c8d8134ae83faacef60",
       "IPY_MODEL_d6f77a106ee74661b286355aaabe8bfe"
      ],
      "layout": "IPY_MODEL_925536a372844f38a919a213df02112a"
     }
    },
    "186fd047946a4267a21b872036152631": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf11d1c4664c4fa6b57bf244405fe02c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3c861e10a8bb43b9a6f904f0a01e0b37",
      "value": "Formattingâ€‡Prompts:â€‡100%"
     }
    },
    "1a29f410f9a84e46b4703bcda436b2cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fa291069e044cb8bebc80cf4064367e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fc00db0c7f4489993af04f6be62023c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20c729b12689453586ac8fffd65139aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2237626406bd4c84abd5970bffd3e71e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a29f410f9a84e46b4703bcda436b2cb",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_610eed07f648491db731465b7ad509d0",
      "value": 2
     }
    },
    "257f3a5412f0493288b1bf4a113f9ce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b502370f284428bbf6e3b768aa6abed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ade1936cec742b4820251605e0bf921",
       "IPY_MODEL_b392907e0ab24948b35d61f56db9147f",
       "IPY_MODEL_e9124e9cb5bf4f0c9773f98deec27642"
      ],
      "layout": "IPY_MODEL_20c729b12689453586ac8fffd65139aa"
     }
    },
    "2d1ba5b1915d483f8745418fd3d964dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e0ccc766a8a42c9bedb712315c79033",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1fa291069e044cb8bebc80cf4064367e",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "306d64d76b5f468ea13008c17fc6d501": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3414656528764ee48254d96b041fd374": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e6e1f476b1420eb55c3e3466450906": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "396744d287d145a98f92bf3b085489a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39a03fa910cf4a389cd5b6de3a866489": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c4cc3e568224b79a7e7db54f6c4739a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88d8434a9a62446aad89a76ea726707a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_614f892dda344f58a84d913f1b5a0f0d",
      "value": "â€‡2/2â€‡[00:01&lt;00:00,â€‡â€‡1.13it/s]"
     }
    },
    "3c861e10a8bb43b9a6f904f0a01e0b37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f1aaf3225c74e15b32c331f0bda4dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "439f660a85304a89a83c7be867be05ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba65d45010984a30b80c1d5185d14b27",
      "max": 4854,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b087538776314773912eacebff185db9",
      "value": 4854
     }
    },
    "44518f5b90cb4c9fb8bb7221e16ba9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc6bb53ce4884b1a931e22229aa3da07",
       "IPY_MODEL_4bcb9fbd5c7f4b38976b2e5e96bcb30b",
       "IPY_MODEL_01c47777c57b48eaa4e7912bdf86b2f9",
       "IPY_MODEL_50e7769e59204a44ba7bc56bb92be843",
       "IPY_MODEL_9e721664efa649368424e41c3d32f5df"
      ],
      "layout": "IPY_MODEL_4b8c08b743f44fdba08e4dc42aac5922"
     }
    },
    "4b8c08b743f44fdba08e4dc42aac5922": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "4bcb9fbd5c7f4b38976b2e5e96bcb30b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_f2094ef9defb4ebb9c44391d346945d6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fb5f496f1de64ff592ce1af36eaa9ed7",
      "value": ""
     }
    },
    "50d511fb23d84631b3d0531c0cc2c32c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_053424666f6b4a14ba88654a81dcd641",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_257f3a5412f0493288b1bf4a113f9ce6",
      "value": 2
     }
    },
    "50e7769e59204a44ba7bc56bb92be843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_39a03fa910cf4a389cd5b6de3a866489",
      "style": "IPY_MODEL_a718d01710b349c390e563ec457cb8b1",
      "tooltip": ""
     }
    },
    "5314f982278b485c96bc5df4bf66dad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d1ba5b1915d483f8745418fd3d964dc",
       "IPY_MODEL_2237626406bd4c84abd5970bffd3e71e",
       "IPY_MODEL_66b6be3998f14136849ed5a8e580858e"
      ],
      "layout": "IPY_MODEL_987375aa842b4cd4a7a2caae2ff1cdea"
     }
    },
    "58ef7327d8cf40fa8d75d02ab8c00821": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a10e22b444a4e99b0a8b12c920168e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ade1936cec742b4820251605e0bf921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_640144e2c3664faeb700dd2e33728cee",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8380557d10b44372ac9bfef345e1b53d",
      "value": "Map:â€‡100%"
     }
    },
    "610eed07f648491db731465b7ad509d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "614f892dda344f58a84d913f1b5a0f0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "637db87c41834c63a5c88bb44ae28e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0c26355951b480ab6528bcab53a1819",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a6b186afdd4a416d81ec07e40b4eceae",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "640144e2c3664faeb700dd2e33728cee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64a10d793961488cb1ff4d70aa43a20c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66b6be3998f14136849ed5a8e580858e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82340180de0a429fba53806470591919",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_34e6e1f476b1420eb55c3e3466450906",
      "value": "â€‡2/2â€‡[00:08&lt;00:00,â€‡â€‡3.90s/it]"
     }
    },
    "6bd4c7ff4c5347db93a23a48bfe6c63d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cfd6730536e4d9ab2d88dd91df9c372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70f52d7d51324b478aa57b40d7fc69f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_effc2f44c1224c088d1bc47886670216",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cfd6730536e4d9ab2d88dd91df9c372",
      "value": 2
     }
    },
    "808d1b7742bb497fa34e821b7352b8fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afbcf8cc4a9d4782a96fe22e61d76e33",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b38cc56427644294a469069eaa22e111",
      "value": "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡2/2â€‡[00:02&lt;00:00,â€‡â€‡1.10s/it]\n"
     }
    },
    "82340180de0a429fba53806470591919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8380557d10b44372ac9bfef345e1b53d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86bf45b291b34f12ab1689a0aa51a3be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "871c6edc326646ffa3535da76ff40a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fea407fa5743459998c90c72dbace8f3",
       "IPY_MODEL_50d511fb23d84631b3d0531c0cc2c32c",
       "IPY_MODEL_808d1b7742bb497fa34e821b7352b8fc"
      ],
      "layout": "IPY_MODEL_396744d287d145a98f92bf3b085489a2"
     }
    },
    "88d8434a9a62446aad89a76ea726707a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f539cd24ac64ea3afeef97f80fbc765": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e165127269f4a4db890d70703e6e66d",
       "IPY_MODEL_439f660a85304a89a83c7be867be05ee",
       "IPY_MODEL_f2b1a9ed13694e03bf7fd886ad6225d3"
      ],
      "layout": "IPY_MODEL_d6bb839ce52341d090a63892e8b00698"
     }
    },
    "925536a372844f38a919a213df02112a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987375aa842b4cd4a7a2caae2ff1cdea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9de8bdc1527d42a6bce57ec58b3def61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e0ccc766a8a42c9bedb712315c79033": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e165127269f4a4db890d70703e6e66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fac7f364701e4b88a556d6e88d6001f3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9de8bdc1527d42a6bce57ec58b3def61",
      "value": "Map:â€‡100%"
     }
    },
    "9e721664efa649368424e41c3d32f5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7e367bb8a1444bf87f7befb6e1ee639",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6bd4c7ff4c5347db93a23a48bfe6c63d",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "a0c26355951b480ab6528bcab53a1819": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f689966e594936975f1bdb2879a51b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6b186afdd4a416d81ec07e40b4eceae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a718d01710b349c390e563ec457cb8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "afbcf8cc4a9d4782a96fe22e61d76e33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b087538776314773912eacebff185db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b38cc56427644294a469069eaa22e111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b392907e0ab24948b35d61f56db9147f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3414656528764ee48254d96b041fd374",
      "max": 4854,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc7db57e7df745848875ea4d98ab2a63",
      "value": 4854
     }
    },
    "ba65d45010984a30b80c1d5185d14b27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf256c07d14047388aa862f5e65564f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf11d1c4664c4fa6b57bf244405fe02c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6bb839ce52341d090a63892e8b00698": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6f77a106ee74661b286355aaabe8bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58ef7327d8cf40fa8d75d02ab8c00821",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e27f8bdfc8fd4aa8b4593f40c2d71d98",
      "value": "â€‡1213/1213â€‡[00:00&lt;00:00,â€‡7091.40it/s]"
     }
    },
    "e1aaf7c13efa4c34b563e6aac893c2e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e27f8bdfc8fd4aa8b4593f40c2d71d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7e367bb8a1444bf87f7befb6e1ee639": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9124e9cb5bf4f0c9773f98deec27642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0253880c85034329894feb5e5f6386cb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_86bf45b291b34f12ab1689a0aa51a3be",
      "value": "â€‡4854/4854â€‡[00:01&lt;00:00,â€‡2898.04â€‡examples/s]"
     }
    },
    "eaf9e495ce8a49818d78861b4cbb2666": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_637db87c41834c63a5c88bb44ae28e94",
       "IPY_MODEL_70f52d7d51324b478aa57b40d7fc69f5",
       "IPY_MODEL_3c4cc3e568224b79a7e7db54f6c4739a"
      ],
      "layout": "IPY_MODEL_306d64d76b5f468ea13008c17fc6d501"
     }
    },
    "effc2f44c1224c088d1bc47886670216": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0f3b1f9d0804c8d8134ae83faacef60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5f689966e594936975f1bdb2879a51b",
      "max": 1213,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f1aaf3225c74e15b32c331f0bda4dec",
      "value": 1213
     }
    },
    "f2094ef9defb4ebb9c44391d346945d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2b1a9ed13694e03bf7fd886ad6225d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64a10d793961488cb1ff4d70aa43a20c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bf256c07d14047388aa862f5e65564f5",
      "value": "â€‡4854/4854â€‡[00:15&lt;00:00,â€‡351.13â€‡examples/s]"
     }
    },
    "f2bce4310d6d496d9ae13bcad425fff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fac7f364701e4b88a556d6e88d6001f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb5f496f1de64ff592ce1af36eaa9ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc6bb53ce4884b1a931e22229aa3da07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a10e22b444a4e99b0a8b12c920168e6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_046f3f9ce3894b0785a0316650a3d971",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "fc7db57e7df745848875ea4d98ab2a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fea407fa5743459998c90c72dbace8f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0765e95d66e249b88b88f572ea71ffdb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1fc00db0c7f4489993af04f6be62023c",
      "value": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
