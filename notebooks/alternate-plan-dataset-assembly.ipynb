{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ba7a4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Dataset output directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/verifier-v2-two-task\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the git repository.\"\"\"\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "# --- Global Constants and Paths ---\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = DATA_DIR / \"sft-datasets/verifier-v2-two-task\"\n",
    "PROCESSED_TEMPLATE_DIR = DATA_DIR / \"template-generated-processed\"\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Seed for reproducibility of shuffling and sampling\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Define the number of core problems to use as a base\n",
    "NUM_CONCEPTUAL_PROBLEMS = 1000\n",
    "NUM_COMPUTATIONAL_PROBLEMS = 1000\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Dataset output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "44d3c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22,645 records from programmatic computational error catalog.\n",
      "Loaded 33,716 records from conceptual error candidate catalog.\n",
      "WARNING: Manual conceptual data not found at /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/final-datasets/conceptual_errors_final.json\n",
      "Loaded 7,473 samples from gsm8k/main train split.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ca42fcbd1f4c2db145fd64098d8648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating GSM8K lookup:   0%|          | 0/7473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GSM8K problem lookup dictionary.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Catalog of Programmatically generated Computational Errors ---\n",
    "# This catalog points to individual JSON files with generated flawed solutions.\n",
    "PROGRAMMATIC_COMPUTATIONAL_DIR = DATA_DIR / \"computational-errors-generated\"\n",
    "PROGRAMMATIC_CATALOG_PATH = PROGRAMMATIC_COMPUTATIONAL_DIR / \"computational_error_catalog.csv\"\n",
    "\n",
    "# --- Load Catalog of Programmatically generated Conceptual Errors ---\n",
    "# This catalog points to individual JSON files with generated conceptual candidates.\n",
    "CONCEPTUAL_CANDIDATES_DIR = DATA_DIR / \"conceptual-error-candidates\"\n",
    "CONCEPTUAL_CATALOG_PATH = CONCEPTUAL_CANDIDATES_DIR / \"conceptual_candidate_catalog.csv\"\n",
    "\n",
    "# --- Load manually validated/corrected conceptual errors ---\n",
    "# This file contains the final, human-approved conceptual error text and explanations.\n",
    "MANUAL_CONCEPTUAL_PATH = DATA_DIR / \"final-datasets/conceptual_errors_final.json\"\n",
    "\n",
    "# --- Load Original GSM8K Dataset for 'Correct' examples and problem text ---\n",
    "GSM8K_DATASET: Dataset = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
    "\n",
    "# --- Loading and Basic Validation ---\n",
    "try:\n",
    "    programmatic_comp_df = pd.read_csv(PROGRAMMATIC_CATALOG_PATH)\n",
    "    print(f\"Loaded {len(programmatic_comp_df):,} records from programmatic computational error catalog.\")\n",
    "except FileNotFoundError:\n",
    "    programmatic_comp_df = pd.DataFrame()\n",
    "    print(f\"WARNING: Programmatic computational catalog not found at {PROGRAMMATIC_CATALOG_PATH}\")\n",
    "\n",
    "try:\n",
    "    conceptual_cand_df = pd.read_csv(CONCEPTUAL_CATALOG_PATH)\n",
    "    print(f\"Loaded {len(conceptual_cand_df):,} records from conceptual error candidate catalog.\")\n",
    "except FileNotFoundError:\n",
    "    conceptual_cand_df = pd.DataFrame()\n",
    "    print(f\"WARNING: Conceptual candidate catalog not found at {CONCEPTUAL_CATALOG_PATH}\")\n",
    "\n",
    "try:\n",
    "    with open(MANUAL_CONCEPTUAL_PATH, 'r', encoding='utf-8') as f:\n",
    "        manual_conceptual_data = json.load(f)\n",
    "    print(f\"Loaded {len(manual_conceptual_data):,} records from manually validated conceptual errors JSON.\")\n",
    "except FileNotFoundError:\n",
    "    manual_conceptual_data = []\n",
    "    print(f\"WARNING: Manual conceptual data not found at {MANUAL_CONCEPTUAL_PATH}\")\n",
    "\n",
    "print(f\"Loaded {len(GSM8K_DATASET):,} samples from gsm8k/main train split.\")\n",
    "\n",
    "# --- Create a quick-access lookup dictionary for original GSM8K problems ---\n",
    "gsm8k_problem_lookup = {\n",
    "    i: {\"question\": sample[\"question\"], \"answer\": sample[\"answer\"]}\n",
    "    for i, sample in enumerate(tqdm(GSM8K_DATASET, desc=\"Creating GSM8K lookup\"))\n",
    "}\n",
    "print(\"Created GSM8K problem lookup dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cedc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core utility functions (revised) defined. Running a quick test case...\n"
     ]
    }
   ],
   "source": [
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces a comprehensive set of problematic Unicode characters with their\n",
    "    ASCII equivalents to prevent model generation and string parsing errors.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    replacements = {\n",
    "        \"\\u2212\": \"-\",  # Minus Sign\n",
    "        \"\\u00d7\": \"*\",  # Multiplication Sign\n",
    "        \"\\u00f7\": \"/\",  # Division Sign\n",
    "        \"\\u22c5\": \"*\",  # Dot Operator\n",
    "        \"\\u201c\": '\"',  # Left Double Quotation Mark\n",
    "        \"\\u201d\": '\"',  # Right Double Quotation Mark\n",
    "        \"\\u2018\": \"'\",  # Left Single Quotation Mark\n",
    "        \"\\u2019\": \"'\",  # Right Single Quotation Mark\n",
    "        \"\\u2014\": \"-\",  # Em Dash\n",
    "        \"\\u2013\": \"-\",  # En Dash\n",
    "        \"\\u2026\": \"...\",# Horizontal Ellipsis\n",
    "        \"\\u00a0\": \" \",  # No-Break Space\n",
    "    }\n",
    "    for uni, ascii_char in replacements.items():\n",
    "        text = text.replace(uni, ascii_char)\n",
    "    return text\n",
    "\n",
    "def clean_and_split_solution(raw_text: str) -> Tuple[str, str | None]:\n",
    "    \"\"\"\n",
    "    Takes a raw solution text, sanitizes it, and separates the reasoning\n",
    "    lines from the final answer line.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing (cleaned_reasoning_text, final_answer_string).\n",
    "        final_answer_string is None if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str):\n",
    "        return \"\", None\n",
    "        \n",
    "    # 1. Sanitize all characters first\n",
    "    sanitized_text = sanitize_text(raw_text)\n",
    "    \n",
    "    # 2. Remove calculator annotations\n",
    "    text_no_annotations = re.sub(r'<<.*?>>', '', sanitized_text)\n",
    "    \n",
    "    # 3. Remove comma separators from numbers\n",
    "    text_no_commas = re.sub(r'(\\d),(\\d)', r'\\1\\2', text_no_annotations)\n",
    "    \n",
    "    lines = text_no_commas.split('\\n')\n",
    "    final_answer = None\n",
    "    \n",
    "    # 4. Find and extract the final answer line\n",
    "    if lines and re.match(r'^\\s*####\\s*.*$', lines[-1]):\n",
    "        final_answer_line = lines.pop().strip()\n",
    "        # Extract just the number part after ####\n",
    "        match = re.search(r'####\\s*(.*)', final_answer_line)\n",
    "        if match:\n",
    "            final_answer = match.group(1).strip()\n",
    "\n",
    "    # 5. Process the remaining reasoning lines\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    reasoning_text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    return reasoning_text, final_answer\n",
    "\n",
    "def convert_solution_to_json_str(cleaned_reasoning: str, final_answer: str | None) -> str:\n",
    "    \"\"\"\n",
    "    Takes cleaned reasoning text and a final answer, and converts them into\n",
    "    a single JSON-formatted string.\n",
    "    \"\"\"\n",
    "    lines = cleaned_reasoning.split('\\n')\n",
    "    solution_dict = {f\"L{i+1}\": line for i, line in enumerate(lines) if line}\n",
    "    \n",
    "    if final_answer is not None:\n",
    "        solution_dict[\"FA\"] = final_answer\n",
    "        \n",
    "    return json.dumps(solution_dict, indent=2)\n",
    "\n",
    "def extract_equations_from_annotations(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses a raw solution text to find all calculator annotations (<<...>>)\n",
    "    and returns them as a JSON-formatted dictionary string.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str):\n",
    "        return \"{}\"\n",
    "        \n",
    "    # Sanitize text first to handle different dash types in equations\n",
    "    sanitized_text = sanitize_text(raw_text)\n",
    "    equations = {}\n",
    "    lines = sanitized_text.split('\\n')\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        annotations = re.findall(r'<<(.*?)>>', line)\n",
    "        if annotations:\n",
    "            # We assume at most one annotation per line\n",
    "            equation_str = annotations[0].strip()\n",
    "            equations[f\"L{i+1}\"] = equation_str\n",
    "            \n",
    "    return json.dumps(equations, indent=2)\n",
    "\n",
    "def create_formatted_prompt(\n",
    "    system_prompt: str,\n",
    "    problem_text: str,\n",
    "    solution_json_str: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Assembles the final prompt string using the Phi-4-mini-instruct chat template.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Problem:\\n```\\n{problem_text}\\n```\\n\\nSolution:\\n```json\\n{solution_json_str}\\n```\"\n",
    "    return f\"<|system|>{system_prompt}<|end|><|user|>{user_prompt}<|end|>\"\n",
    "\n",
    "# --- Verification of New Functions ---\n",
    "print(\"Core utility functions (revised) defined. Running a quick test case...\")\n",
    "\n",
    "# Example usage:\n",
    "test_raw_solution = \"The first number is 1,200. The second is 10.\\nTheir sum is 1,200 + 10 = <<1200+10=1210>>1210.\\n#### 1210\"\n",
    "test_problem = \"What is the sum of 1,200 and 10?\"\n",
    "test_system_prompt = \"[CONCEPTUAL_CHECK]\\nYou are an expert...\"\n",
    "\n",
    "# # 1. Test cleaning and splitting\n",
    "# reasoning, fa = clean_and_split_solution(test_raw_solution)\n",
    "# print(f\"--- Cleaned Reasoning ---\\n{reasoning}\\n--------------------\")\n",
    "# print(f\"--- Final Answer ---\\n{fa}\\n--------------------\")\n",
    "\n",
    "# # 2. Test JSON conversion with FA\n",
    "# solution_json = convert_solution_to_json_str(reasoning, fa)\n",
    "# print(f\"--- Solution as JSON String ---\\n{solution_json}\\n-----------------------------\")\n",
    "\n",
    "# # 3. Test equation extraction (should be unchanged)\n",
    "# equations_json = extract_equations_from_annotations(test_raw_solution)\n",
    "# print(f\"--- Extracted Equations ---\\n{equations_json}\\n-------------------------\")\n",
    "\n",
    "# # 4. Test prompt formatting (should be unchanged)\n",
    "# final_prompt = create_formatted_prompt(test_system_prompt, test_problem, solution_json)\n",
    "# print(f\"--- Final Formatted Prompt ---\\n{final_prompt}\\n----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8db0fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "--- Verifying REVISED Utility Functions on Live Data Samples ---\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- Testing on: Programmatic Computational Errors ---\n",
      "\n",
      "--- Programmatic Sample 1/3 (Index: 3612) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "He has 30*6=<<30*6=810>>810 students\n",
      "So he needs to buy 810*10=<<810*10=8100>>8100 index cards\n",
      "That means he needs to buy 8100/50=<<8100/50=162>>162 packs of index cards\n",
      "So he spends 162*3=$<<162*3=486>>486\n",
      "#### 486\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "He has 30*6=810 students\n",
      "So he needs to buy 810*10=8100 index cards\n",
      "That means he needs to buy 8100/50=162 packs of index cards\n",
      "So he spends 162*3=$486\n",
      "---\n",
      "Final Answer: '486'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"He has 30*6=810 students\",\n",
      "  \"L2\": \"So he needs to buy 810*10=8100 index cards\",\n",
      "  \"L3\": \"That means he needs to buy 8100/50=162 packs of index cards\",\n",
      "  \"L4\": \"So he spends 162*3=$486\",\n",
      "  \"FA\": \"486\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"30*6=810\",\n",
      "  \"L2\": \"810*10=8100\",\n",
      "  \"L3\": \"8100/50=162\",\n",
      "  \"L4\": \"162*3=486\"\n",
      "}\n",
      "---\n",
      "\n",
      "--- Programmatic Sample 2/3 (Index: 991) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "First convert the total length of the journey from kilometers to meters: 9 kilometers * 1000 meters/kilometer = <<9*1000=9000>>9000 meters\n",
      "Then divide this by the length of one step to find the number of steps in the journey: 9000 meters / 3 meters/step = <<9000/3=3000>>3000 steps\n",
      "Then multiply the number of steps by the number of flowers per step to find how many flowers one unicorn creates on the journey: 3000 steps * 4 flowers/step = <<3000*4=1200>>1200 flowers\n",
      "Then multiply the number of flowers per unicorn by the number of unicorns to find the total number of flowers created: 1200 flowers/unicorn * 6 unicorns = 7200 flowers\n",
      "#### 7200\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "First convert the total length of the journey from kilometers to meters: 9 kilometers * 1000 meters/kilometer = 9000 meters\n",
      "Then divide this by the length of one step to find the number of steps in the journey: 9000 meters / 3 meters/step = 3000 steps\n",
      "Then multiply the number of steps by the number of flowers per step to find how many flowers one unicorn creates on the journey: 3000 steps * 4 flowers/step = 1200 flowers\n",
      "Then multiply the number of flowers per unicorn by the number of unicorns to find the total number of flowers created: 1200 flowers/unicorn * 6 unicorns = 7200 flowers\n",
      "---\n",
      "Final Answer: '7200'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"First convert the total length of the journey from kilometers to meters: 9 kilometers * 1000 meters/kilometer = 9000 meters\",\n",
      "  \"L2\": \"Then divide this by the length of one step to find the number of steps in the journey: 9000 meters / 3 meters/step = 3000 steps\",\n",
      "  \"L3\": \"Then multiply the number of steps by the number of flowers per step to find how many flowers one unicorn creates on the journey: 3000 steps * 4 flowers/step = 1200 flowers\",\n",
      "  \"L4\": \"Then multiply the number of flowers per unicorn by the number of unicorns to find the total number of flowers created: 1200 flowers/unicorn * 6 unicorns = 7200 flowers\",\n",
      "  \"FA\": \"7200\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"9*1000=9000\",\n",
      "  \"L2\": \"9000/3=3000\",\n",
      "  \"L3\": \"3000*4=1200\"\n",
      "}\n",
      "---\n",
      "\n",
      "--- Programmatic Sample 3/3 (Index: 305) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "The books will cost $40 because 8 x 5 = <<8*5=40>>40\n",
      "He needs to save up $72 because 40 - 13 = <<40-13=72>>72\n",
      "#### 72\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "The books will cost $40 because 8 x 5 = 40\n",
      "He needs to save up $72 because 40 - 13 = 72\n",
      "---\n",
      "Final Answer: '72'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"The books will cost $40 because 8 x 5 = 40\",\n",
      "  \"L2\": \"He needs to save up $72 because 40 - 13 = 72\",\n",
      "  \"FA\": \"72\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"8*5=40\",\n",
      "  \"L2\": \"40-13=72\"\n",
      "}\n",
      "---\n",
      "\n",
      "\n",
      "--- Testing on: Manually Validated Conceptual Errors ---\n",
      "Skipping manual samples: Data list is empty.\n",
      "\n",
      "\n",
      "--- Testing on: Original GSM8K Correct Solutions ---\n",
      "\n",
      "--- Original GSM8K Sample 1/3 (Index: 3346) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "Let X be the amount Susan had originally. Susan spent 1/5*X in September, 1/4*X in October, and $120 in November.\n",
      "Susan has X - 1/5*X - 1/4*X - $120 = $540 left.\n",
      "Combining like terms, we get 11/20*X - $120 = $540\n",
      "Adding $120 to both sides, we get 11/20*X = $660.\n",
      "Dividing both sides by 11/20 we get X = $1200.\n",
      "#### 1200\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "Let X be the amount Susan had originally. Susan spent 1/5*X in September, 1/4*X in October, and $120 in November.\n",
      "Susan has X - 1/5*X - 1/4*X - $120 = $540 left.\n",
      "Combining like terms, we get 11/20*X - $120 = $540\n",
      "Adding $120 to both sides, we get 11/20*X = $660.\n",
      "Dividing both sides by 11/20 we get X = $1200.\n",
      "---\n",
      "Final Answer: '1200'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"Let X be the amount Susan had originally. Susan spent 1/5*X in September, 1/4*X in October, and $120 in November.\",\n",
      "  \"L2\": \"Susan has X - 1/5*X - 1/4*X - $120 = $540 left.\",\n",
      "  \"L3\": \"Combining like terms, we get 11/20*X - $120 = $540\",\n",
      "  \"L4\": \"Adding $120 to both sides, we get 11/20*X = $660.\",\n",
      "  \"L5\": \"Dividing both sides by 11/20 we get X = $1200.\",\n",
      "  \"FA\": \"1200\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{}\n",
      "---\n",
      "\n",
      "--- Original GSM8K Sample 2/3 (Index: 4260) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "Alex is inviting 84 * 2 / 3 = <<84*2/3=56>>56 guests.\n",
      "Bridgette and Alexâ€™s guests will need 84 + 56 = <<84+56=140>>140 plates.\n",
      "The caterer makes 10 extra plates, so they will need 140 + 10 = <<140+10=150>>150 plates.\n",
      "To put 8 asparagus spears on each plate, the caterer will need 150 * 8 = <<150*8=1200>>1200 asparagus spears.\n",
      "#### 1200\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "Alex is inviting 84 * 2 / 3 = 56 guests.\n",
      "Bridgette and Alex's guests will need 84 + 56 = 140 plates.\n",
      "The caterer makes 10 extra plates, so they will need 140 + 10 = 150 plates.\n",
      "To put 8 asparagus spears on each plate, the caterer will need 150 * 8 = 1200 asparagus spears.\n",
      "---\n",
      "Final Answer: '1200'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"Alex is inviting 84 * 2 / 3 = 56 guests.\",\n",
      "  \"L2\": \"Bridgette and Alex's guests will need 84 + 56 = 140 plates.\",\n",
      "  \"L3\": \"The caterer makes 10 extra plates, so they will need 140 + 10 = 150 plates.\",\n",
      "  \"L4\": \"To put 8 asparagus spears on each plate, the caterer will need 150 * 8 = 1200 asparagus spears.\",\n",
      "  \"FA\": \"1200\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"84*2/3=56\",\n",
      "  \"L2\": \"84+56=140\",\n",
      "  \"L3\": \"140+10=150\",\n",
      "  \"L4\": \"150*8=1200\"\n",
      "}\n",
      "---\n",
      "\n",
      "--- Original GSM8K Sample 3/3 (Index: 4439) ---\n",
      "Original Raw Solution:\n",
      "---\n",
      "First find how much the store makes in profit: $20 * 0.30 = $<<20*0.30=6>>6 for profit\n",
      "Then find the regular selling price: $20 + $6 = $<<20+6=26>>26 total price\n",
      "Then multiply the regular price by the discount to get $26 * 0.50 = $<<26*0.50=13>>13 for the final price\n",
      "#### 13\n",
      "---\n",
      "Cleaned Reasoning:\n",
      "---\n",
      "First find how much the store makes in profit: $20 * 0.30 = $6 for profit\n",
      "Then find the regular selling price: $20 + $6 = $26 total price\n",
      "Then multiply the regular price by the discount to get $26 * 0.50 = $13 for the final price\n",
      "---\n",
      "Final Answer: '13'\n",
      "Solution as JSON:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"First find how much the store makes in profit: $20 * 0.30 = $6 for profit\",\n",
      "  \"L2\": \"Then find the regular selling price: $20 + $6 = $26 total price\",\n",
      "  \"L3\": \"Then multiply the regular price by the discount to get $26 * 0.50 = $13 for the final price\",\n",
      "  \"FA\": \"13\"\n",
      "}\n",
      "---\n",
      "Extracted Equations:\n",
      "---\n",
      "{\n",
      "  \"L1\": \"20*0.30=6\",\n",
      "  \"L2\": \"20+6=26\",\n",
      "  \"L3\": \"26*0.50=13\"\n",
      "}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def verify_utilities_on_samples(num_samples: int = 3):\n",
    "    \"\"\"\n",
    "    Selects random samples from each data source (programmatic, manual, original)\n",
    "    and runs them through the REVISED data transformation utilities to verify their output.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"--- Verifying REVISED Utility Functions on Live Data Samples ---\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- 1. Test on Programmatically Generated Computational Errors ---\n",
    "    print(\"\\n\\n--- Testing on: Programmatic Computational Errors ---\")\n",
    "    if not programmatic_comp_df.empty:\n",
    "        valid_programmatic_samples = programmatic_comp_df.dropna(subset=['filepath']).sample(n=num_samples, random_state=RANDOM_SEED)\n",
    "        \n",
    "        for i, (_, row) in enumerate(valid_programmatic_samples.iterrows()):\n",
    "            print(f\"\\n--- Programmatic Sample {i+1}/{num_samples} (Index: {row['index']}) ---\")\n",
    "            try:\n",
    "                filepath = PROJECT_ROOT / row['filepath']\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                raw_solution = data['flawed_nl_solution']\n",
    "                problem_text = gsm8k_problem_lookup[row['index']]['question']\n",
    "                \n",
    "                print(f\"Original Raw Solution:\\n---\\n{raw_solution}\\n---\")\n",
    "                reasoning, fa = clean_and_split_solution(raw_solution)\n",
    "                print(f\"Cleaned Reasoning:\\n---\\n{reasoning}\\n---\")\n",
    "                print(f\"Final Answer: '{fa}'\")\n",
    "                solution_json = convert_solution_to_json_str(reasoning, fa)\n",
    "                print(f\"Solution as JSON:\\n---\\n{solution_json}\\n---\")\n",
    "                equations_json = extract_equations_from_annotations(raw_solution)\n",
    "                print(f\"Extracted Equations:\\n---\\n{equations_json}\\n---\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing sample: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping programmatic samples: DataFrame is empty.\")\n",
    "\n",
    "    # --- 2. Test on Manually Validated Conceptual Errors ---\n",
    "    print(\"\\n\\n--- Testing on: Manually Validated Conceptual Errors ---\")\n",
    "    if manual_conceptual_data:\n",
    "        k = min(num_samples, len(manual_conceptual_data))\n",
    "        random_manual_samples = random.sample(manual_conceptual_data, k=k)\n",
    "        \n",
    "        for i, sample in enumerate(random_manual_samples):\n",
    "            print(f\"\\n--- Manual Sample {i+1}/{k} (Index: {sample['index']}) ---\")\n",
    "            try:\n",
    "                raw_solution = sample['flawed_nl_solution']\n",
    "                problem_text = sample['question']\n",
    "\n",
    "                print(f\"Original Raw Solution:\\n---\\n{raw_solution}\\n---\")\n",
    "                reasoning, fa = clean_and_split_solution(raw_solution)\n",
    "                print(f\"Cleaned Reasoning:\\n---\\n{reasoning}\\n---\")\n",
    "                print(f\"Final Answer: '{fa}'\")\n",
    "                solution_json = convert_solution_to_json_str(reasoning, fa)\n",
    "                print(f\"Solution as JSON:\\n---\\n{solution_json}\\n---\")\n",
    "                equations_json = extract_equations_from_annotations(raw_solution)\n",
    "                print(f\"Extracted Equations:\\n---\\n{equations_json}\\n---\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing sample: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping manual samples: Data list is empty.\")\n",
    "\n",
    "    # --- 3. Test on Original GSM8K Correct Solutions ---\n",
    "    print(\"\\n\\n--- Testing on: Original GSM8K Correct Solutions ---\")\n",
    "    if gsm8k_problem_lookup:\n",
    "        random_indices = random.sample(list(gsm8k_problem_lookup.keys()), k=num_samples)\n",
    "\n",
    "        for i, idx in enumerate(random_indices):\n",
    "            print(f\"\\n--- Original GSM8K Sample {i+1}/{num_samples} (Index: {idx}) ---\")\n",
    "            try:\n",
    "                sample = gsm8k_problem_lookup[idx]\n",
    "                raw_solution = sample['answer']\n",
    "                problem_text = sample['question']\n",
    "                \n",
    "                print(f\"Original Raw Solution:\\n---\\n{raw_solution}\\n---\")\n",
    "                reasoning, fa = clean_and_split_solution(raw_solution)\n",
    "                print(f\"Cleaned Reasoning:\\n---\\n{reasoning}\\n---\")\n",
    "                print(f\"Final Answer: '{fa}'\")\n",
    "                solution_json = convert_solution_to_json_str(reasoning, fa)\n",
    "                print(f\"Solution as JSON:\\n---\\n{solution_json}\\n---\")\n",
    "                equations_json = extract_equations_from_annotations(raw_solution)\n",
    "                print(f\"Extracted Equations:\\n---\\n{equations_json}\\n---\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing sample: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping original GSM8K samples: Lookup dictionary is empty.\")\n",
    "\n",
    "# Set a new random seed for sampling to get different samples each run if desired\n",
    "random.seed(RANDOM_SEED + 2) \n",
    "verify_utilities_on_samples(num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6300b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1963 records from manually_generated_errors_final.csv\n",
      "\n",
      "================================================================================\n",
      "--- Analyzing Annotation Coverage in Source Datasets (v2) ---\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bdb05b69594aa6bc174f45fc733c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Original GSM8K:   0%|          | 0/7473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original GSM8K Dataset:\n",
      "  - Samples with at least one missing annotation: 1322 / 7473 (17.69%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206bb071cc9f49868f4d4d979a8d1375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Programmatic Errors:   0%|          | 0/6636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Programmatic Computational Errors:\n",
      "  - Samples with at least one missing annotation: 980 / 6636 (14.77%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390ce53b4374138869854af0db45490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Manual Errors CSV:   0%|          | 0/1963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually Generated Errors CSV:\n",
      "  - Correct solutions ('answer' col) with missing annotations: 349 / 1963 (17.78%)\n",
      "  - Flawed solutions ('wrong_answer' col) with missing annotations: 344 / 1963 (17.52%)\n"
     ]
    }
   ],
   "source": [
    "# --- Load the consolidated manual errors CSV ---\n",
    "# This should be defined in Cell 2, but we'll place it here for clarity in this snippet.\n",
    "MANUAL_ERRORS_CSV_PATH = DATA_DIR / \"manually_generated_errors_final.csv\"\n",
    "try:\n",
    "    manual_errors_df = pd.read_csv(MANUAL_ERRORS_CSV_PATH)\n",
    "    print(f\"Successfully loaded {len(manual_errors_df)} records from {MANUAL_ERRORS_CSV_PATH.name}\")\n",
    "except FileNotFoundError:\n",
    "    manual_errors_df = pd.DataFrame()\n",
    "    print(f\"WARNING: Manual errors CSV not found at {MANUAL_ERRORS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "def check_annotation_coverage(solution_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if every line containing an '=' sign also has a calculator annotation.\n",
    "    Returns True if coverage is complete, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(solution_text, str) or pd.isna(solution_text):\n",
    "        return True # Vacuously true for empty/invalid input\n",
    "\n",
    "    # Remove the final answer line to avoid checking it\n",
    "    lines = solution_text.split('\\n')\n",
    "    if lines and re.match(r'^\\s*####\\s*.*$', lines[-1]):\n",
    "        lines.pop()\n",
    "        \n",
    "    for line in lines:\n",
    "        if '=' in line:\n",
    "            if '<<' not in line or '>>' not in line:\n",
    "                return False # Found a line with a calculation but no annotation\n",
    "                \n",
    "    return True\n",
    "\n",
    "# --- Analyze All Data Sources with Corrected Logic for Manual Errors ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Analyzing Annotation Coverage in Source Datasets (v2) ---\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. Analyze Original GSM8K Dataset ---\n",
    "total_gsm8k = len(GSM8K_DATASET)\n",
    "incomplete_gsm8k_count = 0\n",
    "for sample in tqdm(GSM8K_DATASET, desc=\"Analyzing Original GSM8K\"):\n",
    "    if not check_annotation_coverage(sample['answer']):\n",
    "        incomplete_gsm8k_count += 1\n",
    "\n",
    "print(f\"\\nOriginal GSM8K Dataset:\")\n",
    "print(f\"  - Samples with at least one missing annotation: {incomplete_gsm8k_count} / {total_gsm8k} ({incomplete_gsm8k_count/total_gsm8k:.2%})\")\n",
    "\n",
    "\n",
    "# --- 2. Analyze Programmatic Computational Errors ---\n",
    "total_programmatic = 0\n",
    "incomplete_programmatic_count = 0\n",
    "unique_programmatic_indices = programmatic_comp_df.dropna(subset=['filepath'])['index'].unique()\n",
    "\n",
    "for idx in tqdm(unique_programmatic_indices, desc=\"Analyzing Programmatic Errors\"):\n",
    "    row = programmatic_comp_df[programmatic_comp_df['index'] == idx].iloc[0]\n",
    "    try:\n",
    "        filepath = PROJECT_ROOT / row['filepath']\n",
    "        with open(filepath, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "        solution_text = data.get('flawed_nl_solution')\n",
    "        if solution_text:\n",
    "            total_programmatic += 1\n",
    "            if not check_annotation_coverage(solution_text):\n",
    "                incomplete_programmatic_count += 1\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "\n",
    "if total_programmatic > 0:\n",
    "    print(f\"\\nProgrammatic Computational Errors:\")\n",
    "    print(f\"  - Samples with at least one missing annotation: {incomplete_programmatic_count} / {total_programmatic} ({incomplete_programmatic_count/total_programmatic:.2%})\")\n",
    "else:\n",
    "    print(\"\\nProgrammatic Computational Errors: No valid files to analyze.\")\n",
    "\n",
    "\n",
    "# --- 3. Analyze Manually Generated Errors (from the correct CSV) ---\n",
    "total_manual = len(manual_errors_df)\n",
    "incomplete_manual_correct_count = 0\n",
    "incomplete_manual_flawed_count = 0\n",
    "\n",
    "if total_manual > 0:\n",
    "    for _, row in tqdm(manual_errors_df.iterrows(), total=total_manual, desc=\"Analyzing Manual Errors CSV\"):\n",
    "        # Check the 'answer' column (correct solutions)\n",
    "        if not check_annotation_coverage(row['answer']):\n",
    "            incomplete_manual_correct_count += 1\n",
    "        # Check the 'wrong_answer' column (flawed solutions)\n",
    "        if not check_annotation_coverage(row['wrong_answer']):\n",
    "            incomplete_manual_flawed_count += 1\n",
    "            \n",
    "    print(f\"\\nManually Generated Errors CSV:\")\n",
    "    print(f\"  - Correct solutions ('answer' col) with missing annotations: {incomplete_manual_correct_count} / {total_manual} ({incomplete_manual_correct_count/total_manual:.2%})\")\n",
    "    print(f\"  - Flawed solutions ('wrong_answer' col) with missing annotations: {incomplete_manual_flawed_count} / {total_manual} ({incomplete_manual_flawed_count/total_manual:.2%})\")\n",
    "else:\n",
    "    print(\"\\nManually Generated Errors CSV: No data to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b5ae8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt templates defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompts for each task as constants\n",
    "# These will be used by the create_formatted_prompt function.\n",
    "\n",
    "SYSTEM_PROMPT_CONCEPTUAL = \"\"\"[CONCEPTUAL_CHECK]\n",
    "\n",
    "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
    "\n",
    "- Does the solution use the correct numbers from the problem?\n",
    "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
    "- Is the overall logical flow of the steps correct?\n",
    "\n",
    "If the conceptual logic is sound, your entire output must be the single word:\n",
    "None\n",
    "\n",
    "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_EXTRACTION = \"\"\"[EXTRACT_CALCULATIONS]\n",
    "\n",
    "You are a data extraction tool. Your task is to parse the provided solution, which is formatted as a JSON dictionary, and extract every mathematical calculation into a new JSON dictionary.\n",
    "\n",
    "- The keys of the output dictionary should be the line numbers (e.g., \"L1\", \"L2\").\n",
    "- The values should be a string containing the full equation as written (e.g., \"10 + 5 = 15\").\n",
    "- If a line contains no calculation, it should be omitted from the output dictionary.\n",
    "- If the entire solution contains no calculations, return an empty JSON dictionary: {}\n",
    "\n",
    "Your output must be ONLY the JSON object and nothing else.\"\"\"\n",
    "\n",
    "print(\"System prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ec43ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Step 1: Pre-filtering all sources for complete annotation coverage...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82226259bead41728068c1e8a4d9ab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Manual CSV:   0%|          | 0/1963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Found 1,196 manually generated problems with full annotation coverage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595f0b16518d40bca9bac98a08475ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Programmatic Catalog:   0%|          | 0/7304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Found 5,585 programmatically generated problems with full annotation coverage.\n",
      "\n",
      "================================================================================\n",
      "Step 2: Defining dataset quotas and identifying core problems for Task A...\n",
      "Anchor: Found 1030 unique manual conceptual problems (these are the 'core problems').\n",
      "Task A Quotas: 1030 conceptual, 515 correct, 515 computational.\n",
      "Task B Quota: 772 correct/flawed pairs (from non-core problems).\n",
      "\n",
      "================================================================================\n",
      "Step 3: Assembling dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae1f14d1eb84736be030667f845dd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task A - Conceptual Flawed:   0%|          | 0/1030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b585191525b14786bff51c910a268364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task A - Computational (Manual):   0%|          | 0/252 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21609b37e89f4a4ab6ec34bd598e7c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task A - Computational (Prog):   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c92173188bb4b6d9a91fd8afea8f553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task A - Correct:   0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239106bf129048cbb57fc3cf272baba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task B - Extraction Pairs (Manual):   0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c31ae1bdb314fe889dc059b627fe19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Task B - Extraction Pairs (Prog):   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Data Assembly Complete: Final Summary ---\n",
      "Total SFT samples generated: 3,565\n",
      "Total unique problems used for Task A: 1,030\n",
      "Total unique problems used for Task B: 772\n",
      "\n",
      "--- Final Dataset Composition ---\n",
      "               task                 type  count\n",
      "   conceptual_check computational_flawed    476\n",
      "   conceptual_check    conceptual_flawed   1030\n",
      "   conceptual_check     correct_original    515\n",
      "equation_extraction computational_paired    772\n",
      "equation_extraction       correct_paired    772\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Pre-filter for Annotation Coverage (for Task B) ---\n",
    "print(\"=\"*80)\n",
    "print(\"Step 1: Pre-filtering all sources for complete annotation coverage...\")\n",
    "\n",
    "perfect_manual_indices = set()\n",
    "if not manual_errors_df.empty:\n",
    "    for _, row in tqdm(manual_errors_df.iterrows(), total=len(manual_errors_df), desc=\"Analyzing Manual CSV\"):\n",
    "        if check_annotation_coverage(row['answer']) and check_annotation_coverage(row['wrong_answer']):\n",
    "            perfect_manual_indices.add(row['index'])\n",
    "print(f\"-> Found {len(perfect_manual_indices):,} manually generated problems with full annotation coverage.\")\n",
    "\n",
    "perfect_programmatic_indices = set()\n",
    "if not programmatic_comp_df.empty:\n",
    "    unique_indices = programmatic_comp_df['index'].unique()\n",
    "    for idx in tqdm(unique_indices, desc=\"Analyzing Programmatic Catalog\"):\n",
    "        original_answer = gsm8k_problem_lookup.get(idx, {}).get('answer')\n",
    "        if not original_answer or not check_annotation_coverage(original_answer): continue\n",
    "        rows = programmatic_comp_df[programmatic_comp_df['index'] == idx]\n",
    "        is_perfect = False\n",
    "        for _, row in rows.iterrows():\n",
    "            filepath_val = row.get('filepath')\n",
    "            if pd.isna(filepath_val): continue\n",
    "            try:\n",
    "                filepath = PROJECT_ROOT / str(filepath_val)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "                if check_annotation_coverage(data.get('flawed_nl_solution')):\n",
    "                    is_perfect = True\n",
    "                    break\n",
    "            except Exception: continue\n",
    "        if is_perfect: perfect_programmatic_indices.add(idx)\n",
    "print(f\"-> Found {len(perfect_programmatic_indices):,} programmatically generated problems with full annotation coverage.\")\n",
    "all_perfect_indices_for_B = perfect_manual_indices.union(perfect_programmatic_indices)\n",
    "\n",
    "# --- 2. Define Quotas and Core Problems ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: Defining dataset quotas and identifying core problems for Task A...\")\n",
    "\n",
    "conceptual_df = manual_errors_df[manual_errors_df['error_type'] == 'conceptual'].copy()\n",
    "core_problem_indices = sorted(list(conceptual_df['index'].unique()))\n",
    "N_CONCEPTUAL = len(core_problem_indices)\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "rng.shuffle(core_problem_indices) # Shuffle the list of core problems for random assignment\n",
    "\n",
    "QUOTA_A_CONCEPTUAL = N_CONCEPTUAL\n",
    "QUOTA_A_COMPUTATIONAL = N_CONCEPTUAL // 2\n",
    "QUOTA_A_CORRECT = N_CONCEPTUAL - QUOTA_A_COMPUTATIONAL # The remainder\n",
    "QUOTA_B_PAIRS = (N_CONCEPTUAL * 3) // 4\n",
    "\n",
    "print(f\"Anchor: Found {N_CONCEPTUAL} unique manual conceptual problems (these are the 'core problems').\")\n",
    "print(f\"Task A Quotas: {QUOTA_A_CONCEPTUAL} conceptual, {QUOTA_A_CORRECT} correct, {QUOTA_A_COMPUTATIONAL} computational.\")\n",
    "print(f\"Task B Quota: {QUOTA_B_PAIRS} correct/flawed pairs (from non-core problems).\")\n",
    "\n",
    "# --- 3. Assemble Dataset Sequentially ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 3: Assembling dataset...\")\n",
    "\n",
    "sft_samples, metadata_log = [], []\n",
    "used_indices_A, used_indices_B = set(), set()\n",
    "\n",
    "# --- TASK A ASSEMBLY (from Core Problems only) ---\n",
    "# A1: Add all N conceptual flawed samples\n",
    "for idx in tqdm(core_problem_indices, desc=\"Task A - Conceptual Flawed\"):\n",
    "    row = conceptual_df[conceptual_df['index'] == idx].sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "    f_reasoning, f_fa = clean_and_split_solution(row['wrong_answer'])\n",
    "    f_solution_json_str = convert_solution_to_json_str(f_reasoning, f_fa)\n",
    "    explanation = f\"{row['erroneous_line_number']}: {row['explanation']}\"\n",
    "    prompt = create_formatted_prompt(SYSTEM_PROMPT_CONCEPTUAL, row['question'], f_solution_json_str)\n",
    "    sft_samples.append({\"text\": f\"{prompt}<|assistant|>{explanation}<end>\", \"task\": \"conceptual_check\"})\n",
    "    metadata_log.append({\"index\": idx, \"task\": \"conceptual_check\", \"type\": \"conceptual_flawed\", \"source\": row['filepath']})\n",
    "    used_indices_A.add(idx)\n",
    "\n",
    "# A2: Add N/2 computational flawed samples, drawn from core problems\n",
    "indices_for_comp = core_problem_indices[:QUOTA_A_COMPUTATIONAL]\n",
    "indices_for_correct = core_problem_indices[QUOTA_A_COMPUTATIONAL:]\n",
    "\n",
    "# Manual First\n",
    "comp_manual_df = manual_errors_df[(manual_errors_df['error_type'] == 'computational') & (manual_errors_df['index'].isin(indices_for_comp))].copy()\n",
    "manual_indices_found = list(comp_manual_df['index'].unique())\n",
    "for idx in tqdm(manual_indices_found, desc=f\"Task A - Computational (Manual)\"):\n",
    "    row = comp_manual_df[comp_manual_df['index'] == idx].sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "    f_reasoning, f_fa = clean_and_split_solution(row['wrong_answer'])\n",
    "    f_solution_json_str = convert_solution_to_json_str(f_reasoning, f_fa)\n",
    "    prompt = create_formatted_prompt(SYSTEM_PROMPT_CONCEPTUAL, row['question'], f_solution_json_str)\n",
    "    sft_samples.append({\"text\": f\"{prompt}<|assistant|>None<end>\", \"task\": \"conceptual_check\"})\n",
    "    metadata_log.append({\"index\": idx, \"task\": \"conceptual_check\", \"type\": \"computational_flawed\", \"source\": row['filepath']})\n",
    "\n",
    "# Programmatic to fill the rest\n",
    "num_still_needed = QUOTA_A_COMPUTATIONAL - len(manual_indices_found)\n",
    "prog_candidates = [idx for idx in indices_for_comp if idx not in manual_indices_found]\n",
    "prog_indices_to_take = prog_candidates[:num_still_needed]\n",
    "\n",
    "if num_still_needed > 0 and not programmatic_comp_df.empty:\n",
    "    prog_comp_df_A = programmatic_comp_df[programmatic_comp_df['index'].isin(prog_indices_to_take)].copy()\n",
    "    for idx in tqdm(prog_indices_to_take, desc=f\"Task A - Computational (Prog)\"):\n",
    "        rows = prog_comp_df_A[prog_comp_df_A['index'] == idx]\n",
    "        if rows.empty: continue\n",
    "        row = rows.sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "        filepath_val = row.get('filepath')\n",
    "        if pd.isna(filepath_val): continue\n",
    "        try:\n",
    "            filepath = PROJECT_ROOT / str(filepath_val)\n",
    "            with open(filepath, 'r') as f: data = json.load(f)\n",
    "            problem_text = gsm8k_problem_lookup[idx]['question']\n",
    "            f_reasoning, f_fa = clean_and_split_solution(data['flawed_nl_solution'])\n",
    "            f_solution_json_str = convert_solution_to_json_str(f_reasoning, f_fa)\n",
    "            prompt = create_formatted_prompt(SYSTEM_PROMPT_CONCEPTUAL, problem_text, f_solution_json_str)\n",
    "            sft_samples.append({\"text\": f\"{prompt}<|assistant|>None<end>\", \"task\": \"conceptual_check\"})\n",
    "            metadata_log.append({\"index\": idx, \"task\": \"conceptual_check\", \"type\": \"computational_flawed\", \"source\": row['filepath']})\n",
    "        except Exception: continue\n",
    "\n",
    "# A3: Add remaining N/2 correct original samples\n",
    "for idx in tqdm(indices_for_correct, desc=f\"Task A - Correct\"):\n",
    "    problem = gsm8k_problem_lookup[idx]\n",
    "    c_reasoning, c_fa = clean_and_split_solution(problem['answer'])\n",
    "    c_solution_json_str = convert_solution_to_json_str(c_reasoning, c_fa)\n",
    "    prompt = create_formatted_prompt(SYSTEM_PROMPT_CONCEPTUAL, problem['question'], c_solution_json_str)\n",
    "    sft_samples.append({\"text\": f\"{prompt}<|assistant|>None<end>\", \"task\": \"conceptual_check\"})\n",
    "    metadata_log.append({\"index\": idx, \"task\": \"conceptual_check\", \"type\": \"correct_original\", \"source\": \"gsm8k\"})\n",
    "\n",
    "# --- TASK B ASSEMBLY (from non-Core problems only) ---\n",
    "# Manual First\n",
    "b_manual_df = manual_errors_df[(manual_errors_df['error_type'] == 'computational') & (~manual_errors_df['index'].isin(used_indices_A)) & (manual_errors_df['index'].isin(all_perfect_indices_for_B))].copy()\n",
    "b_manual_indices = list(b_manual_df['index'].unique())\n",
    "rng.shuffle(b_manual_indices)\n",
    "b_indices_to_take_manual = b_manual_indices[:QUOTA_B_PAIRS]\n",
    "\n",
    "for idx in tqdm(b_indices_to_take_manual, desc=f\"Task B - Extraction Pairs (Manual)\"):\n",
    "    row = b_manual_df[b_manual_df['index'] == idx].sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "    # Correct\n",
    "    c_reasoning, c_fa = clean_and_split_solution(row['answer'])\n",
    "    c_solution_json_str = convert_solution_to_json_str(c_reasoning, c_fa)\n",
    "    c_equations = extract_equations_from_annotations(row['answer'])\n",
    "    c_prompt = create_formatted_prompt(SYSTEM_PROMPT_EXTRACTION, row['question'], c_solution_json_str)\n",
    "    sft_samples.append({\"text\": f\"{c_prompt}<|assistant|>{c_equations}<end>\", \"task\": \"equation_extraction\"})\n",
    "    metadata_log.append({\"index\": idx, \"task\": \"equation_extraction\", \"type\": \"correct_paired\", \"source\": \"gsm8k\"})\n",
    "    # Flawed\n",
    "    f_reasoning, f_fa = clean_and_split_solution(row['wrong_answer'])\n",
    "    f_solution_json_str = convert_solution_to_json_str(f_reasoning, f_fa)\n",
    "    f_equations = extract_equations_from_annotations(row['wrong_answer'])\n",
    "    f_prompt = create_formatted_prompt(SYSTEM_PROMPT_EXTRACTION, row['question'], f_solution_json_str)\n",
    "    sft_samples.append({\"text\": f\"{f_prompt}<|assistant|>{f_equations}<end>\", \"task\": \"equation_extraction\"})\n",
    "    metadata_log.append({\"index\": idx, \"task\": \"equation_extraction\", \"type\": \"computational_paired\", \"source\": row['filepath']})\n",
    "    used_indices_B.add(idx)\n",
    "\n",
    "# Programmatic to fill the rest\n",
    "num_still_needed_b = QUOTA_B_PAIRS - len(b_indices_to_take_manual)\n",
    "if num_still_needed_b > 0:\n",
    "    b_prog_df = programmatic_comp_df[(~programmatic_comp_df['index'].isin(used_indices_A)) & (~programmatic_comp_df['index'].isin(used_indices_B)) & (programmatic_comp_df['index'].isin(all_perfect_indices_for_B))].copy()\n",
    "    b_prog_indices = list(b_prog_df['index'].unique())\n",
    "    rng.shuffle(b_prog_indices)\n",
    "    b_indices_to_take_prog = b_prog_indices[:num_still_needed_b]\n",
    "    for idx in tqdm(b_indices_to_take_prog, desc=f\"Task B - Extraction Pairs (Prog)\"):\n",
    "        row = b_prog_df[b_prog_df['index'] == idx].sample(n=1, random_state=rng.randint(0, 10**9)).iloc[0]\n",
    "        filepath_val = row.get('filepath')\n",
    "        if pd.isna(filepath_val): continue\n",
    "        try:\n",
    "            filepath = PROJECT_ROOT / str(filepath_val)\n",
    "            with open(filepath, 'r') as f: data = json.load(f)\n",
    "            problem = gsm8k_problem_lookup[idx]\n",
    "            # Correct\n",
    "            c_reasoning, c_fa = clean_and_split_solution(problem['answer'])\n",
    "            c_solution_json_str = convert_solution_to_json_str(c_reasoning, c_fa)\n",
    "            c_equations = extract_equations_from_annotations(problem['answer'])\n",
    "            c_prompt = create_formatted_prompt(SYSTEM_PROMPT_EXTRACTION, problem['question'], c_solution_json_str)\n",
    "            sft_samples.append({\"text\": f\"{c_prompt}<|assistant|>{c_equations}<end>\", \"task\": \"equation_extraction\"})\n",
    "            metadata_log.append({\"index\": idx, \"task\": \"equation_extraction\", \"type\": \"correct_paired\", \"source\": \"gsm8k\"})\n",
    "            # Flawed\n",
    "            f_reasoning, f_fa = clean_and_split_solution(data['flawed_nl_solution'])\n",
    "            f_solution_json_str = convert_solution_to_json_str(f_reasoning, f_fa)\n",
    "            f_equations = extract_equations_from_annotations(data['flawed_nl_solution'])\n",
    "            f_prompt = create_formatted_prompt(SYSTEM_PROMPT_EXTRACTION, problem['question'], f_solution_json_str)\n",
    "            sft_samples.append({\"text\": f\"{f_prompt}<|assistant|>{f_equations}<end>\", \"task\": \"equation_extraction\"})\n",
    "            metadata_log.append({\"index\": idx, \"task\": \"equation_extraction\", \"type\": \"computational_paired\", \"source\": row['filepath']})\n",
    "            used_indices_B.add(idx)\n",
    "        except Exception: continue\n",
    "\n",
    "# --- Final Summary ---\n",
    "final_metadata_df = pd.DataFrame(metadata_log)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Data Assembly Complete: Final Summary ---\")\n",
    "print(f\"Total SFT samples generated: {len(sft_samples):,}\")\n",
    "print(f\"Total unique problems used for Task A: {len(used_indices_A):,}\")\n",
    "print(f\"Total unique problems used for Task B: {len(used_indices_B):,}\")\n",
    "\n",
    "print(\"\\n--- Final Dataset Composition ---\")\n",
    "print(final_metadata_df.groupby(['task', 'type']).size().reset_index(name='count').to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "743b2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Ensure the metadata dataframe is created from the log, and sft_text is added.\n",
    "# This should be run after Cell 5 completes.\n",
    "final_metadata_df = pd.DataFrame(metadata_log)\n",
    "if 'sft_text' not in final_metadata_df.columns:\n",
    "    final_metadata_df['sft_text'] = [s['text'] for s in sft_samples]\n",
    "\n",
    "# Create a new random number generator for this cell to ensure variety\n",
    "rng = random.Random(RANDOM_SEED + 30)\n",
    "\n",
    "def visualize_samples(df: pd.DataFrame, description: str, num_samples: int = 1, **kwargs):\n",
    "    \"\"\"\n",
    "    Finds and prints a specified number of random samples from the dataframe\n",
    "    based on filter criteria.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{description} (Displaying {num_samples} sample(s))\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    query_parts = []\n",
    "    for col, val in kwargs.items():\n",
    "        if isinstance(val, str):\n",
    "            query_parts.append(f'`{col}` == \"{val}\"')\n",
    "        else:\n",
    "            query_parts.append(f'`{col}` == {val}')\n",
    "    query = ' & '.join(query_parts)\n",
    "    \n",
    "    try:\n",
    "        # Select n random samples that match the query\n",
    "        # Use min(num_samples, len(matching_df)) to avoid errors if fewer samples exist\n",
    "        matching_df = df.query(query)\n",
    "        n_to_sample = min(num_samples, len(matching_df))\n",
    "        if n_to_sample == 0:\n",
    "            raise ValueError(\"No samples found matching criteria.\")\n",
    "            \n",
    "        samples = matching_df.sample(n=n_to_sample, random_state=rng.randint(0, 10**9))\n",
    "        \n",
    "        for i, (_, sample) in enumerate(samples.iterrows()):\n",
    "            print(f\"\\n--- Sample {i+1}/{n_to_sample} ---\")\n",
    "            print(f\"METADATA: Index={sample['index']}, Task='{sample['task']}', Type='{sample['type']}'\")\n",
    "            \n",
    "            parts = sample['sft_text'].split('<|assistant|>')\n",
    "            prompt_part = parts[0] + '<|assistant|>'\n",
    "            response_part = parts[1]\n",
    "            \n",
    "            print(\"\\nPROMPT (What the model sees):\")\n",
    "            print(prompt_part)\n",
    "            \n",
    "            print(\"\\nRESPONSE (What the model must learn):\")\n",
    "            print(response_part)\n",
    "            \n",
    "        return samples['index'].tolist() # Return list of indices for paired lookups\n",
    "        \n",
    "    except (IndexError, ValueError) as e:\n",
    "        print(f\"\\nERROR: Could not find enough samples. Details: {e}\\nCriteria: {kwargs}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "17cada86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Request 1: Conceptual Flawed vs. Correct Original from Task A ---\n",
      "\n",
      "================================================================================\n",
      "1a: 'conceptual_flawed' samples (Displaying 2 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/2 ---\n",
      "METADATA: Index=986, Task='conceptual_check', Type='conceptual_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "Camden went swimming 16 times in March and Susannah went 24 times. If the number of times they went throughout the month was divided equally among 4 weeks, how many more times a week did Susannah swim than Camden?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"Camden went swimming 16/4 = 4 times a week\",\n",
      "  \"L2\": \"Susannah went swimming 24/4 = 6 times a week\",\n",
      "  \"L3\": \"Susannah went 6 + 4 = 10 more times a week than Camden\",\n",
      "  \"FA\": \"10\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "L3: Susannah went 6 - 4 =  2 more times a week than Camden, not 6 + 4 = 10<end>\n",
      "\n",
      "--- Sample 2/2 ---\n",
      "METADATA: Index=293, Task='conceptual_check', Type='conceptual_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "For every 1 minute that Carlotta sings on stage during the final opera performance, she spends an additional 3 minutes practicing and 5 minutes throwing temper tantrums. If her final stage performance is 6 minutes long, what is the total combined amount of time, in minutes, that she spends practicing, throwing tantrums, and singing in the final stage performance?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"For each 6 minutes she performs, Carlotta spends 6 * 3 minutes = 18 minutes practicing.\",\n",
      "  \"L2\": \"For each 6 minutes she performs, Carlotta throws tantrums for 6 * 5 minutes = 30 minutes.\",\n",
      "  \"L3\": \"Thus, in total, for each 6 minutes of performance time, she spends 18 minutes + 30 minutes = 48 minutes on all two activities.\",\n",
      "  \"FA\": \"48\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "L3: Forgot to include the 6 minutes performance.<end>\n",
      "\n",
      "================================================================================\n",
      "1b: 'correct_original' samples (Displaying 2 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/2 ---\n",
      "METADATA: Index=271, Task='conceptual_check', Type='correct_original'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "Nadia was sent to the flower shop to buy 20 roses and 3/4 times as many Lillies as roses. If roses cost $5 each and lilies cost twice as much each, calculate the total amount of money Nadia used to buy the flowers.\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"The total cost for roses is 20*5 = $100\",\n",
      "  \"L2\": \"Nadia bought 3/4*20 = 15 lilies.\",\n",
      "  \"L3\": \"Each lily costs 5*2 = $10\",\n",
      "  \"L4\": \"She used 10*15 = $150 in total to buy lilies.\",\n",
      "  \"L5\": \"To buy all the flowers, Nadia used 150+100 = $250\",\n",
      "  \"FA\": \"250\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "None<end>\n",
      "\n",
      "--- Sample 2/2 ---\n",
      "METADATA: Index=1251, Task='conceptual_check', Type='correct_original'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "Josh went to the shopping center. He bought 9 films and 4 books. He also bought 6 CDs. Each film cost $5, each book cost $4 and each CD cost $3. How much did Josh spend in all?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"The cost of the films is 9 * $5 = $45.\",\n",
      "  \"L2\": \"The cost of the books is 4 * $4 = $16.\",\n",
      "  \"L3\": \"The cost of the CDs is 6 * $3 = $18.\",\n",
      "  \"L4\": \"Josh spent $45 + $16 + $18 = $79 in total.\",\n",
      "  \"FA\": \"79\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "None<end>\n",
      "\n",
      "--- Request 2: Conceptual Flawed vs. Computational Flawed from Task A ---\n",
      "\n",
      "================================================================================\n",
      "2a: 'conceptual_flawed' samples (target should be an error string) (Displaying 2 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/2 ---\n",
      "METADATA: Index=823, Task='conceptual_check', Type='conceptual_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "At the beginning of the day there were 74 apples in a basket. If Ricki removes 14 apples and Samson removes twice as many as Ricki. How many apples are left in the basket by the end of the day?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"There are 74-14 = 60 apples left after Ricki removes some.\",\n",
      "  \"L2\": \"Samson removes 14*2 = 28 apples.\",\n",
      "  \"L3\": \"There are 60+28 =  88 apples left after Samson removes some.\",\n",
      "  \"FA\": \"88\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "L3: There are 60-28 =  32 apples left after Samson removes some, not  60+28 =  88.<end>\n",
      "\n",
      "--- Sample 2/2 ---\n",
      "METADATA: Index=705, Task='conceptual_check', Type='conceptual_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "Isabel has some money in her piggy bank. She spent half the amount and bought a toy. She then spent half of the remaining money and bought her brother a book. If she has $51 left, how much money, in dollars, did she have at first?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"After buying the toy, Isabel has 51*2=102 dollars left.\",\n",
      "  \"L2\": \"Isabel had 102/2=51 dollars at first.\",\n",
      "  \"FA\": \"51\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "L2: Isabel had 102*2= 204 dollars at first, not 102/2 = 51<end>\n",
      "\n",
      "================================================================================\n",
      "2b: 'computational_flawed' samples (target should be 'None') (Displaying 2 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/2 ---\n",
      "METADATA: Index=1311, Task='conceptual_check', Type='computational_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "I bought a TV for $1700 excluding tax. What is the price of the television when calculating 15% of the value-added tax?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"The value-added tax is 1700*15% = 250.\",\n",
      "  \"L2\": \"The price of the television including tax 1700+250 = $1950.\",\n",
      "  \"FA\": \"1950\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "None<end>\n",
      "\n",
      "--- Sample 2/2 ---\n",
      "METADATA: Index=730, Task='conceptual_check', Type='computational_flawed'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[CONCEPTUAL_CHECK]\n",
      "\n",
      "You are an expert mathematical reasoning analyst. Your task is to verify the logical soundness of the provided solution based on the problem statement. The solution is formatted as a JSON dictionary mapping line numbers to text. You must IGNORE any potential arithmetic errors in the final calculations and focus ONLY on the conceptual logic.\n",
      "\n",
      "- Does the solution use the correct numbers from the problem?\n",
      "- Does it use the correct mathematical operations (e.g., multiplication where required, not addition)?\n",
      "- Is the overall logical flow of the steps correct?\n",
      "\n",
      "If the conceptual logic is sound, your entire output must be the single word:\n",
      "None\n",
      "\n",
      "If you find a conceptual error, your entire output must be a single line in the format `L{n}: {a brief explanation of the error}`. Do not include any other text.<|end|><|user|>Problem:\n",
      "```\n",
      "Jack bought 3 books a month at $20 each.  He sells them back at the end of the year for $500.  How much money did he lose?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"He bought 3*20=$60 of books a month\",\n",
      "  \"L2\": \"That means he bought 60*12=$720 of books a year\",\n",
      "  \"L3\": \"So he lost 720-500=$202\",\n",
      "  \"FA\": \"202\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "None<end>\n",
      "\n",
      "--- Request 3: Paired 'correct' and 'flawed' samples for Task B ---\n",
      "\n",
      "--- Pair 1/2 for Index 3130 ---\n",
      "\n",
      "================================================================================\n",
      "3a: The 'correct_paired' sample (Displaying 1 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/1 ---\n",
      "METADATA: Index=3130, Task='equation_extraction', Type='correct_paired'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[EXTRACT_CALCULATIONS]\n",
      "\n",
      "You are a data extraction tool. Your task is to parse the provided solution, which is formatted as a JSON dictionary, and extract every mathematical calculation into a new JSON dictionary.\n",
      "\n",
      "- The keys of the output dictionary should be the line numbers (e.g., \"L1\", \"L2\").\n",
      "- The values should be a string containing the full equation as written (e.g., \"10 + 5 = 15\").\n",
      "- If a line contains no calculation, it should be omitted from the output dictionary.\n",
      "- If the entire solution contains no calculations, return an empty JSON dictionary: {}\n",
      "\n",
      "Your output must be ONLY the JSON object and nothing else.<|end|><|user|>Problem:\n",
      "```\n",
      "Jace drives 60 miles per hour. If Jace drives for 4 hours straight, take a 30-minute break, and then drives for another 9 hours straight, how many miles will he travel?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"Jace drives a total of 4+9 = 13 hours.\",\n",
      "  \"L2\": \"Jace will travel 60*13 = 780 miles.\",\n",
      "  \"FA\": \"780\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "{\n",
      "  \"L1\": \"4+9=13\",\n",
      "  \"L2\": \"60*13=780\"\n",
      "}<end>\n",
      "\n",
      "================================================================================\n",
      "3b: The 'computational_paired' sample (Displaying 1 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/1 ---\n",
      "METADATA: Index=3130, Task='equation_extraction', Type='computational_paired'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[EXTRACT_CALCULATIONS]\n",
      "\n",
      "You are a data extraction tool. Your task is to parse the provided solution, which is formatted as a JSON dictionary, and extract every mathematical calculation into a new JSON dictionary.\n",
      "\n",
      "- The keys of the output dictionary should be the line numbers (e.g., \"L1\", \"L2\").\n",
      "- The values should be a string containing the full equation as written (e.g., \"10 + 5 = 15\").\n",
      "- If a line contains no calculation, it should be omitted from the output dictionary.\n",
      "- If the entire solution contains no calculations, return an empty JSON dictionary: {}\n",
      "\n",
      "Your output must be ONLY the JSON object and nothing else.<|end|><|user|>Problem:\n",
      "```\n",
      "Jace drives 60 miles per hour. If Jace drives for 4 hours straight, take a 30-minute break, and then drives for another 9 hours straight, how many miles will he travel?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"Jace drives a total of 4+9 = 31 hours.\",\n",
      "  \"L2\": \"Jace will travel 60*31 = 1860 miles.\",\n",
      "  \"FA\": \"1860\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "{\n",
      "  \"L1\": \"4+9=31\",\n",
      "  \"L2\": \"60*31=1860\"\n",
      "}<end>\n",
      "\n",
      "--- Pair 2/2 for Index 6579 ---\n",
      "\n",
      "================================================================================\n",
      "3a: The 'correct_paired' sample (Displaying 1 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/1 ---\n",
      "METADATA: Index=6579, Task='equation_extraction', Type='correct_paired'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[EXTRACT_CALCULATIONS]\n",
      "\n",
      "You are a data extraction tool. Your task is to parse the provided solution, which is formatted as a JSON dictionary, and extract every mathematical calculation into a new JSON dictionary.\n",
      "\n",
      "- The keys of the output dictionary should be the line numbers (e.g., \"L1\", \"L2\").\n",
      "- The values should be a string containing the full equation as written (e.g., \"10 + 5 = 15\").\n",
      "- If a line contains no calculation, it should be omitted from the output dictionary.\n",
      "- If the entire solution contains no calculations, return an empty JSON dictionary: {}\n",
      "\n",
      "Your output must be ONLY the JSON object and nothing else.<|end|><|user|>Problem:\n",
      "```\n",
      "Megan is an actress. She was the lead actress in 80% of her work. In total, Megan participated in 100 plays. How many times Megan was not the lead actress?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"Megan was the lead actress in 100 * 80% = 80 plays.\",\n",
      "  \"L2\": \"So Megan was not the lead actress in 100 - 80 = 20 plays.\",\n",
      "  \"FA\": \"20\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "{\n",
      "  \"L1\": \"100*80*.01=80\",\n",
      "  \"L2\": \"100-80=20\"\n",
      "}<end>\n",
      "\n",
      "================================================================================\n",
      "3b: The 'computational_paired' sample (Displaying 1 sample(s))\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1/1 ---\n",
      "METADATA: Index=6579, Task='equation_extraction', Type='computational_paired'\n",
      "\n",
      "PROMPT (What the model sees):\n",
      "<|system|>[EXTRACT_CALCULATIONS]\n",
      "\n",
      "You are a data extraction tool. Your task is to parse the provided solution, which is formatted as a JSON dictionary, and extract every mathematical calculation into a new JSON dictionary.\n",
      "\n",
      "- The keys of the output dictionary should be the line numbers (e.g., \"L1\", \"L2\").\n",
      "- The values should be a string containing the full equation as written (e.g., \"10 + 5 = 15\").\n",
      "- If a line contains no calculation, it should be omitted from the output dictionary.\n",
      "- If the entire solution contains no calculations, return an empty JSON dictionary: {}\n",
      "\n",
      "Your output must be ONLY the JSON object and nothing else.<|end|><|user|>Problem:\n",
      "```\n",
      "Megan is an actress. She was the lead actress in 80% of her work. In total, Megan participated in 100 plays. How many times Megan was not the lead actress?\n",
      "```\n",
      "\n",
      "Solution:\n",
      "```json\n",
      "{\n",
      "  \"L1\": \"Megan was the lead actress in 100 * 80% = 80 plays.\",\n",
      "  \"L2\": \"So Megan was not the lead actress in 100 - 80 = 22 plays.\",\n",
      "  \"FA\": \"22\"\n",
      "}\n",
      "```<|end|><|assistant|>\n",
      "\n",
      "RESPONSE (What the model must learn):\n",
      "{\n",
      "  \"L1\": \"100*80*0.01=80\",\n",
      "  \"L2\": \"100-80=22\"\n",
      "}<end>\n"
     ]
    }
   ],
   "source": [
    "# --- Visualization Requests ---\n",
    "\n",
    "# 1. Visualize conceptual vs. correct samples for Task A\n",
    "print(\"\\n--- Request 1: Conceptual Flawed vs. Correct Original from Task A ---\")\n",
    "visualize_samples(\n",
    "    final_metadata_df,\n",
    "    \"1a: 'conceptual_flawed' samples\",\n",
    "    num_samples=2,\n",
    "    task=\"conceptual_check\",\n",
    "    type=\"conceptual_flawed\"\n",
    ")\n",
    "visualize_samples(\n",
    "    final_metadata_df,\n",
    "    \"1b: 'correct_original' samples\",\n",
    "    num_samples=2,\n",
    "    task=\"conceptual_check\",\n",
    "    type=\"correct_original\"\n",
    ")\n",
    "\n",
    "# 2. Visualize conceptual vs. computational samples for Task A\n",
    "print(\"\\n--- Request 2: Conceptual Flawed vs. Computational Flawed from Task A ---\")\n",
    "visualize_samples(\n",
    "    final_metadata_df,\n",
    "    \"2a: 'conceptual_flawed' samples (target should be an error string)\",\n",
    "    num_samples=2,\n",
    "    task=\"conceptual_check\",\n",
    "    type=\"conceptual_flawed\"\n",
    ")\n",
    "visualize_samples(\n",
    "    final_metadata_df,\n",
    "    \"2b: 'computational_flawed' samples (target should be 'None')\",\n",
    "    num_samples=2,\n",
    "    task=\"conceptual_check\",\n",
    "    type=\"computational_flawed\"\n",
    ")\n",
    "\n",
    "# 3. One computational error + original pair from Task B\n",
    "print(\"\\n--- Request 3: Paired 'correct' and 'flawed' samples for Task B ---\")\n",
    "try:\n",
    "    pair_df = final_metadata_df[\n",
    "        (final_metadata_df['task'] == 'equation_extraction') &\n",
    "        (final_metadata_df['type'].isin(['correct_paired', 'computational_paired']))\n",
    "    ]\n",
    "    valid_indices = pair_df['index'].value_counts()\n",
    "    valid_indices = valid_indices[valid_indices == 2].index\n",
    "    \n",
    "    # Select 2 random indices to visualize pairs for\n",
    "    indices_to_show = rng.sample(list(valid_indices), k=min(2, len(valid_indices)))\n",
    "\n",
    "    for i, idx in enumerate(indices_to_show):\n",
    "        print(f\"\\n--- Pair {i+1}/{len(indices_to_show)} for Index {idx} ---\")\n",
    "        visualize_samples(\n",
    "            final_metadata_df,\n",
    "            f\"3a: The 'correct_paired' sample\",\n",
    "            num_samples=1,\n",
    "            task=\"equation_extraction\",\n",
    "            type=\"correct_paired\",\n",
    "            index=idx\n",
    "        )\n",
    "        visualize_samples(\n",
    "            final_metadata_df,\n",
    "            f\"3b: The 'computational_paired' sample\",\n",
    "            num_samples=1,\n",
    "            task=\"equation_extraction\",\n",
    "            type=\"computational_paired\",\n",
    "            index=idx\n",
    "        )\n",
    "except (IndexError, ValueError) as e:\n",
    "    print(f\"\\nERROR: Could not find enough valid computational pairs to display. Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "383c8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final DataFrame created with all samples and metadata ---\n",
      "Total samples: 3,565\n",
      "   index              task               type  \\\n",
      "0   1280  conceptual_check  conceptual_flawed   \n",
      "1    788  conceptual_check  conceptual_flawed   \n",
      "2   1500  conceptual_check  conceptual_flawed   \n",
      "3    840  conceptual_check  conceptual_flawed   \n",
      "4    882  conceptual_check  conceptual_flawed   \n",
      "\n",
      "                                              source  \\\n",
      "0  yewei/gsm8k_data/conceptual/gsm8k_augmented_12...   \n",
      "1  data/manually_gen_incorrect_answers_gsm8k/gsm8...   \n",
      "2                    data/1500_1599_conceptual.jsonl   \n",
      "3  data/manually_gen_incorrect_answers_gsm8k/gsm8...   \n",
      "4  data/manually_gen_incorrect_answers_gsm8k/gsm8...   \n",
      "\n",
      "                                                text  \n",
      "0  <|system|>[CONCEPTUAL_CHECK]\\n\\nYou are an exp...  \n",
      "1  <|system|>[CONCEPTUAL_CHECK]\\n\\nYou are an exp...  \n",
      "2  <|system|>[CONCEPTUAL_CHECK]\\n\\nYou are an exp...  \n",
      "3  <|system|>[CONCEPTUAL_CHECK]\\n\\nYou are an exp...  \n",
      "4  <|system|>[CONCEPTUAL_CHECK]\\n\\nYou are an exp...  \n",
      "\n",
      "--- Splitting dataset into train, validation, and test sets ---\n",
      "\n",
      "--- Saving all artifacts to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/verifier-v2-two-task ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d16cd0b40e04b8c9a4ab69c64218bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbb15af2b4b417c9f3e8c50e3a6a33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc0f211340745b1907e613abc6f5d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - SFT Dataset (train/validation/test splits) saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/verifier-v2-two-task/sft_dataset\n",
      "  - Full metadata catalog saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sft-datasets/verifier-v2-two-task/sft_catalog.csv\n",
      "\n",
      "--- Process Complete ---\n",
      "Final Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'task', 'type', 'source', 'text', '__index_level_0__'],\n",
      "        num_rows: 2846\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'task', 'type', 'source', 'text', '__index_level_0__'],\n",
      "        num_rows: 360\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'task', 'type', 'source', 'text', '__index_level_0__'],\n",
      "        num_rows: 359\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# --- Configuration for Splitting ---\n",
    "TRAIN_SPLIT_RATIO = 0.8  # 80% of unique problems for training\n",
    "VALIDATION_SPLIT_RATIO = 0.1 # 10% for validation, 10% for test\n",
    "\n",
    "def split_dataset_by_index(\n",
    "    df: pd.DataFrame,\n",
    "    train_size: float = 0.8,\n",
    "    val_size: float = 0.1,\n",
    "    seed: int = 42\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train, validation, and test sets based on a unique\n",
    "    'index' column to prevent data leakage.\n",
    "    \"\"\"\n",
    "    # Get all unique problem indices and shuffle them reproducibly\n",
    "    all_indices = sorted(list(df['index'].unique()))\n",
    "    random.Random(seed).shuffle(all_indices)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(len(all_indices) * train_size)\n",
    "    val_end = train_end + int(len(all_indices) * val_size)\n",
    "    \n",
    "    # Create sets of indices for each split\n",
    "    train_indices = set(all_indices[:train_end])\n",
    "    val_indices = set(all_indices[train_end:val_end])\n",
    "    test_indices = set(all_indices[val_end:])\n",
    "    \n",
    "    # Filter the DataFrame for each split\n",
    "    train_df = df[df['index'].isin(train_indices)]\n",
    "    val_df = df[df['index'].isin(val_indices)]\n",
    "    test_df = df[df['index'].isin(test_indices)]\n",
    "    \n",
    "    # Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df),\n",
    "        \"test\": Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "# --- 1. Create the final comprehensive DataFrame ---\n",
    "# This is the same DataFrame used for visualization, ensuring consistency.\n",
    "final_df = pd.DataFrame(metadata_log)\n",
    "final_df['text'] = [s['text'] for s in sft_samples] # Add the full text for SFT\n",
    "\n",
    "# Ensure 'text' is the last column for clarity, and drop the now-redundant 'task' column\n",
    "final_df = final_df[['index', 'task', 'type', 'source', 'text']]\n",
    "\n",
    "print(\"--- Final DataFrame created with all samples and metadata ---\")\n",
    "print(f\"Total samples: {len(final_df):,}\")\n",
    "print(final_df.head())\n",
    "\n",
    "\n",
    "# --- 2. Split the dataset to prevent data leakage ---\n",
    "print(\"\\n--- Splitting dataset into train, validation, and test sets ---\")\n",
    "sft_dataset_dict = split_dataset_by_index(\n",
    "    final_df,\n",
    "    train_size=TRAIN_SPLIT_RATIO,\n",
    "    val_size=VALIDATION_SPLIT_RATIO,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. Save all artifacts to the output directory ---\n",
    "print(f\"\\n--- Saving all artifacts to: {OUTPUT_DIR} ---\")\n",
    "\n",
    "# a. Save the Hugging Face DatasetDict\n",
    "dataset_path = OUTPUT_DIR / \"sft_dataset\"\n",
    "sft_dataset_dict.save_to_disk(dataset_path)\n",
    "print(f\"  - SFT Dataset (train/validation/test splits) saved to: {dataset_path}\")\n",
    "\n",
    "# b. Save the full metadata catalog as a CSV\n",
    "catalog_path = OUTPUT_DIR / \"sft_catalog.csv\"\n",
    "# We save final_df as it contains all metadata plus the final text\n",
    "final_df.to_csv(catalog_path, index=False)\n",
    "print(f\"  - Full metadata catalog saved to: {catalog_path}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(\"Final Dataset Structure:\")\n",
    "print(sft_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9038a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
