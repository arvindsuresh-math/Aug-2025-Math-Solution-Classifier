{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2bd917ad5e9441da6ad3f4ca9e21ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b101a32bbc364db8af6a41b282437ef4",
              "IPY_MODEL_7b5ab137bbc7493798fbe79962b0e54c",
              "IPY_MODEL_e41a8b764ccd4dc7994d8cfd3827a07e"
            ],
            "layout": "IPY_MODEL_9b8496827e284c9aba7a13c15178f4e9"
          }
        },
        "b101a32bbc364db8af6a41b282437ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_107136d00bc443ff81a99ea0b9f113eb",
            "placeholder": "​",
            "style": "IPY_MODEL_18742633796b47f6a3ade3db6afc7ec9",
            "value": "Map: 100%"
          }
        },
        "7b5ab137bbc7493798fbe79962b0e54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3879e362d14062b3b89a7c620492a4",
            "max": 701,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb7e37cc9a314f13bd27ef41f5371c5d",
            "value": 701
          }
        },
        "e41a8b764ccd4dc7994d8cfd3827a07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19df95d3da814ffdacd86f1b2e5203a5",
            "placeholder": "​",
            "style": "IPY_MODEL_37976d74dc694b70af9ebf613ec40a76",
            "value": " 701/701 [00:01&lt;00:00, 603.53 examples/s]"
          }
        },
        "9b8496827e284c9aba7a13c15178f4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107136d00bc443ff81a99ea0b9f113eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18742633796b47f6a3ade3db6afc7ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d3879e362d14062b3b89a7c620492a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7e37cc9a314f13bd27ef41f5371c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19df95d3da814ffdacd86f1b2e5203a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37976d74dc694b70af9ebf613ec40a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6223048f96f14d54ba074359d60f4d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2afcbe827014022b6b2148fb14fefcc",
              "IPY_MODEL_0e12f9f990fe460fbc22b508839bfad7",
              "IPY_MODEL_017d925bf8a4494fbdbb0f327daba382"
            ],
            "layout": "IPY_MODEL_55f3d4a8d6244c59bcfa01f3bee2fd68"
          }
        },
        "a2afcbe827014022b6b2148fb14fefcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d691bbf9347d43e3b91e9bbc2f3cf4f9",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d9bda0e90e4117b67bcf3fa57d8cf1",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "0e12f9f990fe460fbc22b508839bfad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb18090aeef94f40a8b49f0866590c8b",
            "max": 701,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e349baa3f1c49c98d01aff2c8be1b05",
            "value": 701
          }
        },
        "017d925bf8a4494fbdbb0f327daba382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_026b532f782e450181c7b4a8e698f50b",
            "placeholder": "​",
            "style": "IPY_MODEL_edafcfe1c7d1461b96c0d1dd170a551a",
            "value": " 701/701 [00:03&lt;00:00, 242.50 examples/s]"
          }
        },
        "55f3d4a8d6244c59bcfa01f3bee2fd68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d691bbf9347d43e3b91e9bbc2f3cf4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d9bda0e90e4117b67bcf3fa57d8cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb18090aeef94f40a8b49f0866590c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e349baa3f1c49c98d01aff2c8be1b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "026b532f782e450181c7b4a8e698f50b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edafcfe1c7d1461b96c0d1dd170a551a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51b0f45a833e4c70b54405ec24a75376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f89b23abf48c4272b0c9e985a302c0e0",
              "IPY_MODEL_b88060531c644cd9bb129a6c9b5265f1",
              "IPY_MODEL_a48f048575af437088382b865ba8ec32"
            ],
            "layout": "IPY_MODEL_24c73fe6069c4a97b8575c03cb2c4326"
          }
        },
        "f89b23abf48c4272b0c9e985a302c0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac56ff0485f24767a2898fb4c7580212",
            "placeholder": "​",
            "style": "IPY_MODEL_600536fb21904ab595b8107961140c35",
            "value": "Map (num_proc=12): 100%"
          }
        },
        "b88060531c644cd9bb129a6c9b5265f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0124a0cab1f344b39c172da0ea4ed26a",
            "max": 701,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f616244bc38e4911bd127b66614739d4",
            "value": 701
          }
        },
        "a48f048575af437088382b865ba8ec32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43457faf8ea6461886c83fe3452353e6",
            "placeholder": "​",
            "style": "IPY_MODEL_ed2a436dec244f3cb3d9ea760a8e5eac",
            "value": " 701/701 [00:01&lt;00:00, 905.26 examples/s]"
          }
        },
        "24c73fe6069c4a97b8575c03cb2c4326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac56ff0485f24767a2898fb4c7580212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "600536fb21904ab595b8107961140c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0124a0cab1f344b39c172da0ea4ed26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f616244bc38e4911bd127b66614739d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43457faf8ea6461886c83fe3452353e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2a436dec244f3cb3d9ea760a8e5eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Login & Installations ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Please add it.\")\n",
        "# notebook_login(new_session=hf_token) # Unsloth handles token auth automatically"
      ],
      "metadata": {
        "id": "0kxoKB2S4WJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwOPs8bLYp34",
        "outputId": "6745fe7e-5bf4-4764-871f-e7d14d64cc05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.8.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.8.1-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1 cut_cross_entropy-25.1.1 trl-0.21.0 unsloth_zoo-2025.8.1 xformers-0.0.29.post3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Collecting datasets<4.0.0,>=3.4.1\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.12.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.8.1 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.8.1 requires tyro, which is not installed.\n",
            "trl 0.21.0 requires transformers>=4.55.0, but you have transformers 4.54.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.8.1-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.8.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.8/299.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.8.1\n"
          ]
        }
      ],
      "source": [
        "# # Install Unsloth for Google Colab\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# # Standard installations\n",
        "# !pip install -U transformers\n",
        "# !pip install -U datasets\n",
        "# !pip install -U accelerate # Required for Unsloth\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "CONFIG = {\n",
        "    # Core experiment parameters\n",
        "    \"experiment_type\": \"equation_extraction\",\n",
        "    # UPDATED to use Unsloth's 4-bit Gemma 3 1B model\n",
        "    \"model_name\": \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"max_seq_length\": 2048, # Unsloth's FastModel requires this at load time\n",
        "\n",
        "    # Prompting configuration\n",
        "    \"include_examples\": True,\n",
        "    \"few_shot_examples\": [\n",
        "        ('computational_error', 4966),\n",
        "        ('conceptual_error', 1091),\n",
        "    ],\n",
        "\n",
        "    # Training parameters\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"num_epochs\": 1, # Set to 1 as requested\n",
        "    \"batch_size\": 2, # Halved from 4, since Unsloth uses more VRAM initially\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "\n",
        "    # LoRa params (Unsloth defaults are often good)\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 16, # Often set to rank\n",
        "    \"lora_dropout\": 0.05,\n",
        "\n",
        "    # Paths\n",
        "    \"base_dataset_path\": \"/content/equation_extraction_dataset_cleaned.csv\",\n",
        "    \"output_base_dir\": \"/content/experiments\",\n",
        "}\n",
        "\n",
        "# --- Generate Unique Experiment ID ---\n",
        "model_id_short = \"gemma3-1b-unsloth\"\n",
        "experiment_id = f\"{CONFIG['experiment_type']}_{model_id_short}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "CONFIG[\"experiment_id\"] = experiment_id\n",
        "print(f\"Experiment ID: {experiment_id}\")\n",
        "\n",
        "# --- Setup Output Directories ---\n",
        "output_dir = Path(CONFIG[\"output_base_dir\"]) / CONFIG[\"experiment_id\"]\n",
        "(output_dir / \"baseline_results\").mkdir(parents=True, exist_ok=True)\n",
        "(output_dir / \"final_results\").mkdir(parents=True, exist_ok=True)\n",
        "CONFIG[\"output_dir\"] = str(output_dir)\n",
        "CONFIG[\"final_adapter_dir\"] = str(output_dir / \"final_adapter\")\n",
        "CONFIG[\"merged_model_dir\"] = str(output_dir / \"final_merged_model\")\n",
        "\n",
        "with open(output_dir / \"config.json\", 'w') as f: json.dump(CONFIG, f, indent=2)\n",
        "print(f\"Output directory created: {output_dir}\")\n",
        "\n",
        "# --- Set Random Seeds for Reproducibility ---\n",
        "def set_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seeds(42)\n",
        "\n",
        "print(\"\\n✅ Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyVotCt_Y4Eg",
        "outputId": "981481f6-c7d3-44d2-cde8-e86b0b281c26"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment ID: equation_extraction_gemma3-1b-unsloth_20250807_1859\n",
            "Output directory created: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250807_1859\n",
            "\n",
            "✅ Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 3: System Prompt for Equation Extraction\n",
        "\n",
        "SYSTEM_PROMPT = \\\n",
        "\"\"\"[ROLE]\n",
        "You are an expert at parsing mathematical solutions.\n",
        "\n",
        "[TASK]\n",
        "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
        "\n",
        "For each line of the solution, create a key-value pair.\n",
        "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
        "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
        "- If a line contains no calculation, the value must be an empty string.\n",
        "\n",
        "[RESPONSE FORMAT]\n",
        "Your response must be ONLY a single, valid JSON object, with no other text before or after it. The JSON object must map line identifiers to their corresponding equation strings.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iSaFHYaPY468"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 4: Core utilities\n",
        "\n",
        "import pandas as pd\n",
        "from unsloth import FastModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# 4.1 Loading\n",
        "\n",
        "def load_base_dataset():\n",
        "    \"\"\"Loads the base dataset from the specified CSV file.\"\"\"\n",
        "    data = pd.read_csv(CONFIG['base_dataset_path'])\n",
        "    print(f\"Loaded dataset with {len(data)} samples from {CONFIG['base_dataset_path']}\")\n",
        "    return data\n",
        "\n",
        "def load_unsloth_model_and_tokenizer():\n",
        "    \"\"\"\n",
        "    Loads the 4-bit quantized model and tokenizer using Unsloth's FastModel.\n",
        "    \"\"\"\n",
        "    model_name = CONFIG[\"model_name\"]\n",
        "    print(f\"Loading model and tokenizer: {model_name}\")\n",
        "\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = model_name,\n",
        "        max_seq_length = CONFIG[\"max_seq_length\"],\n",
        "        dtype = None, # None for auto detection. Can be torch.bfloat16\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"gemma-3\",\n",
        "    )\n",
        "\n",
        "    # Set padding token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    tokenizer.padding_side = \"left\" # Use left-padding for generation\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_model_for_lora_training(model):\n",
        "    \"\"\"Applies LoRa adapters to the model for fine-tuning.\"\"\"\n",
        "    print(\"Applying LoRa adapters to the model...\")\n",
        "    model = FastModel.get_peft_model(\n",
        "        model,\n",
        "        r = CONFIG[\"lora_rank\"],\n",
        "        lora_alpha = CONFIG[\"lora_alpha\"],\n",
        "        lora_dropout = CONFIG[\"lora_dropout\"],\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = True,\n",
        "        random_state = 42,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 4.2 Formatting\n",
        "\n",
        "def format_user_message(sample: dict) -> str:\n",
        "    \"\"\"\n",
        "    Formats a user message to contain the literal JSON string of the answer mapping.\n",
        "    - Uses 'wrong_answer_mapping' for computational or conceptual errors.\n",
        "    - Uses 'correct_answer_mapping' for correct answers.\n",
        "    \"\"\"\n",
        "    error_type = sample.get('error_type', 'correct')\n",
        "    mapping_str = \"\"\n",
        "\n",
        "    if error_type in ['computational_error', 'conceptual_error']:\n",
        "        mapping_str = sample.get('wrong_answer_mapping', '{}')\n",
        "    else:  # This covers 'correct' and any other cases\n",
        "        mapping_str = sample.get('correct_answer_mapping', '{}')\n",
        "\n",
        "    # Ensure we have a valid string to pass\n",
        "    if not isinstance(mapping_str, str):\n",
        "        mapping_str = '{}'\n",
        "\n",
        "    # The prompt now contains the raw JSON string of the line-by-line solution\n",
        "    return f\"### Solution:\\n{mapping_str.strip()}\"\n",
        "\n",
        "def format_expected_output(sample: dict) -> str:\n",
        "    \"\"\"\n",
        "    Selects the correct equation mapping based on the error type and formats it as a clean JSON string.\n",
        "    - Uses 'wrong_eqn_mapping' for computational or conceptual errors.\n",
        "    - Uses 'correct_eqn_mapping' for correct answers.\n",
        "    \"\"\"\n",
        "    error_type = sample.get('error_type', 'correct')\n",
        "    eqn_map_str = \"\"\n",
        "\n",
        "    if error_type in ['computational_error', 'conceptual_error']:\n",
        "        eqn_map_str = sample.get('wrong_eqn_mapping', '{}')\n",
        "    else: # This covers 'correct' and any other cases\n",
        "        eqn_map_str = sample.get('correct_eqn_mapping', '{}')\n",
        "\n",
        "    if not isinstance(eqn_map_str, str):\n",
        "        return \"{}\"\n",
        "\n",
        "    try:\n",
        "        # Load and re-dump the JSON to ensure consistent, clean formatting for the model to learn.\n",
        "        # This removes extra whitespace and uses double quotes.\n",
        "        parsed_json = json.loads(eqn_map_str)\n",
        "        return json.dumps(parsed_json, indent=2)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        # Return an empty JSON object if the source string is invalid\n",
        "        return \"{}\"\n",
        "\n",
        "# 4.3 Prompt Construction (CORRECTED LOGIC)\n",
        "\n",
        "def _build_conversation_messages(sample, is_training_prompt=True):\n",
        "    \"\"\"\n",
        "    Builds the list of messages for the chat template, correctly placing the system prompt.\n",
        "    \"\"\"\n",
        "    # Prepend the system prompt to the user's message content.\n",
        "    # This is the correct way to provide instructions for the gemma-3 template.\n",
        "    user_content = format_user_message(sample)\n",
        "    instructed_user_content = f\"{SYSTEM_PROMPT}\\n\\n{user_content}\"\n",
        "\n",
        "    # For the few-shot examples, we use a simpler user message\n",
        "    few_shot_messages = []\n",
        "    if CONFIG[\"include_examples\"]:\n",
        "        # NOTE: We use the train_df loaded globally for this.\n",
        "        example_specs = CONFIG[\"few_shot_examples\"]\n",
        "        for error_type, index in example_specs:\n",
        "            example_sample_df = train_df[(train_df['error_type'] == error_type) & (train_df['index'] == index)]\n",
        "            if not example_sample_df.empty:\n",
        "                example_sample = example_sample_df.iloc[0].to_dict()\n",
        "                # The first user turn gets the full instructions\n",
        "                if not few_shot_messages:\n",
        "                     few_shot_user_content = f\"{SYSTEM_PROMPT}\\n\\n{format_user_message(example_sample)}\"\n",
        "                else:\n",
        "                     few_shot_user_content = format_user_message(example_sample)\n",
        "\n",
        "                few_shot_messages.append({\"role\": \"user\", \"content\": few_shot_user_content})\n",
        "                few_shot_messages.append({\"role\": \"assistant\", \"content\": format_expected_output(example_sample)})\n",
        "\n",
        "    # The final conversation list\n",
        "    messages = []\n",
        "    if few_shot_messages:\n",
        "        messages.extend(few_shot_messages)\n",
        "        # The actual query doesn't need the system prompt if few-shot examples are present\n",
        "        messages.append({\"role\": \"user\", \"content\": format_user_message(sample)})\n",
        "    else:\n",
        "        # If no few-shot examples, the main user query gets the instructions\n",
        "        messages.append({\"role\": \"user\", \"content\": instructed_user_content})\n",
        "\n",
        "    # If it's a training prompt, add the final assistant response\n",
        "    if is_training_prompt:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": format_expected_output(sample)})\n",
        "\n",
        "    return messages\n",
        "\n",
        "# 4.4 Prompt creation for inference\n",
        "def create_sample_prompt_for_inference(sample, tokenizer):\n",
        "    \"\"\"Creates a full prompt for a single sample for inference.\"\"\"\n",
        "    messages = _build_conversation_messages(sample, is_training_prompt=False)\n",
        "    # Apply the chat template with the generation prompt\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ],
      "metadata": {
        "id": "RIaOWdIwY-KG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 5: Dataset preparation\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "def prepare_datasets(base_df):\n",
        "    \"\"\"\n",
        "    Splits the base DataFrame into training and testing sets using the 'split' column.\n",
        "    \"\"\"\n",
        "    train_df = base_df[base_df['split'] == 'train'].copy()\n",
        "    test_df = base_df[base_df['split'] == 'test'].copy()\n",
        "    print(f\"Data split using 'split' column: {len(train_df)} training, {len(test_df)} testing samples.\")\n",
        "    return train_df, test_df\n",
        "\n",
        "def create_training_dataset(df, tokenizer):\n",
        "    \"\"\"\n",
        "    Creates the tokenized training dataset object for the SFTTrainer.\n",
        "    \"\"\"\n",
        "    def create_text_for_sample(sample):\n",
        "        \"\"\"Prepares the full conversation text for a single training sample.\"\"\"\n",
        "        messages = _build_conversation_messages(sample, is_training_prompt=True)\n",
        "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
        "\n",
        "    return Dataset.from_pandas(df).map(create_text_for_sample, load_from_cache_file=False)"
      ],
      "metadata": {
        "id": "RM7r4IzZZfMH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 6: Evaluation logic\n",
        "\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "\n",
        "def rigorous_compare_equation_dicts(predicted_dict: dict, expected_dict: dict) -> float:\n",
        "    \"\"\"\n",
        "    Compares two equation dictionaries based on evaluation, operand sequence,\n",
        "    and operator sequence.\n",
        "\n",
        "    Args:\n",
        "        predicted_dict: Dictionary of predicted equation strings.\n",
        "        expected_dict: Dictionary of expected equation strings.\n",
        "\n",
        "    Returns:\n",
        "        A float score [0.0, 1.0] representing the mean structural accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    def _safe_eval(expression: str):\n",
        "        \"\"\"Safely evaluates a string, returning a sentinel on error.\"\"\"\n",
        "        try:\n",
        "            return eval(expression, {\"__builtins__\": None}, {})\n",
        "        except Exception:\n",
        "            return object()\n",
        "\n",
        "    def _extract_components(expression: str) -> tuple[list, list]:\n",
        "        \"\"\"Extracts number and operator sequences from an expression string.\"\"\"\n",
        "        # Regex to find numbers (integers or floats)\n",
        "        numbers = [float(n) for n in re.findall(r'\\d+\\.?\\d*|\\.\\d+', expression)]\n",
        "        # Regex to find basic arithmetic operators\n",
        "        operators = re.findall(r'[+\\-*/]', expression)\n",
        "        return numbers, operators\n",
        "\n",
        "    def _expressions_are_equivalent(pred_expr: str, exp_expr: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if two expression strings are equivalent based on:\n",
        "        1. Evaluated value.\n",
        "        2. Sequence of numbers.\n",
        "        3. Sequence of operators.\n",
        "        \"\"\"\n",
        "        # 1. Check if they evaluate to the same value\n",
        "        pred_val = _safe_eval(pred_expr)\n",
        "        exp_val = _safe_eval(exp_expr)\n",
        "\n",
        "        if not (isinstance(pred_val, (int, float)) and isinstance(exp_val, (int, float))):\n",
        "            return False\n",
        "        if not math.isclose(pred_val, exp_val):\n",
        "            return False\n",
        "\n",
        "        # 2 & 3. Check for same sequence of numbers and operators\n",
        "        pred_nums, pred_ops = _extract_components(pred_expr)\n",
        "        exp_nums, exp_ops = _extract_components(exp_expr)\n",
        "\n",
        "        if pred_nums != exp_nums or pred_ops != exp_ops:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    # --- Main function logic ---\n",
        "    items_to_score = [(k, v) for k, v in expected_dict.items() if v]\n",
        "    if not items_to_score:\n",
        "        return 1.0 if not any(v for v in predicted_dict.values() if isinstance(v, str) and v) else 0.0\n",
        "\n",
        "    scores = []\n",
        "    for key, expected_eqn in items_to_score:\n",
        "        line_score = 0\n",
        "        predicted_eqn = predicted_dict.get(key)\n",
        "\n",
        "        if isinstance(predicted_eqn, str) and predicted_eqn.count('=') == 1:\n",
        "            pred_lhs, pred_rhs = [s.strip() for s in predicted_eqn.split('=', 1)]\n",
        "            exp_lhs, exp_rhs = [s.strip() for s in expected_eqn.split('=', 1)]\n",
        "\n",
        "            # Compare LHS and RHS using the rigorous check\n",
        "            if _expressions_are_equivalent(pred_lhs, exp_lhs) and \\\n",
        "               _expressions_are_equivalent(pred_rhs, exp_rhs):\n",
        "                line_score = 1\n",
        "\n",
        "        scores.append(line_score)\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def extract_json_from_response(response: str) -> dict:\n",
        "    \"\"\"Extracts a JSON object from a model's text response.\"\"\"\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response, re.DOTALL)\n",
        "    json_str = match.group(1) if match else re.search(r'(\\{.*?\\})', response, re.DOTALL)\n",
        "    if not json_str: return {}\n",
        "    try:\n",
        "        # For gemma, which might output python dicts\n",
        "        cleaned_str = json_str.group(0).replace(\"'\", '\"') if hasattr(json_str, 'group') else json_str.replace(\"'\", '\"')\n",
        "        return json.loads(cleaned_str)\n",
        "    except (json.JSONDecodeError, AttributeError):\n",
        "        return {}\n",
        "\n",
        "def run_unsloth_inference(model, tokenizer, df_to_eval, batch_size=32):\n",
        "    \"\"\"\n",
        "    Runs inference using the provided Unsloth model and tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- Running Unsloth native inference ---\")\n",
        "\n",
        "    # Prepare all prompts first, using the corrected prompt creation logic\n",
        "    prompts = [\n",
        "        create_sample_prompt_for_inference(row, tokenizer)\n",
        "        for _, row in df_to_eval.iterrows()\n",
        "    ]\n",
        "\n",
        "    all_predictions = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Inference Batches\"):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "        # print(f\"Before batch {i} input: {tokenizer.padding_side}\")\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True).to(\"cuda\")\n",
        "        # print(f\"Before batch {i} output: {tokenizer.padding_side}\")\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id)\n",
        "        # print(f\"After batch {i} output: {tokenizer.padding_side}\")\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "        all_predictions.extend(decoded_outputs)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(test_df, predictions):\n",
        "    \"\"\"Parses predictions and computes metrics.\"\"\"\n",
        "    results_data = []\n",
        "    for i, pred_text in enumerate(predictions):\n",
        "        original_sample = test_df.iloc[i].to_dict()\n",
        "        predicted_json = extract_json_from_response(pred_text)\n",
        "        expected_json = json.loads(format_expected_output(original_sample))\n",
        "\n",
        "        score = rigorous_compare_equation_dicts(predicted_json, expected_json)\n",
        "        results_data.append({\n",
        "            'problem_index': original_sample.get('index'), 'rigorous_score': score,\n",
        "            'expected_json': json.dumps(expected_json), 'predicted_json': json.dumps(predicted_json),\n",
        "            'full_prediction_text': pred_text.strip(),\n",
        "        })\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "\n",
        "    # Compute final metrics dictionary\n",
        "    parse_failures = (results_df['predicted_json'] == '{}').sum()\n",
        "    metrics = {\n",
        "        \"mean_rigorous_score\": results_df['rigorous_score'].mean(),\n",
        "        \"total_samples\": len(results_df),\n",
        "        \"json_parse_failures\": int(parse_failures),\n",
        "        \"failure_rate\": parse_failures / len(results_df) if len(results_df) > 0 else 0\n",
        "    }\n",
        "    return results_df, metrics"
      ],
      "metadata": {
        "id": "qfhcWcSAZqYR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 7: Fine-tuning function\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "def run_fine_tuning(model, tokenizer, train_dataset):\n",
        "    \"\"\"Runs fine-tuning using Unsloth and SFTTrainer.\"\"\"\n",
        "\n",
        "    # Configure the trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=CONFIG[\"max_seq_length\"],\n",
        "        dataset_num_proc=2,\n",
        "        args=SFTConfig(\n",
        "            per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "            gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "            warmup_steps=5,\n",
        "            num_train_epochs=2,\n",
        "            learning_rate=CONFIG[\"learning_rate\"],\n",
        "            fp16=not torch.cuda.is_bf16_supported(),\n",
        "            bf16=torch.cuda.is_bf16_supported(),\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=42,\n",
        "            output_dir=str(Path(CONFIG[\"output_dir\"]) / \"training_checkpoints\"),\n",
        "            report_to=\"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Use Unsloth's helper to only train on assistant's responses\n",
        "    # This is more efficient than manual masking.\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<start_of_turn>user\",\n",
        "        response_part=\"<start_of_turn>model\",\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Starting fine-tuning for {CONFIG['num_epochs']} epoch(s) ---\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    # Save the final LoRa adapter\n",
        "    print(f\"\\n✅ Fine-tuning finished! Saving final adapter to {CONFIG['final_adapter_dir']}\")\n",
        "    model.save_pretrained(CONFIG[\"final_adapter_dir\"])\n",
        "\n",
        "    return trainer_stats"
      ],
      "metadata": {
        "id": "oWgjzXj4b-vc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 8: Pipeline execution"
      ],
      "metadata": {
        "id": "wrkr2jX5cVNJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.1 Load dataset and few-shot examples\n",
        "base_df = load_base_dataset()\n",
        "train_df, test_df = prepare_datasets(base_df)\n",
        "print(\"\\n✅ Data loaded and split.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijWpdco2cVxJ",
        "outputId": "92bb67ad-ff1d-4188-cf29-f55a43ea6d6f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 877 samples from /content/equation_extraction_dataset_cleaned.csv\n",
            "Data split using 'split' column: 701 training, 176 testing samples.\n",
            "\n",
            "✅ Data loaded and split.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.2 Load model and tokenizer\n",
        "model, tokenizer = load_unsloth_model_and_tokenizer()\n",
        "print(\"\\n✅ Unsloth model and tokenizer loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsX8WaqScZYY",
        "outputId": "83e8f25a-b88e-415b-ca0d-6927a16387e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer: unsloth/gemma-3-1b-it-unsloth-bnb-4bit\n",
            "==((====))==  Unsloth 2025.8.1: Fast Gemma3 patching. Transformers: 4.54.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "✅ Unsloth model and tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.3 Apply Formatting (Inspect Message List)\n",
        "inspection_sample = train_df.iloc[0].to_dict()\n",
        "\n",
        "conversation_messages = _build_conversation_messages(sample=inspection_sample, is_training_prompt=True)\n",
        "\n",
        "import json\n",
        "print(\"Example conversation:\")\n",
        "for message in conversation_messages:\n",
        "    print(f\"{message['role']}:\")\n",
        "    print(message['content'])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51MyJ9smwum7",
        "outputId": "cc51d509-ebb7-480a-d5af-74858fe332c0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example conversation:\n",
            "user:\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "\n",
            "[TASK]\n",
            "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
            "\n",
            "For each line of the solution, create a key-value pair.\n",
            "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
            "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
            "- If a line contains no calculation, the value must be an empty string.\n",
            "\n",
            "[RESPONSE FORMAT]\n",
            "Your response must be ONLY a single, valid JSON object, with no other text before or after it. The JSON object must map line identifiers to their corresponding equation strings.\n",
            "\n",
            "\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}\n",
            "\n",
            "assistant:\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "user:\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
            "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
            "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
            "  \"FA\": \"240\"\n",
            "}\n",
            "\n",
            "assistant:\n",
            "{\n",
            "  \"L1\": \"24/2=12\",\n",
            "  \"L2\": \"240/12=20\",\n",
            "  \"L3\": \"12*20=240.00\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n",
            "user:\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}\n",
            "\n",
            "assistant:\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.4 Apply tokenizer and inspect\n",
        "\n",
        "final_prompt_string = tokenizer.apply_chat_template(\n",
        "    conversation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False # False because it's a training example\n",
        ")\n",
        "\n",
        "print(final_prompt_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvR4eZc11rYf",
        "outputId": "7ac7fd6e-ac1f-4bf9-cb2c-d1200e7449c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "[ROLE]\n",
            "You are an expert at parsing mathematical solutions.\n",
            "\n",
            "[TASK]\n",
            "You are given a mathematical solution. Your task is to extract the calculation performed on each line and represent it as a simple equation.\n",
            "\n",
            "For each line of the solution, create a key-value pair.\n",
            "- The key should be the line identifier (e.g., \"L1\", \"L2\", \"FA\" for the final answer line).\n",
            "- The value should be the extracted equation string (e.g., \"10+5=15\").\n",
            "- If a line contains no calculation, the value must be an empty string.\n",
            "\n",
            "[RESPONSE FORMAT]\n",
            "Your response must be ONLY a single, valid JSON object, with no other text before or after it. The JSON object must map line identifiers to their corresponding equation strings.\n",
            "\n",
            "\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"She drinks 2 bottles a day and there are 24 bottles in a case so a case will last 24/2 = 12 days\",\n",
            "  \"L2\": \"She needs enough to last her 240 days and 1 case will last her 12 days so she needs 240/12 = 20 cases\",\n",
            "  \"L3\": \"Each case is on sale for $12.00 and she needs 20 cases so that's 12*20 = $240.00\",\n",
            "  \"FA\": \"240\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"24/2=12\",\n",
            "  \"L2\": \"240/12=20\",\n",
            "  \"L3\": \"12*20=240.00\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>user\n",
            "### Solution:\n",
            "{\n",
            "  \"L1\": \"2% of $90 is (2/100)*$90 = $1.8\",\n",
            "  \"L2\": \"2% of $60 is (2/100)*$60 = $1.2\",\n",
            "  \"L3\": \"The second transaction was reversed without the service charge so only a total of $90+$1.8+$1.2 = $39 was deducted from his account\",\n",
            "  \"L4\": \"He will have a balance of $400-$39 = $361\",\n",
            "  \"FA\": \"361\"\n",
            "}<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"L1\": \"(2/100)*90=1.8\",\n",
            "  \"L2\": \"(2/100)*60=1.2\",\n",
            "  \"L3\": \"90+1.8+1.2=39\",\n",
            "  \"L4\": \"400-39=361\",\n",
            "  \"FA\": \"\"\n",
            "}<end_of_turn>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# PHASE 1: BASELINE EVALUATION\n",
        "# ===================================================================\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 1: BASELINE EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Run inference on the base model\n",
        "baseline_predictions = run_unsloth_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    df_to_eval=test_df\n",
        ")\n",
        "\n",
        "# Evaluate and save baseline results\n",
        "baseline_results_df, baseline_metrics = evaluate_predictions(test_df, baseline_predictions)\n",
        "baseline_results_path = Path(CONFIG[\"output_dir\"]) / \"baseline_results\" / \"baseline_evaluation_results.csv\"\n",
        "baseline_metrics_path = Path(CONFIG[\"output_dir\"]) / \"baseline_results\" / \"baseline_metrics.json\"\n",
        "baseline_results_df.to_csv(baseline_results_path, index=False)\n",
        "with open(baseline_metrics_path, 'w') as f:\n",
        "    json.dump(baseline_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\n--- Baseline Metrics ---\")\n",
        "print(json.dumps(baseline_metrics, indent=2))\n",
        "print(f\"✅ Baseline results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn8K0rclwu2y",
        "outputId": "64eec8de-8d0a-42e7-b585-a14d4429a5f7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 1: BASELINE EVALUATION\n",
            "==================================================\n",
            "\n",
            "--- Running Unsloth native inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inference Batches: 100%|██████████| 6/6 [02:13<00:00, 22.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Baseline Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.23005952380952382,\n",
            "  \"total_samples\": 176,\n",
            "  \"json_parse_failures\": 3,\n",
            "  \"failure_rate\": 0.017045454545454544\n",
            "}\n",
            "✅ Baseline results saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "F4BAeCLu9GXd",
        "outputId": "09fc0e40-ea11-4096-d418-42b5fb88079d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     problem_index  rigorous_score  \\\n",
              "0             2680        0.000000   \n",
              "1             4479        0.250000   \n",
              "2             5650        0.000000   \n",
              "3             7201        0.000000   \n",
              "4             2796        0.250000   \n",
              "..             ...             ...   \n",
              "171            146        0.000000   \n",
              "172           4165        0.250000   \n",
              "173           1959        0.000000   \n",
              "174           4877        0.500000   \n",
              "175           3068        0.333333   \n",
              "\n",
              "                                         expected_json  \\\n",
              "0    {\"L1\": \"6/12=0.5\", \"L2\": \"2+0.5=2.5\", \"L3\": \"2...   \n",
              "1    {\"L1\": \"21/14=1.5\", \"L2\": \"8+3+12+1.5=24.5\", \"...   \n",
              "2    {\"L1\": \"75/60=1.25\", \"L2\": \"40*1.25=50\", \"L3\":...   \n",
              "3    {\"L1\": \"200/2=100\", \"L2\": \"100*0.3=30\", \"L3\": ...   \n",
              "4    {\"L1\": \"0.4*80=32\", \"L2\": \"2*32=64\", \"L3\": \"10...   \n",
              "..                                                 ...   \n",
              "171  {\"L1\": \"22+3=25\", \"L2\": \"25-4=21\", \"L3\": \"22+2...   \n",
              "172  {\"L1\": \"2*2=4\", \"L2\": \"2*2*2=8\", \"L3\": \"2+4+8=...   \n",
              "173          {\"L1\": \"11-2=9\", \"L2\": \"9-3=6\", \"FA\": \"\"}   \n",
              "174  {\"L1\": \"12*2=24\", \"L2\": \"8-2=6\", \"L3\": \"6+8+6=...   \n",
              "175  {\"L1\": \"20-5=15\", \"L2\": \"3+7=10\", \"L3\": \"15-10...   \n",
              "\n",
              "                                        predicted_json  \\\n",
              "0    {\"L1\": \"6/12=0.5 years\", \"L2\": \"2+0.5=2.5 year...   \n",
              "1    {\"L1\": \"1.5 bushels of corn.\", \"L2\": \"24.5 bus...   \n",
              "2    {\"L1\": \"1.25 pages/minute * 75 = 93.75\", \"L2\":...   \n",
              "3    {\"L1\": \"100 visitors\", \"L2\": \"210\", \"L3\": \"210...   \n",
              "4    {\"L1\": \"32\", \"L2\": \"64\", \"L3\": \"100-64=36\", \"L...   \n",
              "..                                                 ...   \n",
              "171  {\"L1\": \"25 minutes\", \"L2\": \"21 minutes\", \"L3\":...   \n",
              "172  {\"L1\": \"2 x 2 = 4\", \"L2\": \"2 x 2 x 2 = 8\", \"L3...   \n",
              "173          {\"L1\": \"9 - 2 = 7\", \"L2\": \"6\", \"FA\": \"6\"}   \n",
              "174  {\"L1\": \"12 * 2 = 24\", \"L2\": \"24 + 8 - 2 = 20\",...   \n",
              "175  {\"L1\": \"15 - 5 = 10\", \"L2\": \"3 + 7 = 10\", \"L3\"...   \n",
              "\n",
              "                                  full_prediction_text  \n",
              "0    ```json\\n{\\n  \"L1\": \"6/12=0.5 years\",\\n  \"L2\":...  \n",
              "1    ```json\\n{\\n  \"L1\": \"1.5 bushels of corn.\",\\n ...  \n",
              "2    ```json\\n{\\n  \"L1\": \"1.25 pages/minute * 75 = ...  \n",
              "3    ```json\\n{\\n  \"L1\": \"100 visitors\",\\n  \"L2\": \"...  \n",
              "4    ```json\\n{\\n  \"L1\": \"32\",\\n  \"L2\": \"64\",\\n  \"L...  \n",
              "..                                                 ...  \n",
              "171  ```json\\n{\\n  \"L1\": \"25 minutes\",\\n  \"L2\": \"21...  \n",
              "172  ```json\\n{\\n  \"L1\": \"2 x 2 = 4\",\\n  \"L2\": \"2 x...  \n",
              "173  ```json\\n{\\n  \"L1\": \"9 - 2 = 7\",\\n  \"L2\": \"6\",...  \n",
              "174  ```json\\n{\\n  \"L1\": \"12 * 2 = 24\",\\n  \"L2\": \"2...  \n",
              "175  ```json\\n{\\n  \"L1\": \"15 - 5 = 10\",\\n  \"L2\": \"3...  \n",
              "\n",
              "[176 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93117fbb-c6de-482c-ba1c-bff38d27efbc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>problem_index</th>\n",
              "      <th>rigorous_score</th>\n",
              "      <th>expected_json</th>\n",
              "      <th>predicted_json</th>\n",
              "      <th>full_prediction_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2680</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"L1\": \"6/12=0.5\", \"L2\": \"2+0.5=2.5\", \"L3\": \"2...</td>\n",
              "      <td>{\"L1\": \"6/12=0.5 years\", \"L2\": \"2+0.5=2.5 year...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"6/12=0.5 years\",\\n  \"L2\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4479</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>{\"L1\": \"21/14=1.5\", \"L2\": \"8+3+12+1.5=24.5\", \"...</td>\n",
              "      <td>{\"L1\": \"1.5 bushels of corn.\", \"L2\": \"24.5 bus...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"1.5 bushels of corn.\",\\n ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5650</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"L1\": \"75/60=1.25\", \"L2\": \"40*1.25=50\", \"L3\":...</td>\n",
              "      <td>{\"L1\": \"1.25 pages/minute * 75 = 93.75\", \"L2\":...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"1.25 pages/minute * 75 = ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7201</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"L1\": \"200/2=100\", \"L2\": \"100*0.3=30\", \"L3\": ...</td>\n",
              "      <td>{\"L1\": \"100 visitors\", \"L2\": \"210\", \"L3\": \"210...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"100 visitors\",\\n  \"L2\": \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2796</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>{\"L1\": \"0.4*80=32\", \"L2\": \"2*32=64\", \"L3\": \"10...</td>\n",
              "      <td>{\"L1\": \"32\", \"L2\": \"64\", \"L3\": \"100-64=36\", \"L...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"32\",\\n  \"L2\": \"64\",\\n  \"L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>146</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"L1\": \"22+3=25\", \"L2\": \"25-4=21\", \"L3\": \"22+2...</td>\n",
              "      <td>{\"L1\": \"25 minutes\", \"L2\": \"21 minutes\", \"L3\":...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"25 minutes\",\\n  \"L2\": \"21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>4165</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>{\"L1\": \"2*2=4\", \"L2\": \"2*2*2=8\", \"L3\": \"2+4+8=...</td>\n",
              "      <td>{\"L1\": \"2 x 2 = 4\", \"L2\": \"2 x 2 x 2 = 8\", \"L3...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"2 x 2 = 4\",\\n  \"L2\": \"2 x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>1959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"L1\": \"11-2=9\", \"L2\": \"9-3=6\", \"FA\": \"\"}</td>\n",
              "      <td>{\"L1\": \"9 - 2 = 7\", \"L2\": \"6\", \"FA\": \"6\"}</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"9 - 2 = 7\",\\n  \"L2\": \"6\",...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>4877</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>{\"L1\": \"12*2=24\", \"L2\": \"8-2=6\", \"L3\": \"6+8+6=...</td>\n",
              "      <td>{\"L1\": \"12 * 2 = 24\", \"L2\": \"24 + 8 - 2 = 20\",...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"12 * 2 = 24\",\\n  \"L2\": \"2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3068</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>{\"L1\": \"20-5=15\", \"L2\": \"3+7=10\", \"L3\": \"15-10...</td>\n",
              "      <td>{\"L1\": \"15 - 5 = 10\", \"L2\": \"3 + 7 = 10\", \"L3\"...</td>\n",
              "      <td>```json\\n{\\n  \"L1\": \"15 - 5 = 10\",\\n  \"L2\": \"3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>176 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93117fbb-c6de-482c-ba1c-bff38d27efbc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93117fbb-c6de-482c-ba1c-bff38d27efbc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93117fbb-c6de-482c-ba1c-bff38d27efbc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-319ced79-7733-4a96-b03c-92e52c0f9f43\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-319ced79-7733-4a96-b03c-92e52c0f9f43')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-319ced79-7733-4a96-b03c-92e52c0f9f43 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_52caf95e-fb90-40c0-841b-b942a581c957\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('baseline_results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_52caf95e-fb90-40c0-841b-b942a581c957 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('baseline_results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "baseline_results_df",
              "summary": "{\n  \"name\": \"baseline_results_df\",\n  \"rows\": 176,\n  \"fields\": [\n    {\n      \"column\": \"problem_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2279,\n        \"min\": 113,\n        \"max\": 7439,\n        \"num_unique_values\": 175,\n        \"samples\": [\n          6728,\n          7304,\n          387\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rigorous_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32207263515071466,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.2,\n          0.4,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_json\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 175,\n        \"samples\": [\n          \"{\\\"L1\\\": \\\"42/2=21\\\", \\\"L2\\\": \\\"21+17=38\\\", \\\"L3\\\": \\\"38-3=35\\\", \\\"FA\\\": \\\"\\\"}\",\n          \"{\\\"L1\\\": \\\"50+10=60\\\", \\\"L2\\\": \\\"60*20=1200\\\", \\\"L3\\\": \\\"2400-1200=1200\\\", \\\"L4\\\": \\\"1200/50=24\\\", \\\"L5\\\": \\\"20+24=44\\\", \\\"FA\\\": \\\"\\\"}\",\n          \"{\\\"L1\\\": \\\"12/2=6\\\", \\\"L2\\\": \\\"6-3=3\\\", \\\"L3\\\": \\\"12+6=18\\\", \\\"L4\\\": \\\"30-18=12\\\", \\\"L5\\\": \\\"12/3=4\\\", \\\"FA\\\": \\\"\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_json\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 173,\n        \"samples\": [\n          \"{\\\"L1\\\": \\\"8 + 4 = 12\\\", \\\"L2\\\": \\\"12\\\", \\\"L3\\\": \\\"3\\\", \\\"L4\\\": \\\"9\\\"}\",\n          \"{\\\"L1\\\": \\\"10 liters of juice\\\", \\\"L2\\\": \\\"100000\\\", \\\"L3\\\": \\\"400*250=100000\\\", \\\"FA\\\": \\\"400\\\"}\",\n          \"{\\\"L1\\\": \\\"1/6 * 240 students = 40 students. So 40 students read three or more novels.\\\", \\\"L2\\\": \\\"35/100 * 240 students = 84 students. So 84 students read two novels.\\\", \\\"L3\\\": \\\"240 - (40 + 84) = 240 - 124 = 116 students. So 116 students read less than two novels.\\\", \\\"FA\\\": \\\"116\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_prediction_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 175,\n        \"samples\": [\n          \"```json\\n{\\n  \\\"L1\\\": \\\"21\\\",\\n  \\\"L2\\\": \\\"38\\\",\\n  \\\"L3\\\": \\\"35\\\",\\n  \\\"FA\\\": \\\"35\\\"\\n}\\n```\",\n          \"```json\\n{\\n  \\\"L1\\\": \\\"50*20=1000\\\",\\n  \\\"L2\\\": \\\"1200\\\",\\n  \\\"L3\\\": \\\"2400-1200=1200\\\",\\n  \\\"L4\\\": \\\"24\\\",\\n  \\\"L5\\\": \\\"44\\\"\\n}\\n```\",\n          \"```json\\n{\\n  \\\"L1\\\": \\\"6/2=3\\\",\\n  \\\"L2\\\": \\\"6-3=3\\\",\\n  \\\"L3\\\": \\\"12+6=18\\\",\\n  \\\"L4\\\": \\\"30-18=12\\\",\\n  \\\"L5\\\": \\\"12/3=4\\\"\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# PHASE 2: FINE-TUNE\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 2: FINE-TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Apply LoRa adapters to the existing model object for training\n",
        "model = prepare_model_for_lora_training(model)\n",
        "\n",
        "# 2. Prepare the Hugging Face Dataset for the trainer\n",
        "train_dataset = create_training_dataset(train_df, tokenizer)\n",
        "\n",
        "# 3. Run the fine-tuning process\n",
        "training_stats = run_fine_tuning(model, tokenizer, train_dataset)\n",
        "\n",
        "print(\"✅ Fine-tuning complete. The model object in memory is now updated.\")\n",
        "\n",
        "# 4. Save Training Log and Configuration\n",
        "log_history = [log for log in training_stats.log_history if 'loss' in log]\n",
        "log_history_df = pd.DataFrame(log_history)\n",
        "log_path = output_dir / \"training_log.csv\"\n",
        "log_history_df.to_csv(log_path, index=False)\n",
        "print(f\"✅ Training log saved to: {log_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2bd917ad5e9441da6ad3f4ca9e21ffa",
            "b101a32bbc364db8af6a41b282437ef4",
            "7b5ab137bbc7493798fbe79962b0e54c",
            "e41a8b764ccd4dc7994d8cfd3827a07e",
            "9b8496827e284c9aba7a13c15178f4e9",
            "107136d00bc443ff81a99ea0b9f113eb",
            "18742633796b47f6a3ade3db6afc7ec9",
            "7d3879e362d14062b3b89a7c620492a4",
            "eb7e37cc9a314f13bd27ef41f5371c5d",
            "19df95d3da814ffdacd86f1b2e5203a5",
            "37976d74dc694b70af9ebf613ec40a76",
            "6223048f96f14d54ba074359d60f4d74",
            "a2afcbe827014022b6b2148fb14fefcc",
            "0e12f9f990fe460fbc22b508839bfad7",
            "017d925bf8a4494fbdbb0f327daba382",
            "55f3d4a8d6244c59bcfa01f3bee2fd68",
            "d691bbf9347d43e3b91e9bbc2f3cf4f9",
            "f2d9bda0e90e4117b67bcf3fa57d8cf1",
            "eb18090aeef94f40a8b49f0866590c8b",
            "7e349baa3f1c49c98d01aff2c8be1b05",
            "026b532f782e450181c7b4a8e698f50b",
            "edafcfe1c7d1461b96c0d1dd170a551a",
            "51b0f45a833e4c70b54405ec24a75376",
            "f89b23abf48c4272b0c9e985a302c0e0",
            "b88060531c644cd9bb129a6c9b5265f1",
            "a48f048575af437088382b865ba8ec32",
            "24c73fe6069c4a97b8575c03cb2c4326",
            "ac56ff0485f24767a2898fb4c7580212",
            "600536fb21904ab595b8107961140c35",
            "0124a0cab1f344b39c172da0ea4ed26a",
            "f616244bc38e4911bd127b66614739d4",
            "43457faf8ea6461886c83fe3452353e6",
            "ed2a436dec244f3cb3d9ea760a8e5eac"
          ]
        },
        "id": "F4bE83821_Gi",
        "outputId": "e5e4c112-5fb5-4e74-9006-e22c9ffdf1b0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 2: FINE-TUNING\n",
            "==================================================\n",
            "Applying LoRa adapters to the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/701 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2bd917ad5e9441da6ad3f4ca9e21ffa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/701 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6223048f96f14d54ba074359d60f4d74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=12):   0%|          | 0/701 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51b0f45a833e4c70b54405ec24a75376"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting fine-tuning for 1 epoch(s) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 701 | Num Epochs = 2 | Total steps = 88\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [88/88 08:06, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.787500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.735800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.726200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.544100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.386500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.319400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.197700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.139400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.123600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.067600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.063300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.049200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.034600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.010500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.006700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.009700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.021700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.013100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.011100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.041000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.020400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.006400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.004700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.011500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.010500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.012800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.006700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Fine-tuning finished! Saving final adapter to /content/experiments/equation_extraction_gemma3-1b-unsloth_20250807_1859/final_adapter\n",
            "✅ Fine-tuning complete. The model object in memory is now updated.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TrainOutput' object has no attribute 'log_history'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-584187839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 4. Save Training Log and Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mlog_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'loss'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mlog_history_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mlog_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"training_log.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TrainOutput' object has no attribute 'log_history'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# PHASE 3: FINAL EVALUATION\n",
        "# ===================================================================\n",
        "\n",
        "import gc\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 3: FINAL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Run inference with the fine-tuned LoRa model\n",
        "final_predictions = run_unsloth_inference(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    df_to_eval=test_df\n",
        ")\n",
        "\n",
        "# Evaluate and save final results\n",
        "final_results_df, final_metrics = evaluate_predictions(test_df, final_predictions)\n",
        "final_results_path = Path(CONFIG[\"output_dir\"]) / \"final_results\" / \"final_evaluation_results.csv\"\n",
        "final_metrics_path = Path(CONFIG[\"output_dir\"]) / \"final_results\" / \"final_metrics.json\"\n",
        "final_results_df.to_csv(final_results_path, index=False)\n",
        "with open(final_metrics_path, 'w') as f:\n",
        "    json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\n--- Final Metrics ---\")\n",
        "print(json.dumps(final_metrics, indent=2))\n",
        "print(f\"✅ Final results saved.\")\n",
        "\n",
        "# --- Clean up ---\n",
        "del model, tokenizer, final_predictions, final_results_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofz29V2wcrL9",
        "outputId": "8a1540d8-4a75-406a-cba1-bff0362ceb03"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PHASE 3: FINAL EVALUATION\n",
            "==================================================\n",
            "\n",
            "--- Running Unsloth native inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inference Batches: 100%|██████████| 6/6 [02:47<00:00, 27.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.9310673701298703,\n",
            "  \"total_samples\": 176,\n",
            "  \"json_parse_failures\": 0,\n",
            "  \"failure_rate\": 0.0\n",
            "}\n",
            "✅ Final results saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL COMPARISON ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n--- Baseline Metrics ---\")\n",
        "print(json.dumps(baseline_metrics, indent=2))\n",
        "\n",
        "print(\"\\n--- Final Fine-Tuned Metrics ---\")\n",
        "print(json.dumps(final_metrics, indent=2))\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n✅✅✅ Experiment Complete! ✅✅✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C3XhrN6c3OR",
        "outputId": "1447e4e1-7b33-411a-c509-493cffbf211b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PERFORMANCE COMPARISON\n",
            "==================================================\n",
            "\n",
            "--- Baseline Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.23005952380952382,\n",
            "  \"total_samples\": 176,\n",
            "  \"json_parse_failures\": 3,\n",
            "  \"failure_rate\": 0.017045454545454544\n",
            "}\n",
            "\n",
            "--- Final Fine-Tuned Metrics ---\n",
            "{\n",
            "  \"mean_rigorous_score\": 0.9310673701298703,\n",
            "  \"total_samples\": 176,\n",
            "  \"json_parse_failures\": 0,\n",
            "  \"failure_rate\": 0.0\n",
            "}\n",
            "\n",
            "==================================================\n",
            "\n",
            "✅✅✅ Experiment Complete! ✅✅✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Clean up ---\n",
        "del model, tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "hhQuugTf2gh1",
        "outputId": "b0bd23f3-ad74-4a81-b0b8-ddd5a5886ebb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2898057996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Final Clean up ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPRESSING RESULTS FOR DOWNLOAD\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define paths from the global CONFIG\n",
        "output_dir = Path(CONFIG[\"output_dir\"])\n",
        "experiment_id = CONFIG[\"experiment_id\"]\n",
        "adapter_path = Path(CONFIG[\"final_adapter_dir\"])\n",
        "\n",
        "# Define the name and location of the output zip file\n",
        "zip_path = output_dir / f\"{experiment_id}_results.zip\"\n",
        "\n",
        "# List of files and directories to be included in the zip archive\n",
        "files_to_zip = [\n",
        "    output_dir / \"baseline_results\" / \"baseline_evaluation_results.csv\",\n",
        "    output_dir / \"baseline_results\" / \"baseline_metrics.json\",\n",
        "    output_dir / \"final_results\" / \"final_evaluation_results.csv\",\n",
        "    output_dir / \"final_results\" / \"final_metrics.json\",\n",
        "    output_dir / \"training_log.csv\",\n",
        "    output_dir / \"config.json\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        print(f\"Creating zip archive at: {zip_path}\")\n",
        "        for file_path in files_to_zip:\n",
        "            if file_path.exists():\n",
        "                # The arcname is the path of the file relative to the experiment directory,\n",
        "                # which keeps the folder structure (e.g., 'baseline_results/...') inside the zip.\n",
        "                arcname = file_path.relative_to(output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                print(f\"  - Adding: {arcname}\")\n",
        "            else:\n",
        "                print(f\"  - Skipping (not found): {file_path}\")\n",
        "\n",
        "    print(f\"\\n✅ Successfully created results zip archive at: {zip_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An error occurred while creating the zip file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEVICvv4cLPV",
        "outputId": "07945b0f-b120-44e4-d38b-2ca32f69b8f3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "COMPRESSING RESULTS FOR DOWNLOAD\n",
            "==================================================\n",
            "Creating zip archive at: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250807_1859/equation_extraction_gemma3-1b-unsloth_20250807_1859_results.zip\n",
            "  - Adding: baseline_results/baseline_evaluation_results.csv\n",
            "  - Adding: baseline_results/baseline_metrics.json\n",
            "  - Adding: final_results/final_evaluation_results.csv\n",
            "  - Adding: final_results/final_metrics.json\n",
            "  - Skipping (not found): /content/experiments/equation_extraction_gemma3-1b-unsloth_20250807_1859/training_log.csv\n",
            "  - Adding: config.json\n",
            "\n",
            "✅ Successfully created results zip archive at: /content/experiments/equation_extraction_gemma3-1b-unsloth_20250807_1859/equation_extraction_gemma3-1b-unsloth_20250807_1859_results.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmMYHwSBINog"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}