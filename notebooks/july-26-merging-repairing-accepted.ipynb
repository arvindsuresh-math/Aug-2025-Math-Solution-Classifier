{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40e2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali: 341\n",
      "Arvind: 91\n",
      "Ling: 110\n",
      "Mauro: 312\n",
      "Yewei: 290\n",
      "Total: 1144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "\n",
    "df_ali = pd.read_csv(parent_dir + \"ali.csv\")\n",
    "df_arvind = pd.read_csv(parent_dir + \"arvind.csv\")\n",
    "df_ling = pd.read_csv(parent_dir + \"ling.csv\")\n",
    "df_mauro = pd.read_csv(parent_dir + \"mauro.csv\")\n",
    "df_yewei = pd.read_csv(parent_dir + \"yewei.csv\")\n",
    "\n",
    "df_ali = df_ali[df_ali[\"status\"] == \"accepted\"]\n",
    "df_arvind = df_arvind[df_arvind[\"status\"] == \"accepted\"]\n",
    "df_ling = df_ling[df_ling[\"status\"] == \"accepted\"]\n",
    "df_mauro = df_mauro[df_mauro[\"status\"] == \"accepted\"]\n",
    "df_yewei = df_yewei[df_yewei[\"status\"] == \"accepted\"]\n",
    "\n",
    "print(\"Ali:\", len(df_ali))\n",
    "print(\"Arvind:\", len(df_arvind))\n",
    "print(\"Ling:\", len(df_ling))\n",
    "print(\"Mauro:\", len(df_mauro))\n",
    "print(\"Yewei:\", len(df_yewei))\n",
    "print(\"Total:\", len(df_ali) + len(df_arvind) + len(df_ling) + len(df_mauro) + len(df_yewei))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dc56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_catalogs = {\n",
    "    \"ali\": df_ali,\n",
    "    \"arvind\": df_arvind,\n",
    "    \"ling\": df_ling,\n",
    "    \"mauro\": df_mauro,\n",
    "    \"yewei\": df_yewei\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6623ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC REPORT: Validation Catalog vs Accepted Files\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   341    341\n",
      "Total      341    341\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 341\n",
      "   • Samples marked 'accepted': 341\n",
      "   • Files actually found: 341\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted    91     91\n",
      "Total       91     91\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 91\n",
      "   • Samples marked 'accepted': 91\n",
      "   • Files actually found: 91\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   110    110\n",
      "Total      110    110\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 110\n",
      "   • Samples marked 'accepted': 110\n",
      "   • Files actually found: 110\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   312    312\n",
      "Total      312    312\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 312\n",
      "   • Samples marked 'accepted': 312\n",
      "   • Files actually found: 312\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   290    290\n",
      "Total      290    290\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 290\n",
      "   • Samples marked 'accepted': 290\n",
      "   • Files actually found: 290\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total reviewers checked: 5\n",
      "   • Total inconsistencies across all reviewers: 0\n",
      "   • 🎉 All validation catalogs are consistent with accepted files!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to check if a file exists in the accepted folder\n",
    "def check_file_exists_in_accepted(filepath, base_dir=\"../data/conceptual-errors-accepted/\"):\n",
    "    \"\"\"\n",
    "    Check if a file exists in the conceptual-errors-accepted folder.\n",
    "    The filepath in the catalog points to the accepted location if accepted.\n",
    "    \"\"\"\n",
    "    if pd.isna(filepath) or filepath == \"\":\n",
    "        return False\n",
    "    \n",
    "    # Convert to Path object and normalize path separators\n",
    "    # Replace Windows backslashes with forward slashes for cross-platform compatibility\n",
    "    normalized_filepath = str(filepath).replace('\\\\', '/')\n",
    "    file_path = Path(normalized_filepath)\n",
    "    \n",
    "    # If it's a relative path, make it relative to the notebook location\n",
    "    if not file_path.is_absolute():\n",
    "        # Assuming we're running from notebooks/ directory\n",
    "        full_path = Path(\"..\") / file_path\n",
    "    else:\n",
    "        full_path = file_path\n",
    "    \n",
    "    return full_path.exists()\n",
    "\n",
    "# Main diagnostic function\n",
    "def run_diagnostics(validation_catalogs):\n",
    "    \"\"\"\n",
    "    Run diagnostic checks on validation catalogs vs actual files in accepted folder.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC REPORT: Validation Catalog vs Accepted Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_inconsistencies = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        df_check = df.copy()\n",
    "        \n",
    "        # Add the 'added' column by checking if files exist\n",
    "        df_check['added'] = df_check['filepath'].apply(check_file_exists_in_accepted)\n",
    "        \n",
    "        # Create cross-tabulation\n",
    "        crosstab = pd.crosstab(df_check['status'], df_check['added'], \n",
    "                              margins=True, margins_name='Total')\n",
    "        \n",
    "        print(\"Cross-tabulation (Status vs File Actually Added):\")\n",
    "        print(crosstab)\n",
    "        print()\n",
    "        \n",
    "        # Identify specific inconsistencies\n",
    "        accepted_but_not_added = df_check[(df_check['status'] == 'accepted') & \n",
    "                                         (df_check['added'] == False)]\n",
    "        not_accepted_but_added = df_check[(df_check['status'] != 'accepted') & \n",
    "                                         (df_check['added'] == True)]\n",
    "        \n",
    "        inconsistency_count = len(accepted_but_not_added) + len(not_accepted_but_added)\n",
    "        total_inconsistencies += inconsistency_count\n",
    "        \n",
    "        if inconsistency_count > 0:\n",
    "            print(\"🚨 INCONSISTENCIES FOUND:\")\n",
    "            \n",
    "            if len(accepted_but_not_added) > 0:\n",
    "                print(f\"   • {len(accepted_but_not_added)} samples marked 'accepted' but file NOT found:\")\n",
    "                for idx, row in accepted_but_not_added.iterrows():\n",
    "                    print(f\"     - Index {row['index']}: {row['tier']} | {row['mutation_type']} | {row['target_variable']}\")\n",
    "                    print(f\"       Expected file: {row['filepath']}\")\n",
    "                    # Show normalized path for debugging\n",
    "                    normalized_path = str(row['filepath']).replace('\\\\', '/')\n",
    "                    full_check_path = Path(\"..\") / normalized_path\n",
    "                    print(f\"       Checking path: {full_check_path}\")\n",
    "                    print(f\"       Path exists: {full_check_path.exists()}\")\n",
    "                print()\n",
    "            \n",
    "            if len(not_accepted_but_added) > 0:\n",
    "                print(f\"   • {len(not_accepted_but_added)} samples NOT marked 'accepted' but file exists:\")\n",
    "                for idx, row in not_accepted_but_added.iterrows():\n",
    "                    print(f\"     - Index {row['index']}: Status='{row['status']}' but file exists at {row['filepath']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"✅ No inconsistencies found!\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_samples = len(df_check)\n",
    "        accepted_count = len(df_check[df_check['status'] == 'accepted'])\n",
    "        files_found = len(df_check[df_check['added'] == True])\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Total samples in catalog: {total_samples}\")\n",
    "        print(f\"   • Samples marked 'accepted': {accepted_count}\")\n",
    "        print(f\"   • Files actually found: {files_found}\")\n",
    "        print(f\"   • Inconsistencies: {inconsistency_count}\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        if accepted_count > 0:\n",
    "            success_rate = ((accepted_count - len(accepted_but_not_added)) / accepted_count) * 100\n",
    "            print(f\"   • Success rate (accepted → file created): {success_rate:.1f}%\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"🎯 OVERALL SUMMARY:\")\n",
    "    print(f\"   • Total reviewers checked: {len(validation_catalogs)}\")\n",
    "    print(f\"   • Total inconsistencies across all reviewers: {total_inconsistencies}\")\n",
    "    if total_inconsistencies == 0:\n",
    "        print(\"   • 🎉 All validation catalogs are consistent with accepted files!\")\n",
    "    else:\n",
    "        print(f\"   • ⚠️  Found {total_inconsistencies} inconsistencies that need investigation.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run the diagnostics\n",
    "run_diagnostics(validation_catalogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af998fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Ling's copied catalog...\n",
      "================================================================================\n",
      "DIAGNOSTIC REPORT: Ling's Copied Files\n",
      "================================================================================\n",
      "📋 LING'S COPIED CATALOG\n",
      "--------------------------------------------------\n",
      "Total entries in catalog: 388\n",
      "Status distribution:\n",
      "   • todo: 187\n",
      "   • accepted: 110\n",
      "   • rejected: 87\n",
      "   • skipped: 4\n",
      "\n",
      "Accepted entries: 110\n",
      "Cross-tabulation (Status vs File Actually Added in Copy):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   110    110\n",
      "Total      110    110\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total accepted samples in catalog: 110\n",
      "   • Files actually found in copy folder: 110\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file found in copy): 100.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load Ling's copied catalog\n",
    "def load_ling_copy_catalog():\n",
    "    \"\"\"Load Ling's copied validation catalog\"\"\"\n",
    "    ling_copy_path = \"../data/conceptual-error-candidates/validation_catalog_ling_copy.csv\"\n",
    "    df_ling_copy = pd.read_csv(ling_copy_path)\n",
    "    return df_ling_copy\n",
    "\n",
    "# Function to check if a file exists in Ling's copied accepted folder\n",
    "def check_file_exists_in_ling_copy(filepath, base_dir=\"../data/conceptual-errors-accepted-copy/\"):\n",
    "    \"\"\"\n",
    "    Check if a file exists in Ling's copied conceptual-errors-accepted folder.\n",
    "    \"\"\"\n",
    "    if pd.isna(filepath) or filepath == \"\":\n",
    "        return False\n",
    "    \n",
    "    # Convert to Path object and normalize path separators\n",
    "    normalized_filepath = str(filepath).replace('\\\\', '/')\n",
    "    file_path = Path(normalized_filepath)\n",
    "    \n",
    "    # Extract the relative path from conceptual-errors-accepted onwards\n",
    "    # Handle both cases: full path or relative path\n",
    "    if \"conceptual-errors-accepted\" in str(file_path):\n",
    "        # Extract everything after \"conceptual-errors-accepted\"\n",
    "        parts = file_path.parts\n",
    "        try:\n",
    "            accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "            relative_path = Path(*parts[accepted_idx + 1:])\n",
    "        except StopIteration:\n",
    "            # If we can't find the pattern, use the full path as is\n",
    "            relative_path = file_path\n",
    "    else:\n",
    "        relative_path = file_path\n",
    "    \n",
    "    # Build the full path to Ling's copy\n",
    "    if not relative_path.is_absolute():\n",
    "        full_path = Path(\"..\") / \"data\" / \"conceptual-errors-accepted-copy\" / relative_path\n",
    "    else:\n",
    "        full_path = relative_path\n",
    "    \n",
    "    return full_path.exists()\n",
    "\n",
    "# Main diagnostic function for Ling's copy\n",
    "def run_ling_copy_diagnostics():\n",
    "    \"\"\"\n",
    "    Run diagnostic checks on Ling's copied validation catalog vs copied accepted files.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC REPORT: Ling's Copied Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load Ling's copied catalog\n",
    "    df_ling_copy = load_ling_copy_catalog()\n",
    "    \n",
    "    print(f\"📋 LING'S COPIED CATALOG\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total entries in catalog: {len(df_ling_copy)}\")\n",
    "    \n",
    "    # Show status distribution\n",
    "    status_counts = df_ling_copy['status'].value_counts()\n",
    "    print(\"Status distribution:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"   • {status}: {count}\")\n",
    "    print()\n",
    "    \n",
    "    # Filter for accepted entries only\n",
    "    df_accepted = df_ling_copy[df_ling_copy['status'] == 'accepted'].copy()\n",
    "    print(f\"Accepted entries: {len(df_accepted)}\")\n",
    "    \n",
    "    if len(df_accepted) == 0:\n",
    "        print(\"No accepted entries found in Ling's catalog.\")\n",
    "        return\n",
    "    \n",
    "    # Add the 'added' column by checking if files exist in the copy folder\n",
    "    df_accepted['added'] = df_accepted['filepath'].apply(check_file_exists_in_ling_copy)\n",
    "    \n",
    "    # Create cross-tabulation\n",
    "    crosstab = pd.crosstab(df_accepted['status'], df_accepted['added'], \n",
    "                          margins=True, margins_name='Total')\n",
    "    \n",
    "    print(\"Cross-tabulation (Status vs File Actually Added in Copy):\")\n",
    "    print(crosstab)\n",
    "    print()\n",
    "    \n",
    "    # Identify specific inconsistencies\n",
    "    accepted_but_not_added = df_accepted[(df_accepted['status'] == 'accepted') & \n",
    "                                       (df_accepted['added'] == False)]\n",
    "    \n",
    "    inconsistency_count = len(accepted_but_not_added)\n",
    "    \n",
    "    if inconsistency_count > 0:\n",
    "        print(\"🚨 INCONSISTENCIES FOUND:\")\n",
    "        print(f\"   • {len(accepted_but_not_added)} samples marked 'accepted' but file NOT found in copy:\")\n",
    "        \n",
    "        for idx, row in accepted_but_not_added.iterrows():\n",
    "            print(f\"     - Index {row['index']}: {row['tier']} | {row['mutation_type']} | {row['target_variable']}\")\n",
    "            print(f\"       Expected file: {row['filepath']}\")\n",
    "            \n",
    "            # Show what path we're actually checking\n",
    "            normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "            file_path = Path(normalized_filepath)\n",
    "            \n",
    "            if \"conceptual-errors-accepted\" in str(file_path):\n",
    "                parts = file_path.parts\n",
    "                try:\n",
    "                    accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "                    relative_path = Path(*parts[accepted_idx + 1:])\n",
    "                except StopIteration:\n",
    "                    relative_path = file_path\n",
    "            else:\n",
    "                relative_path = file_path\n",
    "                \n",
    "            full_check_path = Path(\"..\") / \"data\" / \"conceptual-errors-accepted-copy\" / relative_path\n",
    "            print(f\"       Checking path: {full_check_path}\")\n",
    "            print(f\"       Path exists: {full_check_path.exists()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"✅ No inconsistencies found!\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_accepted = len(df_accepted)\n",
    "    files_found = len(df_accepted[df_accepted['added'] == True])\n",
    "    \n",
    "    print(f\"📊 SUMMARY:\")\n",
    "    print(f\"   • Total accepted samples in catalog: {total_accepted}\")\n",
    "    print(f\"   • Files actually found in copy folder: {files_found}\")\n",
    "    print(f\"   • Inconsistencies: {inconsistency_count}\")\n",
    "    \n",
    "    # Calculate success rate\n",
    "    if total_accepted > 0:\n",
    "        success_rate = (files_found / total_accepted) * 100\n",
    "        print(f\"   • Success rate (accepted → file found in copy): {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return df_accepted\n",
    "\n",
    "# Load and analyze Ling's copied files\n",
    "print(\"Loading Ling's copied catalog...\")\n",
    "df_ling_copy_analysis = run_ling_copy_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6bf329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MOVING LING'S FILES TO MAIN FOLDERS\n",
      "================================================================================\n",
      "📋 Loading Ling's copy catalog...\n",
      "Found 110 accepted entries to process\n",
      "\n",
      "📁 Moving accepted files...\n",
      "✅ Moving: tier4/5691/google_gemini-2.5-flash_incomplete_calculation_full_meal_cost_before_voucher_.json\n",
      "✅ Moving: tier4/7184/google_gemini-2.5-flash_operator_swap_total_profits_.json\n",
      "✅ Moving: tier4/7146/google_gemini-2.5-flash_operator_swap_ginger_tsp_.json\n",
      "✅ Moving: tier4/2407/google_gemini-2.5-flash_operator_swap_wanda_treats_.json\n",
      "✅ Moving: tier4/1111/google_gemini-2.5-flash_operator_swap_total_steak_pounds_.json\n",
      "✅ Moving: tier4/6580/google_gemini-2.5-flash_incorrect_operand_cost_per_gallon_dollars_cost_1_cent.json\n",
      "✅ Moving: tier4/3517/google_gemini-2.5-flash_operator_swap_sale_cost_dollars_.json\n",
      "✅ Moving: tier4/2512/google_gemini-2.5-flash_incorrect_operand_produce_cost_eggplant_pounds.json\n",
      "✅ Moving: tier4/7413/google_gemini-2.5-flash_operator_swap_total_profit_.json\n",
      "✅ Moving: tier4/2844/google_gemini-2.5-flash_operator_swap_total_dolls_.json\n",
      "✅ Moving: tier3/7399/google_gemini-2.5-flash_incomplete_calculation_total_fruit_.json\n",
      "✅ Moving: tier3/4305/google_gemini-2.5-flash_input_misrepresentation_total_sales_tuesday_sales.json\n",
      "✅ Moving: tier3/6682/google_gemini-2.5-flash_input_misrepresentation_total_coins_after_gift_gift_coins_in_solution.json\n",
      "✅ Moving: tier3/6146/google_gemini-2.5-flash_input_misrepresentation_total_cost_erika_savings.json\n",
      "✅ Moving: tier3/3001/google_gemini-2.5-flash_input_misrepresentation_sue_total_miles_num_other_ladies.json\n",
      "✅ Moving: tier3/824/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_students_from_25_desk_classrooms.json\n",
      "✅ Moving: tier3/2368/google_gemini-2.5-flash_incomplete_calculation_total_points_first_three_games_.json\n",
      "✅ Moving: tier3/2488/google_gemini-2.5-flash_operator_swap_timothy_movies_2010_.json\n",
      "✅ Moving: tier3/5572/google_gemini-2.5-flash_operator_swap_total_presents_.json\n",
      "✅ Moving: tier3/6454/google_gemini-2.5-flash_operator_swap_remaining_turtles_.json\n",
      "✅ Moving: tier3/1215/google_gemini-2.5-flash_incomplete_calculation_total_people_.json\n",
      "✅ Moving: tier3/841/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_initial_dogs.json\n",
      "✅ Moving: tier3/1192/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_men_and_women.json\n",
      "⚠️  Overwriting: tier3/4534/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_eggs_per_unit.json\n",
      "✅ Moving: tier3/4789/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_burgers.json\n",
      "⚠️  Overwriting: tier3/4789/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_burgers.json\n",
      "⚠️  Overwriting: tier3/7154/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_isabella_dresses.json\n",
      "✅ Moving: tier3/4490/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_number_of_blocks.json\n",
      "⚠️  Overwriting: tier3/1229/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_itzayana_height.json\n",
      "✅ Moving: tier3/1351/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_jake_sold.json\n",
      "✅ Moving: tier3/1679/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_park_area.json\n",
      "✅ Moving: tier3/2454/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_students_wearing_shorts.json\n",
      "✅ Moving: tier3/2677/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_children_cost.json\n",
      "✅ Moving: tier3/2781/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_manu_total_time.json\n",
      "✅ Moving: tier3/3639/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_eaten_slices.json\n",
      "✅ Moving: tier3/2428/google_gemini-2.5-flash_operator_swap_times_per_small_hill_.json\n",
      "✅ Moving: tier3/5354/google_gemini-2.5-flash_incomplete_calculation_total_carrots_.json\n",
      "✅ Moving: tier3/6120/google_gemini-2.5-flash_incorrect_operand_combined_yellow_blue_students_like_red.json\n",
      "✅ Moving: tier3/1729/google_gemini-2.5-flash_incomplete_calculation_total_insects_.json\n",
      "✅ Moving: tier3/7376/google_gemini-2.5-flash_incomplete_calculation_daily_jerky_consumption_.json\n",
      "✅ Moving: tier3/5716/google_gemini-2.5-flash_incomplete_calculation_total_cups_desired_.json\n",
      "✅ Moving: tier3/486/google_gemini-2.5-flash_incorrect_operand_total_friday_rain_thursday_rain.json\n",
      "✅ Moving: tier3/4945/google_gemini-2.5-flash_incorrect_operand_female_cows_total_cows.json\n",
      "✅ Moving: tier3/6429/google_gemini-2.5-flash_incomplete_calculation_total_protein_consumed_.json\n",
      "✅ Moving: tier3/5925/google_gemini-2.5-flash_incomplete_calculation_total_cases_.json\n",
      "✅ Moving: tier3/3760/google_gemini-2.5-flash_incomplete_calculation_total_cookies_.json\n",
      "✅ Moving: tier3/2153/google_gemini-2.5-flash_incomplete_calculation_total_fills_.json\n",
      "✅ Moving: tier3/6625/google_gemini-2.5-flash_incorrect_operand_apples_leftover_num_mini_pies.json\n",
      "✅ Moving: tier3/3941/google_gemini-2.5-flash_incomplete_calculation_total_trial_speed_.json\n",
      "✅ Moving: tier3/4222/google_gemini-2.5-flash_incomplete_calculation_sum_of_first_three_baskets_.json\n",
      "✅ Moving: tier2/6250/google_gemini-2.5-flash_input_misrepresentation_second_distance_first_speed.json\n",
      "✅ Moving: tier2/6048/google_gemini-2.5-flash_input_misrepresentation_cost_fruit_drinks_cost_sandwiches.json\n",
      "✅ Moving: tier2/5194/google_gemini-2.5-flash_input_misrepresentation_total_revenue_total_cost_patches.json\n",
      "✅ Moving: tier2/4439/google_gemini-2.5-flash_input_misrepresentation_final_price_profit_amount.json\n",
      "✅ Moving: tier2/2323/google_gemini-2.5-flash_input_misrepresentation_num_showed_up_rsvp_rate.json\n",
      "✅ Moving: tier2/1625/google_gemini-2.5-flash_operator_swap_new_price_.json\n",
      "✅ Moving: tier2/5184/google_gemini-2.5-flash_incorrect_operand_fabric_still_needed_fabric_has_yards.json\n",
      "✅ Moving: tier2/3893/google_gemini-2.5-flash_incomplete_calculation_quarters_value_.json\n",
      "✅ Moving: tier2/898/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_loss_from_knots.json\n",
      "✅ Moving: tier2/1548/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_bugs_eaten_by_spiders.json\n",
      "✅ Moving: tier2/1489/google_gemini-2.5-flash_operator_swap_jerome_lost_pounds_.json\n",
      "✅ Moving: tier2/5863/google_gemini-2.5-flash_incomplete_calculation_total_volume_.json\n",
      "✅ Moving: tier2/3878/google_gemini-2.5-flash_operator_swap_total_candy_bars_sold_.json\n",
      "✅ Moving: tier2/5883/google_gemini-2.5-flash_operator_swap_final_pennies_left_.json\n",
      "✅ Moving: tier2/153/google_gemini-2.5-flash_operator_swap_total_paid_.json\n",
      "✅ Moving: tier2/445/google_gemini-2.5-flash_operator_swap_bag_price_after_discount_.json\n",
      "✅ Moving: tier2/1293/google_gemini-2.5-flash_input_misrepresentation_irene_shirts_cost_cost_per_short.json\n",
      "✅ Moving: tier2/7306/google_gemini-2.5-flash_operator_swap_cost_lemonade_.json\n",
      "✅ Moving: tier2/4460/google_gemini-2.5-flash_incorrect_operand_total_cost_his_vest_vest_cost.json\n",
      "✅ Moving: tier2/5783/google_gemini-2.5-flash_incomplete_calculation_total_saving_.json\n",
      "✅ Moving: tier2/7437/google_gemini-2.5-flash_operator_swap_total_earnings_.json\n",
      "✅ Moving: tier2/736/google_gemini-2.5-flash_incorrect_operand_food_subtotal_mango_lassi_cost.json\n",
      "✅ Moving: tier1/2444/google_gemini-2.5-flash_incomplete_calculation_total_collected_thus_far_.json\n",
      "✅ Moving: tier1/1021/google_gemini-2.5-flash_incomplete_calculation_total_homework_minutes_spent_.json\n",
      "✅ Moving: tier1/7391/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_dryer_cost.json\n",
      "✅ Moving: tier1/7319/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_distance.json\n",
      "✅ Moving: tier1/5537/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_steps_week4.json\n",
      "✅ Moving: tier1/5496/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_people_after_second_stop.json\n",
      "✅ Moving: tier1/4009/google_gemini-2.5-flash_incomplete_calculation_total_cards_given_away_.json\n",
      "⚠️  Overwriting: tier1/5496/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_people_after_second_stop.json\n",
      "✅ Moving: tier1/6654/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_students.json\n",
      "✅ Moving: tier1/7054/google_gemini-2.5-flash_incomplete_calculation_total_sold_.json\n",
      "✅ Moving: tier1/4165/google_gemini-2.5-flash_incorrect_operand_sandwiches_day3_multiplier_double.json\n",
      "✅ Moving: tier1/5034/google_gemini-2.5-flash_incomplete_calculation_total_spent_hours_.json\n",
      "✅ Moving: tier1/4957/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_doubled_speed.json\n",
      "✅ Moving: tier1/2161/google_gemini-2.5-flash_incorrect_operand_money_after_casino_money_after_donation.json\n",
      "✅ Moving: tier1/7065/google_gemini-2.5-flash_incomplete_calculation_total_wheels_.json\n",
      "✅ Moving: tier1/6478/google_gemini-2.5-flash_incomplete_calculation_total_growth_.json\n",
      "✅ Moving: tier1/3456/google_gemini-2.5-flash_incomplete_calculation_students_in_first_three_groups_.json\n",
      "✅ Moving: tier1/1599/google_gemini-2.5-flash_incomplete_calculation_total_cost_.json\n",
      "✅ Moving: tier1/4689/google_gemini-2.5-flash_incomplete_calculation_total_eggs_dozen_.json\n",
      "✅ Moving: tier1/4181/google_gemini-2.5-flash_input_misrepresentation_savings_week2_multiplier_twice.json\n",
      "✅ Moving: tier1/5160/google_gemini-2.5-flash_operator_swap_rayden_more_geese_.json\n",
      "✅ Moving: tier1/7070/google_gemini-2.5-flash_input_misrepresentation_total_bread_double_meat_total_bread_regular.json\n",
      "✅ Moving: tier1/3761/google_gemini-2.5-flash_operator_swap_fleas_before_treatments_.json\n",
      "✅ Moving: tier1/4794/google_gemini-2.5-flash_incomplete_calculation_yuri_sum_.json\n",
      "✅ Moving: tier1/604/google_gemini-2.5-flash_incomplete_calculation_brothers_bday_second_half_.json\n",
      "✅ Moving: tier1/423/google_gemini-2.5-flash_operator_swap_money_left_.json\n",
      "✅ Moving: tier1/4735/google_gemini-2.5-flash_incorrect_operand_total_ounces_per_day_water_ounces_per_day.json\n",
      "✅ Moving: tier1/4638/google_gemini-2.5-flash_operator_swap_soda_total_cost_.json\n",
      "✅ Moving: tier1/1160/google_gemini-2.5-flash_input_misrepresentation_time_with_new_shoes_time_with_normal_shoes.json\n",
      "✅ Moving: tier1/2263/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_leo_marbles.json\n",
      "✅ Moving: tier1/2443/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_tickets_cost.json\n",
      "✅ Moving: tier1/2449/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_spent.json\n",
      "✅ Moving: tier1/3969/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_minutes_lost_from_Ds.json\n",
      "⚠️  Overwriting: tier1/1230/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_time_per_coat_cycle.json\n",
      "✅ Moving: tier1/887/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_julian_age.json\n",
      "✅ Moving: tier1/7012/google_gemini-2.5-flash_input_misrepresentation_jake_total_earnings_jacob_hourly_wage.json\n",
      "✅ Moving: tier1/615/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_gap_sister_grandma.json\n",
      "⚠️  Overwriting: tier1/615/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_gap_sister_grandma.json\n",
      "\n",
      "📊 File Movement Summary:\n",
      "   • Successfully moved: 110\n",
      "   • Overwritten existing files: 7\n",
      "   • Errors: 0\n",
      "\n",
      "📋 Updating Ling's main validation catalog...\n",
      "✅ Created backup: ../data/conceptual-error-candidates/validation_catalog_ling.csv.backup\n",
      "✅ Updated main catalog: ../data/conceptual-error-candidates/validation_catalog_ling.csv\n",
      "✅ Verification: New main catalog has 110 accepted entries\n",
      "\n",
      "================================================================================\n",
      "🎉 MIGRATION COMPLETE!\n",
      "================================================================================\n",
      "Summary:\n",
      "   • Moved 110 accepted files to main folder\n",
      "   • Updated main validation catalog\n",
      "   • 110 accepted entries now in main catalog\n",
      "   • 7 existing files were overwritten\n",
      "\n",
      "Next steps:\n",
      "   1. Verify the files are in the correct locations\n",
      "   2. Re-run your original diagnostics to confirm consistency\n",
      "   3. Consider cleaning up the copy folders if everything looks good\n",
      "\n",
      "========================================\n",
      "VERIFICATION CHECK\n",
      "========================================\n",
      "Updated Ling catalog has 110 accepted entries\n",
      "Checking 3 sample files...\n",
      "   ✅ Index 5691: True\n",
      "   ✅ Index 7184: True\n",
      "   ✅ Index 7146: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def move_ling_files_to_main():\n",
    "    \"\"\"\n",
    "    Move all of Ling's accepted files from the copy folder to the main accepted folder\n",
    "    and update Ling's validation catalog.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MOVING LING'S FILES TO MAIN FOLDERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Paths\n",
    "    copy_folder = Path(\"../data/conceptual-errors-accepted-copy\")\n",
    "    main_folder = Path(\"../data/conceptual-errors-accepted\")\n",
    "    copy_catalog_path = Path(\"../data/conceptual-error-candidates/validation_catalog_ling_copy.csv\")\n",
    "    main_catalog_path = Path(\"../data/conceptual-error-candidates/validation_catalog_ling.csv\")\n",
    "    \n",
    "    # Step 1: Load the copy catalog to see what we're working with\n",
    "    print(\"📋 Loading Ling's copy catalog...\")\n",
    "    df_ling_copy = pd.read_csv(copy_catalog_path)\n",
    "    accepted_entries = df_ling_copy[df_ling_copy['status'] == 'accepted']\n",
    "    \n",
    "    print(f\"Found {len(accepted_entries)} accepted entries to process\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Move accepted files\n",
    "    print(\"📁 Moving accepted files...\")\n",
    "    moved_count = 0\n",
    "    error_count = 0\n",
    "    overwritten_count = 0\n",
    "    \n",
    "    for idx, row in accepted_entries.iterrows():\n",
    "        try:\n",
    "            # Parse the filepath to get the relative path structure\n",
    "            filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "            file_path = Path(filepath)\n",
    "            \n",
    "            # Extract relative path from conceptual-errors-accepted onwards\n",
    "            if \"conceptual-errors-accepted\" in str(file_path):\n",
    "                parts = file_path.parts\n",
    "                try:\n",
    "                    accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "                    relative_path = Path(*parts[accepted_idx + 1:])\n",
    "                except StopIteration:\n",
    "                    relative_path = file_path\n",
    "            else:\n",
    "                relative_path = file_path\n",
    "            \n",
    "            # Source and destination paths\n",
    "            source_path = copy_folder / relative_path\n",
    "            dest_path = main_folder / relative_path\n",
    "            \n",
    "            # Check if source exists\n",
    "            if not source_path.exists():\n",
    "                print(f\"❌ Source file not found: {source_path}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create destination directory if it doesn't exist\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Check if destination already exists\n",
    "            will_overwrite = dest_path.exists()\n",
    "            if will_overwrite:\n",
    "                overwritten_count += 1\n",
    "                print(f\"⚠️  Overwriting: {relative_path}\")\n",
    "            else:\n",
    "                print(f\"✅ Moving: {relative_path}\")\n",
    "            \n",
    "            # Copy the file (this will overwrite if destination exists)\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            moved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {row['index']}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print()\n",
    "    print(f\"📊 File Movement Summary:\")\n",
    "    print(f\"   • Successfully moved: {moved_count}\")\n",
    "    print(f\"   • Overwritten existing files: {overwritten_count}\")\n",
    "    print(f\"   • Errors: {error_count}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Replace the main catalog with the copy\n",
    "    print(\"📋 Updating Ling's main validation catalog...\")\n",
    "    \n",
    "    try:\n",
    "        # Create backup of original catalog if it exists\n",
    "        if main_catalog_path.exists():\n",
    "            backup_path = main_catalog_path.with_suffix('.csv.backup')\n",
    "            shutil.copy2(main_catalog_path, backup_path)\n",
    "            print(f\"✅ Created backup: {backup_path}\")\n",
    "        \n",
    "        # Copy the catalog\n",
    "        shutil.copy2(copy_catalog_path, main_catalog_path)\n",
    "        print(f\"✅ Updated main catalog: {main_catalog_path}\")\n",
    "        \n",
    "        # Verify the update\n",
    "        df_new_main = pd.read_csv(main_catalog_path)\n",
    "        accepted_in_new = len(df_new_main[df_new_main['status'] == 'accepted'])\n",
    "        print(f\"✅ Verification: New main catalog has {accepted_in_new} accepted entries\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error updating catalog: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎉 MIGRATION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Summary:\")\n",
    "    print(f\"   • Moved {moved_count} accepted files to main folder\")\n",
    "    print(f\"   • Updated main validation catalog\")\n",
    "    print(f\"   • {accepted_in_new} accepted entries now in main catalog\")\n",
    "    if overwritten_count > 0:\n",
    "        print(f\"   • {overwritten_count} existing files were overwritten\")\n",
    "    print()\n",
    "    print(\"Next steps:\")\n",
    "    print(\"   1. Verify the files are in the correct locations\")\n",
    "    print(\"   2. Re-run your original diagnostics to confirm consistency\")\n",
    "    print(\"   3. Consider cleaning up the copy folders if everything looks good\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the migration\n",
    "success = move_ling_files_to_main()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"VERIFICATION CHECK\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Quick verification: reload the updated catalog and check consistency\n",
    "    df_ling_updated = pd.read_csv(\"../data/conceptual-error-candidates/validation_catalog_ling.csv\")\n",
    "    df_ling_accepted = df_ling_updated[df_ling_updated['status'] == 'accepted']\n",
    "    \n",
    "    print(f\"Updated Ling catalog has {len(df_ling_accepted)} accepted entries\")\n",
    "    \n",
    "    # Check a few files to make sure they're in the right place\n",
    "    sample_size = min(3, len(df_ling_accepted))\n",
    "    print(f\"Checking {sample_size} sample files...\")\n",
    "    \n",
    "    for idx, (_, row) in enumerate(df_ling_accepted.head(sample_size).iterrows()):\n",
    "        filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "        full_path = Path(\"..\") / filepath\n",
    "        exists = full_path.exists()\n",
    "        status = \"✅\" if exists else \"❌\"\n",
    "        print(f\"   {status} Index {row['index']}: {exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3015d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def diagnose_null_erroneous_line_numbers():\n",
    "    \"\"\"\n",
    "    Diagnose how many accepted files have null erroneous_line_number values for each reviewer.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load all validation catalogs\n",
    "    validation_catalogs = {}\n",
    "    parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "    \n",
    "    for reviewer in ['ali', 'arvind', 'ling', 'mauro', 'yewei']:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{parent_dir}{reviewer}.csv\")\n",
    "            validation_catalogs[reviewer] = df[df['status'] == 'accepted']\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  Catalog not found for {reviewer}\")\n",
    "            validation_catalogs[reviewer] = pd.DataFrame()\n",
    "    \n",
    "    total_null_count = 0\n",
    "    total_files_checked = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        if len(df) == 0:\n",
    "            print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(\"No accepted files found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        null_count = 0\n",
    "        valid_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Normalize the file path\n",
    "                normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "                file_path = Path(\"..\") / normalized_filepath\n",
    "                \n",
    "                if not file_path.exists():\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load and check the JSON\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                erroneous_line = json_data.get('error_details', {}).get('erroneous_line_number')\n",
    "                \n",
    "                if erroneous_line is None or erroneous_line == \"null\":\n",
    "                    null_count += 1\n",
    "                    # Show a few examples\n",
    "                    if null_count <= 3:\n",
    "                        print(f\"   • Index {row['index']}: {row['mutation_type']} - NULL erroneous_line_number\")\n",
    "                else:\n",
    "                    valid_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"   ❌ Error reading file for index {row['index']}: {e}\")\n",
    "        \n",
    "        total_files_checked += len(df)\n",
    "        total_null_count += null_count\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Total accepted files: {len(df)}\")\n",
    "        print(f\"   • Files with NULL erroneous_line_number: {null_count}\")\n",
    "        print(f\"   • Files with valid erroneous_line_number: {valid_count}\")\n",
    "        print(f\"   • Files with read errors: {error_count}\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            null_percentage = (null_count / len(df)) * 100\n",
    "            print(f\"   • Percentage with NULL: {null_percentage:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"🎯 OVERALL SUMMARY:\")\n",
    "    print(f\"   • Total accepted files checked: {total_files_checked}\")\n",
    "    print(f\"   • Total files with NULL erroneous_line_number: {total_null_count}\")\n",
    "    if total_files_checked > 0:\n",
    "        overall_percentage = (total_null_count / total_files_checked) * 100\n",
    "        print(f\"   • Overall percentage with NULL: {overall_percentage:.1f}%\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return total_null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14bab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 341\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 341\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 91\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 91\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 110\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 312\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 290\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 290\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 1144\n",
      "   • Total files with NULL erroneous_line_number: 0\n",
      "   • Overall percentage with NULL: 0.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "null_count = diagnose_null_erroneous_line_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24540b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "erroneous_line_number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e5ec4a30-2edc-447b-b2c8-ceacfda77a54",
       "rows": [
        [
         "FA",
         "538"
        ],
        [
         "L1",
         "503"
        ],
        [
         "L2",
         "369"
        ],
        [
         "L3",
         "291"
        ],
        [
         "L4",
         "156"
        ],
        [
         "L5",
         "56"
        ],
        [
         "L6",
         "16"
        ],
        [
         "L7",
         "7"
        ],
        [
         "L8",
         "5"
        ],
        [
         "L9",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "erroneous_line_number\n",
       "FA    538\n",
       "L1    503\n",
       "L2    369\n",
       "L3    291\n",
       "L4    156\n",
       "L5     56\n",
       "L6     16\n",
       "L7      7\n",
       "L8      5\n",
       "L9      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shortlist = pd.read_csv(\"../data/conceptual-error-candidates/conceptual_candidates_shortlist.csv\")\n",
    "\n",
    "df_shortlist[\"erroneous_line_number\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede5bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running diagnostic...\n",
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "   • Index 370: operator_swap - NULL erroneous_line_number\n",
      "   • Index 2214: incorrect_operand - NULL erroneous_line_number\n",
      "   • Index 389: incorrect_operand - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 341\n",
      "   • Files with NULL erroneous_line_number: 41\n",
      "   • Files with valid erroneous_line_number: 300\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 12.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 91\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 91\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 110\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 312\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 290\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 290\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 1144\n",
      "   • Total files with NULL erroneous_line_number: 41\n",
      "   • Overall percentage with NULL: 3.6%\n",
      "================================================================================\n",
      "\n",
      "🔧 Running repair...\n",
      "================================================================================\n",
      "REPAIR: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "✅ Loaded shortlist with 1942 entries\n",
      "✅ Created lookup table with 1744 entries\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "   ✅ Repaired Index 370: None → L2\n",
      "   ✅ Repaired Index 2214: None → L3\n",
      "   ✅ Repaired Index 389: None → L2\n",
      "   ✅ Repaired Index 7156: None → FA\n",
      "   ✅ Repaired Index 7062: None → L3\n",
      "   ✅ Repaired Index 2666: None → L1\n",
      "   ✅ Repaired Index 1759: None → L3\n",
      "   ✅ Repaired Index 638: None → L4\n",
      "   ✅ Repaired Index 6177: None → FA\n",
      "   ✅ Repaired Index 6227: None → L2\n",
      "   ✅ Repaired Index 1297: None → L2\n",
      "   ✅ Repaired Index 5204: None → L2\n",
      "   ✅ Repaired Index 5267: None → L3\n",
      "   ✅ Repaired Index 1191: None → L1\n",
      "   ✅ Repaired Index 4797: None → L3\n",
      "   ✅ Repaired Index 108: None → L3\n",
      "   ✅ Repaired Index 469: None → L1\n",
      "   ✅ Repaired Index 5233: None → L1\n",
      "   ✅ Repaired Index 6666: None → L1\n",
      "   ✅ Repaired Index 3333: None → L1\n",
      "   ✅ Repaired Index 7322: None → FA\n",
      "   ✅ Repaired Index 6967: None → FA\n",
      "   ✅ Repaired Index 6095: None → FA\n",
      "   ✅ Repaired Index 4037: None → L2\n",
      "   ✅ Repaired Index 7244: None → L3\n",
      "   ✅ Repaired Index 5782: None → L2\n",
      "   ✅ Repaired Index 1428: None → L5\n",
      "   ✅ Repaired Index 663: None → L5\n",
      "   ✅ Repaired Index 3340: None → L3\n",
      "   ✅ Repaired Index 1212: None → L3\n",
      "   ✅ Repaired Index 4967: None → L2\n",
      "   ✅ Repaired Index 728: None → L5\n",
      "   ✅ Repaired Index 6931: None → L1\n",
      "   ✅ Repaired Index 191: None → L2\n",
      "   ✅ Repaired Index 6775: None → L1\n",
      "   ✅ Repaired Index 1815: None → FA\n",
      "   ✅ Repaired Index 2611: None → FA\n",
      "   ✅ Repaired Index 4890: None → FA\n",
      "   ✅ Repaired Index 1220: None → FA\n",
      "   ✅ Repaired Index 1309: None → FA\n",
      "   ✅ Repaired Index 1535: None → FA\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 41\n",
      "   • Files already valid: 300\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 91\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 110\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 312\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 290\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "================================================================================\n",
      "🎯 REPAIR SUMMARY:\n",
      "   • Total files repaired: 41\n",
      "   • Total files already valid: 1103\n",
      "   • Total files not found in shortlist: 0\n",
      "================================================================================\n",
      "✅ Repair completed! Backup files (.json.backup) were created for all modified files.\n",
      "\n",
      "🔍 Running post-repair diagnostic...\n",
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 341\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 341\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 91\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 91\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 110\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 312\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 290\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 290\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 1144\n",
      "   • Total files with NULL erroneous_line_number: 0\n",
      "   • Overall percentage with NULL: 0.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def repair_null_erroneous_line_numbers():\n",
    "    \"\"\"\n",
    "    Repair null erroneous_line_number values using the conceptual_candidates_shortlist.csv file.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REPAIR: NULL ERRONEOUS LINE NUMBERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the shortlist CSV for reference\n",
    "    shortlist_path = \"../data/conceptual-error-candidates/conceptual_candidates_shortlist.csv\"\n",
    "    try:\n",
    "        df_shortlist = pd.read_csv(shortlist_path)\n",
    "        print(f\"✅ Loaded shortlist with {len(df_shortlist)} entries\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Shortlist file not found: {shortlist_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Create a lookup dictionary: (index, tier, mutation_type) -> erroneous_line_number\n",
    "    lookup = {}\n",
    "    for _, row in df_shortlist.iterrows():\n",
    "        key = (row['index'], row['tier'], row['mutation_type'])\n",
    "        lookup[key] = row['erroneous_line_number']\n",
    "    \n",
    "    print(f\"✅ Created lookup table with {len(lookup)} entries\")\n",
    "    print()\n",
    "    \n",
    "    # Load all validation catalogs\n",
    "    validation_catalogs = {}\n",
    "    parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "    \n",
    "    for reviewer in ['ali', 'arvind', 'ling', 'mauro', 'yewei']:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{parent_dir}{reviewer}.csv\")\n",
    "            validation_catalogs[reviewer] = df[df['status'] == 'accepted']\n",
    "        except FileNotFoundError:\n",
    "            validation_catalogs[reviewer] = pd.DataFrame()\n",
    "    \n",
    "    total_repaired = 0\n",
    "    total_not_found = 0\n",
    "    total_already_valid = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        repaired_count = 0\n",
    "        not_found_count = 0\n",
    "        already_valid_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Normalize the file path\n",
    "                normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "                file_path = Path(\"..\") / normalized_filepath\n",
    "                \n",
    "                if not file_path.exists():\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load the JSON\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                current_erroneous_line = json_data.get('error_details', {}).get('erroneous_line_number')\n",
    "                \n",
    "                # Check if repair is needed\n",
    "                if current_erroneous_line is not None and current_erroneous_line != \"null\":\n",
    "                    already_valid_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Look up the correct value\n",
    "                key = (row['index'], row['tier'], row['mutation_type'])\n",
    "                if key in lookup:\n",
    "                    correct_erroneous_line = lookup[key]\n",
    "                    \n",
    "                    # Create backup\n",
    "                    backup_path = file_path.with_suffix('.json.backup')\n",
    "                    shutil.copy2(file_path, backup_path)\n",
    "                    \n",
    "                    # Update the JSON\n",
    "                    json_data['error_details']['erroneous_line_number'] = correct_erroneous_line\n",
    "                    \n",
    "                    # Save the updated JSON\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    print(f\"   ✅ Repaired Index {row['index']}: {current_erroneous_line} → {correct_erroneous_line}\")\n",
    "                    repaired_count += 1\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ❌ Not found in shortlist: Index {row['index']}, Tier {row['tier']}, Type {row['mutation_type']}\")\n",
    "                    not_found_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"   ❌ Error processing index {row['index']}: {e}\")\n",
    "        \n",
    "        total_repaired += repaired_count\n",
    "        total_not_found += not_found_count\n",
    "        total_already_valid += already_valid_count\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Files repaired: {repaired_count}\")\n",
    "        print(f\"   • Files already valid: {already_valid_count}\")\n",
    "        print(f\"   • Files not found in shortlist: {not_found_count}\")\n",
    "        print(f\"   • Files with errors: {error_count}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"🎯 REPAIR SUMMARY:\")\n",
    "    print(f\"   • Total files repaired: {total_repaired}\")\n",
    "    print(f\"   • Total files already valid: {total_already_valid}\")\n",
    "    print(f\"   • Total files not found in shortlist: {total_not_found}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if total_repaired > 0:\n",
    "        print(\"✅ Repair completed! Backup files (.json.backup) were created for all modified files.\")\n",
    "    \n",
    "    return total_repaired > 0\n",
    "\n",
    "# Run diagnostic first\n",
    "print(\"🔍 Running diagnostic...\")\n",
    "null_count = diagnose_null_erroneous_line_numbers()\n",
    "\n",
    "if null_count > 0:\n",
    "    print(\"\\n\" + \"🔧 Running repair...\")\n",
    "    repair_success = repair_null_erroneous_line_numbers()\n",
    "    \n",
    "    if repair_success:\n",
    "        print(\"\\n\" + \"🔍 Running post-repair diagnostic...\")\n",
    "        diagnose_null_erroneous_line_numbers()\n",
    "else:\n",
    "    print(\"\\n✅ No null erroneous_line_number values found. No repair needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef8d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
