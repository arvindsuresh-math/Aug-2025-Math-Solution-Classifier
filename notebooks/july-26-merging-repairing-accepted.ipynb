{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40e2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali: 195\n",
      "Arvind: 47\n",
      "Ling: 0\n",
      "Mauro: 312\n",
      "Yewei: 0\n",
      "Total: 554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "\n",
    "df_ali = pd.read_csv(parent_dir + \"ali.csv\")\n",
    "df_arvind = pd.read_csv(parent_dir + \"arvind.csv\")\n",
    "df_ling = pd.read_csv(parent_dir + \"ling.csv\")\n",
    "df_mauro = pd.read_csv(parent_dir + \"mauro.csv\")\n",
    "df_yewei = pd.read_csv(parent_dir + \"yewei.csv\")\n",
    "\n",
    "df_ali = df_ali[df_ali[\"status\"] == \"accepted\"]\n",
    "df_arvind = df_arvind[df_arvind[\"status\"] == \"accepted\"]\n",
    "df_ling = df_ling[df_ling[\"status\"] == \"accepted\"]\n",
    "df_mauro = df_mauro[df_mauro[\"status\"] == \"accepted\"]\n",
    "df_yewei = df_yewei[df_yewei[\"status\"] == \"accepted\"]\n",
    "\n",
    "print(\"Ali:\", len(df_ali))\n",
    "print(\"Arvind:\", len(df_arvind))\n",
    "print(\"Ling:\", len(df_ling))\n",
    "print(\"Mauro:\", len(df_mauro))\n",
    "print(\"Yewei:\", len(df_yewei))\n",
    "print(\"Total:\", len(df_ali) + len(df_arvind) + len(df_ling) + len(df_mauro) + len(df_yewei))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dc56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_catalogs = {\n",
    "    \"ali\": df_ali,\n",
    "    \"arvind\": df_arvind,\n",
    "    \"ling\": df_ling,\n",
    "    \"mauro\": df_mauro,\n",
    "    \"yewei\": df_yewei\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6623ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC REPORT: Validation Catalog vs Accepted Files\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   195    195\n",
      "Total      195    195\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 195\n",
      "   • Samples marked 'accepted': 195\n",
      "   • Files actually found: 195\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted    47     47\n",
      "Total       47     47\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 47\n",
      "   • Samples marked 'accepted': 47\n",
      "   • Files actually found: 47\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 0\n",
      "   • Samples marked 'accepted': 0\n",
      "   • Files actually found: 0\n",
      "   • Inconsistencies: 0\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   312    312\n",
      "Total      312    312\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 312\n",
      "   • Samples marked 'accepted': 312\n",
      "   • Files actually found: 312\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 0\n",
      "   • Samples marked 'accepted': 0\n",
      "   • Files actually found: 0\n",
      "   • Inconsistencies: 0\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total reviewers checked: 5\n",
      "   • Total inconsistencies across all reviewers: 0\n",
      "   • 🎉 All validation catalogs are consistent with accepted files!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to check if a file exists in the accepted folder\n",
    "def check_file_exists_in_accepted(filepath, base_dir=\"../data/conceptual-errors-accepted/\"):\n",
    "    \"\"\"\n",
    "    Check if a file exists in the conceptual-errors-accepted folder.\n",
    "    The filepath in the catalog points to the accepted location if accepted.\n",
    "    \"\"\"\n",
    "    if pd.isna(filepath) or filepath == \"\":\n",
    "        return False\n",
    "    \n",
    "    # Convert to Path object and normalize path separators\n",
    "    # Replace Windows backslashes with forward slashes for cross-platform compatibility\n",
    "    normalized_filepath = str(filepath).replace('\\\\', '/')\n",
    "    file_path = Path(normalized_filepath)\n",
    "    \n",
    "    # If it's a relative path, make it relative to the notebook location\n",
    "    if not file_path.is_absolute():\n",
    "        # Assuming we're running from notebooks/ directory\n",
    "        full_path = Path(\"..\") / file_path\n",
    "    else:\n",
    "        full_path = file_path\n",
    "    \n",
    "    return full_path.exists()\n",
    "\n",
    "# Main diagnostic function\n",
    "def run_diagnostics(validation_catalogs):\n",
    "    \"\"\"\n",
    "    Run diagnostic checks on validation catalogs vs actual files in accepted folder.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC REPORT: Validation Catalog vs Accepted Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_inconsistencies = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        df_check = df.copy()\n",
    "        \n",
    "        # Add the 'added' column by checking if files exist\n",
    "        df_check['added'] = df_check['filepath'].apply(check_file_exists_in_accepted)\n",
    "        \n",
    "        # Create cross-tabulation\n",
    "        crosstab = pd.crosstab(df_check['status'], df_check['added'], \n",
    "                              margins=True, margins_name='Total')\n",
    "        \n",
    "        print(\"Cross-tabulation (Status vs File Actually Added):\")\n",
    "        print(crosstab)\n",
    "        print()\n",
    "        \n",
    "        # Identify specific inconsistencies\n",
    "        accepted_but_not_added = df_check[(df_check['status'] == 'accepted') & \n",
    "                                         (df_check['added'] == False)]\n",
    "        not_accepted_but_added = df_check[(df_check['status'] != 'accepted') & \n",
    "                                         (df_check['added'] == True)]\n",
    "        \n",
    "        inconsistency_count = len(accepted_but_not_added) + len(not_accepted_but_added)\n",
    "        total_inconsistencies += inconsistency_count\n",
    "        \n",
    "        if inconsistency_count > 0:\n",
    "            print(\"🚨 INCONSISTENCIES FOUND:\")\n",
    "            \n",
    "            if len(accepted_but_not_added) > 0:\n",
    "                print(f\"   • {len(accepted_but_not_added)} samples marked 'accepted' but file NOT found:\")\n",
    "                for idx, row in accepted_but_not_added.iterrows():\n",
    "                    print(f\"     - Index {row['index']}: {row['tier']} | {row['mutation_type']} | {row['target_variable']}\")\n",
    "                    print(f\"       Expected file: {row['filepath']}\")\n",
    "                    # Show normalized path for debugging\n",
    "                    normalized_path = str(row['filepath']).replace('\\\\', '/')\n",
    "                    full_check_path = Path(\"..\") / normalized_path\n",
    "                    print(f\"       Checking path: {full_check_path}\")\n",
    "                    print(f\"       Path exists: {full_check_path.exists()}\")\n",
    "                print()\n",
    "            \n",
    "            if len(not_accepted_but_added) > 0:\n",
    "                print(f\"   • {len(not_accepted_but_added)} samples NOT marked 'accepted' but file exists:\")\n",
    "                for idx, row in not_accepted_but_added.iterrows():\n",
    "                    print(f\"     - Index {row['index']}: Status='{row['status']}' but file exists at {row['filepath']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"✅ No inconsistencies found!\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_samples = len(df_check)\n",
    "        accepted_count = len(df_check[df_check['status'] == 'accepted'])\n",
    "        files_found = len(df_check[df_check['added'] == True])\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Total samples in catalog: {total_samples}\")\n",
    "        print(f\"   • Samples marked 'accepted': {accepted_count}\")\n",
    "        print(f\"   • Files actually found: {files_found}\")\n",
    "        print(f\"   • Inconsistencies: {inconsistency_count}\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        if accepted_count > 0:\n",
    "            success_rate = ((accepted_count - len(accepted_but_not_added)) / accepted_count) * 100\n",
    "            print(f\"   • Success rate (accepted → file created): {success_rate:.1f}%\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"🎯 OVERALL SUMMARY:\")\n",
    "    print(f\"   • Total reviewers checked: {len(validation_catalogs)}\")\n",
    "    print(f\"   • Total inconsistencies across all reviewers: {total_inconsistencies}\")\n",
    "    if total_inconsistencies == 0:\n",
    "        print(\"   • 🎉 All validation catalogs are consistent with accepted files!\")\n",
    "    else:\n",
    "        print(f\"   • ⚠️  Found {total_inconsistencies} inconsistencies that need investigation.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run the diagnostics\n",
    "run_diagnostics(validation_catalogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968e7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REVIEWING ACCEPTED 'incorrect_operand' FILES - ARVIND\n",
      "Found 10 samples to review\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 1/10\n",
      "Index: 6555 | Tier: tier4 | Target Variable: miles_to_add_per_week\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/6555/google_gemini-2.5-flash_incorrect_operand_miles_to_add_per_week_total_days.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L4\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'total_days' (value: 280) was used instead of 'target_miles_per_week' (value: 120.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 7\\nHe wants to run 100*0.2=<<100*0.2=20>>20 miles more than he used to\\nSo he needs to run 100+20=<<100+20=120>>120 miles\\nHe is doing this in 280/7=<<280/7=40>>40 weeks\\nSo he needs to add 280/40=<<280/40=7>>7 miles per week\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L4\n",
      "   • Flawed Solution Length: 226 characters\n",
      "   • Solution Preview: #### 7\n",
      "He wants to run 100*0.2=<<100*0.2=20>>20 miles more than he used to\n",
      "So he needs to run 100+20=<<100+20=120>>120 miles\n",
      "He is doing this in 280/7=<<280/7=40>>40 weeks\n",
      "So he needs to add 280/40=<<...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 2/10\n",
      "Index: 6702 | Tier: tier4 | Target Variable: boys_count\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/6702/google_gemini-2.5-flash_incorrect_operand_boys_count_total_attendees.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L4\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'total_attendees' (value: 100) was used instead of 'remaining_students' (value: 90.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 40\\nOf 100 people in attendance at the school dance, 10% were school faculty and staff, or a total of 0.1*100=<<100*0.1=10>>10 people.\\nThis leaves 100-10=<<100-10=90>>90 students in attendance.\\nOf the remaining attendees, two-thirds were girls, or (2/3)*90=<<(2/3)*90=60>>60 attendees were girls.\\nThus, the remaining 100-60=<<100-60=40>>40 students were boys who attended the school dance.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L4\n",
      "   • Flawed Solution Length: 393 characters\n",
      "   • Solution Preview: #### 40\n",
      "Of 100 people in attendance at the school dance, 10% were school faculty and staff, or a total of 0.1*100=<<100*0.1=10>>10 people.\n",
      "This leaves 100-10=<<100-10=90>>90 students in attendance.\n",
      "Of...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 3/10\n",
      "Index: 6424 | Tier: tier4 | Target Variable: bc_grade_students\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/6424/google_gemini-2.5-flash_incorrect_operand_bc_grade_students_total_students.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L3\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'total_students' (value: 32) was used instead of 'remaining_students_after_a' (value: 24.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 16\\nA grade:32(0.25)=8\\n32-8=<<32-8=24>>24 students remaining\\nB/C grade:32/4=<<32/4=8>>8\\n24-8=<<24-8=16>>16 students fail\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L3\n",
      "   • Flawed Solution Length: 124 characters\n",
      "   • Solution Preview: #### 16\n",
      "A grade:32(0.25)=8\n",
      "32-8=<<32-8=24>>24 students remaining\n",
      "B/C grade:32/4=<<32/4=8>>8\n",
      "24-8=<<24-8=16>>16 students fail\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 4/10\n",
      "Index: 4991 | Tier: tier4 | Target Variable: votes_to_win\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/4991/google_gemini-2.5-flash_incorrect_operand_votes_to_win_total_students.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L2\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'total_students' (value: 200) was used instead of 'votes_for_50_percent' (value: 100.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 150\\n50% of the vote is 100 votes because 200 x 0.5 = <<200*0.5=100>>100\\nJust over 50% is 201 votes because 200 + 1 = 201\\n120 students are boys because 200 x 0.6 = <<200*0.6=120>>120\\n80 students are girls because 200 - 120 = <<200-120=80>>80\\nShe gets 81 votes from boys because 120 x 0.68 = <<120*0.68=81>>81\\nShe needs 120 votes from girls because 201 - 81 = <<201-81=120>>120\\nShe needs a proportion of 1.5 of girl's votes to win because 120 / 80 = <<120/80=1.5>>1.5\\nThe percentage she needs is 150 because 100 x 1.5 = <<100*1.5=150>>150\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L2\n",
      "   • Flawed Solution Length: 541 characters\n",
      "   • Solution Preview: #### 150\n",
      "50% of the vote is 100 votes because 200 x 0.5 = <<200*0.5=100>>100\n",
      "Just over 50% is 201 votes because 200 + 1 = 201\n",
      "120 students are boys because 200 x 0.6 = <<200*0.6=120>>120\n",
      "80 students a...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 5/10\n",
      "Index: 4629 | Tier: tier4 | Target Variable: total_cost_4_bottles\n",
      "Manual Edits: L1\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/4629/google_gemini-2.5-flash_incorrect_operand_total_cost_4_bottles_num_bottles_desired.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L2\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'num_bottles_desired' (value: 4) was used instead of 'cost_per_bottle' (value: 0.5).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 16\\nA bottle costs 1.50 € / 3 = <<1.5/3=0.5>>0.50 €.\\nSo, four bottles cost 4 € * 4 = <<4*4=16>>16 €.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L2\n",
      "   • Flawed Solution Length: 104 characters\n",
      "   • Solution Preview: #### 16\n",
      "A bottle costs 1.50 € / 3 = <<1.5/3=0.5>>0.50 €.\n",
      "So, four bottles cost 4 € * 4 = <<4*4=16>>16 €.\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 6/10\n",
      "Index: 3006 | Tier: tier4 | Target Variable: flavors_to_try_this_year\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/3006/google_gemini-2.5-flash_incorrect_operand_flavors_to_try_this_year_flavors_two_years_ago.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L4\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'flavors_two_years_ago' (value: 25.0) was used instead of 'total_flavors_tried_so_far' (value: 75.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 75\\nThe shop offers 100 flavors and she tried 1/4 of the flavors so she had 100*0.25 = <<100*0.25=25>>25 flavors 2 years ago\\nLast year she ate double the amount from the previous year so she had 25*2 = <<25*2=50>>50 flavors\\n2 years ago she ate 25 flavors and last year she had 50 flavors so she has had 25+50 = <<25+50=75>>75 flavors\\nThe shop offers 100 flavors and she has had 25 flavors so she still needs to try 100-25 = <<100-25=75>>75 flavors\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L4\n",
      "   • Flawed Solution Length: 451 characters\n",
      "   • Solution Preview: #### 75\n",
      "The shop offers 100 flavors and she tried 1/4 of the flavors so she had 100*0.25 = <<100*0.25=25>>25 flavors 2 years ago\n",
      "Last year she ate double the amount from the previous year so she had 2...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 7/10\n",
      "Index: 2987 | Tier: tier4 | Target Variable: percentage_comic_books\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/2987/google_gemini-2.5-flash_incorrect_operand_percentage_comic_books_num_graphic_novels.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L3\",\n",
      "    \"explanation\": \"Incorrect operand. The percentage of graphic novels should be 15, not 18, so the percentage of comic books will be 100-65-15=20.\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 17\\nThe proportion of graphic novels is .15 because 18 / 120 = <<18/120=0.15>>0.15\\nThe percentage of graphic novels is 15 because 100 x 0.15 = <<100*0.15=15>>15\\nThe percentage of comic books is 20 because 100 - 65 - 18 = <<100-65-18=17>>17\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L3\n",
      "   • Flawed Solution Length: 243 characters\n",
      "   • Solution Preview: #### 17\n",
      "The proportion of graphic novels is .15 because 18 / 120 = <<18/120=0.15>>0.15\n",
      "The percentage of graphic novels is 15 because 100 x 0.15 = <<100*0.15=15>>15\n",
      "The percentage of comic books is 20...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 8/10\n",
      "Index: 2762 | Tier: tier4 | Target Variable: bottles_can_buy\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/2762/google_gemini-2.5-flash_incorrect_operand_bottles_can_buy_months_per_year.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L3\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'months_per_year' (value: 12) was used instead of 'total_refund_amount' (value: 18.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 4\\nHe drinks 15 bottles every month and there are 12 months in 1 year so that's 15*12 = <<15*12=180>>180 bottles\\nHe earns $0.1 on every bottle that he recycles so that's 0.1*180 = $<<0.1*180=18>>18\\nHe has $12 and each bottle costs $3 so he can buy 12/3 = <<12/3=4>>4 bottles\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L3\n",
      "   • Flawed Solution Length: 278 characters\n",
      "   • Solution Preview: #### 4\n",
      "He drinks 15 bottles every month and there are 12 months in 1 year so that's 15*12 = <<15*12=180>>180 bottles\n",
      "He earns $0.1 on every bottle that he recycles so that's 0.1*180 = $<<0.1*180=18>>1...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 9/10\n",
      "Index: 1909 | Tier: tier4 | Target Variable: kids_under_8_min\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/1909/google_gemini-2.5-flash_incorrect_operand_kids_under_8_min_percent_under_6_min_num.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L2\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'percent_under_6_min_num' (value: 10) was used instead of 'kids_under_6_min' (value: 4.0).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 1\\nFirst find how many kids finish in under 6 minutes: 40 kids * 10% = <<40*10*0.01=4>>4 kids\\nThen triple that number to find how many finish in less than 8 minutes: 10 kids * 3 = <<10*3=30>>30 kids\\nThen subtract those two numbers from the total number of kids to find the number of remaining kids: 40 kids - 4 kids - 30 kids = <<40-4-30=6>>6 kids\\nThen divide that number by 6 to find how many kids take more than 14 minutes: 6 kids / 6 = <<6/6=1>>1 kids\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L2\n",
      "   • Flawed Solution Length: 458 characters\n",
      "   • Solution Preview: #### 1\n",
      "First find how many kids finish in under 6 minutes: 40 kids * 10% = <<40*10*0.01=4>>4 kids\n",
      "Then triple that number to find how many finish in less than 8 minutes: 10 kids * 3 = <<10*3=30>>30 ki...\n",
      "================================================================================\n",
      "\n",
      "📁 SAMPLE 10/10\n",
      "Index: 6237 | Tier: tier4 | Target Variable: total_hours_worked\n",
      "Manual Edits: nan\n",
      "------------------------------------------------------------\n",
      "File path: ../data/conceptual-errors-accepted/tier4/6237/google_gemini-2.5-flash_incorrect_operand_total_tips_earned_hours_per_8hr_shift.json\n",
      "📄 JSON CONTENT:\n",
      "{\n",
      "  \"verdict\": \"Flawed\",\n",
      "  \"error_details\": {\n",
      "    \"error_type\": \"incorrect_operand\",\n",
      "    \"erroneous_line_number\": \"L4\",\n",
      "    \"explanation\": \"Incorrect operand. The variable 'hours_per_8hr_shift' (value: 8) was used instead of 'total_hours_worked' (value: 28).\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"question\": \"N/A\",\n",
      "    \"flawed_solution\": \"#### 40\\nFirst find the total number of hours Brendan works for the 8-hour shifts: 2 shifts * 8 hours/shift = <<2*8=16>>16 hours\\nThen find the total number of hours Brendan works by adding those hours to the 12-hour shift: 16 hours + 12 hours = <<16+12=28>>28 hours\\nNow find Brendan's total wage earnings by multiplying his $6/hour wage by the number of hours he works: $6/hour * 28 hours = $<<6*28=168>>168/week\\nNow figure out how much Brendan makes in tips by multiplying the number of hours he works by the amount of tips he makes per hour: $12/hour * 8 hours/week = $<<12*8=96>>96/week\\nNow divide that number by 3, since Brendan only reports a third of his tips: $96/week / 3 = $<<96/3=32>>32/week\\nNow add Brendan's wage earnings to that number to find how much income he reports to the IRS each week: $168/week + $32/week = $<<168+32=200>>200/week\\nFinally, multiply Brendan's reported income by his tax rate to find how much he pays in taxes each week: $200/week * 0.2 = $<<200*0.2=40>>40/week\"\n",
      "  }\n",
      "}\n",
      "\n",
      "🔍 QUICK SUMMARY:\n",
      "   • Verdict: Flawed\n",
      "   • Error Type: incorrect_operand\n",
      "   • Erroneous Line: L4\n",
      "   • Flawed Solution Length: 997 characters\n",
      "   • Solution Preview: #### 40\n",
      "First find the total number of hours Brendan works for the 8-hour shifts: 2 shifts * 8 hours/shift = <<2*8=16>>16 hours\n",
      "Then find the total number of hours Brendan works by adding those hours ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def review_accepted_incorrect_operand_files(df_arvind):\n",
    "    \"\"\"\n",
    "    Review all accepted incorrect_operand samples from Arvind's catalog\n",
    "    by loading and displaying their corresponding JSON files.\n",
    "    \"\"\"\n",
    "    # Filter for incorrect_operand mutation type\n",
    "    incorrect_operand_samples = df_arvind[df_arvind['mutation_type'] == 'incorrect_operand']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"REVIEWING ACCEPTED 'incorrect_operand' FILES - ARVIND\")\n",
    "    print(f\"Found {len(incorrect_operand_samples)} samples to review\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(incorrect_operand_samples.iterrows(), 1):\n",
    "        print(f\"\\n📁 SAMPLE {idx}/{len(incorrect_operand_samples)}\")\n",
    "        print(f\"Index: {row['index']} | Tier: {row['tier']} | Target Variable: {row['target_variable']}\")\n",
    "        print(f\"Manual Edits: {row['manual_edits']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Normalize the file path (handle Windows vs Unix path separators)\n",
    "        normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "        file_path = Path(\"..\") / normalized_filepath\n",
    "        \n",
    "        print(f\"File path: {file_path}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not file_path.exists():\n",
    "            print(\"❌ FILE NOT FOUND!\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load and display the JSON content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            print(\"📄 JSON CONTENT:\")\n",
    "            print(json.dumps(json_data, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            # Extract key information for quick review\n",
    "            verdict = json_data.get('verdict', 'N/A')\n",
    "            error_type = json_data.get('error_details', {}).get('error_type', 'N/A')\n",
    "            erroneous_line = json_data.get('error_details', {}).get('erroneous_line_number', 'N/A')\n",
    "            \n",
    "            print(f\"\\n🔍 QUICK SUMMARY:\")\n",
    "            print(f\"   • Verdict: {verdict}\")\n",
    "            print(f\"   • Error Type: {error_type}\")\n",
    "            print(f\"   • Erroneous Line: {erroneous_line}\")\n",
    "            \n",
    "            # Check if there's a flawed_solution in context\n",
    "            flawed_solution = json_data.get('context', {}).get('flawed_solution', '')\n",
    "            if flawed_solution:\n",
    "                print(f\"   • Flawed Solution Length: {len(flawed_solution)} characters\")\n",
    "                # Show first 200 characters as preview\n",
    "                preview = flawed_solution[:200] + \"...\" if len(flawed_solution) > 200 else flawed_solution\n",
    "                print(f\"   • Solution Preview: {preview}\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON DECODE ERROR: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR READING FILE: {e}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Add a pause option for manual review\n",
    "        if idx < len(incorrect_operand_samples):\n",
    "            response = input(\"Press Enter to continue to next file, or 'q' to quit: \")\n",
    "            if response.lower() == 'q':\n",
    "                break\n",
    "\n",
    "# Run the review\n",
    "review_accepted_incorrect_operand_files(df_arvind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af998fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Ling's copied catalog...\n",
      "================================================================================\n",
      "DIAGNOSTIC REPORT: Ling's Copied Files\n",
      "================================================================================\n",
      "📋 LING'S COPIED CATALOG\n",
      "--------------------------------------------------\n",
      "Total entries in catalog: 388\n",
      "Status distribution:\n",
      "   • todo: 187\n",
      "   • accepted: 110\n",
      "   • rejected: 87\n",
      "   • skipped: 4\n",
      "\n",
      "Accepted entries: 110\n",
      "Cross-tabulation (Status vs File Actually Added in Copy):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   110    110\n",
      "Total      110    110\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total accepted samples in catalog: 110\n",
      "   • Files actually found in copy folder: 110\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file found in copy): 100.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load Ling's copied catalog\n",
    "def load_ling_copy_catalog():\n",
    "    \"\"\"Load Ling's copied validation catalog\"\"\"\n",
    "    ling_copy_path = \"../data/conceptual-error-candidates/validation_catalog_ling_copy.csv\"\n",
    "    df_ling_copy = pd.read_csv(ling_copy_path)\n",
    "    return df_ling_copy\n",
    "\n",
    "# Function to check if a file exists in Ling's copied accepted folder\n",
    "def check_file_exists_in_ling_copy(filepath, base_dir=\"../data/conceptual-errors-accepted-copy/\"):\n",
    "    \"\"\"\n",
    "    Check if a file exists in Ling's copied conceptual-errors-accepted folder.\n",
    "    \"\"\"\n",
    "    if pd.isna(filepath) or filepath == \"\":\n",
    "        return False\n",
    "    \n",
    "    # Convert to Path object and normalize path separators\n",
    "    normalized_filepath = str(filepath).replace('\\\\', '/')\n",
    "    file_path = Path(normalized_filepath)\n",
    "    \n",
    "    # Extract the relative path from conceptual-errors-accepted onwards\n",
    "    # Handle both cases: full path or relative path\n",
    "    if \"conceptual-errors-accepted\" in str(file_path):\n",
    "        # Extract everything after \"conceptual-errors-accepted\"\n",
    "        parts = file_path.parts\n",
    "        try:\n",
    "            accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "            relative_path = Path(*parts[accepted_idx + 1:])\n",
    "        except StopIteration:\n",
    "            # If we can't find the pattern, use the full path as is\n",
    "            relative_path = file_path\n",
    "    else:\n",
    "        relative_path = file_path\n",
    "    \n",
    "    # Build the full path to Ling's copy\n",
    "    if not relative_path.is_absolute():\n",
    "        full_path = Path(\"..\") / \"data\" / \"conceptual-errors-accepted-copy\" / relative_path\n",
    "    else:\n",
    "        full_path = relative_path\n",
    "    \n",
    "    return full_path.exists()\n",
    "\n",
    "# Main diagnostic function for Ling's copy\n",
    "def run_ling_copy_diagnostics():\n",
    "    \"\"\"\n",
    "    Run diagnostic checks on Ling's copied validation catalog vs copied accepted files.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC REPORT: Ling's Copied Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load Ling's copied catalog\n",
    "    df_ling_copy = load_ling_copy_catalog()\n",
    "    \n",
    "    print(f\"📋 LING'S COPIED CATALOG\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total entries in catalog: {len(df_ling_copy)}\")\n",
    "    \n",
    "    # Show status distribution\n",
    "    status_counts = df_ling_copy['status'].value_counts()\n",
    "    print(\"Status distribution:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"   • {status}: {count}\")\n",
    "    print()\n",
    "    \n",
    "    # Filter for accepted entries only\n",
    "    df_accepted = df_ling_copy[df_ling_copy['status'] == 'accepted'].copy()\n",
    "    print(f\"Accepted entries: {len(df_accepted)}\")\n",
    "    \n",
    "    if len(df_accepted) == 0:\n",
    "        print(\"No accepted entries found in Ling's catalog.\")\n",
    "        return\n",
    "    \n",
    "    # Add the 'added' column by checking if files exist in the copy folder\n",
    "    df_accepted['added'] = df_accepted['filepath'].apply(check_file_exists_in_ling_copy)\n",
    "    \n",
    "    # Create cross-tabulation\n",
    "    crosstab = pd.crosstab(df_accepted['status'], df_accepted['added'], \n",
    "                          margins=True, margins_name='Total')\n",
    "    \n",
    "    print(\"Cross-tabulation (Status vs File Actually Added in Copy):\")\n",
    "    print(crosstab)\n",
    "    print()\n",
    "    \n",
    "    # Identify specific inconsistencies\n",
    "    accepted_but_not_added = df_accepted[(df_accepted['status'] == 'accepted') & \n",
    "                                       (df_accepted['added'] == False)]\n",
    "    \n",
    "    inconsistency_count = len(accepted_but_not_added)\n",
    "    \n",
    "    if inconsistency_count > 0:\n",
    "        print(\"🚨 INCONSISTENCIES FOUND:\")\n",
    "        print(f\"   • {len(accepted_but_not_added)} samples marked 'accepted' but file NOT found in copy:\")\n",
    "        \n",
    "        for idx, row in accepted_but_not_added.iterrows():\n",
    "            print(f\"     - Index {row['index']}: {row['tier']} | {row['mutation_type']} | {row['target_variable']}\")\n",
    "            print(f\"       Expected file: {row['filepath']}\")\n",
    "            \n",
    "            # Show what path we're actually checking\n",
    "            normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "            file_path = Path(normalized_filepath)\n",
    "            \n",
    "            if \"conceptual-errors-accepted\" in str(file_path):\n",
    "                parts = file_path.parts\n",
    "                try:\n",
    "                    accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "                    relative_path = Path(*parts[accepted_idx + 1:])\n",
    "                except StopIteration:\n",
    "                    relative_path = file_path\n",
    "            else:\n",
    "                relative_path = file_path\n",
    "                \n",
    "            full_check_path = Path(\"..\") / \"data\" / \"conceptual-errors-accepted-copy\" / relative_path\n",
    "            print(f\"       Checking path: {full_check_path}\")\n",
    "            print(f\"       Path exists: {full_check_path.exists()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"✅ No inconsistencies found!\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_accepted = len(df_accepted)\n",
    "    files_found = len(df_accepted[df_accepted['added'] == True])\n",
    "    \n",
    "    print(f\"📊 SUMMARY:\")\n",
    "    print(f\"   • Total accepted samples in catalog: {total_accepted}\")\n",
    "    print(f\"   • Files actually found in copy folder: {files_found}\")\n",
    "    print(f\"   • Inconsistencies: {inconsistency_count}\")\n",
    "    \n",
    "    # Calculate success rate\n",
    "    if total_accepted > 0:\n",
    "        success_rate = (files_found / total_accepted) * 100\n",
    "        print(f\"   • Success rate (accepted → file found in copy): {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return df_accepted\n",
    "\n",
    "# Load and analyze Ling's copied files\n",
    "print(\"Loading Ling's copied catalog...\")\n",
    "df_ling_copy_analysis = run_ling_copy_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6bf329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MOVING LING'S FILES TO MAIN FOLDERS\n",
      "================================================================================\n",
      "📋 Loading Ling's copy catalog...\n",
      "Found 110 accepted entries to process\n",
      "\n",
      "📁 Moving accepted files...\n",
      "✅ Moving: tier4/5691/google_gemini-2.5-flash_incomplete_calculation_full_meal_cost_before_voucher_.json\n",
      "✅ Moving: tier4/7184/google_gemini-2.5-flash_operator_swap_total_profits_.json\n",
      "✅ Moving: tier4/7146/google_gemini-2.5-flash_operator_swap_ginger_tsp_.json\n",
      "✅ Moving: tier4/2407/google_gemini-2.5-flash_operator_swap_wanda_treats_.json\n",
      "✅ Moving: tier4/1111/google_gemini-2.5-flash_operator_swap_total_steak_pounds_.json\n",
      "✅ Moving: tier4/6580/google_gemini-2.5-flash_incorrect_operand_cost_per_gallon_dollars_cost_1_cent.json\n",
      "✅ Moving: tier4/3517/google_gemini-2.5-flash_operator_swap_sale_cost_dollars_.json\n",
      "✅ Moving: tier4/2512/google_gemini-2.5-flash_incorrect_operand_produce_cost_eggplant_pounds.json\n",
      "✅ Moving: tier4/7413/google_gemini-2.5-flash_operator_swap_total_profit_.json\n",
      "✅ Moving: tier4/2844/google_gemini-2.5-flash_operator_swap_total_dolls_.json\n",
      "✅ Moving: tier3/7399/google_gemini-2.5-flash_incomplete_calculation_total_fruit_.json\n",
      "✅ Moving: tier3/4305/google_gemini-2.5-flash_input_misrepresentation_total_sales_tuesday_sales.json\n",
      "✅ Moving: tier3/6682/google_gemini-2.5-flash_input_misrepresentation_total_coins_after_gift_gift_coins_in_solution.json\n",
      "✅ Moving: tier3/6146/google_gemini-2.5-flash_input_misrepresentation_total_cost_erika_savings.json\n",
      "✅ Moving: tier3/3001/google_gemini-2.5-flash_input_misrepresentation_sue_total_miles_num_other_ladies.json\n",
      "✅ Moving: tier3/824/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_students_from_25_desk_classrooms.json\n",
      "✅ Moving: tier3/2368/google_gemini-2.5-flash_incomplete_calculation_total_points_first_three_games_.json\n",
      "✅ Moving: tier3/2488/google_gemini-2.5-flash_operator_swap_timothy_movies_2010_.json\n",
      "✅ Moving: tier3/5572/google_gemini-2.5-flash_operator_swap_total_presents_.json\n",
      "✅ Moving: tier3/6454/google_gemini-2.5-flash_operator_swap_remaining_turtles_.json\n",
      "✅ Moving: tier3/1215/google_gemini-2.5-flash_incomplete_calculation_total_people_.json\n",
      "✅ Moving: tier3/841/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_initial_dogs.json\n",
      "✅ Moving: tier3/1192/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_men_and_women.json\n",
      "⚠️  Overwriting: tier3/4534/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_eggs_per_unit.json\n",
      "✅ Moving: tier3/4789/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_burgers.json\n",
      "⚠️  Overwriting: tier3/4789/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_burgers.json\n",
      "⚠️  Overwriting: tier3/7154/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_isabella_dresses.json\n",
      "✅ Moving: tier3/4490/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_number_of_blocks.json\n",
      "⚠️  Overwriting: tier3/1229/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_itzayana_height.json\n",
      "✅ Moving: tier3/1351/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_jake_sold.json\n",
      "✅ Moving: tier3/1679/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_park_area.json\n",
      "✅ Moving: tier3/2454/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_students_wearing_shorts.json\n",
      "✅ Moving: tier3/2677/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_children_cost.json\n",
      "✅ Moving: tier3/2781/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_manu_total_time.json\n",
      "✅ Moving: tier3/3639/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_eaten_slices.json\n",
      "✅ Moving: tier3/2428/google_gemini-2.5-flash_operator_swap_times_per_small_hill_.json\n",
      "✅ Moving: tier3/5354/google_gemini-2.5-flash_incomplete_calculation_total_carrots_.json\n",
      "✅ Moving: tier3/6120/google_gemini-2.5-flash_incorrect_operand_combined_yellow_blue_students_like_red.json\n",
      "✅ Moving: tier3/1729/google_gemini-2.5-flash_incomplete_calculation_total_insects_.json\n",
      "✅ Moving: tier3/7376/google_gemini-2.5-flash_incomplete_calculation_daily_jerky_consumption_.json\n",
      "✅ Moving: tier3/5716/google_gemini-2.5-flash_incomplete_calculation_total_cups_desired_.json\n",
      "✅ Moving: tier3/486/google_gemini-2.5-flash_incorrect_operand_total_friday_rain_thursday_rain.json\n",
      "✅ Moving: tier3/4945/google_gemini-2.5-flash_incorrect_operand_female_cows_total_cows.json\n",
      "✅ Moving: tier3/6429/google_gemini-2.5-flash_incomplete_calculation_total_protein_consumed_.json\n",
      "✅ Moving: tier3/5925/google_gemini-2.5-flash_incomplete_calculation_total_cases_.json\n",
      "✅ Moving: tier3/3760/google_gemini-2.5-flash_incomplete_calculation_total_cookies_.json\n",
      "✅ Moving: tier3/2153/google_gemini-2.5-flash_incomplete_calculation_total_fills_.json\n",
      "✅ Moving: tier3/6625/google_gemini-2.5-flash_incorrect_operand_apples_leftover_num_mini_pies.json\n",
      "✅ Moving: tier3/3941/google_gemini-2.5-flash_incomplete_calculation_total_trial_speed_.json\n",
      "✅ Moving: tier3/4222/google_gemini-2.5-flash_incomplete_calculation_sum_of_first_three_baskets_.json\n",
      "✅ Moving: tier2/6250/google_gemini-2.5-flash_input_misrepresentation_second_distance_first_speed.json\n",
      "✅ Moving: tier2/6048/google_gemini-2.5-flash_input_misrepresentation_cost_fruit_drinks_cost_sandwiches.json\n",
      "✅ Moving: tier2/5194/google_gemini-2.5-flash_input_misrepresentation_total_revenue_total_cost_patches.json\n",
      "✅ Moving: tier2/4439/google_gemini-2.5-flash_input_misrepresentation_final_price_profit_amount.json\n",
      "✅ Moving: tier2/2323/google_gemini-2.5-flash_input_misrepresentation_num_showed_up_rsvp_rate.json\n",
      "✅ Moving: tier2/1625/google_gemini-2.5-flash_operator_swap_new_price_.json\n",
      "✅ Moving: tier2/5184/google_gemini-2.5-flash_incorrect_operand_fabric_still_needed_fabric_has_yards.json\n",
      "✅ Moving: tier2/3893/google_gemini-2.5-flash_incomplete_calculation_quarters_value_.json\n",
      "✅ Moving: tier2/898/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_loss_from_knots.json\n",
      "✅ Moving: tier2/1548/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_bugs_eaten_by_spiders.json\n",
      "✅ Moving: tier2/1489/google_gemini-2.5-flash_operator_swap_jerome_lost_pounds_.json\n",
      "✅ Moving: tier2/5863/google_gemini-2.5-flash_incomplete_calculation_total_volume_.json\n",
      "✅ Moving: tier2/3878/google_gemini-2.5-flash_operator_swap_total_candy_bars_sold_.json\n",
      "✅ Moving: tier2/5883/google_gemini-2.5-flash_operator_swap_final_pennies_left_.json\n",
      "✅ Moving: tier2/153/google_gemini-2.5-flash_operator_swap_total_paid_.json\n",
      "✅ Moving: tier2/445/google_gemini-2.5-flash_operator_swap_bag_price_after_discount_.json\n",
      "✅ Moving: tier2/1293/google_gemini-2.5-flash_input_misrepresentation_irene_shirts_cost_cost_per_short.json\n",
      "✅ Moving: tier2/7306/google_gemini-2.5-flash_operator_swap_cost_lemonade_.json\n",
      "✅ Moving: tier2/4460/google_gemini-2.5-flash_incorrect_operand_total_cost_his_vest_vest_cost.json\n",
      "✅ Moving: tier2/5783/google_gemini-2.5-flash_incomplete_calculation_total_saving_.json\n",
      "✅ Moving: tier2/7437/google_gemini-2.5-flash_operator_swap_total_earnings_.json\n",
      "✅ Moving: tier2/736/google_gemini-2.5-flash_incorrect_operand_food_subtotal_mango_lassi_cost.json\n",
      "✅ Moving: tier1/2444/google_gemini-2.5-flash_incomplete_calculation_total_collected_thus_far_.json\n",
      "✅ Moving: tier1/1021/google_gemini-2.5-flash_incomplete_calculation_total_homework_minutes_spent_.json\n",
      "✅ Moving: tier1/7391/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_dryer_cost.json\n",
      "✅ Moving: tier1/7319/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_distance.json\n",
      "✅ Moving: tier1/5537/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_steps_week4.json\n",
      "✅ Moving: tier1/5496/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_people_after_second_stop.json\n",
      "✅ Moving: tier1/4009/google_gemini-2.5-flash_incomplete_calculation_total_cards_given_away_.json\n",
      "⚠️  Overwriting: tier1/5496/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_people_after_second_stop.json\n",
      "✅ Moving: tier1/6654/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_students.json\n",
      "✅ Moving: tier1/7054/google_gemini-2.5-flash_incomplete_calculation_total_sold_.json\n",
      "✅ Moving: tier1/4165/google_gemini-2.5-flash_incorrect_operand_sandwiches_day3_multiplier_double.json\n",
      "✅ Moving: tier1/5034/google_gemini-2.5-flash_incomplete_calculation_total_spent_hours_.json\n",
      "✅ Moving: tier1/4957/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_doubled_speed.json\n",
      "✅ Moving: tier1/2161/google_gemini-2.5-flash_incorrect_operand_money_after_casino_money_after_donation.json\n",
      "✅ Moving: tier1/7065/google_gemini-2.5-flash_incomplete_calculation_total_wheels_.json\n",
      "✅ Moving: tier1/6478/google_gemini-2.5-flash_incomplete_calculation_total_growth_.json\n",
      "✅ Moving: tier1/3456/google_gemini-2.5-flash_incomplete_calculation_students_in_first_three_groups_.json\n",
      "✅ Moving: tier1/1599/google_gemini-2.5-flash_incomplete_calculation_total_cost_.json\n",
      "✅ Moving: tier1/4689/google_gemini-2.5-flash_incomplete_calculation_total_eggs_dozen_.json\n",
      "✅ Moving: tier1/4181/google_gemini-2.5-flash_input_misrepresentation_savings_week2_multiplier_twice.json\n",
      "✅ Moving: tier1/5160/google_gemini-2.5-flash_operator_swap_rayden_more_geese_.json\n",
      "✅ Moving: tier1/7070/google_gemini-2.5-flash_input_misrepresentation_total_bread_double_meat_total_bread_regular.json\n",
      "✅ Moving: tier1/3761/google_gemini-2.5-flash_operator_swap_fleas_before_treatments_.json\n",
      "✅ Moving: tier1/4794/google_gemini-2.5-flash_incomplete_calculation_yuri_sum_.json\n",
      "✅ Moving: tier1/604/google_gemini-2.5-flash_incomplete_calculation_brothers_bday_second_half_.json\n",
      "✅ Moving: tier1/423/google_gemini-2.5-flash_operator_swap_money_left_.json\n",
      "✅ Moving: tier1/4735/google_gemini-2.5-flash_incorrect_operand_total_ounces_per_day_water_ounces_per_day.json\n",
      "✅ Moving: tier1/4638/google_gemini-2.5-flash_operator_swap_soda_total_cost_.json\n",
      "✅ Moving: tier1/1160/google_gemini-2.5-flash_input_misrepresentation_time_with_new_shoes_time_with_normal_shoes.json\n",
      "✅ Moving: tier1/2263/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_leo_marbles.json\n",
      "✅ Moving: tier1/2443/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_tickets_cost.json\n",
      "✅ Moving: tier1/2449/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_total_spent.json\n",
      "✅ Moving: tier1/3969/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_minutes_lost_from_Ds.json\n",
      "⚠️  Overwriting: tier1/1230/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_time_per_coat_cycle.json\n",
      "✅ Moving: tier1/887/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_julian_age.json\n",
      "✅ Moving: tier1/7012/google_gemini-2.5-flash_input_misrepresentation_jake_total_earnings_jacob_hourly_wage.json\n",
      "✅ Moving: tier1/615/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_gap_sister_grandma.json\n",
      "⚠️  Overwriting: tier1/615/google_gemini-2.5-flash_incorrect_final_answer_selection_answer_gap_sister_grandma.json\n",
      "\n",
      "📊 File Movement Summary:\n",
      "   • Successfully moved: 110\n",
      "   • Overwritten existing files: 7\n",
      "   • Errors: 0\n",
      "\n",
      "📋 Updating Ling's main validation catalog...\n",
      "✅ Created backup: ../data/conceptual-error-candidates/validation_catalog_ling.csv.backup\n",
      "✅ Updated main catalog: ../data/conceptual-error-candidates/validation_catalog_ling.csv\n",
      "✅ Verification: New main catalog has 110 accepted entries\n",
      "\n",
      "================================================================================\n",
      "🎉 MIGRATION COMPLETE!\n",
      "================================================================================\n",
      "Summary:\n",
      "   • Moved 110 accepted files to main folder\n",
      "   • Updated main validation catalog\n",
      "   • 110 accepted entries now in main catalog\n",
      "   • 7 existing files were overwritten\n",
      "\n",
      "Next steps:\n",
      "   1. Verify the files are in the correct locations\n",
      "   2. Re-run your original diagnostics to confirm consistency\n",
      "   3. Consider cleaning up the copy folders if everything looks good\n",
      "\n",
      "========================================\n",
      "VERIFICATION CHECK\n",
      "========================================\n",
      "Updated Ling catalog has 110 accepted entries\n",
      "Checking 3 sample files...\n",
      "   ✅ Index 5691: True\n",
      "   ✅ Index 7184: True\n",
      "   ✅ Index 7146: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def move_ling_files_to_main():\n",
    "    \"\"\"\n",
    "    Move all of Ling's accepted files from the copy folder to the main accepted folder\n",
    "    and update Ling's validation catalog.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MOVING LING'S FILES TO MAIN FOLDERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Paths\n",
    "    copy_folder = Path(\"../data/conceptual-errors-accepted-copy\")\n",
    "    main_folder = Path(\"../data/conceptual-errors-accepted\")\n",
    "    copy_catalog_path = Path(\"../data/conceptual-error-candidates/validation_catalog_ling_copy.csv\")\n",
    "    main_catalog_path = Path(\"../data/conceptual-error-candidates/validation_catalog_ling.csv\")\n",
    "    \n",
    "    # Step 1: Load the copy catalog to see what we're working with\n",
    "    print(\"📋 Loading Ling's copy catalog...\")\n",
    "    df_ling_copy = pd.read_csv(copy_catalog_path)\n",
    "    accepted_entries = df_ling_copy[df_ling_copy['status'] == 'accepted']\n",
    "    \n",
    "    print(f\"Found {len(accepted_entries)} accepted entries to process\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Move accepted files\n",
    "    print(\"📁 Moving accepted files...\")\n",
    "    moved_count = 0\n",
    "    error_count = 0\n",
    "    overwritten_count = 0\n",
    "    \n",
    "    for idx, row in accepted_entries.iterrows():\n",
    "        try:\n",
    "            # Parse the filepath to get the relative path structure\n",
    "            filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "            file_path = Path(filepath)\n",
    "            \n",
    "            # Extract relative path from conceptual-errors-accepted onwards\n",
    "            if \"conceptual-errors-accepted\" in str(file_path):\n",
    "                parts = file_path.parts\n",
    "                try:\n",
    "                    accepted_idx = next(i for i, part in enumerate(parts) if \"conceptual-errors-accepted\" in part)\n",
    "                    relative_path = Path(*parts[accepted_idx + 1:])\n",
    "                except StopIteration:\n",
    "                    relative_path = file_path\n",
    "            else:\n",
    "                relative_path = file_path\n",
    "            \n",
    "            # Source and destination paths\n",
    "            source_path = copy_folder / relative_path\n",
    "            dest_path = main_folder / relative_path\n",
    "            \n",
    "            # Check if source exists\n",
    "            if not source_path.exists():\n",
    "                print(f\"❌ Source file not found: {source_path}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create destination directory if it doesn't exist\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Check if destination already exists\n",
    "            will_overwrite = dest_path.exists()\n",
    "            if will_overwrite:\n",
    "                overwritten_count += 1\n",
    "                print(f\"⚠️  Overwriting: {relative_path}\")\n",
    "            else:\n",
    "                print(f\"✅ Moving: {relative_path}\")\n",
    "            \n",
    "            # Copy the file (this will overwrite if destination exists)\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            moved_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {row['index']}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print()\n",
    "    print(f\"📊 File Movement Summary:\")\n",
    "    print(f\"   • Successfully moved: {moved_count}\")\n",
    "    print(f\"   • Overwritten existing files: {overwritten_count}\")\n",
    "    print(f\"   • Errors: {error_count}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Replace the main catalog with the copy\n",
    "    print(\"📋 Updating Ling's main validation catalog...\")\n",
    "    \n",
    "    try:\n",
    "        # Create backup of original catalog if it exists\n",
    "        if main_catalog_path.exists():\n",
    "            backup_path = main_catalog_path.with_suffix('.csv.backup')\n",
    "            shutil.copy2(main_catalog_path, backup_path)\n",
    "            print(f\"✅ Created backup: {backup_path}\")\n",
    "        \n",
    "        # Copy the catalog\n",
    "        shutil.copy2(copy_catalog_path, main_catalog_path)\n",
    "        print(f\"✅ Updated main catalog: {main_catalog_path}\")\n",
    "        \n",
    "        # Verify the update\n",
    "        df_new_main = pd.read_csv(main_catalog_path)\n",
    "        accepted_in_new = len(df_new_main[df_new_main['status'] == 'accepted'])\n",
    "        print(f\"✅ Verification: New main catalog has {accepted_in_new} accepted entries\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error updating catalog: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎉 MIGRATION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Summary:\")\n",
    "    print(f\"   • Moved {moved_count} accepted files to main folder\")\n",
    "    print(f\"   • Updated main validation catalog\")\n",
    "    print(f\"   • {accepted_in_new} accepted entries now in main catalog\")\n",
    "    if overwritten_count > 0:\n",
    "        print(f\"   • {overwritten_count} existing files were overwritten\")\n",
    "    print()\n",
    "    print(\"Next steps:\")\n",
    "    print(\"   1. Verify the files are in the correct locations\")\n",
    "    print(\"   2. Re-run your original diagnostics to confirm consistency\")\n",
    "    print(\"   3. Consider cleaning up the copy folders if everything looks good\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the migration\n",
    "success = move_ling_files_to_main()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"VERIFICATION CHECK\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Quick verification: reload the updated catalog and check consistency\n",
    "    df_ling_updated = pd.read_csv(\"../data/conceptual-error-candidates/validation_catalog_ling.csv\")\n",
    "    df_ling_accepted = df_ling_updated[df_ling_updated['status'] == 'accepted']\n",
    "    \n",
    "    print(f\"Updated Ling catalog has {len(df_ling_accepted)} accepted entries\")\n",
    "    \n",
    "    # Check a few files to make sure they're in the right place\n",
    "    sample_size = min(3, len(df_ling_accepted))\n",
    "    print(f\"Checking {sample_size} sample files...\")\n",
    "    \n",
    "    for idx, (_, row) in enumerate(df_ling_accepted.head(sample_size).iterrows()):\n",
    "        filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "        full_path = Path(\"..\") / filepath\n",
    "        exists = full_path.exists()\n",
    "        status = \"✅\" if exists else \"❌\"\n",
    "        print(f\"   {status} Index {row['index']}: {exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb4f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC REPORT: Validation Catalog vs Accepted Files\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   195    195\n",
      "Total      195    195\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 195\n",
      "   • Samples marked 'accepted': 195\n",
      "   • Files actually found: 195\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted    47     47\n",
      "Total       47     47\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 47\n",
      "   • Samples marked 'accepted': 47\n",
      "   • Files actually found: 47\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 0\n",
      "   • Samples marked 'accepted': 0\n",
      "   • Files actually found: 0\n",
      "   • Inconsistencies: 0\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "added     True  Total\n",
      "status               \n",
      "accepted   312    312\n",
      "Total      312    312\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 312\n",
      "   • Samples marked 'accepted': 312\n",
      "   • Files actually found: 312\n",
      "   • Inconsistencies: 0\n",
      "   • Success rate (accepted → file created): 100.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "Cross-tabulation (Status vs File Actually Added):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "✅ No inconsistencies found!\n",
      "📊 SUMMARY:\n",
      "   • Total samples in catalog: 0\n",
      "   • Samples marked 'accepted': 0\n",
      "   • Files actually found: 0\n",
      "   • Inconsistencies: 0\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total reviewers checked: 5\n",
      "   • Total inconsistencies across all reviewers: 0\n",
      "   • 🎉 All validation catalogs are consistent with accepted files!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "run_diagnostics(validation_catalogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3015d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def diagnose_null_erroneous_line_numbers():\n",
    "    \"\"\"\n",
    "    Diagnose how many accepted files have null erroneous_line_number values for each reviewer.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load all validation catalogs\n",
    "    validation_catalogs = {}\n",
    "    parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "    \n",
    "    for reviewer in ['ali', 'arvind', 'ling', 'mauro', 'yewei']:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{parent_dir}{reviewer}.csv\")\n",
    "            validation_catalogs[reviewer] = df[df['status'] == 'accepted']\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  Catalog not found for {reviewer}\")\n",
    "            validation_catalogs[reviewer] = pd.DataFrame()\n",
    "    \n",
    "    total_null_count = 0\n",
    "    total_files_checked = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        if len(df) == 0:\n",
    "            print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(\"No accepted files found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        null_count = 0\n",
    "        valid_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Normalize the file path\n",
    "                normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "                file_path = Path(\"..\") / normalized_filepath\n",
    "                \n",
    "                if not file_path.exists():\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load and check the JSON\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                erroneous_line = json_data.get('error_details', {}).get('erroneous_line_number')\n",
    "                \n",
    "                if erroneous_line is None or erroneous_line == \"null\":\n",
    "                    null_count += 1\n",
    "                    # Show a few examples\n",
    "                    if null_count <= 3:\n",
    "                        print(f\"   • Index {row['index']}: {row['mutation_type']} - NULL erroneous_line_number\")\n",
    "                else:\n",
    "                    valid_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"   ❌ Error reading file for index {row['index']}: {e}\")\n",
    "        \n",
    "        total_files_checked += len(df)\n",
    "        total_null_count += null_count\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Total accepted files: {len(df)}\")\n",
    "        print(f\"   • Files with NULL erroneous_line_number: {null_count}\")\n",
    "        print(f\"   • Files with valid erroneous_line_number: {valid_count}\")\n",
    "        print(f\"   • Files with read errors: {error_count}\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            null_percentage = (null_count / len(df)) * 100\n",
    "            print(f\"   • Percentage with NULL: {null_percentage:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"🎯 OVERALL SUMMARY:\")\n",
    "    print(f\"   • Total accepted files checked: {total_files_checked}\")\n",
    "    print(f\"   • Total files with NULL erroneous_line_number: {total_null_count}\")\n",
    "    if total_files_checked > 0:\n",
    "        overall_percentage = (total_null_count / total_files_checked) * 100\n",
    "        print(f\"   • Overall percentage with NULL: {overall_percentage:.1f}%\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return total_null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14bab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "   • Index 1229: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "   • Index 7154: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "   • Index 1230: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 195\n",
      "   • Files with NULL erroneous_line_number: 3\n",
      "   • Files with valid erroneous_line_number: 192\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 1.5%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 51\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 51\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "   • Index 7184: operator_swap - NULL erroneous_line_number\n",
      "   • Index 7146: operator_swap - NULL erroneous_line_number\n",
      "   • Index 2407: operator_swap - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 109\n",
      "   • Files with valid erroneous_line_number: 1\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 99.1%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "   • Index 4534: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 1\n",
      "   • Files with valid erroneous_line_number: 311\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.3%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "No accepted files found.\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 668\n",
      "   • Total files with NULL erroneous_line_number: 113\n",
      "   • Overall percentage with NULL: 16.9%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "null_count = diagnose_null_erroneous_line_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24540b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "erroneous_line_number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e5ec4a30-2edc-447b-b2c8-ceacfda77a54",
       "rows": [
        [
         "FA",
         "538"
        ],
        [
         "L1",
         "503"
        ],
        [
         "L2",
         "369"
        ],
        [
         "L3",
         "291"
        ],
        [
         "L4",
         "156"
        ],
        [
         "L5",
         "56"
        ],
        [
         "L6",
         "16"
        ],
        [
         "L7",
         "7"
        ],
        [
         "L8",
         "5"
        ],
        [
         "L9",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "erroneous_line_number\n",
       "FA    538\n",
       "L1    503\n",
       "L2    369\n",
       "L3    291\n",
       "L4    156\n",
       "L5     56\n",
       "L6     16\n",
       "L7      7\n",
       "L8      5\n",
       "L9      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shortlist = pd.read_csv(\"../data/conceptual-error-candidates/conceptual_candidates_shortlist.csv\")\n",
    "\n",
    "df_shortlist[\"erroneous_line_number\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ede5bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running diagnostic...\n",
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "   • Index 1229: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "   • Index 7154: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "   • Index 1230: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 195\n",
      "   • Files with NULL erroneous_line_number: 3\n",
      "   • Files with valid erroneous_line_number: 192\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 1.5%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 51\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 51\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "   • Index 7184: operator_swap - NULL erroneous_line_number\n",
      "   • Index 7146: operator_swap - NULL erroneous_line_number\n",
      "   • Index 2407: operator_swap - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 109\n",
      "   • Files with valid erroneous_line_number: 1\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 99.1%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "   • Index 4534: incorrect_final_answer_selection - NULL erroneous_line_number\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 1\n",
      "   • Files with valid erroneous_line_number: 311\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.3%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "No accepted files found.\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 668\n",
      "   • Total files with NULL erroneous_line_number: 113\n",
      "   • Overall percentage with NULL: 16.9%\n",
      "================================================================================\n",
      "\n",
      "🔧 Running repair...\n",
      "================================================================================\n",
      "REPAIR: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "✅ Loaded shortlist with 1942 entries\n",
      "✅ Created lookup table with 1744 entries\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "   ✅ Repaired Index 1229: None → FA\n",
      "   ✅ Repaired Index 7154: None → FA\n",
      "   ✅ Repaired Index 1230: None → FA\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 3\n",
      "   • Files already valid: 192\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 51\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "   ✅ Repaired Index 7184: None → L1\n",
      "   ✅ Repaired Index 7146: None → L1\n",
      "   ✅ Repaired Index 2407: None → L1\n",
      "   ✅ Repaired Index 1111: None → L1\n",
      "   ✅ Repaired Index 6580: None → L3\n",
      "   ✅ Repaired Index 3517: None → L1\n",
      "   ✅ Repaired Index 2512: None → L2\n",
      "   ✅ Repaired Index 7413: None → L3\n",
      "   ✅ Repaired Index 2844: None → L4\n",
      "   ✅ Repaired Index 7399: None → L4\n",
      "   ✅ Repaired Index 4305: None → L1\n",
      "   ✅ Repaired Index 6682: None → L1\n",
      "   ✅ Repaired Index 6146: None → L3\n",
      "   ✅ Repaired Index 3001: None → L1\n",
      "   ✅ Repaired Index 824: None → FA\n",
      "   ✅ Repaired Index 2368: None → L1\n",
      "   ✅ Repaired Index 2488: None → L1\n",
      "   ✅ Repaired Index 5572: None → L1\n",
      "   ✅ Repaired Index 6454: None → L2\n",
      "   ✅ Repaired Index 1215: None → L1\n",
      "   ✅ Repaired Index 841: None → FA\n",
      "   ✅ Repaired Index 1192: None → FA\n",
      "   ✅ Repaired Index 4534: None → FA\n",
      "   ✅ Repaired Index 4789: None → FA\n",
      "   ✅ Repaired Index 4490: None → FA\n",
      "   ✅ Repaired Index 1351: None → FA\n",
      "   ✅ Repaired Index 1679: None → FA\n",
      "   ✅ Repaired Index 2454: None → FA\n",
      "   ✅ Repaired Index 2677: None → FA\n",
      "   ✅ Repaired Index 2781: None → FA\n",
      "   ✅ Repaired Index 3639: None → FA\n",
      "   ✅ Repaired Index 2428: None → L2\n",
      "   ✅ Repaired Index 5354: None → L1\n",
      "   ✅ Repaired Index 6120: None → L3\n",
      "   ✅ Repaired Index 1729: None → L3\n",
      "   ✅ Repaired Index 7376: None → L1\n",
      "   ✅ Repaired Index 5716: None → L1\n",
      "   ✅ Repaired Index 486: None → L4\n",
      "   ✅ Repaired Index 4945: None → L1\n",
      "   ✅ Repaired Index 6429: None → L2\n",
      "   ✅ Repaired Index 5925: None → L3\n",
      "   ✅ Repaired Index 3760: None → L4\n",
      "   ✅ Repaired Index 2153: None → L5\n",
      "   ✅ Repaired Index 6625: None → L2\n",
      "   ✅ Repaired Index 3941: None → L1\n",
      "   ✅ Repaired Index 4222: None → L1\n",
      "   ✅ Repaired Index 6250: None → L1\n",
      "   ✅ Repaired Index 6048: None → L2\n",
      "   ✅ Repaired Index 5194: None → L2\n",
      "   ✅ Repaired Index 4439: None → L3\n",
      "   ✅ Repaired Index 2323: None → L2\n",
      "   ✅ Repaired Index 1625: None → L2\n",
      "   ✅ Repaired Index 5184: None → L3\n",
      "   ✅ Repaired Index 3893: None → L2\n",
      "   ✅ Repaired Index 898: None → FA\n",
      "   ✅ Repaired Index 1548: None → FA\n",
      "   ✅ Repaired Index 1489: None → L2\n",
      "   ✅ Repaired Index 5863: None → L2\n",
      "   ✅ Repaired Index 3878: None → L1\n",
      "   ✅ Repaired Index 5883: None → L1\n",
      "   ✅ Repaired Index 153: None → L4\n",
      "   ✅ Repaired Index 445: None → L2\n",
      "   ✅ Repaired Index 1293: None → L2\n",
      "   ✅ Repaired Index 7306: None → L1\n",
      "   ✅ Repaired Index 4460: None → L2\n",
      "   ✅ Repaired Index 5783: None → L4\n",
      "   ✅ Repaired Index 7437: None → L1\n",
      "   ✅ Repaired Index 736: None → L3\n",
      "   ✅ Repaired Index 2444: None → L4\n",
      "   ✅ Repaired Index 1021: None → L2\n",
      "   ✅ Repaired Index 7391: None → FA\n",
      "   ✅ Repaired Index 7319: None → FA\n",
      "   ✅ Repaired Index 5537: None → FA\n",
      "   ✅ Repaired Index 5496: None → FA\n",
      "   ✅ Repaired Index 4009: None → L4\n",
      "   ✅ Repaired Index 6654: None → FA\n",
      "   ✅ Repaired Index 7054: None → L2\n",
      "   ✅ Repaired Index 4165: None → L2\n",
      "   ✅ Repaired Index 5034: None → L1\n",
      "   ✅ Repaired Index 4957: None → FA\n",
      "   ✅ Repaired Index 2161: None → L4\n",
      "   ✅ Repaired Index 7065: None → L5\n",
      "   ✅ Repaired Index 6478: None → L3\n",
      "   ✅ Repaired Index 3456: None → L1\n",
      "   ✅ Repaired Index 1599: None → L1\n",
      "   ✅ Repaired Index 4689: None → L3\n",
      "   ✅ Repaired Index 4181: None → L1\n",
      "   ✅ Repaired Index 5160: None → L4\n",
      "   ✅ Repaired Index 7070: None → L1\n",
      "   ✅ Repaired Index 3761: None → L1\n",
      "   ✅ Repaired Index 4794: None → L1\n",
      "   ✅ Repaired Index 604: None → L1\n",
      "   ✅ Repaired Index 423: None → L2\n",
      "   ✅ Repaired Index 4735: None → L3\n",
      "   ✅ Repaired Index 4638: None → L1\n",
      "   ✅ Repaired Index 1160: None → L2\n",
      "   ✅ Repaired Index 2263: None → FA\n",
      "   ✅ Repaired Index 2443: None → FA\n",
      "   ✅ Repaired Index 2449: None → FA\n",
      "   ✅ Repaired Index 3969: None → FA\n",
      "   ✅ Repaired Index 887: None → FA\n",
      "   ✅ Repaired Index 7012: None → L2\n",
      "   ✅ Repaired Index 615: None → FA\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 103\n",
      "   • Files already valid: 7\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Files repaired: 0\n",
      "   • Files already valid: 312\n",
      "   • Files not found in shortlist: 0\n",
      "   • Files with errors: 0\n",
      "\n",
      "================================================================================\n",
      "🎯 REPAIR SUMMARY:\n",
      "   • Total files repaired: 106\n",
      "   • Total files already valid: 562\n",
      "   • Total files not found in shortlist: 0\n",
      "================================================================================\n",
      "✅ Repair completed! Backup files (.json.backup) were created for all modified files.\n",
      "\n",
      "🔍 Running post-repair diagnostic...\n",
      "================================================================================\n",
      "DIAGNOSTIC: NULL ERRONEOUS LINE NUMBERS\n",
      "================================================================================\n",
      "\n",
      "📋 REVIEWER: ALI\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 195\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 195\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: ARVIND\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 51\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 51\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: LING\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 110\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 110\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: MAURO\n",
      "--------------------------------------------------\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 312\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "No accepted files found.\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 668\n",
      "   • Total files with NULL erroneous_line_number: 0\n",
      "   • Overall percentage with NULL: 0.0%\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   • Total accepted files: 312\n",
      "   • Files with NULL erroneous_line_number: 0\n",
      "   • Files with valid erroneous_line_number: 312\n",
      "   • Files with read errors: 0\n",
      "   • Percentage with NULL: 0.0%\n",
      "\n",
      "📋 REVIEWER: YEWEI\n",
      "--------------------------------------------------\n",
      "No accepted files found.\n",
      "\n",
      "================================================================================\n",
      "🎯 OVERALL SUMMARY:\n",
      "   • Total accepted files checked: 668\n",
      "   • Total files with NULL erroneous_line_number: 0\n",
      "   • Overall percentage with NULL: 0.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def repair_null_erroneous_line_numbers():\n",
    "    \"\"\"\n",
    "    Repair null erroneous_line_number values using the conceptual_candidates_shortlist.csv file.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REPAIR: NULL ERRONEOUS LINE NUMBERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the shortlist CSV for reference\n",
    "    shortlist_path = \"../data/conceptual-error-candidates/conceptual_candidates_shortlist.csv\"\n",
    "    try:\n",
    "        df_shortlist = pd.read_csv(shortlist_path)\n",
    "        print(f\"✅ Loaded shortlist with {len(df_shortlist)} entries\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Shortlist file not found: {shortlist_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Create a lookup dictionary: (index, tier, mutation_type) -> erroneous_line_number\n",
    "    lookup = {}\n",
    "    for _, row in df_shortlist.iterrows():\n",
    "        key = (row['index'], row['tier'], row['mutation_type'])\n",
    "        lookup[key] = row['erroneous_line_number']\n",
    "    \n",
    "    print(f\"✅ Created lookup table with {len(lookup)} entries\")\n",
    "    print()\n",
    "    \n",
    "    # Load all validation catalogs\n",
    "    validation_catalogs = {}\n",
    "    parent_dir = \"../data/conceptual-error-candidates/validation_catalog_\"\n",
    "    \n",
    "    for reviewer in ['ali', 'arvind', 'ling', 'mauro', 'yewei']:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{parent_dir}{reviewer}.csv\")\n",
    "            validation_catalogs[reviewer] = df[df['status'] == 'accepted']\n",
    "        except FileNotFoundError:\n",
    "            validation_catalogs[reviewer] = pd.DataFrame()\n",
    "    \n",
    "    total_repaired = 0\n",
    "    total_not_found = 0\n",
    "    total_already_valid = 0\n",
    "    \n",
    "    for reviewer_name, df in validation_catalogs.items():\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"📋 REVIEWER: {reviewer_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        repaired_count = 0\n",
    "        not_found_count = 0\n",
    "        already_valid_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Normalize the file path\n",
    "                normalized_filepath = str(row['filepath']).replace('\\\\', '/')\n",
    "                file_path = Path(\"..\") / normalized_filepath\n",
    "                \n",
    "                if not file_path.exists():\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load the JSON\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    json_data = json.load(f)\n",
    "                \n",
    "                current_erroneous_line = json_data.get('error_details', {}).get('erroneous_line_number')\n",
    "                \n",
    "                # Check if repair is needed\n",
    "                if current_erroneous_line is not None and current_erroneous_line != \"null\":\n",
    "                    already_valid_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Look up the correct value\n",
    "                key = (row['index'], row['tier'], row['mutation_type'])\n",
    "                if key in lookup:\n",
    "                    correct_erroneous_line = lookup[key]\n",
    "                    \n",
    "                    # Create backup\n",
    "                    backup_path = file_path.with_suffix('.json.backup')\n",
    "                    shutil.copy2(file_path, backup_path)\n",
    "                    \n",
    "                    # Update the JSON\n",
    "                    json_data['error_details']['erroneous_line_number'] = correct_erroneous_line\n",
    "                    \n",
    "                    # Save the updated JSON\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    print(f\"   ✅ Repaired Index {row['index']}: {current_erroneous_line} → {correct_erroneous_line}\")\n",
    "                    repaired_count += 1\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ❌ Not found in shortlist: Index {row['index']}, Tier {row['tier']}, Type {row['mutation_type']}\")\n",
    "                    not_found_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"   ❌ Error processing index {row['index']}: {e}\")\n",
    "        \n",
    "        total_repaired += repaired_count\n",
    "        total_not_found += not_found_count\n",
    "        total_already_valid += already_valid_count\n",
    "        \n",
    "        print(f\"📊 SUMMARY:\")\n",
    "        print(f\"   • Files repaired: {repaired_count}\")\n",
    "        print(f\"   • Files already valid: {already_valid_count}\")\n",
    "        print(f\"   • Files not found in shortlist: {not_found_count}\")\n",
    "        print(f\"   • Files with errors: {error_count}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"🎯 REPAIR SUMMARY:\")\n",
    "    print(f\"   • Total files repaired: {total_repaired}\")\n",
    "    print(f\"   • Total files already valid: {total_already_valid}\")\n",
    "    print(f\"   • Total files not found in shortlist: {total_not_found}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if total_repaired > 0:\n",
    "        print(\"✅ Repair completed! Backup files (.json.backup) were created for all modified files.\")\n",
    "    \n",
    "    return total_repaired > 0\n",
    "\n",
    "# Run diagnostic first\n",
    "print(\"🔍 Running diagnostic...\")\n",
    "null_count = diagnose_null_erroneous_line_numbers()\n",
    "\n",
    "if null_count > 0:\n",
    "    print(\"\\n\" + \"🔧 Running repair...\")\n",
    "    repair_success = repair_null_erroneous_line_numbers()\n",
    "    \n",
    "    if repair_success:\n",
    "        print(\"\\n\" + \"🔍 Running post-repair diagnostic...\")\n",
    "        diagnose_null_erroneous_line_numbers()\n",
    "else:\n",
    "    print(\"\\n✅ No null erroneous_line_number values found. No repair needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef8d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
