{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a58d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Hugging Face Login ---\n",
    "# from google.colab import userdata\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# if not hf_token:\n",
    "#     raise ValueError(\"HF_TOKEN not found in Colab Secrets. Please complete the prerequisite steps.\")\n",
    "# notebook_login(hf_token)\n",
    "\n",
    "# !pip install -Uq transformers==4.53.2\n",
    "# !pip install -Uq peft\n",
    "# !pip install -Uq trl\n",
    "# !pip install -Uq accelerate\n",
    "# !pip install -Uq datasets\n",
    "# !pip install -Uq bitsandbytes\n",
    "\n",
    "# # Install Flash Attention 2\n",
    "# !pip install flash-attn==2.7.4.post1 \\\n",
    "#   --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "#   --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90267fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",  # \"discriminative\" or \"generative\"\n",
    "    \"classification_type\": \"ternary\",   # \"binary\" or \"ternary\"\n",
    "    \"dataset_strategy\": \"3N\",          # \"4N\" or \"3N\" (generative only)\n",
    "    \"include_explanation\": True,      # True or False (generative only)\n",
    "    \"include_eln\": True,              # True or False (generative only)\n",
    "    \"solution_format\": \"nl\",        # \"dict\" or \"nl\" (generative only)\n",
    "    \"model_name\": \"microsoft/phi-4-mini-instruct\",  # or \"Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # Prompting configuration\n",
    "    \"system_prompt\": None,  # Will auto-generate if None, or use custom string\n",
    "    \"include_examples\": False,\n",
    "    \"num_examples\": 3,\n",
    "    \"example_strategy\": \"balanced\",  # \"balanced\", \"error_focused\", \"custom\"\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": 1600,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "\n",
    "    # LoRa params\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Paths and tokens\n",
    "    # \"base_dataset_dir\": \"/content/drive/MyDrive/sft_datasets\",\n",
    "    \"base_dataset_dir\": \"../data/base-datasets-sanitized\",\n",
    "    \"output_base_dir\": \"/content/drive/MyDrive/sft_experiments\",\n",
    "    # \"hf_token\": \"your_huggingface_token_here\",\n",
    "    # \"wandb_project\": \"math_error_classification\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "    \"use_wandb\": False\n",
    "}\n",
    "\n",
    "# Generate experiment ID\n",
    "import datetime\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],  # \"gene\" or \"disc\"\n",
    "    CONFIG[\"classification_type\"][:3],  # \"bin\" or \"ter\"\n",
    "    CONFIG[\"dataset_strategy\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"qwen\" if \"Qwen\" in CONFIG[\"model_name\"] else \"phi4\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "\n",
    "def setup_output_directory(config):\n",
    "    \"\"\"Creates organized output directory structure\"\"\"\n",
    "    \n",
    "    output_dir = Path(config[\"output_base_dir\"]) / config[\"experiment_id\"]\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"baseline\", \"training\", \"final\", \"checkpoints\"]\n",
    "    for subdir in subdirs:\n",
    "        (output_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = output_dir / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# # Setup output directory\n",
    "# output_dir = setup_output_directory(CONFIG)\n",
    "# CONFIG[\"output_dir\"] = str(output_dir)\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"Dependencies imported and seeds set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_prompt(config):\n",
    "    \"\"\"Auto-generates appropriate system prompt based on config\"\"\"\n",
    "    \n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        return \"You are a mathematics tutor. Classify the given solution.\"\n",
    "    \n",
    "    # Generative prompts\n",
    "    base_prompt = \"You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format.\"\n",
    "    \n",
    "    # Add classification instructions\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        base_prompt += \" Determine if the solution is 'correct' or 'flawed'.\"\n",
    "    else:\n",
    "        base_prompt += \" Classify as 'correct', 'conceptual_error', or 'computational_error'.\"\n",
    "    \n",
    "    # Add field instructions\n",
    "    fields = []\n",
    "    if config[\"include_eln\"]:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            fields.append(\"identify the erroneous line number (e.g., 'L1', 'FA')\")\n",
    "        else:\n",
    "            fields.append(\"quote the full erroneous line text\")\n",
    "    \n",
    "    if config[\"include_explanation\"]:\n",
    "        fields.append(\"provide a brief explanation of any error\")\n",
    "    \n",
    "    if fields:\n",
    "        base_prompt += f\" Also {', and '.join(fields)}.\"\n",
    "    \n",
    "    base_prompt += \" Respond only with valid JSON.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Auto-generate system prompt if not provided\n",
    "if CONFIG[\"system_prompt\"] is None:\n",
    "    CONFIG[\"system_prompt\"] = generate_system_prompt(CONFIG)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(CONFIG[\"system_prompt\"])\n",
    "print()\n",
    "\n",
    "# Allow manual override\n",
    "print(\"To customize the system prompt, run:\")\n",
    "print('CONFIG[\"system_prompt\"] = \"Your custom prompt here\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset, config):\n",
    "        # Convert DataFrame to list of dicts if needed\n",
    "        if hasattr(base_dataset, 'to_dict'):  # It's a DataFrame\n",
    "            self.samples = base_dataset.to_dict('records')\n",
    "        else:\n",
    "            self.samples = base_dataset  # Already a list of dicts\n",
    "            \n",
    "        self.config = config\n",
    "        self._prepare_examples_by_problem()\n",
    "    \n",
    "    def _prepare_examples_by_problem(self):\n",
    "        \"\"\"Organizes samples by problem index and error type\"\"\"\n",
    "        self.problems_by_type = {\n",
    "            \"correct\": {},\n",
    "            \"conceptual_error\": {},\n",
    "            \"computational_error\": {}\n",
    "        }\n",
    "        \n",
    "        # Group samples by problem index and error type\n",
    "        for sample in self.samples:\n",
    "            problem_index = sample[\"index\"]\n",
    "            error_type = sample[\"error_type\"]\n",
    "            \n",
    "            if problem_index not in self.problems_by_type[error_type]:\n",
    "                self.problems_by_type[error_type][problem_index] = []\n",
    "            self.problems_by_type[error_type][problem_index].append(sample)\n",
    "        \n",
    "        print(f\"Problems by type: correct={len(self.problems_by_type['correct'])}, \"\n",
    "              f\"conceptual={len(self.problems_by_type['conceptual_error'])}, \"\n",
    "              f\"computational={len(self.problems_by_type['computational_error'])}\")\n",
    "    \n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns examples based on dataset strategy\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        num_examples = self.config[\"num_examples\"]\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        examples = []\n",
    "        \n",
    "        import random\n",
    "        if dataset_strategy == \"3N\":\n",
    "            # Choose num_examples distinct problem indices that have all 3 versions\n",
    "            available_problems = set(self.problems_by_type[\"correct\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"conceptual_error\"].keys()) & \\\n",
    "                               set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            # Sample problem indices\n",
    "            selected_problems = random.sample(list(available_problems), num_examples)\n",
    "            for problem_index in selected_problems:\n",
    "                # Add all 3 versions: correct, conceptual_error, computational_error\n",
    "                examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "                examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "            \n",
    "        elif dataset_strategy == \"4N\":\n",
    "            import math\n",
    "            # Get problems that have conceptual errors (with correct versions)\n",
    "            conceptual_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"conceptual_error\"].keys())\n",
    "            )\n",
    "            # Get problems that have computational errors (with correct versions)\n",
    "            computational_problems = list(\n",
    "                set(self.problems_by_type[\"correct\"].keys()) & \n",
    "                set(self.problems_by_type[\"computational_error\"].keys())\n",
    "            )\n",
    "            # Calculate splits: floor(n/2) conceptual, ceil(n/2) computational\n",
    "            n_conceptual = num_examples // 2  # This is floor(n/2)\n",
    "            n_computational = math.ceil(num_examples / 2)\n",
    "            \n",
    "            # Sample conceptual problems\n",
    "            if conceptual_problems and n_conceptual > 0:\n",
    "                selected_conceptual = random.sample(conceptual_problems,n_conceptual)\n",
    "                for problem_index in selected_conceptual:\n",
    "                    # Add correct + conceptual_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"conceptual_error\"][problem_index][0])\n",
    "            \n",
    "            # Sample computational problems\n",
    "            if computational_problems and n_computational > 0:\n",
    "                selected_computational = random.sample(computational_problems,n_computational)\n",
    "                for problem_index in selected_computational:\n",
    "                    # Add correct + computational_error pair\n",
    "                    examples.append(self.problems_by_type[\"correct\"][problem_index][0])\n",
    "                    examples.append(self.problems_by_type[\"computational_error\"][problem_index][0])\n",
    "            \n",
    "            return examples\n",
    "        \n",
    "        else:\n",
    "            print(f\"Warning: Unknown dataset strategy '{dataset_strategy}'\")\n",
    "            return []\n",
    "\n",
    "print(\"Updated ExampleManager class loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bde813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_base_dataset():\n",
    "    \"\"\"Loads the appropriate base dataset\"\"\"\n",
    "    dataset_strategy = CONFIG[\"dataset_strategy\"]\n",
    "    base_dir = Path(CONFIG[\"base_dataset_dir\"])\n",
    "    dataset_file = base_dir / f\"base_{dataset_strategy}_dataset_sanitized.csv\"\n",
    "    data = pd.read_csv(dataset_file)\n",
    "    print(f\"Loaded base {dataset_strategy} dataset with {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "def make_solution_mapping(solution_text: str):\n",
    "    \"\"\"Creates a mapping of line numbers to solution lines.\"\"\"\n",
    "    solution_lines = solution_text.strip().split('\\n')\n",
    "    solution_mapping = {f\"L{i+1}\": line.strip() for i, line in enumerate(solution_lines[:-1]) if line.strip()}\n",
    "    solution_mapping[\"FA\"] = solution_lines[-1].strip()\n",
    "    return solution_mapping\n",
    "\n",
    "def format_solution(sample):\n",
    "    \"\"\"Formats solution according to config - updated for CSV structure\"\"\"\n",
    "    if sample[\"error_type\"] == \"correct\":\n",
    "        solution_text = sample.get(\"correct_answer\", \"\").strip()\n",
    "    else:\n",
    "        solution_text = sample.get(\"wrong_answer\", \"\").strip()\n",
    "    if CONFIG[\"solution_format\"] == \"dict\":\n",
    "        return make_solution_mapping(solution_text)\n",
    "    else:\n",
    "        return solution_text\n",
    "\n",
    "def format_expected_output(sample):\n",
    "    \"\"\"Creates the expected JSON output for a sample - updated for CSV structure\"\"\"\n",
    "    output = {}\n",
    "    output[\"verdict\"] = sample[\"error_type\"]\n",
    "    if CONFIG[\"classification_type\"] == \"binary\" and sample[\"error_type\"] != \"correct\":\n",
    "        output[\"verdict\"] = \"flawed\"\n",
    "    \n",
    "    # ELN\n",
    "    if CONFIG[\"include_eln\"]:\n",
    "        # If the sample is correct, set ELN or EL to None\n",
    "        if sample[\"error_type\"] == \"correct\":\n",
    "            if CONFIG[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = None\n",
    "            else:\n",
    "                output[\"erroneous_line\"] = None\n",
    "        # If sample is erroneous, extract ELN or EL\n",
    "        else:\n",
    "            eln = sample[\"erroneous_line_number\"]\n",
    "            if CONFIG[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = eln\n",
    "            else:\n",
    "                solution_text = sample[\"wrong_answer\"]\n",
    "                solution_mapping = make_solution_mapping(solution_text)\n",
    "                el = solution_mapping[eln] # get the actual line\n",
    "                output[\"erroneous_line\"] = el\n",
    "    \n",
    "    # Explanation\n",
    "    if CONFIG[\"include_explanation\"]:\n",
    "        output[\"explanation\"] = sample[\"explanation\"]\n",
    "    \n",
    "    return json.dumps(output)\n",
    "\n",
    "# def format_user_message(sample):\n",
    "#     \"\"\"Format a sample into a user message.\"\"\"\n",
    "#     return f\"### Question:\\n{sample['question']}\\n\\n### Answer:\\n{format_solution(sample)}\"\n",
    "\n",
    "def format_user_message(sample):\n",
    "    \"\"\"Format a sample into a user message, ensuring all parts are strings.\"\"\"\n",
    "    question_text = sample.get('question', '') or '' # Ensures it's a string, not None\n",
    "    solution_text = format_solution(sample)\n",
    "    return f\"### Question:\\n{question_text}\\n\\n### Answer:\\n{solution_text}\"\n",
    "\n",
    "def create_sample_messages(sample, examples):\n",
    "    \"\"\"Create complete message list for a sample.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # System message\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": CONFIG[\"system_prompt\"]\n",
    "    })\n",
    "    \n",
    "    # Few-shot examples\n",
    "    if CONFIG[\"include_examples\"]:\n",
    "        for example in examples:\n",
    "            user_content = format_user_message(example)\n",
    "            assistant_content = format_expected_output(example)\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "    \n",
    "    # Actual sample\n",
    "    user_content = format_user_message(sample)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "print(\"Updated formatting functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer with proper configuration\"\"\"\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\"  # Ensure left padding for causal models\n",
    "    \n",
    "    print(f\"✓ Tokenizer loaded successfully!\")\n",
    "    return tokenizer\n",
    "\n",
    "def apply_chat_template(\n",
    "        messages, \n",
    "        tokenizer, \n",
    "        add_generation_prompt=False, \n",
    "        tokenize=True, \n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Applies chat template to messages with consistent interface\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' keys\n",
    "        tokenizer: The tokenizer to use for formatting\n",
    "        add_generation_prompt: Whether to add generation prompt (for inference)\n",
    "        tokenize: Whether to return tokens (True) or text (False)\n",
    "        **kwargs: Additional arguments for tokenizer (like return_tensors, max_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        If tokenize=True: tokenizer output dict with input_ids, attention_mask, etc.\n",
    "        If tokenize=False: formatted text string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if this is a Qwen3 model and disable thinking if so\n",
    "    template_kwargs = {}\n",
    "    if CONFIG[\"model_name\"].startswith(\"Qwen\"):\n",
    "        template_kwargs['enable_thinking'] = False\n",
    "    \n",
    "    # Apply chat template to get formatted text\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        **template_kwargs\n",
    "    )\n",
    "    \n",
    "    # Return text if not tokenizing\n",
    "    if not tokenize:\n",
    "        return formatted_text\n",
    "    \n",
    "    # Tokenize and return tensor format\n",
    "    return tokenizer(formatted_text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9775f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "def step1_create_conversations(sample, examples):\n",
    "    \"\"\"[STEP 1] Creates the list of message dictionaries for a sample.\"\"\"\n",
    "    messages = create_sample_messages(sample, examples)\n",
    "    expected_output = format_expected_output(sample)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": expected_output})\n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg['content'] is None:\n",
    "            raise TypeError(f\"Message content is None at index {i} for sample ID {sample.get('id', 'N/A')}. Message: {msg}\")\n",
    "    return {\"conversation\": messages}\n",
    "\n",
    "def step2_apply_chat_template(sample, tokenizer):\n",
    "    \"\"\"[STEP 2] Applies the tokenizer's chat template to a conversation.\"\"\"\n",
    "    formatted_text = apply_chat_template(\n",
    "        sample[\"conversation\"],\n",
    "        tokenizer,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "def step3_tokenize_text(sample, tokenizer):\n",
    "    \"\"\"[STEP 3] Tokenizes the formatted text.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        padding=False\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# New Cell after the modular functions\n",
    "\n",
    "def prepare_dataset(config, tokenizer):\n",
    "    \"\"\"Orchestrates the modular data preparation pipeline.\"\"\"\n",
    "    base_df = load_base_dataset()\n",
    "    raw_dataset = Dataset.from_pandas(base_df)\n",
    "    example_manager = ExampleManager(base_df, config)\n",
    "    examples = example_manager.get_examples()\n",
    "    system_prompt = config[\"system_prompt\"]\n",
    "    if system_prompt is None:\n",
    "        raise ValueError(\"System prompt is None! Check cell 3.\")\n",
    "\n",
    "    print(\"Executing Step 1: Creating conversations...\")\n",
    "    ds_step1 = raw_dataset.map(\n",
    "        lambda x: step1_create_conversations(x, examples)\n",
    "    )\n",
    "    print(\"✅ Step 1 complete.\")\n",
    "\n",
    "    print(\"\\nExecuting Step 2: Applying chat template...\")\n",
    "    ds_step2 = ds_step1.map(\n",
    "        lambda x: step2_apply_chat_template(x, tokenizer)\n",
    "    )\n",
    "    print(\"✅ Step 2 complete.\")\n",
    "\n",
    "    print(\"\\nExecuting Step 3: Tokenizing text...\")\n",
    "    processed_dataset = ds_step2.map(\n",
    "        lambda x: step3_tokenize_text(x, tokenizer),\n",
    "        remove_columns=ds_step2.column_names\n",
    "    )\n",
    "    print(\"✅ Step 3 complete.\")\n",
    "\n",
    "    split_dataset = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    print(f\"\\nDataset prepared: {len(split_dataset['train'])} training, {len(split_dataset['test'])} evaluation samples\")\n",
    "\n",
    "    return split_dataset['train'], split_dataset['test'], examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Loads model with appropriate configuration\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Configure quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=CONFIG[\"lora_rank\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        target_modules=\"all-linear\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    # Prepare model for 4-bit training with LoRA\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Check params\n",
    "    model.print_trainable_parameters()\n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for flexible comparison (removes spaces, converts to lowercase).\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', '', str(text).lower().strip())\n",
    "\n",
    "def extract_json_from_response(response):\n",
    "    \"\"\"Extract JSON from model response, handling various formatting issues.\"\"\"\n",
    "    if not response: return {}\n",
    "    response = response.strip()\n",
    "    patterns = [r'\\{.*\\}', r'```json\\s*(\\{.*\\})\\s*```', r'```\\s*(\\{.*\\})\\s*```']\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try: return json.loads(match)\n",
    "            except json.JSONDecodeError: continue\n",
    "    try: return json.loads(response)\n",
    "    except json.JSONDecodeError: return {}\n",
    "\n",
    "def compute_metrics_for_trainer(eval_pred: EvalPrediction, tokenizer):\n",
    "    \"\"\"Computes metrics from trainer's predictions.\"\"\"\n",
    "    # With predict_with_generate=True, 'predictions' are the generated token IDs, not logits\n",
    "    predicted_ids, labels = eval_pred\n",
    "    \n",
    "    # The .argmax() call is no longer needed\n",
    "    # REMOVED: predicted_ids = predictions.argmax(axis=-1)\n",
    "    \n",
    "    # Decode the predicted IDs and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    \n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    verdict_expected, verdict_predicted = [], []\n",
    "    eln_expected, eln_predicted = [], []\n",
    "    parse_failures = 0\n",
    "\n",
    "    for pred_text, label_text in zip(decoded_preds, decoded_labels):\n",
    "        pred_json = extract_json_from_response(pred_text)\n",
    "        expected_json = extract_json_from_response(label_text)\n",
    "\n",
    "        if not pred_json:\n",
    "            parse_failures += 1\n",
    "        \n",
    "        verdict_expected.append(expected_json.get(\"verdict\", \"unknown\"))\n",
    "        verdict_predicted.append(pred_json.get(\"verdict\", \"unknown\"))\n",
    "        \n",
    "        if CONFIG[\"include_eln\"]:\n",
    "            key = \"erroneous_line\" if CONFIG[\"solution_format\"] == \"nl\" else \"erroneous_line_number\"\n",
    "            expected_line = str(expected_json.get(key, \"\"))\n",
    "            predicted_line = str(pred_json.get(key, \"\"))\n",
    "            eln_expected.append(normalize_text(expected_line))\n",
    "            eln_predicted.append(normalize_text(predicted_line))\n",
    "\n",
    "    # Calculate metrics\n",
    "    verdict_accuracy = accuracy_score(verdict_expected, verdict_predicted)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(verdict_expected, verdict_predicted, average='macro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        \"verdict_accuracy\": verdict_accuracy,\n",
    "        \"verdict_precision\": precision,\n",
    "        \"verdict_recall\": recall,\n",
    "        \"verdict_f1\": f1,\n",
    "        \"parse_failures\": parse_failures\n",
    "    }\n",
    "\n",
    "    if CONFIG[\"include_eln\"]:\n",
    "        metrics[\"eln_accuracy\"] = accuracy_score(eln_expected, eln_predicted)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Print simple metrics summary.\"\"\"\n",
    "    print(f\"\\n{stage_name} Results:\")\n",
    "    print(f\"Total samples: {metrics['total_samples']} (Parse failures: {metrics['parse_failures']})\")\n",
    "    print(f\"Verdict - Accuracy: {metrics['verdict_accuracy']:.3f}, Precision: {metrics['verdict_precision']:.3f}, Recall: {metrics['verdict_recall']:.3f}, F1: {metrics['verdict_f1']:.3f}\")\n",
    "    \n",
    "    if \"eln_accuracy\" in metrics:\n",
    "        print(f\"ELN Accuracy: {metrics['eln_accuracy']:.3f}\")\n",
    "\n",
    "# Simple evaluation function\n",
    "def evaluate_results(results, tokenizer, stage_name=\"Evaluation\"):\n",
    "    \"\"\"Evaluate results with simple metrics.\"\"\"\n",
    "    metrics = compute_metrics_for_trainer(results, tokenizer)\n",
    "    print_metrics(metrics, stage_name)\n",
    "    return metrics\n",
    "\n",
    "print(\"✅ Simplified metrics functions loaded (with text normalization)!\")\n",
    "print(\"\\nTo evaluate your baseline results, run:\")\n",
    "print(\"baseline_metrics = evaluate_results(baseline_results, CONFIG, 'Baseline')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "def setup_trainer(model, tokenizer, train_dataset, eval_dataset):\n",
    "    \"\"\"Sets up the Trainer for fine-tuning.\"\"\"\n",
    "    \n",
    "    # Data collator for language modeling (pads batches dynamically)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    \n",
    "    output_dir = Path(CONFIG[\"output_dir\"]) / \"training\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "\n",
    "        # Basic training parameters\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "\n",
    "        # Evaluation and saving strategy\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=25,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=25,\n",
    "        save_total_limit=1,\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        eval_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],  \n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eln_accuracy\", # Make sure this metric exists\n",
    "        greater_is_better=True,\n",
    "\n",
    "        # Other settings\n",
    "        logging_steps=25,\n",
    "        fp16=True,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Use a partial function to pass config and tokenizer to the metrics function\n",
    "    compute_metrics_with_config = partial(\n",
    "        compute_metrics_for_trainer, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics_with_config, # Use the new metrics function\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Trainer initialized successfully!\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955167ab",
   "metadata": {},
   "source": [
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe83dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load tokenizer\n",
    "tokenizer = load_tokenizer(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_base_dataset()\n",
    "\n",
    "for i in range(5):\n",
    "    sample = df.iloc[i].to_dict()\n",
    "    example_manager = ExampleManager(df, CONFIG)\n",
    "    examples = example_manager.get_examples()\n",
    "\n",
    "    # Step 1: Create conversation dict (not a list)\n",
    "    conversation = step1_create_conversations(sample, examples)\n",
    "\n",
    "    print(\"Conversation:\")\n",
    "    print(conversation[\"conversation\"])\n",
    "\n",
    "    # Step 2: Process conversation into single prompt\n",
    "    formatted_text = step2_apply_chat_template(conversation, tokenizer)\n",
    "    print(\"Formatted Text:\")\n",
    "    print(formatted_text[\"text\"])\n",
    "\n",
    "    # Step 3: Tokenize\n",
    "    tokenizer_output = step3_tokenize_text(formatted_text, tokenizer)\n",
    "    print(\"Tokenized Output:\")\n",
    "    print(tokenizer_output)\n",
    "\n",
    "    print(len(tokenizer_output['input_ids']), \"tokens generated.\")\n",
    "    print(len(tokenizer_output['attention_mask']), \"attention mask tokens generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and process datasets\n",
    "train_dataset, eval_dataset, examples = prepare_dataset(CONFIG, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load model\n",
    "model = load_model(CONFIG[\"model_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
