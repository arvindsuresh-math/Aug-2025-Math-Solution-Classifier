{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa10bfe",
   "metadata": {},
   "source": [
    "Cell 1: Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14c05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: gene_ter_4N_exp_eln_nl_phi4_20250731_132248\n",
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===== EXPERIMENT CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Core experiment parameters\n",
    "    \"experiment_type\": \"generative\",  # \"discriminative\" or \"generative\"\n",
    "    \"classification_type\": \"ternary\",   # \"binary\" or \"ternary\"\n",
    "    \"dataset_strategy\": \"4N\",          # \"4N\" or \"3N\" (generative only)\n",
    "    \"include_explanation\": True,      # True or False (generative only)\n",
    "    \"include_eln\": True,              # True or False (generative only)\n",
    "    \"solution_format\": \"nl\",        # \"dict\" or \"nl\" (generative only)\n",
    "    \"model_name\": \"microsoft/phi-4-mini-instruct\",  # or \"Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # Prompting configuration\n",
    "    \"system_prompt\": None,  # Will auto-generate if None, or use custom string\n",
    "    \"include_examples\": False,\n",
    "    \"num_examples\": 3,\n",
    "    \"example_strategy\": \"balanced\",  # \"balanced\", \"error_focused\", \"custom\"\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_length\": 512,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    # Infrastructure\n",
    "    \"use_lora\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Paths and tokens\n",
    "    # \"base_dataset_dir\": \"/content/drive/MyDrive/sft_datasets\",\n",
    "    \"base_dataset_dir\": \"../data/base-datasets-sanitized\",\n",
    "    \"output_base_dir\": \"/content/drive/MyDrive/sft_experiments\",\n",
    "    # \"hf_token\": \"your_huggingface_token_here\",\n",
    "    # \"wandb_project\": \"math_error_classification\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"save_to_hf\": True,\n",
    "    \"save_locally\": True,\n",
    "    \"use_wandb\": False\n",
    "}\n",
    "\n",
    "# Generate experiment ID\n",
    "import datetime\n",
    "experiment_components = [\n",
    "    CONFIG[\"experiment_type\"][:4],  # \"gene\" or \"disc\"\n",
    "    CONFIG[\"classification_type\"][:3],  # \"bin\" or \"ter\"\n",
    "    CONFIG[\"dataset_strategy\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"exp\" if CONFIG[\"include_explanation\"] else \"no_exp\",\n",
    "    \"eln\" if CONFIG[\"include_eln\"] else \"no_eln\",\n",
    "    CONFIG[\"solution_format\"] if CONFIG[\"experiment_type\"] == \"generative\" else \"\",\n",
    "    \"qwen\" if \"Qwen\" in CONFIG[\"model_name\"] else \"phi4\"\n",
    "]\n",
    "experiment_id = \"_\".join([c for c in experiment_components if c]) + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "CONFIG[\"experiment_id\"] = experiment_id\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766c573",
   "metadata": {},
   "source": [
    "Cell 2: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c7a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported and seeds set!\n"
     ]
    }
   ],
   "source": [
    "# # Core libraries\n",
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "# import datetime\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Any, Optional\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# # Data handling\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset, load_dataset\n",
    "\n",
    "# # ML libraries\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, \n",
    "#     AutoModelForCausalLM,\n",
    "#     TrainingArguments, \n",
    "#     Trainer, \n",
    "#     DataCollatorForLanguageModeling,\n",
    "#     BitsAndBytesConfig\n",
    "# )\n",
    "# from peft import (\n",
    "#     LoraConfig, \n",
    "#     get_peft_model, \n",
    "#     TaskType, \n",
    "#     prepare_model_for_kbit_training\n",
    "# )\n",
    "\n",
    "# # Logging and tracking\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # Google Drive mounting (for Colab)\n",
    "# try:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     print(\"Google Drive mounted successfully!\")\n",
    "# except ImportError:\n",
    "#     print(\"Not running in Colab - skipping Drive mount\")\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"Dependencies imported and seeds set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd140f7",
   "metadata": {},
   "source": [
    "Cell 3: System Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ef498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Classify as 'correct', 'conceptual_error', or 'computational_error'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.\n",
      "\n",
      "To customize the system prompt, run:\n",
      "CONFIG[\"system_prompt\"] = \"Your custom prompt here\"\n"
     ]
    }
   ],
   "source": [
    "def generate_system_prompt(config):\n",
    "    \"\"\"Auto-generates appropriate system prompt based on config\"\"\"\n",
    "    \n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        return \"You are a mathematics tutor. Classify the given solution.\"\n",
    "    \n",
    "    # Generative prompts\n",
    "    base_prompt = \"You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format.\"\n",
    "    \n",
    "    # Add classification instructions\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        base_prompt += \" Determine if the solution is 'correct' or 'flawed'.\"\n",
    "    else:\n",
    "        base_prompt += \" Classify as 'correct', 'conceptual_error', or 'computational_error'.\"\n",
    "    \n",
    "    # Add field instructions\n",
    "    fields = []\n",
    "    if config[\"include_eln\"]:\n",
    "        if config[\"solution_format\"] == \"dict\":\n",
    "            fields.append(\"identify the erroneous line number (e.g., 'L1', 'FA')\")\n",
    "        else:\n",
    "            fields.append(\"quote the full erroneous line text\")\n",
    "    \n",
    "    if config[\"include_explanation\"]:\n",
    "        fields.append(\"provide a brief explanation of any error\")\n",
    "    \n",
    "    if fields:\n",
    "        base_prompt += f\" Also {', and '.join(fields)}.\"\n",
    "    \n",
    "    base_prompt += \" Respond only with valid JSON.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Auto-generate system prompt if not provided\n",
    "if CONFIG[\"system_prompt\"] is None:\n",
    "    CONFIG[\"system_prompt\"] = generate_system_prompt(CONFIG)\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(CONFIG[\"system_prompt\"])\n",
    "print()\n",
    "\n",
    "# Allow manual override\n",
    "print(\"To customize the system prompt, run:\")\n",
    "print('CONFIG[\"system_prompt\"] = \"Your custom prompt here\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41b208",
   "metadata": {},
   "source": [
    "Cell 4: Example Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3522a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ExampleManager class loaded!\n"
     ]
    }
   ],
   "source": [
    "class ExampleManager:\n",
    "    def __init__(self, base_dataset, config):\n",
    "        # Convert DataFrame to list of dicts if needed\n",
    "        if hasattr(base_dataset, 'to_dict'):  # It's a DataFrame\n",
    "            self.samples = base_dataset.to_dict('records')\n",
    "        else:\n",
    "            self.samples = base_dataset  # Already a list of dicts\n",
    "            \n",
    "        self.config = config\n",
    "        self._prepare_examples_by_problem()\n",
    "    \n",
    "    def _prepare_examples_by_problem(self):\n",
    "        \"\"\"Organizes samples by problem index and error type for problem-based sampling\"\"\"\n",
    "        self.problems_by_type = {\n",
    "            \"correct\": {},\n",
    "            \"conceptual_error\": {},\n",
    "            \"computational_error\": {}\n",
    "        }\n",
    "        \n",
    "        # Group samples by problem index and error type\n",
    "        for sample in self.samples:\n",
    "            problem_index = sample[\"index\"]\n",
    "            error_type = sample[\"error_type\"]\n",
    "            \n",
    "            if problem_index not in self.problems_by_type[error_type]:\n",
    "                self.problems_by_type[error_type][problem_index] = []\n",
    "            self.problems_by_type[error_type][problem_index].append(sample)\n",
    "        \n",
    "        # Convert to lists of problem indices for easier sampling\n",
    "        self.problem_indices_by_type = {\n",
    "            \"correct\": list(self.problems_by_type[\"correct\"].keys()),\n",
    "            \"conceptual_error\": list(self.problems_by_type[\"conceptual_error\"].keys()),\n",
    "            \"computational_error\": list(self.problems_by_type[\"computational_error\"].keys())\n",
    "        }\n",
    "        \n",
    "        print(f\"Problems by type: {[(k, len(v)) for k, v in self.problem_indices_by_type.items()]}\")\n",
    "    \n",
    "    def get_examples(self):\n",
    "        \"\"\"Returns appropriate few-shot examples based on distinct problems\"\"\"\n",
    "        if not self.config[\"include_examples\"]:\n",
    "            return []\n",
    "        \n",
    "        strategy = self.config[\"example_strategy\"]\n",
    "        num_problems = self.config[\"num_examples\"]  # Now refers to number of distinct problems\n",
    "        \n",
    "        if strategy == \"balanced\":\n",
    "            return self._get_balanced_examples(num_problems)\n",
    "        elif strategy == \"error_focused\":\n",
    "            return self._get_error_focused_examples(num_problems)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def _get_samples_for_problem(self, problem_index, error_types_needed):\n",
    "        \"\"\"Get all required samples for a specific problem\"\"\"\n",
    "        samples = []\n",
    "        for error_type in error_types_needed:\n",
    "            if problem_index in self.problems_by_type[error_type]:\n",
    "                # Take the first sample of this error type for this problem\n",
    "                samples.append(self.problems_by_type[error_type][problem_index][0])\n",
    "        return samples\n",
    "    \n",
    "    def _get_balanced_examples(self, n):\n",
    "        \"\"\"Gets balanced representation across error types, sampling by distinct problems\"\"\"\n",
    "        examples = []\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        classification_type = self.config[\"classification_type\"]\n",
    "        \n",
    "        if classification_type == \"binary\":\n",
    "            # For binary: each problem contributes correct + flawed versions\n",
    "            if dataset_strategy == \"3N\":\n",
    "                # 3N dataset: each problem has all 3 types, choose correct + one error type\n",
    "                available_problems = set(self.problem_indices_by_type[\"correct\"]) & \\\n",
    "                                set(self.problem_indices_by_type[\"conceptual_error\"]) & \\\n",
    "                                set(self.problem_indices_by_type[\"computational_error\"])\n",
    "                available_problems = list(available_problems)\n",
    "                \n",
    "                selected_problems = random.sample(available_problems, min(n, len(available_problems)))\n",
    "                \n",
    "                for problem_index in selected_problems:\n",
    "                    # Add correct version\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\"]))\n",
    "                    # Add one error type (randomly choose between conceptual and computational)\n",
    "                    error_type = random.choice([\"conceptual_error\", \"computational_error\"])\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [error_type]))\n",
    "                    \n",
    "            elif dataset_strategy == \"4N\":\n",
    "                # 4N dataset: each problem has correct + one specific error type\n",
    "                # Sample from problems that have both correct and error versions\n",
    "                available_problems_conceptual = set(self.problem_indices_by_type[\"correct\"]) & \\\n",
    "                                            set(self.problem_indices_by_type[\"conceptual_error\"])\n",
    "                available_problems_computational = set(self.problem_indices_by_type[\"correct\"]) & \\\n",
    "                                                set(self.problem_indices_by_type[\"computational_error\"])\n",
    "                \n",
    "                # Balance between conceptual and computational problems\n",
    "                n_conceptual = n // 2\n",
    "                n_computational = n - n_conceptual\n",
    "                \n",
    "                selected_conceptual = random.sample(\n",
    "                    list(available_problems_conceptual), \n",
    "                    min(n_conceptual, len(available_problems_conceptual))\n",
    "                )\n",
    "                selected_computational = random.sample(\n",
    "                    list(available_problems_computational), \n",
    "                    min(n_computational, len(available_problems_computational))\n",
    "                )\n",
    "                \n",
    "                # Add samples for conceptual problems\n",
    "                for problem_index in selected_conceptual:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\", \"conceptual_error\"]))\n",
    "                \n",
    "                # Add samples for computational problems  \n",
    "                for problem_index in selected_computational:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\", \"computational_error\"]))\n",
    "                    \n",
    "        else:  # ternary classification\n",
    "            if dataset_strategy == \"3N\":\n",
    "                # 3N dataset: each problem should have all 3 types\n",
    "                available_problems = set(self.problem_indices_by_type[\"correct\"]) & \\\n",
    "                                set(self.problem_indices_by_type[\"conceptual_error\"]) & \\\n",
    "                                set(self.problem_indices_by_type[\"computational_error\"])\n",
    "                available_problems = list(available_problems)\n",
    "                \n",
    "                selected_problems = random.sample(available_problems, min(n, len(available_problems)))\n",
    "                \n",
    "                for problem_index in selected_problems:\n",
    "                    # Add all 3 versions of each selected problem\n",
    "                    examples.extend(self._get_samples_for_problem(\n",
    "                        problem_index, \n",
    "                        [\"correct\", \"conceptual_error\", \"computational_error\"]\n",
    "                    ))\n",
    "                    \n",
    "            elif dataset_strategy == \"4N\":\n",
    "                # Make n even to ensure balanced sampling\n",
    "                n_adjusted = n + (n % 2)\n",
    "                \n",
    "                # Get problems that have conceptual errors\n",
    "                conceptual_problems = list(set(self.problem_indices_by_type[\"correct\"]) & \n",
    "                                        set(self.problem_indices_by_type[\"conceptual_error\"]))\n",
    "                # Get problems that have computational errors  \n",
    "                computational_problems = list(set(self.problem_indices_by_type[\"correct\"]) & \n",
    "                                            set(self.problem_indices_by_type[\"computational_error\"]))\n",
    "                \n",
    "                # Use exactly n/2 of each type\n",
    "                n_per_type = n_adjusted // 2\n",
    "                \n",
    "                selected_conceptual = random.sample(\n",
    "                    conceptual_problems, \n",
    "                    min(n_per_type, len(conceptual_problems))\n",
    "                )\n",
    "                selected_computational = random.sample(\n",
    "                    computational_problems, \n",
    "                    min(n_per_type, len(computational_problems))\n",
    "                )\n",
    "                \n",
    "                # Add samples for conceptual problems (correct + conceptual_error)\n",
    "                for problem_index in selected_conceptual:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\", \"conceptual_error\"]))\n",
    "                    \n",
    "                # Add samples for computational problems (correct + computational_error)\n",
    "                for problem_index in selected_computational:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\", \"computational_error\"]))\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def _get_error_focused_examples(self, n):\n",
    "        \"\"\"Gets examples focused on error types, sampling by distinct problems\"\"\"\n",
    "        examples = []\n",
    "        dataset_strategy = self.config[\"dataset_strategy\"]\n",
    "        \n",
    "        if dataset_strategy == \"3N\":\n",
    "            # For 3N, prioritize problems that have errors, include fewer correct-only examples\n",
    "            available_problems = set(self.problem_indices_by_type[\"correct\"]) & \\\n",
    "                               set(self.problem_indices_by_type[\"conceptual_error\"]) & \\\n",
    "                               set(self.problem_indices_by_type[\"computational_error\"])\n",
    "            available_problems = list(available_problems)\n",
    "            \n",
    "            selected_problems = random.sample(available_problems, min(n, len(available_problems)))\n",
    "            \n",
    "            for problem_index in selected_problems:\n",
    "                # For error-focused, include both error types but only sometimes the correct version\n",
    "                examples.extend(self._get_samples_for_problem(\n",
    "                    problem_index, \n",
    "                    [\"conceptual_error\", \"computational_error\"]\n",
    "                ))\n",
    "                \n",
    "                # Add correct version for only some problems (1/3 chance)\n",
    "                if random.random() < 0.33:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"correct\"]))\n",
    "                    \n",
    "        elif dataset_strategy == \"4N\":\n",
    "            # For 4N, prioritize error problems over correct-only problems\n",
    "            error_problems = list(\n",
    "                (set(self.problem_indices_by_type[\"conceptual_error\"]) | \n",
    "                 set(self.problem_indices_by_type[\"computational_error\"])) &\n",
    "                set(self.problem_indices_by_type[\"correct\"])\n",
    "            )\n",
    "            \n",
    "            selected_problems = random.sample(error_problems, min(n, len(error_problems)))\n",
    "            \n",
    "            for problem_index in selected_problems:\n",
    "                # Add correct version\n",
    "                examples.extend(self._get_samples_for_problem(problem_index, [\"correct\"]))\n",
    "                \n",
    "                # Add the error version that exists for this problem\n",
    "                if problem_index in self.problem_indices_by_type[\"conceptual_error\"]:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"conceptual_error\"]))\n",
    "                elif problem_index in self.problem_indices_by_type[\"computational_error\"]:\n",
    "                    examples.extend(self._get_samples_for_problem(problem_index, [\"computational_error\"]))\n",
    "        \n",
    "        return examples\n",
    "\n",
    "print(\"Updated ExampleManager class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246c873",
   "metadata": {},
   "source": [
    "Cell 5: Dataset Loading and Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc8d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated formatting functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_base_dataset(config):\n",
    "    \"\"\"Loads the appropriate base dataset\"\"\"\n",
    "    dataset_strategy = config[\"dataset_strategy\"]\n",
    "    base_dir = Path(config[\"base_dataset_dir\"])\n",
    "    \n",
    "    dataset_file = base_dir / f\"base_{dataset_strategy}_dataset_sanitized.csv\"\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        raise FileNotFoundError(f\"Base dataset not found: {dataset_file}\")\n",
    "\n",
    "    data = pd.read_csv(dataset_file)\n",
    "\n",
    "    print(f\"Loaded base {dataset_strategy} dataset with {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "def format_solution(sample, config):\n",
    "    \"\"\"Formats solution according to config - updated for CSV structure\"\"\"\n",
    "    # Use wrong_answer for the solution (this contains the solution steps)\n",
    "    solution_text = sample.get('wrong_answer', sample.get('correct_answer', ''))\n",
    "    \n",
    "    if config[\"solution_format\"] == \"dict\":\n",
    "        # Split solution into lines and format as dict\n",
    "        lines = solution_text.strip().split('\\n')\n",
    "        solution = {}\n",
    "        for i, line in enumerate(lines[:-1]):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                solution[f\"L{i+1}\"] = line.strip()\n",
    "        if lines and lines[-1].strip():\n",
    "            solution[\"FA\"] = lines[-1].strip()\n",
    "        return json.dumps(solution, indent=2)\n",
    "    else:\n",
    "        return solution_text.strip()\n",
    "\n",
    "def format_expected_output(sample, config):\n",
    "    \"\"\"Creates the expected JSON output for a sample - updated for CSV structure\"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    # Verdict\n",
    "    if config[\"classification_type\"] == \"binary\":\n",
    "        output[\"verdict\"] = \"correct\" if sample[\"error_type\"] == \"correct\" else \"flawed\"\n",
    "    else:\n",
    "        output[\"verdict\"] = sample[\"error_type\"]\n",
    "    \n",
    "    # ELN (Erroneous Line Number)\n",
    "    if config[\"include_eln\"]:\n",
    "        if sample[\"error_type\"] != \"correct\":\n",
    "            if config[\"solution_format\"] == \"dict\":\n",
    "                output[\"erroneous_line_number\"] = sample.get(\"erroneous_line_number\", None)\n",
    "            else:\n",
    "                # For natural language format, try to extract the actual erroneous line text\n",
    "                eln = sample.get(\"erroneous_line_number\")\n",
    "                if eln and pd.notna(eln):\n",
    "                    # Extract line number (e.g., \"L3\" -> 3)\n",
    "                    try:\n",
    "                        if eln.startswith('L'):\n",
    "                            line_num = int(eln[1:]) - 1\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            if 0 <= line_num < len(solution_lines):\n",
    "                                output[\"erroneous_line\"] = solution_lines[line_num].strip()\n",
    "                            else:\n",
    "                                output[\"erroneous_line\"] = eln  # Fallback to the ELN itself\n",
    "                        elif eln == 'FA':\n",
    "                            solution_lines = sample.get('wrong_answer', '').strip().split('\\n')\n",
    "                            output[\"erroneous_line\"] = solution_lines[-1].strip() if solution_lines else None\n",
    "                        else:\n",
    "                            output[\"erroneous_line\"] = eln\n",
    "                    except:\n",
    "                        output[\"erroneous_line\"] = eln\n",
    "                else:\n",
    "                    output[\"erroneous_line\"] = None\n",
    "        else:\n",
    "            key = \"erroneous_line_number\" if config[\"solution_format\"] == \"dict\" else \"erroneous_line\"\n",
    "            output[key] = None\n",
    "    \n",
    "    # Explanation\n",
    "    if config[\"include_explanation\"]:\n",
    "        explanation = sample.get(\"explanation\")\n",
    "        output[\"explanation\"] = explanation if pd.notna(explanation) and sample[\"error_type\"] != \"correct\" else None\n",
    "    \n",
    "    return json.dumps(output)\n",
    "\n",
    "def format_user_message(sample, config):\n",
    "    \"\"\"Format a sample into a user message.\"\"\"\n",
    "    return f\"### Question:\\n{sample['question']}\\n\\n### Answer:\\n{format_solution(sample, config)}\"\n",
    "\n",
    "def create_sample_messages(sample, examples, config):\n",
    "    \"\"\"Create complete message list for a sample.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # System message\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": config[\"system_prompt\"]\n",
    "    })\n",
    "    \n",
    "    # Few-shot examples\n",
    "    if config[\"include_examples\"]:\n",
    "        for example in examples:\n",
    "            user_content = format_user_message(example, config)\n",
    "            assistant_content = format_expected_output(example, config)\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "    \n",
    "    # Actual sample\n",
    "    user_content = format_user_message(sample, config)\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "print(\"Updated formatting functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455adbd",
   "metadata": {},
   "source": [
    "Cell 6: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f0a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(config):\n",
    "    \"\"\"Loads and formats complete dataset for training/evaluation\"\"\"\n",
    "    \n",
    "    # Load base dataset\n",
    "    base_samples = load_base_dataset(config)\n",
    "    \n",
    "    # Initialize example manager - convert DataFrame to list of dicts\n",
    "    example_manager = ExampleManager(base_samples, config)\n",
    "    examples = example_manager.get_examples()\n",
    "    \n",
    "    print(f\"Using {len(examples)} few-shot examples\")\n",
    "    \n",
    "    # Format all samples\n",
    "    formatted_data = []\n",
    "    \n",
    "    # Iterate over DataFrame rows\n",
    "    for idx, sample in base_samples.iterrows():\n",
    "        # Convert pandas Series to dict for easier access\n",
    "        sample_dict = sample.to_dict()\n",
    "        \n",
    "        # Use modular function to create messages\n",
    "        messages = create_sample_messages(sample_dict, examples, config)\n",
    "        \n",
    "        # Expected output\n",
    "        expected_output = format_expected_output(sample_dict, config)\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"id\": sample_dict.get(\"id\", f\"sample_{len(formatted_data)}\"),\n",
    "            \"messages\": messages,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"metadata\": {\n",
    "                \"error_type\": sample_dict[\"error_type\"],\n",
    "                \"tier\": sample_dict.get(\"tier\", \"unknown\"),\n",
    "                \"source\": sample_dict.get(\"source\", \"unknown\")\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Split into train/eval\n",
    "    split_point = int(0.8 * len(formatted_data))\n",
    "    train_data = formatted_data[:split_point]\n",
    "    eval_data = formatted_data[split_point:]\n",
    "    \n",
    "    print(f\"Dataset prepared: {len(train_data)} training, {len(eval_data)} evaluation samples\")\n",
    "    \n",
    "    return train_data, eval_data, examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3af4e",
   "metadata": {},
   "source": [
    "Cell 7: Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4817613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer with proper configuration\"\"\"\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"✓ Tokenizer loaded successfully!\")\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(model_name, config):\n",
    "    \"\"\"Loads model with appropriate configuration\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Configure quantization if using LoRA\n",
    "    bnb_config = None\n",
    "    if config[\"use_lora\"]:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "    \n",
    "    # Load model based on experiment type\n",
    "    if config[\"experiment_type\"] == \"discriminative\":\n",
    "        # For discriminative, we need a classification model\n",
    "        num_labels = 2 if config[\"classification_type\"] == \"binary\" else 3\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        # For generative, use causal LM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    \n",
    "    # Apply LoRA if configured\n",
    "    if config[\"use_lora\"]:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Configure LoRA based on experiment type\n",
    "        if config[\"experiment_type\"] == \"discriminative\":\n",
    "            task_type = TaskType.SEQ_CLS\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "        else:\n",
    "            task_type = TaskType.CAUSAL_LM\n",
    "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            r=config[\"lora_rank\"],\n",
    "            lora_alpha=config[\"lora_alpha\"],\n",
    "            lora_dropout=config[\"lora_dropout\"],\n",
    "            target_modules=target_modules,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def apply_chat_template(messages, tokenizer, add_generation_prompt=False, tokenize=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies chat template to messages with consistent interface\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' keys\n",
    "        tokenizer: The tokenizer to use for formatting\n",
    "        add_generation_prompt: Whether to add generation prompt (for inference)\n",
    "        tokenize: Whether to return tokens (True) or text (False)\n",
    "        **kwargs: Additional arguments for tokenizer (like return_tensors, max_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        If tokenize=True: tokenizer output dict with input_ids, attention_mask, etc.\n",
    "        If tokenize=False: formatted text string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply chat template to get formatted text\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=add_generation_prompt\n",
    "    )\n",
    "    \n",
    "    # Return text if not tokenizing\n",
    "    if not tokenize:\n",
    "        return formatted_text\n",
    "    \n",
    "    # Tokenize and return tensor format\n",
    "    return tokenizer(formatted_text, **kwargs)\n",
    "\n",
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"\n",
    "    Convenience function that loads both model and tokenizer\n",
    "    Uses the modular functions above\n",
    "    \"\"\"\n",
    "    model_name = config[\"model_name\"]\n",
    "    \n",
    "    # Load components separately\n",
    "    tokenizer = load_tokenizer(model_name)\n",
    "    model = load_model(model_name, config)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cfe5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "COMPREHENSIVE TOKEN COUNT AND CHARACTER ANALYSIS\n",
      "====================================================================================================\n",
      "Loading sample dataset for analysis...\n",
      "Loaded base 4N dataset with 7524 samples\n",
      "✓ Loaded 50 samples for analysis\n",
      "\n",
      "Loading tokenizer for microsoft/phi-4-mini-instruct...\n",
      "Loading tokenizer: microsoft/phi-4-mini-instruct\n",
      "✓ Tokenizer loaded successfully!\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 1/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 544-807 (avg: 681)\n",
      "  Tokens: 130-223 (avg: 167)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-72 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 168, avg expected tokens: 57\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 167, avg expected tokens: 53\n",
      "  correct: 6 samples, avg prompt tokens: 166, avg expected tokens: 21\n",
      "\n",
      "SAMPLE FORMATTED PROMPT (first 500 chars):\n",
      "<|system|>You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Determine if the solution is 'correct' or 'flawed'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.<|end|><|user|>### Question:\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "### Answer:\n",
      "Weng earns 12/60 = $0.2 per minute.\n",
      "Working 50 minutes, she earned...\n",
      "\n",
      "SAMPLE EXPECTED OUTPUT:\n",
      "{\"verdict\": \"flawed\", \"erroneous_line\": \"Working 50 minutes, she earned 0.2 x 50 = $11.\", \"explanation\": \"the last multiplication is wrong.\"}\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 2/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1688-1951 (avg: 1825)\n",
      "  Tokens: 452-545 (avg: 489)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-72 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 490, avg expected tokens: 57\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 489, avg expected tokens: 53\n",
      "  correct: 6 samples, avg prompt tokens: 488, avg expected tokens: 21\n",
      "\n",
      "SAMPLE FORMATTED PROMPT (first 500 chars):\n",
      "<|system|>You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Determine if the solution is 'correct' or 'flawed'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.<|end|><|user|>### Question:\n",
      "Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many piec...\n",
      "\n",
      "SAMPLE EXPECTED OUTPUT:\n",
      "{\"verdict\": \"flawed\", \"erroneous_line\": \"Working 50 minutes, she earned 0.2 x 50 = $11.\", \"explanation\": \"the last multiplication is wrong.\"}\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 3/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2636-2899 (avg: 2773)\n",
      "  Tokens: 724-817 (avg: 761)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-72 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 762, avg expected tokens: 57\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 761, avg expected tokens: 53\n",
      "  correct: 6 samples, avg prompt tokens: 760, avg expected tokens: 21\n",
      "\n",
      "SAMPLE FORMATTED PROMPT (first 500 chars):\n",
      "<|system|>You are a mathematics tutor. Analyze the given solution and provide your assessment in JSON format. Determine if the solution is 'correct' or 'flawed'. Also quote the full erroneous line text, and provide a brief explanation of any error. Respond only with valid JSON.<|end|><|user|>### Question:\n",
      "Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all on his farm?\n",
      "\n",
      "### Answer:\n",
      "Half of the number of Ra...\n",
      "\n",
      "SAMPLE EXPECTED OUTPUT:\n",
      "{\"verdict\": \"flawed\", \"erroneous_line\": \"Working 50 minutes, she earned 0.2 x 50 = $11.\", \"explanation\": \"the last multiplication is wrong.\"}\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 4/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 561-824 (avg: 698)\n",
      "  Tokens: 134-227 (avg: 171)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-73 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 172, avg expected tokens: 58\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 171, avg expected tokens: 54\n",
      "  correct: 6 samples, avg prompt tokens: 170, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 5/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 3\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 3\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1871-2134 (avg: 2008)\n",
      "  Tokens: 500-593 (avg: 537)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-73 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 538, avg expected tokens: 58\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 537, avg expected tokens: 54\n",
      "  correct: 6 samples, avg prompt tokens: 536, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 6/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 6\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 6\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4537-4800 (avg: 4674)\n",
      "  Tokens: 1240-1333 (avg: 1277)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-73 (avg: 45)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 1278, avg expected tokens: 58\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 1277, avg expected tokens: 54\n",
      "  correct: 6 samples, avg prompt tokens: 1276, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 7/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 544-1075 (avg: 780)\n",
      "  Tokens: 130-257 (avg: 190)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-72 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 182, avg expected tokens: 54\n",
      "  correct: 10 samples, avg prompt tokens: 190, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 221, avg expected tokens: 56\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 8/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2806-3337 (avg: 3042)\n",
      "  Tokens: 675-802 (avg: 735)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-72 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 727, avg expected tokens: 54\n",
      "  correct: 10 samples, avg prompt tokens: 735, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 766, avg expected tokens: 56\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 9/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2962-3493 (avg: 3198)\n",
      "  Tokens: 796-923 (avg: 856)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-72 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 848, avg expected tokens: 54\n",
      "  correct: 10 samples, avg prompt tokens: 856, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 887, avg expected tokens: 56\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 10/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 561-1092 (avg: 797)\n",
      "  Tokens: 134-261 (avg: 194)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-73 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 186, avg expected tokens: 56\n",
      "  correct: 10 samples, avg prompt tokens: 194, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 225, avg expected tokens: 57\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 11/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3171-3702 (avg: 3407)\n",
      "  Tokens: 877-1004 (avg: 937)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-73 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 929, avg expected tokens: 56\n",
      "  correct: 10 samples, avg prompt tokens: 937, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 968, avg expected tokens: 57\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 12/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2970-3501 (avg: 3206)\n",
      "  Tokens: 824-951 (avg: 884)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-73 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 876, avg expected tokens: 56\n",
      "  correct: 10 samples, avg prompt tokens: 884, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 915, avg expected tokens: 57\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 13/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 599-906 (avg: 750)\n",
      "  Tokens: 159-276 (avg: 204)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-60 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 204, avg expected tokens: 44\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 204, avg expected tokens: 42\n",
      "  correct: 6 samples, avg prompt tokens: 203, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 14/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1691-1998 (avg: 1842)\n",
      "  Tokens: 486-603 (avg: 531)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-60 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 531, avg expected tokens: 44\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 531, avg expected tokens: 42\n",
      "  correct: 6 samples, avg prompt tokens: 530, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 15/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3405-3712 (avg: 3556)\n",
      "  Tokens: 936-1053 (avg: 981)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-60 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 981, avg expected tokens: 44\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 981, avg expected tokens: 42\n",
      "  correct: 6 samples, avg prompt tokens: 980, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 16/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 616-923 (avg: 767)\n",
      "  Tokens: 163-280 (avg: 208)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-61 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 208, avg expected tokens: 45\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 208, avg expected tokens: 43\n",
      "  correct: 6 samples, avg prompt tokens: 207, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 17/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 3\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 3\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2565-2872 (avg: 2716)\n",
      "  Tokens: 716-833 (avg: 761)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-61 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 761, avg expected tokens: 45\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 761, avg expected tokens: 43\n",
      "  correct: 6 samples, avg prompt tokens: 760, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 18/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 6\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 6\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4511-4818 (avg: 4662)\n",
      "  Tokens: 1409-1526 (avg: 1454)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-61 (avg: 37)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 1454, avg expected tokens: 45\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 1454, avg expected tokens: 43\n",
      "  correct: 6 samples, avg prompt tokens: 1453, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 19/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 599-1163 (avg: 851)\n",
      "  Tokens: 159-304 (avg: 228)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-59 (avg: 31)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 220, avg expected tokens: 41\n",
      "  correct: 10 samples, avg prompt tokens: 228, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 262, avg expected tokens: 40\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 20/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2043-2607 (avg: 2295)\n",
      "  Tokens: 570-715 (avg: 639)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-59 (avg: 31)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 630, avg expected tokens: 41\n",
      "  correct: 10 samples, avg prompt tokens: 639, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 673, avg expected tokens: 40\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 21/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3370-3934 (avg: 3622)\n",
      "  Tokens: 1004-1149 (avg: 1073)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-59 (avg: 31)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1064, avg expected tokens: 41\n",
      "  correct: 10 samples, avg prompt tokens: 1073, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 1107, avg expected tokens: 40\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 22/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 616-1180 (avg: 868)\n",
      "  Tokens: 163-308 (avg: 232)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 32)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 224, avg expected tokens: 42\n",
      "  correct: 10 samples, avg prompt tokens: 232, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 266, avg expected tokens: 42\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 23/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3435-3999 (avg: 3687)\n",
      "  Tokens: 991-1136 (avg: 1060)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 32)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1052, avg expected tokens: 42\n",
      "  correct: 10 samples, avg prompt tokens: 1060, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 1094, avg expected tokens: 42\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 24/48\n",
      "Model: phi-4-mini-instruct\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3017-3581 (avg: 3269)\n",
      "  Tokens: 865-1010 (avg: 934)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 32)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 926, avg expected tokens: 42\n",
      "  correct: 10 samples, avg prompt tokens: 934, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 968, avg expected tokens: 42\n",
      "Loading tokenizer for Qwen/Qwen3-4B...\n",
      "Loading tokenizer: Qwen/Qwen3-4B\n",
      "✓ Tokenizer loaded successfully!\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 25/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 579-842 (avg: 716)\n",
      "  Tokens: 148-271 (avg: 192)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 193, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 192, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 191, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 26/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1980-2243 (avg: 2117)\n",
      "  Tokens: 532-655 (avg: 576)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 577, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 576, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 575, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 27/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3097-3360 (avg: 3234)\n",
      "  Tokens: 1018-1141 (avg: 1062)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 144)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 1063, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 1062, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 1061, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 28/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 596-859 (avg: 733)\n",
      "  Tokens: 151-274 (avg: 195)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 196, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 195, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 194, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 29/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 3\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 3\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2559-2822 (avg: 2696)\n",
      "  Tokens: 725-848 (avg: 769)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 770, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 769, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 768, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 30/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 6\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 6\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3996-4259 (avg: 4133)\n",
      "  Tokens: 1158-1281 (avg: 1202)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 152)\n",
      "  Tokens: 21-78 (avg: 49)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 1203, avg expected tokens: 64\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 1202, avg expected tokens: 57\n",
      "  correct: 6 samples, avg prompt tokens: 1201, avg expected tokens: 21\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 31/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 579-1110 (avg: 815)\n",
      "  Tokens: 148-287 (avg: 214)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 204, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 214, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 254, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 32/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1978-2509 (avg: 2214)\n",
      "  Tokens: 553-692 (avg: 619)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 610, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 619, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 658, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 33/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4025-4556 (avg: 4261)\n",
      "  Tokens: 1168-1307 (avg: 1234)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-221 (avg: 117)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1224, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 1234, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 1274, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 34/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 596-1127 (avg: 832)\n",
      "  Tokens: 151-290 (avg: 217)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 208, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 217, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 256, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 35/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4810-5341 (avg: 5046)\n",
      "  Tokens: 1416-1555 (avg: 1482)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1472, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 1482, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 1522, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 36/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: nl\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3587-4118 (avg: 3823)\n",
      "  Tokens: 1079-1218 (avg: 1145)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 67-231 (avg: 123)\n",
      "  Tokens: 21-78 (avg: 41)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1136, avg expected tokens: 59\n",
      "  correct: 10 samples, avg prompt tokens: 1145, avg expected tokens: 21\n",
      "  computational_error: 2 samples, avg prompt tokens: 1184, avg expected tokens: 66\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 37/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 634-941 (avg: 785)\n",
      "  Tokens: 177-324 (avg: 229)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 230, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 228, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 228, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 38/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 1547-1854 (avg: 1698)\n",
      "  Tokens: 475-622 (avg: 527)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 528, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 526, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 526, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 39/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2842-3149 (avg: 2993)\n",
      "  Tokens: 925-1072 (avg: 977)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 122)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 978, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 976, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 976, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 40/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 651-958 (avg: 802)\n",
      "  Tokens: 180-327 (avg: 232)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 233, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 231, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 231, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 41/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 3\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 3\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 2803-3110 (avg: 2954)\n",
      "  Tokens: 795-942 (avg: 847)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 848, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 846, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 846, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 42/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 3N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Loaded base 3N dataset with 5319 samples\n",
      "Problems by type: [('correct', 16), ('conceptual_error', 17), ('computational_error', 17)]\n",
      "Examples generated: 6\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 6\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 5325-5632 (avg: 5476)\n",
      "  Tokens: 1773-1920 (avg: 1825)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 130)\n",
      "  Tokens: 22-62 (avg: 38)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  computational_error: 7 samples, avg prompt tokens: 1826, avg expected tokens: 46\n",
      "  conceptual_error: 7 samples, avg prompt tokens: 1824, avg expected tokens: 44\n",
      "  correct: 6 samples, avg prompt tokens: 1824, avg expected tokens: 22\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 43/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 634-1198 (avg: 886)\n",
      "  Tokens: 177-332 (avg: 252)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 242, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 252, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 294, avg expected tokens: 45\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 44/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 2\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 2\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3002-3566 (avg: 3254)\n",
      "  Tokens: 868-1023 (avg: 943)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 933, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 943, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 986, avg expected tokens: 45\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 45/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: binary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4316-4880 (avg: 4568)\n",
      "  Tokens: 1278-1433 (avg: 1353)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-183 (avg: 100)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1343, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 1353, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 1396, avg expected tokens: 45\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 46/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 0\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 0\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 0\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 651-1215 (avg: 903)\n",
      "  Tokens: 180-335 (avg: 255)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 245, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 255, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 298, avg expected tokens: 45\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 47/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 1\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 4334-4898 (avg: 4586)\n",
      "  Tokens: 1264-1419 (avg: 1339)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1329, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 1339, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 1382, avg expected tokens: 45\n",
      "\n",
      "================================================================================\n",
      "COMBINATION 48/48\n",
      "Model: Qwen3-4B\n",
      "Solution Format: dict\n",
      "Dataset Strategy: 4N\n",
      "Classification Type: ternary\n",
      "Num Examples: 2\n",
      "================================================================================\n",
      "Problems by type: [('correct', 25), ('conceptual_error', 19), ('computational_error', 6)]\n",
      "Examples generated: 4\n",
      "Analyzing samples...\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Samples analyzed: 20\n",
      "Examples in prompts: 4\n",
      "\n",
      "PROMPT STATISTICS:\n",
      "  Characters: 3599-4163 (avg: 3851)\n",
      "  Tokens: 1115-1270 (avg: 1190)\n",
      "\n",
      "EXPECTED OUTPUT STATISTICS:\n",
      "  Characters: 74-193 (avg: 106)\n",
      "  Tokens: 22-60 (avg: 33)\n",
      "\n",
      "BY ERROR TYPE:\n",
      "  conceptual_error: 8 samples, avg prompt tokens: 1180, avg expected tokens: 43\n",
      "  correct: 10 samples, avg prompt tokens: 1190, avg expected tokens: 22\n",
      "  computational_error: 2 samples, avg prompt tokens: 1232, avg expected tokens: 45\n",
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE COMPARISON TABLE\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "KEY INSIGHTS\n",
      "====================================================================================================\n",
      "Prompt Token Range: 166 - 1824 tokens\n",
      "Expected Output Token Range: 31 - 48 tokens\n",
      "Maximum Single Prompt: 1920 tokens\n",
      "Maximum Single Expected Output: 78 tokens\n",
      "\n",
      "Impact of Examples on Prompt Length:\n",
      "  No examples: 211 tokens\n",
      "  1 example: 799 tokens\n",
      "  2 examples: 1138 tokens\n",
      "  Token increase per example: ~463 tokens\n",
      "\n",
      "Model Differences (average across all configs):\n",
      "  phi-4-mini-instruct: 645 avg prompt tokens\n",
      "  Qwen3-4B: 786 avg prompt tokens\n",
      "\n",
      "Format Differences (average across all configs):\n",
      "  nl: 671 prompt tokens, 42 expected tokens\n",
      "  dict: 761 prompt tokens, 34 expected tokens\n",
      "\n",
      "✅ ANALYSIS COMPLETED!\n",
      "Analyzed 48 configurations successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Format</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Examples</th>\n",
       "      <th>Actual_Examples</th>\n",
       "      <th>Avg_Prompt_Chars</th>\n",
       "      <th>Avg_Prompt_Tokens</th>\n",
       "      <th>Avg_Expected_Chars</th>\n",
       "      <th>Avg_Expected_Tokens</th>\n",
       "      <th>Max_Prompt_Tokens</th>\n",
       "      <th>Max_Expected_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785</td>\n",
       "      <td>228</td>\n",
       "      <td>121</td>\n",
       "      <td>38</td>\n",
       "      <td>324</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>802</td>\n",
       "      <td>231</td>\n",
       "      <td>129</td>\n",
       "      <td>38</td>\n",
       "      <td>327</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "      <td>252</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>332</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>255</td>\n",
       "      <td>105</td>\n",
       "      <td>32</td>\n",
       "      <td>335</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>716</td>\n",
       "      <td>191</td>\n",
       "      <td>144</td>\n",
       "      <td>48</td>\n",
       "      <td>271</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>733</td>\n",
       "      <td>194</td>\n",
       "      <td>152</td>\n",
       "      <td>48</td>\n",
       "      <td>274</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>814</td>\n",
       "      <td>214</td>\n",
       "      <td>117</td>\n",
       "      <td>40</td>\n",
       "      <td>287</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>831</td>\n",
       "      <td>217</td>\n",
       "      <td>122</td>\n",
       "      <td>40</td>\n",
       "      <td>290</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1698</td>\n",
       "      <td>526</td>\n",
       "      <td>121</td>\n",
       "      <td>38</td>\n",
       "      <td>622</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2954</td>\n",
       "      <td>846</td>\n",
       "      <td>129</td>\n",
       "      <td>38</td>\n",
       "      <td>942</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3254</td>\n",
       "      <td>943</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>1023</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4586</td>\n",
       "      <td>1339</td>\n",
       "      <td>105</td>\n",
       "      <td>32</td>\n",
       "      <td>1419</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2117</td>\n",
       "      <td>575</td>\n",
       "      <td>144</td>\n",
       "      <td>48</td>\n",
       "      <td>655</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2696</td>\n",
       "      <td>768</td>\n",
       "      <td>152</td>\n",
       "      <td>48</td>\n",
       "      <td>848</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2213</td>\n",
       "      <td>619</td>\n",
       "      <td>117</td>\n",
       "      <td>40</td>\n",
       "      <td>692</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5045</td>\n",
       "      <td>1482</td>\n",
       "      <td>122</td>\n",
       "      <td>40</td>\n",
       "      <td>1555</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2993</td>\n",
       "      <td>976</td>\n",
       "      <td>121</td>\n",
       "      <td>38</td>\n",
       "      <td>1072</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5476</td>\n",
       "      <td>1824</td>\n",
       "      <td>129</td>\n",
       "      <td>38</td>\n",
       "      <td>1920</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4568</td>\n",
       "      <td>1353</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>1433</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3851</td>\n",
       "      <td>1190</td>\n",
       "      <td>105</td>\n",
       "      <td>32</td>\n",
       "      <td>1270</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3234</td>\n",
       "      <td>1061</td>\n",
       "      <td>144</td>\n",
       "      <td>48</td>\n",
       "      <td>1141</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4133</td>\n",
       "      <td>1201</td>\n",
       "      <td>152</td>\n",
       "      <td>48</td>\n",
       "      <td>1281</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4260</td>\n",
       "      <td>1234</td>\n",
       "      <td>117</td>\n",
       "      <td>40</td>\n",
       "      <td>1307</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Qwen3-4B</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3822</td>\n",
       "      <td>1145</td>\n",
       "      <td>122</td>\n",
       "      <td>40</td>\n",
       "      <td>1218</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>203</td>\n",
       "      <td>121</td>\n",
       "      <td>36</td>\n",
       "      <td>276</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>207</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>280</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "      <td>228</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>304</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>868</td>\n",
       "      <td>232</td>\n",
       "      <td>105</td>\n",
       "      <td>31</td>\n",
       "      <td>308</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>681</td>\n",
       "      <td>166</td>\n",
       "      <td>144</td>\n",
       "      <td>44</td>\n",
       "      <td>223</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>698</td>\n",
       "      <td>170</td>\n",
       "      <td>152</td>\n",
       "      <td>45</td>\n",
       "      <td>227</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>779</td>\n",
       "      <td>190</td>\n",
       "      <td>117</td>\n",
       "      <td>37</td>\n",
       "      <td>257</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>194</td>\n",
       "      <td>122</td>\n",
       "      <td>38</td>\n",
       "      <td>261</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1842</td>\n",
       "      <td>530</td>\n",
       "      <td>121</td>\n",
       "      <td>36</td>\n",
       "      <td>603</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2716</td>\n",
       "      <td>760</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>833</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2295</td>\n",
       "      <td>639</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>715</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3687</td>\n",
       "      <td>1060</td>\n",
       "      <td>105</td>\n",
       "      <td>31</td>\n",
       "      <td>1136</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1825</td>\n",
       "      <td>488</td>\n",
       "      <td>144</td>\n",
       "      <td>44</td>\n",
       "      <td>545</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>536</td>\n",
       "      <td>152</td>\n",
       "      <td>45</td>\n",
       "      <td>593</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3041</td>\n",
       "      <td>735</td>\n",
       "      <td>117</td>\n",
       "      <td>37</td>\n",
       "      <td>802</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3406</td>\n",
       "      <td>937</td>\n",
       "      <td>122</td>\n",
       "      <td>38</td>\n",
       "      <td>1004</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3556</td>\n",
       "      <td>980</td>\n",
       "      <td>121</td>\n",
       "      <td>36</td>\n",
       "      <td>1053</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4662</td>\n",
       "      <td>1453</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>1526</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3622</td>\n",
       "      <td>1073</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>1149</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>dict</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3269</td>\n",
       "      <td>934</td>\n",
       "      <td>105</td>\n",
       "      <td>31</td>\n",
       "      <td>1010</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2773</td>\n",
       "      <td>760</td>\n",
       "      <td>144</td>\n",
       "      <td>44</td>\n",
       "      <td>817</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>3N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4674</td>\n",
       "      <td>1276</td>\n",
       "      <td>152</td>\n",
       "      <td>45</td>\n",
       "      <td>1333</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>binary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3197</td>\n",
       "      <td>856</td>\n",
       "      <td>117</td>\n",
       "      <td>37</td>\n",
       "      <td>923</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phi-4-mini-instruct</td>\n",
       "      <td>nl</td>\n",
       "      <td>4N</td>\n",
       "      <td>ternary</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3205</td>\n",
       "      <td>884</td>\n",
       "      <td>122</td>\n",
       "      <td>38</td>\n",
       "      <td>951</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Format Strategy Classification  Examples  \\\n",
       "36             Qwen3-4B   dict       3N         binary         0   \n",
       "39             Qwen3-4B   dict       3N        ternary         0   \n",
       "42             Qwen3-4B   dict       4N         binary         0   \n",
       "45             Qwen3-4B   dict       4N        ternary         0   \n",
       "24             Qwen3-4B     nl       3N         binary         0   \n",
       "27             Qwen3-4B     nl       3N        ternary         0   \n",
       "30             Qwen3-4B     nl       4N         binary         0   \n",
       "33             Qwen3-4B     nl       4N        ternary         0   \n",
       "37             Qwen3-4B   dict       3N         binary         1   \n",
       "40             Qwen3-4B   dict       3N        ternary         1   \n",
       "43             Qwen3-4B   dict       4N         binary         1   \n",
       "46             Qwen3-4B   dict       4N        ternary         1   \n",
       "25             Qwen3-4B     nl       3N         binary         1   \n",
       "28             Qwen3-4B     nl       3N        ternary         1   \n",
       "31             Qwen3-4B     nl       4N         binary         1   \n",
       "34             Qwen3-4B     nl       4N        ternary         1   \n",
       "38             Qwen3-4B   dict       3N         binary         2   \n",
       "41             Qwen3-4B   dict       3N        ternary         2   \n",
       "44             Qwen3-4B   dict       4N         binary         2   \n",
       "47             Qwen3-4B   dict       4N        ternary         2   \n",
       "26             Qwen3-4B     nl       3N         binary         2   \n",
       "29             Qwen3-4B     nl       3N        ternary         2   \n",
       "32             Qwen3-4B     nl       4N         binary         2   \n",
       "35             Qwen3-4B     nl       4N        ternary         2   \n",
       "12  phi-4-mini-instruct   dict       3N         binary         0   \n",
       "15  phi-4-mini-instruct   dict       3N        ternary         0   \n",
       "18  phi-4-mini-instruct   dict       4N         binary         0   \n",
       "21  phi-4-mini-instruct   dict       4N        ternary         0   \n",
       "0   phi-4-mini-instruct     nl       3N         binary         0   \n",
       "3   phi-4-mini-instruct     nl       3N        ternary         0   \n",
       "6   phi-4-mini-instruct     nl       4N         binary         0   \n",
       "9   phi-4-mini-instruct     nl       4N        ternary         0   \n",
       "13  phi-4-mini-instruct   dict       3N         binary         1   \n",
       "16  phi-4-mini-instruct   dict       3N        ternary         1   \n",
       "19  phi-4-mini-instruct   dict       4N         binary         1   \n",
       "22  phi-4-mini-instruct   dict       4N        ternary         1   \n",
       "1   phi-4-mini-instruct     nl       3N         binary         1   \n",
       "4   phi-4-mini-instruct     nl       3N        ternary         1   \n",
       "7   phi-4-mini-instruct     nl       4N         binary         1   \n",
       "10  phi-4-mini-instruct     nl       4N        ternary         1   \n",
       "14  phi-4-mini-instruct   dict       3N         binary         2   \n",
       "17  phi-4-mini-instruct   dict       3N        ternary         2   \n",
       "20  phi-4-mini-instruct   dict       4N         binary         2   \n",
       "23  phi-4-mini-instruct   dict       4N        ternary         2   \n",
       "2   phi-4-mini-instruct     nl       3N         binary         2   \n",
       "5   phi-4-mini-instruct     nl       3N        ternary         2   \n",
       "8   phi-4-mini-instruct     nl       4N         binary         2   \n",
       "11  phi-4-mini-instruct     nl       4N        ternary         2   \n",
       "\n",
       "    Actual_Examples  Avg_Prompt_Chars  Avg_Prompt_Tokens  Avg_Expected_Chars  \\\n",
       "36                0               785                228                 121   \n",
       "39                0               802                231                 129   \n",
       "42                0               886                252                 100   \n",
       "45                0               903                255                 105   \n",
       "24                0               716                191                 144   \n",
       "27                0               733                194                 152   \n",
       "30                0               814                214                 117   \n",
       "33                0               831                217                 122   \n",
       "37                2              1698                526                 121   \n",
       "40                3              2954                846                 129   \n",
       "43                2              3254                943                 100   \n",
       "46                4              4586               1339                 105   \n",
       "25                2              2117                575                 144   \n",
       "28                3              2696                768                 152   \n",
       "31                2              2213                619                 117   \n",
       "34                4              5045               1482                 122   \n",
       "38                4              2993                976                 121   \n",
       "41                6              5476               1824                 129   \n",
       "44                4              4568               1353                 100   \n",
       "47                4              3851               1190                 105   \n",
       "26                4              3234               1061                 144   \n",
       "29                6              4133               1201                 152   \n",
       "32                4              4260               1234                 117   \n",
       "35                4              3822               1145                 122   \n",
       "12                0               750                203                 121   \n",
       "15                0               767                207                 129   \n",
       "18                0               851                228                 100   \n",
       "21                0               868                232                 105   \n",
       "0                 0               681                166                 144   \n",
       "3                 0               698                170                 152   \n",
       "6                 0               779                190                 117   \n",
       "9                 0               796                194                 122   \n",
       "13                2              1842                530                 121   \n",
       "16                3              2716                760                 129   \n",
       "19                2              2295                639                 100   \n",
       "22                4              3687               1060                 105   \n",
       "1                 2              1825                488                 144   \n",
       "4                 3              2008                536                 152   \n",
       "7                 2              3041                735                 117   \n",
       "10                4              3406                937                 122   \n",
       "14                4              3556                980                 121   \n",
       "17                6              4662               1453                 129   \n",
       "20                4              3622               1073                 100   \n",
       "23                4              3269                934                 105   \n",
       "2                 4              2773                760                 144   \n",
       "5                 6              4674               1276                 152   \n",
       "8                 4              3197                856                 117   \n",
       "11                4              3205                884                 122   \n",
       "\n",
       "    Avg_Expected_Tokens  Max_Prompt_Tokens  Max_Expected_Tokens  \n",
       "36                   38                324                   62  \n",
       "39                   38                327                   62  \n",
       "42                   32                332                   60  \n",
       "45                   32                335                   60  \n",
       "24                   48                271                   78  \n",
       "27                   48                274                   78  \n",
       "30                   40                287                   78  \n",
       "33                   40                290                   78  \n",
       "37                   38                622                   62  \n",
       "40                   38                942                   62  \n",
       "43                   32               1023                   60  \n",
       "46                   32               1419                   60  \n",
       "25                   48                655                   78  \n",
       "28                   48                848                   78  \n",
       "31                   40                692                   78  \n",
       "34                   40               1555                   78  \n",
       "38                   38               1072                   62  \n",
       "41                   38               1920                   62  \n",
       "44                   32               1433                   60  \n",
       "47                   32               1270                   60  \n",
       "26                   48               1141                   78  \n",
       "29                   48               1281                   78  \n",
       "32                   40               1307                   78  \n",
       "35                   40               1218                   78  \n",
       "12                   36                276                   60  \n",
       "15                   37                280                   61  \n",
       "18                   31                304                   59  \n",
       "21                   31                308                   60  \n",
       "0                    44                223                   72  \n",
       "3                    45                227                   73  \n",
       "6                    37                257                   72  \n",
       "9                    38                261                   73  \n",
       "13                   36                603                   60  \n",
       "16                   37                833                   61  \n",
       "19                   31                715                   59  \n",
       "22                   31               1136                   60  \n",
       "1                    44                545                   72  \n",
       "4                    45                593                   73  \n",
       "7                    37                802                   72  \n",
       "10                   38               1004                   73  \n",
       "14                   36               1053                   60  \n",
       "17                   37               1526                   61  \n",
       "20                   31               1149                   59  \n",
       "23                   31               1010                   60  \n",
       "2                    44                817                   72  \n",
       "5                    45               1333                   73  \n",
       "8                    37                923                   72  \n",
       "11                   38                951                   73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== TOKEN COUNT AND CHARACTER ANALYSIS =====\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE TOKEN COUNT AND CHARACTER ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration combinations to test\n",
    "models = [\"microsoft/phi-4-mini-instruct\", \"Qwen/Qwen3-4B\"]\n",
    "solution_formats = [\"nl\", \"dict\"]\n",
    "dataset_strategies = [\"3N\", \"4N\"]\n",
    "classification_types = [\"binary\", \"ternary\"]\n",
    "num_examples_list = [0, 1, 2]  # 0 for no examples, 1 and 2 for few-shot\n",
    "\n",
    "# Storage for all statistics\n",
    "all_stats = []\n",
    "\n",
    "try:\n",
    "    # Load a sample of data for analysis\n",
    "    print(\"Loading sample dataset for analysis...\")\n",
    "    sample_config = CONFIG.copy()\n",
    "    sample_config[\"dataset_strategy\"] = \"4N\"  # Start with 4N\n",
    "    base_data = load_base_dataset(sample_config)\n",
    "    sample_data = base_data.head(50)  # Use 50 samples for analysis\n",
    "    print(f\"✓ Loaded {len(sample_data)} samples for analysis\\n\")\n",
    "\n",
    "    total_combinations = len(models) * len(solution_formats) * len(dataset_strategies) * len(classification_types) * len(num_examples_list)\n",
    "    combo_count = 0\n",
    "\n",
    "    for model_name in models:\n",
    "        # Load tokenizer once per model\n",
    "        print(f\"Loading tokenizer for {model_name}...\")\n",
    "        tokenizer = load_tokenizer(model_name)\n",
    "        \n",
    "        for solution_format in solution_formats:\n",
    "            for dataset_strategy in dataset_strategies:\n",
    "                for classification_type in classification_types:\n",
    "                    for num_examples in num_examples_list:\n",
    "                        combo_count += 1\n",
    "                        \n",
    "                        print(f\"\\n{'='*80}\")\n",
    "                        print(f\"COMBINATION {combo_count}/{total_combinations}\")\n",
    "                        print(f\"Model: {model_name.split('/')[-1]}\")\n",
    "                        print(f\"Solution Format: {solution_format}\")\n",
    "                        print(f\"Dataset Strategy: {dataset_strategy}\")\n",
    "                        print(f\"Classification Type: {classification_type}\")\n",
    "                        print(f\"Num Examples: {num_examples}\")\n",
    "                        print(f\"{'='*80}\")\n",
    "                        \n",
    "                        # Create test configuration\n",
    "                        test_config = CONFIG.copy()\n",
    "                        test_config.update({\n",
    "                            'model_name': model_name,\n",
    "                            'solution_format': solution_format,\n",
    "                            'dataset_strategy': dataset_strategy,\n",
    "                            'classification_type': classification_type,\n",
    "                            'include_examples': num_examples > 0,\n",
    "                            'num_examples': num_examples,\n",
    "                            'system_prompt': generate_system_prompt({\n",
    "                                **test_config,\n",
    "                                'solution_format': solution_format,\n",
    "                                'classification_type': classification_type\n",
    "                            })\n",
    "                        })\n",
    "                        \n",
    "                        try:\n",
    "                            # Load appropriate dataset\n",
    "                            if dataset_strategy != sample_config[\"dataset_strategy\"]:\n",
    "                                test_data = load_base_dataset(test_config)\n",
    "                                analysis_data = test_data.head(50)\n",
    "                            else:\n",
    "                                analysis_data = sample_data\n",
    "                            \n",
    "                            # Create example manager\n",
    "                            example_manager = ExampleManager(analysis_data, test_config)\n",
    "                            examples = example_manager.get_examples()\n",
    "                            \n",
    "                            print(f\"Examples generated: {len(examples)}\")\n",
    "                            \n",
    "                            # Analyze a subset of samples\n",
    "                            analysis_samples = analysis_data.head(20).to_dict('records')\n",
    "                            \n",
    "                            # Statistics storage\n",
    "                            stats = {\n",
    "                                'config': {\n",
    "                                    'model': model_name.split('/')[-1],\n",
    "                                    'solution_format': solution_format,\n",
    "                                    'dataset_strategy': dataset_strategy,\n",
    "                                    'classification_type': classification_type,\n",
    "                                    'num_examples': num_examples,\n",
    "                                    'actual_examples_used': len(examples)\n",
    "                                },\n",
    "                                'prompt_stats': [],\n",
    "                                'expected_output_stats': [],\n",
    "                                'by_error_type': defaultdict(list)\n",
    "                            }\n",
    "                            \n",
    "                            print(\"Analyzing samples...\")\n",
    "                            for i, sample in enumerate(analysis_samples):\n",
    "                                # Create messages\n",
    "                                messages = create_sample_messages(sample, examples, test_config)\n",
    "                                expected_output = format_expected_output(sample, test_config)\n",
    "                                \n",
    "                                # Format prompt\n",
    "                                formatted_prompt = tokenizer.apply_chat_template(\n",
    "                                    messages, tokenize=False, add_generation_prompt=True\n",
    "                                )\n",
    "                                \n",
    "                                # Tokenize\n",
    "                                tokenized_prompt = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "                                tokenized_expected = tokenizer(expected_output, return_tensors=\"pt\")\n",
    "                                \n",
    "                                # Calculate statistics\n",
    "                                prompt_chars = len(formatted_prompt)\n",
    "                                prompt_tokens = len(tokenized_prompt[\"input_ids\"][0])\n",
    "                                expected_chars = len(expected_output)\n",
    "                                expected_tokens = len(tokenized_expected[\"input_ids\"][0])\n",
    "                                \n",
    "                                # Store prompt stats\n",
    "                                prompt_stat = {\n",
    "                                    'chars': prompt_chars,\n",
    "                                    'tokens': prompt_tokens,\n",
    "                                    'chars_per_token': prompt_chars / prompt_tokens if prompt_tokens > 0 else 0,\n",
    "                                    'error_type': sample['error_type']\n",
    "                                }\n",
    "                                stats['prompt_stats'].append(prompt_stat)\n",
    "                                \n",
    "                                # Store expected output stats\n",
    "                                expected_stat = {\n",
    "                                    'chars': expected_chars,\n",
    "                                    'tokens': expected_tokens,\n",
    "                                    'chars_per_token': expected_chars / expected_tokens if expected_tokens > 0 else 0,\n",
    "                                    'error_type': sample['error_type']\n",
    "                                }\n",
    "                                stats['expected_output_stats'].append(expected_stat)\n",
    "                                \n",
    "                                # Group by error type\n",
    "                                stats['by_error_type'][sample['error_type']].append({\n",
    "                                    'prompt_chars': prompt_chars,\n",
    "                                    'prompt_tokens': prompt_tokens,\n",
    "                                    'expected_chars': expected_chars,\n",
    "                                    'expected_tokens': expected_tokens\n",
    "                                })\n",
    "                            \n",
    "                            # Calculate summary statistics\n",
    "                            prompt_chars = [s['chars'] for s in stats['prompt_stats']]\n",
    "                            prompt_tokens = [s['tokens'] for s in stats['prompt_stats']]\n",
    "                            expected_chars = [s['chars'] for s in stats['expected_output_stats']]\n",
    "                            expected_tokens = [s['tokens'] for s in stats['expected_output_stats']]\n",
    "                            \n",
    "                            # Add summary stats\n",
    "                            stats['summary'] = {\n",
    "                                'prompt': {\n",
    "                                    'chars': {\n",
    "                                        'min': min(prompt_chars),\n",
    "                                        'max': max(prompt_chars),\n",
    "                                        'mean': sum(prompt_chars) / len(prompt_chars),\n",
    "                                        'median': sorted(prompt_chars)[len(prompt_chars)//2]\n",
    "                                    },\n",
    "                                    'tokens': {\n",
    "                                        'min': min(prompt_tokens),\n",
    "                                        'max': max(prompt_tokens),\n",
    "                                        'mean': sum(prompt_tokens) / len(prompt_tokens),\n",
    "                                        'median': sorted(prompt_tokens)[len(prompt_tokens)//2]\n",
    "                                    }\n",
    "                                },\n",
    "                                'expected_output': {\n",
    "                                    'chars': {\n",
    "                                        'min': min(expected_chars),\n",
    "                                        'max': max(expected_chars),\n",
    "                                        'mean': sum(expected_chars) / len(expected_chars),\n",
    "                                        'median': sorted(expected_chars)[len(expected_chars)//2]\n",
    "                                    },\n",
    "                                    'tokens': {\n",
    "                                        'min': min(expected_tokens),\n",
    "                                        'max': max(expected_tokens),\n",
    "                                        'mean': sum(expected_tokens) / len(expected_tokens),\n",
    "                                        'median': sorted(expected_tokens)[len(expected_tokens)//2]\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                            \n",
    "                            # Print summary for this combination\n",
    "                            print(f\"\\nSUMMARY STATISTICS:\")\n",
    "                            print(f\"Samples analyzed: {len(analysis_samples)}\")\n",
    "                            print(f\"Examples in prompts: {len(examples)}\")\n",
    "                            \n",
    "                            print(f\"\\nPROMPT STATISTICS:\")\n",
    "                            print(f\"  Characters: {stats['summary']['prompt']['chars']['min']}-{stats['summary']['prompt']['chars']['max']} \"\n",
    "                                  f\"(avg: {stats['summary']['prompt']['chars']['mean']:.0f})\")\n",
    "                            print(f\"  Tokens: {stats['summary']['prompt']['tokens']['min']}-{stats['summary']['prompt']['tokens']['max']} \"\n",
    "                                  f\"(avg: {stats['summary']['prompt']['tokens']['mean']:.0f})\")\n",
    "                            \n",
    "                            print(f\"\\nEXPECTED OUTPUT STATISTICS:\")\n",
    "                            print(f\"  Characters: {stats['summary']['expected_output']['chars']['min']}-{stats['summary']['expected_output']['chars']['max']} \"\n",
    "                                  f\"(avg: {stats['summary']['expected_output']['chars']['mean']:.0f})\")\n",
    "                            print(f\"  Tokens: {stats['summary']['expected_output']['tokens']['min']}-{stats['summary']['expected_output']['tokens']['max']} \"\n",
    "                                  f\"(avg: {stats['summary']['expected_output']['tokens']['mean']:.0f})\")\n",
    "                            \n",
    "                            # Error type breakdown\n",
    "                            print(f\"\\nBY ERROR TYPE:\")\n",
    "                            for error_type, error_stats in stats['by_error_type'].items():\n",
    "                                if error_stats:\n",
    "                                    avg_prompt_tokens = sum(s['prompt_tokens'] for s in error_stats) / len(error_stats)\n",
    "                                    avg_expected_tokens = sum(s['expected_tokens'] for s in error_stats) / len(error_stats)\n",
    "                                    print(f\"  {error_type}: {len(error_stats)} samples, \"\n",
    "                                          f\"avg prompt tokens: {avg_prompt_tokens:.0f}, \"\n",
    "                                          f\"avg expected tokens: {avg_expected_tokens:.0f}\")\n",
    "                            \n",
    "                            # Store for final comparison\n",
    "                            all_stats.append(stats)\n",
    "                            \n",
    "                            # Show a sample prompt for first few combinations\n",
    "                            if combo_count <= 3:\n",
    "                                print(f\"\\nSAMPLE FORMATTED PROMPT (first 500 chars):\")\n",
    "                                sample_messages = create_sample_messages(analysis_samples[0], examples, test_config)\n",
    "                                sample_formatted = tokenizer.apply_chat_template(\n",
    "                                    sample_messages, tokenize=False, add_generation_prompt=True\n",
    "                                )\n",
    "                                print(f\"{sample_formatted[:500]}...\")\n",
    "                                print(f\"\\nSAMPLE EXPECTED OUTPUT:\")\n",
    "                                print(format_expected_output(analysis_samples[0], test_config))\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Error in combination: {e}\")\n",
    "                            continue\n",
    "\n",
    "    # Final comparison table\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"COMPREHENSIVE COMPARISON TABLE\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for stat in all_stats:\n",
    "        row = {\n",
    "            'Model': stat['config']['model'],\n",
    "            'Format': stat['config']['solution_format'],\n",
    "            'Strategy': stat['config']['dataset_strategy'],\n",
    "            'Classification': stat['config']['classification_type'],\n",
    "            'Examples': stat['config']['num_examples'],\n",
    "            'Actual_Examples': stat['config']['actual_examples_used'],\n",
    "            'Avg_Prompt_Chars': int(stat['summary']['prompt']['chars']['mean']),\n",
    "            'Avg_Prompt_Tokens': int(stat['summary']['prompt']['tokens']['mean']),\n",
    "            'Avg_Expected_Chars': int(stat['summary']['expected_output']['chars']['mean']),\n",
    "            'Avg_Expected_Tokens': int(stat['summary']['expected_output']['tokens']['mean']),\n",
    "            'Max_Prompt_Tokens': stat['summary']['prompt']['tokens']['max'],\n",
    "            'Max_Expected_Tokens': stat['summary']['expected_output']['tokens']['max']\n",
    "        }\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    df_sorted = df.sort_values(['Model', 'Examples', 'Format', 'Strategy', 'Classification'])\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Token count ranges\n",
    "    min_prompt_tokens = df['Avg_Prompt_Tokens'].min()\n",
    "    max_prompt_tokens = df['Avg_Prompt_Tokens'].max()\n",
    "    min_expected_tokens = df['Avg_Expected_Tokens'].min()\n",
    "    max_expected_tokens = df['Avg_Expected_Tokens'].max()\n",
    "    \n",
    "    print(f\"Prompt Token Range: {min_prompt_tokens} - {max_prompt_tokens} tokens\")\n",
    "    print(f\"Expected Output Token Range: {min_expected_tokens} - {max_expected_tokens} tokens\")\n",
    "    print(f\"Maximum Single Prompt: {df['Max_Prompt_Tokens'].max()} tokens\")\n",
    "    print(f\"Maximum Single Expected Output: {df['Max_Expected_Tokens'].max()} tokens\")\n",
    "    \n",
    "    # Impact of examples\n",
    "    no_examples = df[df['Examples'] == 0]['Avg_Prompt_Tokens'].mean()\n",
    "    one_example = df[df['Examples'] == 1]['Avg_Prompt_Tokens'].mean()\n",
    "    two_examples = df[df['Examples'] == 2]['Avg_Prompt_Tokens'].mean()\n",
    "    \n",
    "    print(f\"\\nImpact of Examples on Prompt Length:\")\n",
    "    print(f\"  No examples: {no_examples:.0f} tokens\")\n",
    "    print(f\"  1 example: {one_example:.0f} tokens\")\n",
    "    print(f\"  2 examples: {two_examples:.0f} tokens\")\n",
    "    print(f\"  Token increase per example: ~{(two_examples - no_examples) / 2:.0f} tokens\")\n",
    "    \n",
    "    # Model differences\n",
    "    print(f\"\\nModel Differences (average across all configs):\")\n",
    "    for model in df['Model'].unique():\n",
    "        model_avg = df[df['Model'] == model]['Avg_Prompt_Tokens'].mean()\n",
    "        print(f\"  {model}: {model_avg:.0f} avg prompt tokens\")\n",
    "    \n",
    "    # Format differences\n",
    "    print(f\"\\nFormat Differences (average across all configs):\")\n",
    "    for fmt in df['Format'].unique():\n",
    "        fmt_avg_prompt = df[df['Format'] == fmt]['Avg_Prompt_Tokens'].mean()\n",
    "        fmt_avg_expected = df[df['Format'] == fmt]['Avg_Expected_Tokens'].mean()\n",
    "        print(f\"  {fmt}: {fmt_avg_prompt:.0f} prompt tokens, {fmt_avg_expected:.0f} expected tokens\")\n",
    "    \n",
    "    print(f\"\\n✅ ANALYSIS COMPLETED!\")\n",
    "    print(f\"Analyzed {len(all_stats)} configurations successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR DURING ANALYSIS:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Print formatted table\n",
    "display(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d9eb1",
   "metadata": {},
   "source": [
    "Cell 8: Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a089c87",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sft_experiments/gene_ter_4N_exp_eln_nl_phi4_20250731_132248/baseline'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sft_experiments/gene_ter_4N_exp_eln_nl_phi4_20250731_132248'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sft_experiments'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output_dir\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Setup output directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m output_dir = \u001b[43msetup_output_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m CONFIG[\u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(output_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36msetup_output_directory\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m      7\u001b[39m subdirs = [\u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfinal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m subdirs:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Save configuration\u001b[39;00m\n\u001b[32m     12\u001b[39m config_path = output_dir / \u001b[33m\"\u001b[39m\u001b[33mconfig.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1315\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1315\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Path.mkdir at line 1315 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1315\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/erdos-dl/lib/python3.12/pathlib.py:1311\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1307\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[33;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[32m   1309\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/content'"
     ]
    }
   ],
   "source": [
    "def setup_output_directory(config):\n",
    "    \"\"\"Creates organized output directory structure\"\"\"\n",
    "    \n",
    "    output_dir = Path(config[\"output_base_dir\"]) / config[\"experiment_id\"]\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"baseline\", \"training\", \"final\", \"checkpoints\"]\n",
    "    for subdir in subdirs:\n",
    "        (output_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = output_dir / \"config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = setup_output_directory(CONFIG)\n",
    "CONFIG[\"output_dir\"] = str(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f1ffe",
   "metadata": {},
   "source": [
    "Cell 9: Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdc7def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inference_batch(messages_batch, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Prepares a batch of messages for inference by applying chat templates and tokenizing.\n",
    "    \n",
    "    Args:\n",
    "        messages_batch: List of message conversations (each is a list of message dicts)\n",
    "        tokenizer: The tokenizer to use\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        dict: Batch with input_ids, attention_mask, and metadata\n",
    "    \"\"\"\n",
    "    batch_data = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"metadata\": {\n",
    "            \"formatted_prompts\": [],\n",
    "            \"input_token_counts\": [],\n",
    "            \"conversation_lengths\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for messages in messages_batch:\n",
    "        # Apply chat template to get formatted prompt\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted prompt\n",
    "        tokenized = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False  # We'll pad at batch level\n",
    "        )\n",
    "        \n",
    "        # Store batch data\n",
    "        batch_data[\"input_ids\"].append(tokenized[\"input_ids\"].squeeze(0))\n",
    "        batch_data[\"attention_mask\"].append(tokenized[\"attention_mask\"].squeeze(0))\n",
    "        \n",
    "        # Store metadata\n",
    "        batch_data[\"metadata\"][\"formatted_prompts\"].append(formatted_prompt)\n",
    "        batch_data[\"metadata\"][\"input_token_counts\"].append(len(tokenized[\"input_ids\"][0]))\n",
    "        batch_data[\"metadata\"][\"conversation_lengths\"].append(len(messages))\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "def apply_batch_padding(batch_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Applies padding to a batch of tokenized sequences.\n",
    "    \n",
    "    Args:\n",
    "        batch_data: Output from prepare_inference_batch\n",
    "        tokenizer: The tokenizer (for pad_token_id)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Padded tensors ready for model input\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    input_ids_padded = pad_sequence(\n",
    "        batch_data[\"input_ids\"],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    attention_mask_padded = pad_sequence(\n",
    "        batch_data[\"attention_mask\"],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"metadata\": batch_data[\"metadata\"]\n",
    "    }\n",
    "\n",
    "def decode_batch_outputs(outputs, input_lengths, tokenizer):\n",
    "    \"\"\"\n",
    "    Decodes model outputs for a batch, extracting only the generated portions.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Model generation outputs (batch_size, sequence_length)\n",
    "        input_lengths: List of input sequence lengths for each item in batch\n",
    "        tokenizer: The tokenizer for decoding\n",
    "        \n",
    "    Returns:\n",
    "        list: Decoded responses (only the generated parts)\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for i, output_sequence in enumerate(outputs):\n",
    "        # Extract only the generated tokens (after input)\n",
    "        input_length = input_lengths[i]\n",
    "        generated_tokens = output_sequence[input_length:]\n",
    "        \n",
    "        # Decode to text\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        responses.append(response.strip())\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def create_attention_masks(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates attention masks for tokenized inputs.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Tokenized input sequences\n",
    "        tokenizer: The tokenizer (for pad_token_id)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Attention masks\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    if isinstance(input_ids, list):\n",
    "        input_ids = torch.stack(input_ids)\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return attention_mask\n",
    "\n",
    "def run_inference(model, tokenizer, prepared_inputs, batch_size=1):\n",
    "    \"\"\"\n",
    "    Pure inference function that accepts pre-processed inputs and returns results.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for inference\n",
    "        tokenizer: The tokenizer (only used for pad_token_id in generation)\n",
    "        prepared_inputs: Pre-processed batch of inputs with input_ids, attention_mask, metadata\n",
    "        batch_size: Batch size for processing (legacy parameter for compatibility)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (responses, generation_metadata)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import time\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move inputs to model device\n",
    "        input_ids = prepared_inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = prepared_inputs[\"attention_mask\"].to(model.device)\n",
    "        \n",
    "        # Generate responses\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode outputs\n",
    "        input_lengths = prepared_inputs[\"metadata\"][\"input_token_counts\"]\n",
    "        responses = decode_batch_outputs(outputs.sequences, input_lengths, tokenizer)\n",
    "        \n",
    "        # Calculate generation metadata\n",
    "        generation_metadata = {\n",
    "            \"total_inference_time\": end_time - start_time,\n",
    "            \"batch_size\": len(responses),\n",
    "            \"avg_inference_time_per_sample\": (end_time - start_time) / len(responses),\n",
    "            \"input_token_counts\": input_lengths,\n",
    "            \"output_token_counts\": [len(outputs.sequences[i]) - input_lengths[i] for i in range(len(responses))],\n",
    "            \"total_tokens_generated\": sum(len(outputs.sequences[i]) - input_lengths[i] for i in range(len(responses)))\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            generation_metadata[\"gpu_memory_used\"] = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "    \n",
    "    return responses, generation_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, metadata, stage, config):\n",
    "    \"\"\"Saves results and metadata to appropriate locations\"\"\"\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(config[\"output_dir\"]) / stage\n",
    "    \n",
    "    # Save results\n",
    "    results_path = output_dir / f\"results_{timestamp}.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = output_dir / f\"metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return results_path, metadata_path\n",
    "\n",
    "print(\"Inference functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b884d",
   "metadata": {},
   "source": [
    "Cell 10: Baseline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace74c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BASELINE INFERENCE =====\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING BASELINE INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Step 1: Prepare dataset\n",
    "    print(\"Step 1: Preparing dataset...\")\n",
    "    train_data, eval_data, examples = prepare_dataset(CONFIG)\n",
    "    print(f\"✓ Dataset prepared: {len(eval_data)} evaluation samples\")\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    print(\"\\nStep 2: Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(CONFIG)\n",
    "    print(\"✓ Model and tokenizer loaded successfully\")\n",
    "    \n",
    "    # Step 3: Prepare evaluation data for inference\n",
    "    print(\"\\nStep 3: Preparing evaluation data for batched inference...\")\n",
    "    eval_messages = [sample[\"messages\"] for sample in eval_data]\n",
    "    print(f\"✓ Extracted {len(eval_messages)} message conversations\")\n",
    "    \n",
    "    # Step 4: Create inference batches\n",
    "    print(\"\\nStep 4: Creating inference batches...\")\n",
    "    batch_data = prepare_inference_batch(eval_messages, tokenizer, max_length=1024)\n",
    "    prepared_inputs = apply_batch_padding(batch_data, tokenizer)\n",
    "    \n",
    "    print(f\"✓ Prepared batch with shape: {prepared_inputs['input_ids'].shape}\")\n",
    "    print(f\"✓ Average tokens per sample: {sum(batch_data['metadata']['input_token_counts']) / len(batch_data['metadata']['input_token_counts']):.1f}\")\n",
    "    \n",
    "    # Step 5: Run inference\n",
    "    print(\"\\nStep 5: Running baseline inference...\")\n",
    "    print(f\"Processing {len(eval_data)} samples...\")\n",
    "    \n",
    "    baseline_responses, baseline_metadata = run_inference(model, tokenizer, prepared_inputs)\n",
    "    \n",
    "    print(f\"✓ Inference completed in {baseline_metadata['total_inference_time']:.2f}s\")\n",
    "    print(f\"✓ Average time per sample: {baseline_metadata['avg_inference_time_per_sample']:.3f}s\")\n",
    "    print(f\"✓ Total tokens generated: {baseline_metadata['total_tokens_generated']}\")\n",
    "    \n",
    "    # Step 6: Format results for saving\n",
    "    print(\"\\nStep 6: Formatting results...\")\n",
    "    baseline_results = []\n",
    "    \n",
    "    for i in range(len(baseline_responses)):\n",
    "        result = {\n",
    "            \"sample_id\": eval_data[i][\"id\"],\n",
    "            \"expected_output\": eval_data[i][\"expected_output\"],\n",
    "            \"model_response\": baseline_responses[i],\n",
    "            \"sample_metadata\": eval_data[i][\"metadata\"],\n",
    "            \"input_tokens\": batch_data['metadata']['input_token_counts'][i],\n",
    "            \"output_tokens\": baseline_metadata['output_token_counts'][i],\n",
    "            \"formatted_prompt\": batch_data['metadata']['formatted_prompts'][i][:200] + \"...\" if len(batch_data['metadata']['formatted_prompts'][i]) > 200 else batch_data['metadata']['formatted_prompts'][i]  # Truncated for file size\n",
    "        }\n",
    "        baseline_results.append(result)\n",
    "    \n",
    "    print(f\"✓ Formatted {len(baseline_results)} results\")\n",
    "    \n",
    "    # Step 7: Save results\n",
    "    print(\"\\nStep 7: Saving results...\")\n",
    "    baseline_results_path, baseline_metadata_path = save_results(\n",
    "        baseline_results, baseline_metadata, \"baseline\", CONFIG\n",
    "    )\n",
    "    \n",
    "    # Step 8: Display sample results\n",
    "    print(\"\\nStep 8: Sample results preview:\")\n",
    "    print(\"=\"*60)\n",
    "    for i in range(min(3, len(baseline_results))):\n",
    "        sample = baseline_results[i]\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Sample ID: {sample['sample_id']}\")\n",
    "        print(f\"Error Type: {sample['sample_metadata']['error_type']}\")\n",
    "        print(f\"Expected: {sample['expected_output']}\")\n",
    "        print(f\"Model Response: {sample['model_response']}\")\n",
    "        print(f\"Input/Output Tokens: {sample['input_tokens']}/{sample['output_tokens']}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Step 9: Summary statistics\n",
    "    print(\"\\nStep 9: Summary Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total samples processed: {len(baseline_results)}\")\n",
    "    print(f\"Total inference time: {baseline_metadata['total_inference_time']:.2f}s\")\n",
    "    print(f\"Average time per sample: {baseline_metadata['avg_inference_time_per_sample']:.3f}s\")\n",
    "    print(f\"Total input tokens: {sum(baseline_metadata['input_token_counts'])}\")\n",
    "    print(f\"Total output tokens: {baseline_metadata['total_tokens_generated']}\")\n",
    "    print(f\"Average output tokens per sample: {baseline_metadata['total_tokens_generated'] / len(baseline_results):.1f}\")\n",
    "    \n",
    "    if 'gpu_memory_used' in baseline_metadata:\n",
    "        print(f\"GPU memory used: {baseline_metadata['gpu_memory_used']:.2f} GB\")\n",
    "    \n",
    "    # Error type distribution\n",
    "    error_types = {}\n",
    "    for result in baseline_results:\n",
    "        error_type = result['sample_metadata']['error_type']\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nError type distribution:\")\n",
    "    for error_type, count in error_types.items():\n",
    "        print(f\"  {error_type}: {count} samples ({count/len(baseline_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ BASELINE INFERENCE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Results saved to: {baseline_results_path}\")\n",
    "    print(f\"Metadata saved to: {baseline_metadata_path}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR DURING BASELINE INFERENCE:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "    # Import traceback for detailed error info\n",
    "    import traceback\n",
    "    print(f\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n💡 Troubleshooting tips:\")\n",
    "    print(\"1. Check if all previous cells have been run\")\n",
    "    print(\"2. Verify CONFIG is properly set\")\n",
    "    print(\"3. Ensure dataset files exist in the specified path\")\n",
    "    print(\"4. Check GPU memory availability\")\n",
    "    print(\"5. Try reducing batch size if out of memory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
