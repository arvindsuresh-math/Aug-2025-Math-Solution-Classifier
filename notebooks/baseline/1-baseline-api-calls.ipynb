{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da0cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root identified at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Baseline directory is: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/baseline\n",
      "Raw API outputs will be saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/baseline/results/raw_api_outputs\n",
      "\n",
      "✅ Setup complete. Paths and API key are configured.\n"
     ]
    }
   ],
   "source": [
    "### Cell 1: Setup and Configuration\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# The notebook is inside the 'baseline' directory. The project root is its parent.\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "BASELINE_DIR = NOTEBOOK_DIR\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_OUTPUT_DIR = BASELINE_DIR / \"results\" / \"raw_api_outputs\"\n",
    "\n",
    "# --- Create output directories ---\n",
    "# This structure will be created inside the 'baseline' directory\n",
    "(RAW_OUTPUT_DIR / \"conceptual_check\" / \"sft_test_set\").mkdir(parents=True, exist_ok=True)\n",
    "(RAW_OUTPUT_DIR / \"conceptual_check\" / \"final_test_set\").mkdir(parents=True, exist_ok=True)\n",
    "(RAW_OUTPUT_DIR / \"computational_check\" / \"sft_test_set\").mkdir(parents=True, exist_ok=True)\n",
    "(RAW_OUTPUT_DIR / \"computational_check\" / \"final_test_set\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root identified at: {PROJECT_ROOT}\")\n",
    "print(f\"Baseline directory is: {BASELINE_DIR}\")\n",
    "print(f\"Raw API outputs will be saved to: {RAW_OUTPUT_DIR}\")\n",
    "\n",
    "# --- API Key Configuration ---\n",
    "# The .env file should be located at the project root\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in the .env file at the project root.\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "print(\"\\n✅ Setup complete. Paths and API key are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781deccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1214 samples from the SFT test set.\n",
      "Loaded 302 samples from the final test set.\n",
      "\n",
      "SFT Test Set Columns: ['index', 'question', 'answer', 'error_type']\n",
      "Final Test Set Columns: ['index', 'question', 'answer', 'error_type']\n"
     ]
    }
   ],
   "source": [
    "### Cell 2: Load Test Data\n",
    "\n",
    "SFT_TEST_SET_PATH = DATA_DIR / \"final-datasets\" / \"error_detection_dataset.csv\"\n",
    "FINAL_TEST_SET_PATH = DATA_DIR / \"final-datasets\" / \"final-test-with-wrong-answers.csv\"\n",
    "\n",
    "# --- 1. Process the Final Test Set (Identical to final-testing.md) ---\n",
    "final_test_raw_df = pd.read_csv(FINAL_TEST_SET_PATH)\n",
    "\n",
    "final_test_data = []\n",
    "for idx, row in final_test_raw_df.iterrows():\n",
    "    # Append the correct version of the solution\n",
    "    final_test_data.append({\n",
    "        \"index\": idx, # use the test set index\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": row[\"correct_answer\"],\n",
    "        \"error_type\": \"correct\"\n",
    "    })\n",
    "    # Append the flawed version of the solution\n",
    "    error = \"conceptual_error\" if row[\"error_type\"] == \"concep\" else \"computational_error\"\n",
    "    final_test_data.append({\n",
    "        \"index\": idx,\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": row[\"wrong_answer\"],\n",
    "        \"error_type\": error\n",
    "    })\n",
    "final_test_df = pd.DataFrame(final_test_data)\n",
    "\n",
    "\n",
    "# --- 2. Process the SFT Test Set (Identical to final-testing.md) ---\n",
    "sft_full_df = pd.read_csv(SFT_TEST_SET_PATH)\n",
    "sft_test_df_filtered = sft_full_df[sft_full_df['split'] == 'test']\n",
    "\n",
    "sft_test_data = []\n",
    "for _, row in sft_test_df_filtered.iterrows():\n",
    "    if row[\"error_type\"] == \"correct\":\n",
    "        answer = row[\"correct_answer\"]\n",
    "    else:\n",
    "        answer = row[\"wrong_answer\"]\n",
    "    sft_test_data.append({\n",
    "        \"index\": row[\"index\"], # use the original gsm8k index\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": answer,\n",
    "        \"error_type\": row[\"error_type\"]\n",
    "    })\n",
    "sft_test_df = pd.DataFrame(sft_test_data)\n",
    "\n",
    "\n",
    "# --- 3. Final Verification ---\n",
    "print(f\"Loaded {len(sft_test_df)} samples from the SFT test set.\")\n",
    "print(f\"Loaded {len(final_test_df)} samples from the final test set.\")\n",
    "\n",
    "# Verify that the 'answer' column exists and other columns are consistent\n",
    "print(f\"\\nSFT Test Set Columns: {sft_test_df.columns.tolist()}\")\n",
    "print(f\"Final Test Set Columns: {final_test_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4813ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates are defined.\n"
     ]
    }
   ],
   "source": [
    "### Cell 3: Prompt Templates\n",
    "\n",
    "SYSTEM_PROMPT_CONCEPTUAL = \"\"\"You are a mathematics tutor.\n",
    "You will be given a math word problem and a solution written by a student.\n",
    "Carefully analyze the problem and solution LINE-BY-LINE and determine whether there are any errors in the solution.\n",
    "\n",
    "IMPORTANT: Your entire response should consist of ONLY the single word 'correct' or 'flawed'. Do NOT provide any explanation or surrounding text.\"\"\"\n",
    "\n",
    "def format_conceptual_prompt(row):\n",
    "    return f\"### Problem:\\n{row['question']}\\n\\n### Student's Solution:\\n{row['answer']}\\n\\n### Verdict:\"\n",
    "\n",
    "SYSTEM_PROMPT_COMPUTATIONAL = \\\n",
    "\"\"\"[ROLE]\n",
    "You are an expert at parsing mathematical solutions.\n",
    "\n",
    "[TASK]\n",
    "You are given a single line from a mathematical solution. Your task is to extract the calculation from this line.\n",
    "\n",
    "**This is a literal transcription task. Follow these rules with extreme precision:**\n",
    "- **RULE 1: Transcribe EXACTLY.** Do not correct mathematical errors. If a line implies `2+2=5`, your output for that line must be `2+2=5`.\n",
    "- **RULE 2: Isolate the Equation.** Your output must contain ONLY the equation, with no surrounding text, units, or currency symbols.\n",
    "\n",
    "[RESPONSE FORMAT]\n",
    "Your response must ONLY contain the extracted equation, wrapped in <eq> and </eq> tags.\n",
    "If the line contains no calculation, respond with empty tags: <eq></eq>.\n",
    "\n",
    "[EXAMPLE 1]\n",
    "### Input:\n",
    "First find how many liters of the seawater are salt: 2 liters * 20% = .4 liters\n",
    "### Output:\n",
    "<eq>2*20*.01=.4</eq>\n",
    "\n",
    "[EXAMPLE 2]\n",
    "### Input:\n",
    "Therefore, there are 2 $10 bills.\n",
    "### Output:\n",
    "<eq></eq>\n",
    "\n",
    "[EXAMPLE 3]\n",
    "### Input:\n",
    "There are 100-94 = 11 people in the theater with green eyes.\n",
    "### Output:\n",
    "<eq>100-94=11</eq>\n",
    "\"\"\"\n",
    "\n",
    "def format_computational_prompt(line):\n",
    "    return f\"### Solution Line:\\n{line}\"\n",
    "\n",
    "print(\"Prompt templates are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7511c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 4: Asynchronous API Call Logic\n",
    "\n",
    "async def call_gemini_api_with_retries(prompt: str, system_prompt: str, max_attempts: int = 5):\n",
    "    \"\"\"Calls the Gemini API with a simple exponential backoff retry mechanism.\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash-latest\",\n",
    "        system_instruction=system_prompt\n",
    "    )\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = await model.generate_content_async(prompt, generation_config={\"temperature\": 0})\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(f\"API call failed after {max_attempts} attempts. Error: {e}\")\n",
    "                return f\"--- ERROR: {e} ---\"\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"API call failed (attempt {attempt + 1}). Retrying in {wait_time}s...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "    return \"--- ERROR: Max retries exceeded ---\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bba40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 5: Main Execution Loop\n",
    "\n",
    "async def process_sample(index, row, test_set_name):\n",
    "    \"\"\"Processes a single sample: one conceptual call and multiple computational calls.\"\"\"\n",
    "    if test_set_name == \"sft_test_set\":\n",
    "        file_index = row['index']\n",
    "    else:\n",
    "        file_index = index\n",
    "\n",
    "    # --- 1. Conceptual Check ---\n",
    "    conceptual_output_path = RAW_OUTPUT_DIR / \"conceptual_check\" / test_set_name / f\"{file_index}.txt\"\n",
    "    if not conceptual_output_path.exists():\n",
    "        conceptual_prompt = format_conceptual_prompt(row)\n",
    "        response = await call_gemini_api_with_retries(conceptual_prompt, SYSTEM_PROMPT_CONCEPTUAL)\n",
    "        conceptual_output_path.write_text(response, encoding='utf-8')\n",
    "\n",
    "    # --- 2. Computational Check ---\n",
    "    solution_lines = [line.strip() for line in row['answer'].strip().split('\\n') if line.strip()]\n",
    "    for i, line in enumerate(solution_lines):\n",
    "        computational_output_path = RAW_OUTPUT_DIR / \"computational_check\" / test_set_name / f\"{file_index}_line_{i+1}.txt\"\n",
    "        if not computational_output_path.exists():\n",
    "            computational_prompt = format_computational_prompt(line)\n",
    "            response = await call_gemini_api_with_retries(computational_prompt, SYSTEM_PROMPT_COMPUTATIONAL)\n",
    "            computational_output_path.write_text(response, encoding='utf-8')\n",
    "\n",
    "# async def process_sample(index, row, test_set_name):\n",
    "#     \"\"\"Processes a single sample: one conceptual call and multiple computational calls.\"\"\"\n",
    "#     # --- 1. Conceptual Check ---\n",
    "#     conceptual_output_path = RAW_OUTPUT_DIR / \"conceptual_check\" / test_set_name / f\"{index}.txt\"\n",
    "#     if not conceptual_output_path.exists():\n",
    "#         conceptual_prompt = format_conceptual_prompt(row)\n",
    "#         response = await call_gemini_api_with_retries(conceptual_prompt, SYSTEM_PROMPT_CONCEPTUAL)\n",
    "#         conceptual_output_path.write_text(response, encoding='utf-8')\n",
    "\n",
    "#     # --- 2. Computational Check ---\n",
    "#     solution_lines = [line.strip() for line in row['answer'].strip().split('\\n') if line.strip()]\n",
    "#     for i, line in enumerate(solution_lines):\n",
    "#         computational_output_path = RAW_OUTPUT_DIR / \"computational_check\" / test_set_name / f\"{index}_line_{i+1}.txt\"\n",
    "#         if not computational_output_path.exists():\n",
    "#             computational_prompt = format_computational_prompt(line)\n",
    "#             response = await call_gemini_api_with_retries(computational_prompt, SYSTEM_PROMPT_COMPUTATIONAL)\n",
    "#             computational_output_path.write_text(response, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56f0fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a test batch with 30 samples from the SFT test set.\n",
      "Created a test batch with 30 samples from the final test set.\n",
      "\n",
      "🚀 Starting small test runs...\n",
      "\n",
      "--- Preparing tasks for the SFT Test Set (Subset) ---\n",
      "Starting 30 sample processing tasks for the SFT test batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 4269.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ SFT test run complete. ---\n",
      "\n",
      "--- Preparing tasks for the Final Test Set (Subset) ---\n",
      "Starting 30 sample processing tasks for the final test batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 8522.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Final test run complete. ---\n",
      "\n",
      "🎉 Both test runs are finished.\n",
      "Please check the 'results/raw_api_outputs' directory to verify the output files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# --- 1. Create Subsets for the Test Run ---\n",
    "NUM_TEST_SAMPLES = 30\n",
    "sft_test_subset = sft_test_df.head(NUM_TEST_SAMPLES)\n",
    "final_test_subset = final_test_df.head(NUM_TEST_SAMPLES)\n",
    "\n",
    "print(f\"Created a test batch with {len(sft_test_subset)} samples from the SFT test set.\")\n",
    "print(f\"Created a test batch with {len(final_test_subset)} samples from the final test set.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Specific Functions for the Test Runs ---\n",
    "async def run_sft_test_batch():\n",
    "    \"\"\"An asynchronous function to process the SFT test batch.\"\"\"\n",
    "    tasks = []\n",
    "    print(\"\\n--- Preparing tasks for the SFT Test Set (Subset) ---\")\n",
    "    for index, row in sft_test_subset.iterrows():\n",
    "        # The `process_sample` function correctly checks if files exist and skips\n",
    "        # API calls if this test is re-run.\n",
    "        tasks.append(process_sample(index, row, \"sft_test_set\"))\n",
    "    \n",
    "    if tasks:\n",
    "        print(f\"Starting {len(tasks)} sample processing tasks for the SFT test batch...\")\n",
    "        await tqdm_asyncio.gather(*tasks)\n",
    "    else:\n",
    "        print(\"All SFT test samples already have output files. No API calls needed.\")\n",
    "    print(\"--- ✅ SFT test run complete. ---\")\n",
    "\n",
    "\n",
    "async def run_final_test_batch():\n",
    "    \"\"\"An asynchronous function to process the final test batch.\"\"\"\n",
    "    tasks = []\n",
    "    print(\"\\n--- Preparing tasks for the Final Test Set (Subset) ---\")\n",
    "    for index, row in final_test_subset.iterrows():\n",
    "        tasks.append(process_sample(index, row, \"final_test_set\"))\n",
    "\n",
    "    if tasks:\n",
    "        print(f\"Starting {len(tasks)} sample processing tasks for the final test batch...\")\n",
    "        await tqdm_asyncio.gather(*tasks)\n",
    "    else:\n",
    "        print(\"All final test samples already have output files. No API calls needed.\")\n",
    "    print(\"--- ✅ Final test run complete. ---\")\n",
    "\n",
    "\n",
    "# --- 3. Execute the Test Runs Sequentially ---\n",
    "print(\"\\n🚀 Starting small test runs...\")\n",
    "\n",
    "# Run the SFT test batch\n",
    "await run_sft_test_batch()\n",
    "\n",
    "# Run the final test batch\n",
    "await run_final_test_batch()\n",
    "\n",
    "print(\"\\n🎉 Both test runs are finished.\")\n",
    "print(\"Please check the 'results/raw_api_outputs' directory to verify the output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def final_test_main():\n",
    "    \"\"\"Processes all samples in the final test set.\"\"\"\n",
    "    tasks = []\n",
    "    print(\"--- Preparing tasks for the Final Test Set ---\")\n",
    "    for index, row in final_test_df.iterrows():\n",
    "        tasks.append(process_sample(index, row, \"final_test_set\"))\n",
    "    \n",
    "    print(f\"\\nCreated a total of {len(tasks)} sample processing tasks for the final test set.\")\n",
    "    print(\"Starting concurrent API calls...\")\n",
    "    \n",
    "    await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\n--- ✅ Final Test Set processing complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1793dc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting full baseline generation for the Final Test Set...\n",
      "--- Preparing tasks for the Final Test Set ---\n",
      "\n",
      "Created a total of 302 sample processing tasks for the final test set.\n",
      "Starting concurrent API calls...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 302/302 [00:09<00:00, 33.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ Final Test Set processing complete. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🚀 Starting full baseline generation for the Final Test Set...\")\n",
    "await final_test_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3617679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sft_test_main():\n",
    "    \"\"\"\n",
    "    Processes all samples in the SFT test set by splitting them into\n",
    "    manageable chunks to avoid overwhelming the API with too many concurrent requests.\n",
    "    \"\"\"\n",
    "    CHUNK_SIZE = 150  # Number of samples to process in each sequential batch\n",
    "    num_chunks = (len(sft_test_df) + CHUNK_SIZE - 1) // CHUNK_SIZE  # Ceiling division\n",
    "\n",
    "    print(f\"--- Preparing to process {len(sft_test_df)} SFT samples in {num_chunks} chunks of size {CHUNK_SIZE} ---\")\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_index = i * CHUNK_SIZE\n",
    "        end_index = start_index + CHUNK_SIZE\n",
    "        chunk_df = sft_test_df.iloc[start_index:end_index]\n",
    "        \n",
    "        print(f\"\\n--- Processing Chunk {i + 1} of {num_chunks} (Samples {start_index} to {end_index-1}) ---\")\n",
    "\n",
    "        tasks = []\n",
    "        for index, row in chunk_df.iterrows():\n",
    "            tasks.append(process_sample(index, row, \"sft_test_set\"))\n",
    "\n",
    "        if tasks:\n",
    "            print(f\"Starting {len(tasks)} sample processing tasks for this chunk...\")\n",
    "            await tqdm_asyncio.gather(*tasks)\n",
    "            print(f\"--- ✅ Chunk {i + 1} complete. ---\")\n",
    "        \n",
    "        if (i + 1) < num_chunks:\n",
    "            # Add a small delay between chunks as a courtesy to the API\n",
    "            print(\"Waiting 5 seconds before starting the next chunk...\")\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    print(\"\\n--- ✅ All SFT Test Set chunks have been processed. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c046a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting full baseline generation for the SFT Test Set...\n",
      "--- Preparing to process 1214 SFT samples in 9 chunks of size 150 ---\n",
      "\n",
      "--- Processing Chunk 1 of 9 (Samples 0 to 149) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 4683.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 1 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 2 of 9 (Samples 150 to 299) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 5693.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 2 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 3 of 9 (Samples 300 to 449) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 6194.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 3 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 4 of 9 (Samples 450 to 599) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 5455.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 4 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 5 of 9 (Samples 600 to 749) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 5763.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 5 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 6 of 9 (Samples 750 to 899) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 6628.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 6 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 7 of 9 (Samples 900 to 1049) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 6770.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 7 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 8 of 9 (Samples 1050 to 1199) ---\n",
      "Starting 150 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 5414.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 8 complete. ---\n",
      "Waiting 5 seconds before starting the next chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Chunk 9 of 9 (Samples 1200 to 1349) ---\n",
      "Starting 14 sample processing tasks for this chunk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 4874.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ✅ Chunk 9 complete. ---\n",
      "\n",
      "--- ✅ All SFT Test Set chunks have been processed. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Starting full baseline generation for the SFT Test Set...\")\n",
    "await sft_test_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
