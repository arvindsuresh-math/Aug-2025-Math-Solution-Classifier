{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149d9c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./template-preprocessing.ipynb -> markdown-versions/template-preprocessing.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./template-preprocessing.ipynb to markdown\n",
      "[NbConvertApp] Writing 5128 bytes to markdown-versions/template-preprocessing.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./conceptual-error-injection.ipynb -> markdown-versions/conceptual-error-injection.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./conceptual-error-injection.ipynb to markdown\n",
      "[NbConvertApp] Writing 39840 bytes to markdown-versions/conceptual-error-injection.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./manual-error-files-reformatting.ipynb -> markdown-versions/manual-error-files-reformatting.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./manual-error-files-reformatting.ipynb to markdown\n",
      "[NbConvertApp] Support files will be in manual-error-files-reformatting_files/\n",
      "[NbConvertApp] Making directory markdown-versions/manual-error-files-reformatting_files\n",
      "[NbConvertApp] Writing 79192 bytes to markdown-versions/manual-error-files-reformatting.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./temp.ipynb -> markdown-versions/temp.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./temp.ipynb to markdown\n",
      "[NbConvertApp] Writing 6674 bytes to markdown-versions/temp.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./template-generation.ipynb -> markdown-versions/template-generation.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./template-generation.ipynb to markdown\n",
      "[NbConvertApp] Writing 50825 bytes to markdown-versions/template-generation.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./computational-error-injection.ipynb -> markdown-versions/computational-error-injection.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./computational-error-injection.ipynb to markdown\n",
      "[NbConvertApp] Writing 31834 bytes to markdown-versions/computational-error-injection.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./level-1-phi-4-mini.ipynb -> markdown-versions/level-1-phi-4-mini.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./level-1-phi-4-mini.ipynb to markdown\n",
      "[NbConvertApp] Writing 26588 bytes to markdown-versions/level-1-phi-4-mini.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./error-sampling.ipynb -> markdown-versions/error-sampling.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./error-sampling.ipynb to markdown\n",
      "[NbConvertApp] Writing 74961 bytes to markdown-versions/error-sampling.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./template-validation.ipynb -> markdown-versions/template-validation.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./template-validation.ipynb to markdown\n",
      "[NbConvertApp] Writing 20834 bytes to markdown-versions/template-validation.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: ./level-1-dataset-arvind.ipynb -> markdown-versions/level-1-dataset-arvind.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ./level-1-dataset-arvind.ipynb to markdown\n",
      "[NbConvertApp] Support files will be in level-1-dataset-arvind_files/\n",
      "[NbConvertApp] Making directory markdown-versions/level-1-dataset-arvind_files\n",
      "[NbConvertApp] Writing 24374 bytes to markdown-versions/level-1-dataset-arvind.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "notebooks_dir = \".\"\n",
    "output_dir = \"markdown-versions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(notebooks_dir):\n",
    "    if fname.endswith(\".ipynb\"):\n",
    "        input_path = os.path.join(notebooks_dir, fname)\n",
    "        output_path = os.path.join(output_dir, fname.replace(\".ipynb\", \".md\"))\n",
    "        cmd = [\n",
    "            \"jupyter\", \"nbconvert\",\n",
    "            \"--to\", \"markdown\",\n",
    "            \"--output\", os.path.splitext(os.path.basename(output_path))[0],\n",
    "            \"--output-dir\", output_dir,\n",
    "            input_path\n",
    "        ]\n",
    "        print(\"Converting:\", input_path, \"->\", output_path)\n",
    "        subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0613f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Input (Processed Templates): /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/template-generated-processed\n",
      "Output (Generated Errors): /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/computational-errors-generated\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "import inspect\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from typing import Callable, Any, Dict, List\n",
    "from fractions import Fraction as BuiltinFraction\n",
    "import datetime\n",
    "import functools\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "# from datasets import load_dataset, Dataset\n",
    "\n",
    "# --- Path and Directory Definitions ---\n",
    "def find_project_root(marker: str = \".git\") -> Path:\n",
    "    current_path = Path.cwd().resolve()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / marker).exists():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(f\"Could not find project root. Marker '{marker}' not found.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "PROCESSED_TEMPLATE_DIR = DATA_DIR / \"template-generated-processed\"\n",
    "GENERATED_ERRORS_DIR = DATA_DIR / \"computational-errors-generated\"\n",
    "\n",
    "MODELS = ['openai_gpt-4.1', 'google_gemini-2.5-flash']\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Input (Processed Templates): {PROCESSED_TEMPLATE_DIR}\")\n",
    "print(f\"Output (Generated Errors): {GENERATED_ERRORS_DIR}\")\n",
    "\n",
    "# --- Ensure Directories Exist ---\n",
    "PROCESSED_TEMPLATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GENERATED_ERRORS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(GENERATED_ERRORS_DIR / 'computational_error_catalog.csv')\n",
    "sample = df.sample(5, random_state=42)\n",
    "sample.to_csv(GENERATED_ERRORS_DIR / 'catalog_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b64805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / 'sft-datasets' / 'level-1-binary' / 'sft_level1_catalog.csv')\n",
    "\n",
    "sample = df.head()\n",
    "sample.to_csv(DATA_DIR / 'sft-datasets' / 'level-1-binary' / 'sft_level1_catalog_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf6ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier definitions loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load GSM8K Dataset ---\n",
    "from datasets import load_dataset, Dataset\n",
    "GSM8K_TRAIN: Dataset = load_dataset(\"gsm8k\", \"main\")[\"train\"] #type: ignore\n",
    "\n",
    "# --- Tier Definition Functions ---\n",
    "def has_computational_division(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'/\\s*\\d')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def has_float(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'(?<!\\d)\\.\\d+|\\d+\\.\\d+')\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def is_symbolic(solution_text: str) -> bool:\n",
    "    pattern = re.compile(r'^Let [a-zA-Z] ', re.MULTILINE)\n",
    "    return bool(pattern.search(solution_text))\n",
    "\n",
    "def mutually_disjoint_tiers(dataset: Dataset) -> dict[str, list[int]]:\n",
    "    tiers = {}\n",
    "    symbolic_set = set(idx for idx, sample in enumerate(dataset) if is_symbolic(sample.get(\"answer\", \"\")))\n",
    "    non_symbolic_indices = [idx for idx in range(len(dataset)) if idx not in symbolic_set]\n",
    "    tiers[\"tier1\"] = sorted([idx for idx in non_symbolic_indices if not has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "    tiers[\"tier2\"] = sorted([idx for idx in non_symbolic_indices if has_float(dataset[idx].get(\"answer\", \"\")) and not has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "    tiers[\"tier3\"] = sorted([idx for idx in non_symbolic_indices if not has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "    tiers[\"tier4\"] = sorted([idx for idx in non_symbolic_indices if has_float(dataset[idx].get(\"answer\", \"\")) and has_computational_division(dataset[idx].get(\"answer\", \"\"))])\n",
    "    tiers[\"tier5\"] = sorted(list(symbolic_set))\n",
    "    return tiers\n",
    "\n",
    "TIER_LISTS = mutually_disjoint_tiers(GSM8K_TRAIN)\n",
    "print(\"Tier definitions loaded.\")\n",
    "\n",
    "def get_tier(index: int, tier_lists: dict[str, list[int]]) -> str:\n",
    "    found = None\n",
    "    for tier, indices in tier_lists.items():\n",
    "        if index in indices:\n",
    "            found = tier\n",
    "            break\n",
    "    if found:\n",
    "        return found\n",
    "    else:\n",
    "        raise ValueError(f\"Index {index} not found in any tier lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd98a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/manually_generated_errors_final.csv')\n",
    "\n",
    "# create a df with 3 conceptual error samples and 3 computational error samples\n",
    "def create_error_samples(df: pd.DataFrame, num_samples: int = 3) -> pd.DataFrame:\n",
    "    conceptual_errors = df[df['error_type'] == 'conceptual'].sample(n=num_samples, random_state=42)\n",
    "    computational_errors = df[df['error_type'] == 'computational'].sample(n=num_samples, random_state=42)\n",
    "    return pd.concat([conceptual_errors, computational_errors]).reset_index(drop=True)\n",
    "\n",
    "sample_df = create_error_samples(df)\n",
    "# Save the sample DataFrame to a CSV file\n",
    "sample_df.to_csv('../data/sampled_errors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d83a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c64bb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the error samples json file\n",
    "with open(DATA_DIR / 'conceptual_error_samples.json', 'r') as json_file:\n",
    "    conceptual_error_samples = json.load(json_file)\n",
    "\n",
    "# add a \"tier\" field to each error sample\n",
    "for error_type, details in conceptual_error_samples.items():\n",
    "    for sample in details['samples']:\n",
    "        sample['tier'] = get_tier(sample['index'], TIER_LISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb2c1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample in the error samples, add the raw template from DATA_DIR / 'template-generated-raw'\n",
    "\n",
    "for error_type, details in conceptual_error_samples.items():\n",
    "    for sample in details['samples']:\n",
    "        tier = sample['tier']\n",
    "        if tier == 'tier5':\n",
    "            continue  # Skip symbolic tier as it doesn't have raw templates\n",
    "\n",
    "        idx = str(sample['index'])\n",
    "        template_folder = DATA_DIR / 'template-generated-raw' / tier / idx\n",
    "\n",
    "        # Choose a random template file from the folder\n",
    "        template_files = list(template_folder.glob('*.txt'))\n",
    "        if not template_files:\n",
    "            raise FileNotFoundError(f\"No template files found in {template_folder}\")\n",
    "        file = template_files[0]  # For simplicity, just take the first file\n",
    "        with open(file, 'r') as f:\n",
    "            sample['formalization_template'] = json.loads(f.read())\n",
    "\n",
    "# Save the updated error samples with tiers back to the JSON file\n",
    "with open(DATA_DIR / 'conceptual_error_samples_with_tiers.json', 'w') as json_file:\n",
    "    json.dump(conceptual_error_samples, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af497a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptual_error_samples_small = copy.deepcopy(conceptual_error_samples)\n",
    "\n",
    "# Keep only 3 samples per error type\n",
    "for error_type, details in conceptual_error_samples_small.items():\n",
    "    details['samples'] = details['samples'][:3]\n",
    "\n",
    "# Save the small error samples to a new JSON file\n",
    "with open(DATA_DIR / 'conceptual_error_samples_small.json', 'w') as json_file:\n",
    "    json.dump(conceptual_error_samples_small, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
