{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853a5fdc",
   "metadata": {},
   "source": [
    "### Basic confuguration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9998035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n",
      "API clients initialized successfully.\n",
      "API concurrency limits set to: {'google': 2, 'anthropic': 2, 'openai': 2}\n",
      "Available models: ['anthropic_claude-sonnet-4-20250514', 'google_gemini-2.5-pro']\n",
      "Source directory: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_raw\n",
      "Destination directory for cleaned JSON: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json\n",
      "Project root found at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "Data directory found at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data\n",
      "Sample manifests directory found at: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sample_manifests\n",
      "Raw manifest output directory set to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_raw\n",
      "Source directory for raw manifests: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_raw\n",
      "Destination directory for manifest json files: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- API Client and Concurrency Configuration --- #\n",
    "\n",
    "# This must be done once per kernel to allow asyncio to run in a Jupyter notebook..\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load API Keys from .env file\n",
    "load_dotenv()\n",
    "print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "# Initialize Asynchronous API Clients\n",
    "try:\n",
    "    openai_client_async = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    anthropic_client_async = AsyncClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    print(\"API clients initialized successfully.\")\n",
    "except TypeError:\n",
    "    print(\"API key not found for one or more services. Please check your .env file.\")\n",
    "    # Assign None to prevent errors in subsequent cells\n",
    "    openai_client_async = None\n",
    "    anthropic_client_async = None\n",
    "\n",
    "# Define API Concurrency Limits to prevent 429 \"Too Many Requests\" errors.\n",
    "API_CONCURRENCY_LIMITS = {\n",
    "    \"google\": 2,    # Gemini API has low RPM limits, so keep this low.\n",
    "    \"anthropic\": 2, # Anthropic's token-based limits are complex, 2 is a safe start.\n",
    "    \"openai\": 2,    # OpenAI APIs are generally more permissive.\n",
    "}\n",
    "print(f\"API concurrency limits set to: {API_CONCURRENCY_LIMITS}\")\n",
    "\n",
    "MODEL_DICT = {\n",
    "    \"anthropic\": \"claude-sonnet-4-20250514\",\n",
    "    \"google\": \"gemini-2.5-pro\"}\n",
    "\n",
    "MODELS = [f\"{provider}_{model}\" for provider, model in MODEL_DICT.items()]\n",
    "print(f\"Available models: {MODELS}\")\n",
    "\n",
    "\n",
    "# --- Set directories and paths --- #\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "SAMPLE_MANIFESTS_DIR = DATA_DIR / 'sample_manifests'\n",
    "MANIFEST_OUTPUT_DIR = DATA_DIR / 'generated_manifests_raw'\n",
    "MANIFEST_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SOURCE_DIR = DATA_DIR / 'generated_manifests_raw'\n",
    "DEST_DIR = DATA_DIR / 'generated_manifests_json'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Source directory: {SOURCE_DIR}\")\n",
    "print(f\"Destination directory for cleaned JSON: {DEST_DIR}\")\n",
    "\n",
    "print(f\"Project root found at: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory found at: {DATA_DIR}\")\n",
    "print(f\"Sample manifests directory found at: {SAMPLE_MANIFESTS_DIR}\")\n",
    "print(f\"Raw manifest output directory set to: {MANIFEST_OUTPUT_DIR}\")\n",
    "print(f\"Source directory for raw manifests: {SOURCE_DIR}\")\n",
    "print(f\"Destination directory for manifest json files: {DEST_DIR}\")\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "GSM8K_TRAIN = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "# Few shot example indices (chosen for maximum diversity)\n",
    "EXAMPLE_INDICES = [54, 72, 310]\n",
    "\n",
    "# Indices to generate manifests for\n",
    "INDICES_TO_GENERATE = list(range(10)) \n",
    "\n",
    "\n",
    "# --- Helper Functions for Manifest loading and displaying --- #\n",
    "\n",
    "def build_solution_mapping(\n",
    "        index: int, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN,\n",
    "        exclude_FA: bool = True\n",
    "    ) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts the natural language solution for a given problem index,\n",
    "    cleans it, and structures it into a line-numbered dictionary.\n",
    "    \"\"\"\n",
    "    solution_mapping = {}\n",
    "    solution_text = dataset[index][\"answer\"]\n",
    "    lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "\n",
    "    # Improved regex to handle commas in the final answer\n",
    "    if lines and re.match(r\"^####\\s*[\\d\\.,]+$\", lines[-1]):\n",
    "        solution_mapping[\"FA\"] = lines.pop(-1).strip()\n",
    "\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        solution_mapping[f\"L{i}\"] = line\n",
    "\n",
    "    if exclude_FA and \"FA\" in solution_mapping:\n",
    "        del solution_mapping[\"FA\"]\n",
    "\n",
    "    return solution_mapping\n",
    "\n",
    "def load_manifest(index: int, manifest_dir: Path):\n",
    "    \"\"\"Loads the manifest for a given index.\"\"\"\n",
    "    manifest_path = manifest_dir / f'_{index}_alt.json'\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f\"Manifest for index {index} not found at {manifest_path}\")\n",
    "    with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def display_manifest(\n",
    "        index: int, \n",
    "        manifest_dir: Path, \n",
    "        dataset: 'datasets.Dataset' = GSM8K_TRAIN):\n",
    "    \"\"\"Loads and displays the final, streamlined Formalization Manifest.\"\"\"\n",
    "    try:\n",
    "        manifest = load_manifest(index, manifest_dir)\n",
    "\n",
    "        # --- Extract info and display Top-Level ---\n",
    "        code = manifest.get('function_code', '# Code not found')\n",
    "        display(Markdown(f\"# Manifest for Index: **{index}**\"))\n",
    "        display(Markdown(\"## Question\"))\n",
    "        display(Markdown(f\"> {dataset[index]['question']}\"))\n",
    "\n",
    "        # --- Display Function Code ---\n",
    "        display(Markdown(\"## Function Code\"))\n",
    "        display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "        # --- Display Logical Steps in a DataFrame ---\n",
    "        display(Markdown(\"## Logical Steps\"))\n",
    "        steps = manifest.get('logical_steps', [])\n",
    "        if not steps:\n",
    "            print(\"No logical steps found in the manifest.\")\n",
    "            return\n",
    "        \n",
    "        df_steps = pd.DataFrame(steps)\n",
    "        original_solution = build_solution_mapping(\n",
    "            index=index, \n",
    "            dataset=dataset, \n",
    "            exclude_FA=True\n",
    "        )\n",
    "        \n",
    "        df_steps['original_solution_line'] = df_steps['line_number'].apply(\n",
    "            lambda ln: original_solution.get(ln, \"N/A\")\n",
    "        )\n",
    "        \n",
    "        column_order = [\n",
    "            'line_number', \n",
    "            'original_solution_line',\n",
    "            'solution_line_template',\n",
    "            'new_inputs',\n",
    "            'output_variable'\n",
    "        ]\n",
    "        \n",
    "        existing_columns_ordered = [col for col in column_order if col in df_steps.columns]\n",
    "        df_steps = df_steps[existing_columns_ordered]\n",
    "        \n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display(df_steps)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def display_generated_manifest(\n",
    "        index: int, \n",
    "        model_name: str,\n",
    "        manifest_dir: Path = DEST_DIR):\n",
    "    \"\"\"\n",
    "    Loads and displays a generated manifest from a specific model and index.\n",
    "\n",
    "    Args:\n",
    "        index: The problem index.\n",
    "        model_name: The name of the model (e.g., 'openai_gpt-4.1-mini').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the path to the specific JSON file\n",
    "        manifest_path = manifest_dir / str(index) / f\"{model_name}.json\"\n",
    "        \n",
    "        if not manifest_path.exists():\n",
    "            print(f\"❌ Error: Manifest file not found at {manifest_path}\")\n",
    "            return\n",
    "\n",
    "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "            manifest = json.load(f)\n",
    "\n",
    "        # --- Display Top-Level Information ---\n",
    "        display(Markdown(f\"# Generated Manifest for Index: `{index}` | Model: `{model_name}`\"))\n",
    "        display(Markdown(\"## Question\"))\n",
    "        display(Markdown(f\"> {GSM8K_TRAIN[index]['question']}\"))\n",
    "\n",
    "        # --- Display Function Code ---\n",
    "        code = manifest.get('function_code', '# Code not found')\n",
    "        display(Markdown(\"## Function Code\"))\n",
    "        display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "        # --- Display Logical Steps in a DataFrame ---\n",
    "        display(Markdown(\"## Logical Steps\"))\n",
    "        steps = manifest.get('logical_steps', [])\n",
    "        if not steps:\n",
    "            print(\"No logical steps found in the manifest.\")\n",
    "            return\n",
    "        \n",
    "        df_steps = pd.DataFrame(steps)\n",
    "        \n",
    "        # Get the original solution text for comparison\n",
    "        original_solution = build_solution_mapping(index=index, exclude_FA=True)\n",
    "        \n",
    "        df_steps['original_solution_line'] = df_steps['line_number'].apply(\n",
    "            lambda ln: original_solution.get(ln, \"N/A\")\n",
    "        )\n",
    "        \n",
    "        column_order = [\n",
    "            'line_number', \n",
    "            'original_solution_line',\n",
    "            'solution_line_template',\n",
    "            'new_inputs',\n",
    "            'output_variable'\n",
    "        ]\n",
    "        \n",
    "        # Ensure columns exist before reordering\n",
    "        existing_columns = [col for col in column_order if col in df_steps.columns]\n",
    "        df_steps = df_steps[existing_columns]\n",
    "        \n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display(df_steps)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while displaying the manifest for index {index}, model {model_name}: {e}\")\n",
    "\n",
    "def process_and_clean_manifests(\n",
    "    source_dir: Path = SOURCE_DIR,\n",
    "    dest_dir: Path = DEST_DIR\n",
    "):\n",
    "    \"\"\"\n",
    "    Traverses the source directory, cleans raw .txt model outputs,\n",
    "    validates them as JSON, and writes them as .json files to the destination.\n",
    "\n",
    "    - If a ```json ... ``` fence is found, its content is extracted.\n",
    "    - Otherwise, the entire file content is used.\n",
    "    - The extracted string is validated as JSON before being written.\n",
    "    \"\"\"\n",
    "    if not source_dir.is_dir():\n",
    "        print(f\"❌ Error: Source directory '{source_dir}' not found.\")\n",
    "        return\n",
    "\n",
    "    files_processed = 0\n",
    "    files_written = 0\n",
    "    files_failed = 0\n",
    "\n",
    "    # Regex to find a JSON block fenced by ```json ... ``` or ``` ... ```\n",
    "    # It is non-greedy and handles potential whitespace.\n",
    "    _JSON_FENCE_RE = re.compile(r\"```(?:json)?\\s*\\r?\\n(.*?)\\r?\\n```\", re.DOTALL)\n",
    "    \n",
    "    # Recursively find all .txt files in the source directory and its subfolders\n",
    "    for txt_path in sorted(source_dir.rglob(\"*.txt\")):\n",
    "        files_processed += 1\n",
    "        cleaned_json_str = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Determine the corresponding output path, preserving subdirectories\n",
    "            relative_path = txt_path.relative_to(source_dir)\n",
    "            dest_path = dest_dir / relative_path.with_suffix('.json')\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Read the raw content from the source file\n",
    "            raw_content = txt_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "            # Try to find and extract a fenced JSON block\n",
    "            match = _JSON_FENCE_RE.search(raw_content)\n",
    "            if match:\n",
    "                cleaned_json_str = match.group(1).strip()\n",
    "                print(f\"✓ Extracted fenced JSON from: {relative_path}\")\n",
    "            else:\n",
    "                # If no fence is found, assume the whole file is the JSON string\n",
    "                cleaned_json_str = raw_content.strip()\n",
    "                print(f\"⚠ No fence found, using full file: {relative_path}\")\n",
    "\n",
    "            # Validate that the extracted string is valid JSON before writing\n",
    "            json.loads(cleaned_json_str) # This will raise an error if invalid\n",
    "            \n",
    "            # Write the clean, valid JSON to the destination file\n",
    "            dest_path.write_text(cleaned_json_str, encoding=\"utf-8\")\n",
    "            files_written += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"❌ Invalid JSON content in: {relative_path}. File not written.\")\n",
    "            files_failed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ An unexpected error occurred while processing {relative_path}: {e}\")\n",
    "            files_failed += 1\n",
    "    \n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    print(f\"Total files found: {files_processed}\")\n",
    "    print(f\"Successfully written: {files_written}\")\n",
    "    print(f\"Failed to process: {files_failed}\")\n",
    "\n",
    "def concat_json_texts(\n",
    "        indices: List[int], \n",
    "        model_name: str, \n",
    "        parent_dir: Path):\n",
    "    \"\"\"\n",
    "    Reads JSON files for the given indices and model_name from parent_dir,\n",
    "    concatenates their text content, and returns as a single string.\n",
    "    \"\"\"\n",
    "    parent_dir = Path(parent_dir)\n",
    "    all_text = []\n",
    "    for idx in indices:\n",
    "        file_path = parent_dir / str(idx) / f\"{model_name}.json\"\n",
    "        if file_path.exists():\n",
    "            text = file_path.read_text(encoding=\"utf-8\")\n",
    "            all_text.append(text)\n",
    "        else:\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def save_concatenated_outputs(\n",
    "        indices: List[int] = INDICES_TO_GENERATE, \n",
    "        models: List[str] = MODELS, \n",
    "        parent_dir: Path = DEST_DIR):\n",
    "    \"\"\"\n",
    "    Concatenates the JSON outputs for each model and saves them as text files.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names to process.\n",
    "        parent_dir: Directory where the JSON files are stored.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for model in models:\n",
    "        try:\n",
    "            outputs[model] = concat_json_texts(\n",
    "                indices=indices, \n",
    "                model_name=model, \n",
    "                parent_dir=parent_dir\n",
    "            )\n",
    "            print(f\"Concatenated output for model {model} with {len(outputs[model])} characters.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing model {model}: {e}\")\n",
    "\n",
    "    # Save as text files\n",
    "    for model, text in outputs.items():\n",
    "        output_path = SAMPLE_MANIFESTS_DIR / f\"{model}_output.txt\"\n",
    "        output_path.write_text(text, encoding='utf-8')\n",
    "        print(f\"Saved concatenated output for {model} to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8514b",
   "metadata": {},
   "source": [
    "### Visualization of complete manifests for chosen examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd37a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in EXAMPLE_INDICES:\n",
    "#     display_manifest(index, SAMPLE_MANIFESTS_DIR)\n",
    "#     print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f85cd9",
   "metadata": {},
   "source": [
    "### Assembly of user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36f02775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static user prompt prefix generated successfully. Length: 18019 characters.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a data formalization expert who excels in mathematical reasoning and writing python code. You will be presented with math word problems accompanied by step-by-step natural language solutions. You goal is to carefully and meticulously analyze the given question and solution, and formalize it by converting it into a structured json object that deconstructs the logic of the solution.\n",
    "\n",
    "You MUST follow all rules and formatting instructions provided in the user prompt without deviation. Your entire output MUST be a single JSON object wrapped in ```json ... ```. Do not include any text or explanation before or after the JSON object.\"\"\"\n",
    "\n",
    "FORMAT_GUIDELINES = \"\"\"In the TASK below, you will be given a math problem and its corresponding step-by-step solution. Each step in the solution is numbered (e.g. \"L1\", \"L2\" and so on), and many of the steps include calculator annotations (e.g. \"<<20*0.1=2>>\"). Your goal is to convert this information into a structured JSON object according to the following schema and detailed instructions.\n",
    "\n",
    "# JSON Schema Definition\n",
    "\n",
    "Your output must adhere to the following JSON structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"function_code\": \"A single string containing a complete, self-contained Python function that constitutes an end-to-end formalization of the solution.\",\n",
    "  \"logical_steps\": [\n",
    "    {\n",
    "      \"line_number\": \"The line number from the original solution (e.g., 'L1', 'L2').\",\n",
    "      \"new_inputs\": \"A (possibly empty) list of strings, where each string is the name of a variable being defined for the first time in this step.\",\n",
    "      \"output_variable\": \"The name of the variable being assigned as the result of the main computation in this step.\",\n",
    "      \"solution_line_template\": \"The complete original line from the solution, including the calculator annotation, with all computational numbers replaced by `{variable_name}` placeholders.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Detailed Field Instructions\n",
    "\n",
    "## \"function_code\"\n",
    "\n",
    "This string must contain a Python function with the following characteristics:\n",
    "\n",
    "1. **Conditional Imports:** The function_code string should contain no imports, with one exception: if the function body uses the Fraction class (e.g., rate = Fraction(1, 10)), then the very first line of the function_code string MUST be from fractions import Fraction. If not, then the very first line MUST be the function definition (i.e. `def solve():`).\n",
    "\n",
    "2.  **Function Naming & Docstring:** The function must be named `solve`, and it should not have any args. It must begin with a docstring that has exactly two lines:\n",
    "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
    "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
    "\n",
    "3. **Line comments:** For each solution line that is used to compute the final answer, include a comment of the form `# L1`, `# L2` and so on, which references the line number. \n",
    "    *   Such a comment must immediately be followed by a code block that precisely formalizes the corresponding solution line. \n",
    "    *   If a solution line does not contain any computation relevant to the final answer, then omit it completely from the function code and do NOT add a corresponding line comment.\n",
    "\n",
    "4. **Code blocks:** Each code block must consist of the following:\n",
    "    *   First, define the `new_inputs`. These are the variables (if any) needed for the computation in the solution line, which have not already been defined, and whose values are stated in (or can be extracted from) the `question`. Each new input variable definition MUST be followed on the same line by a comment (`#`) that quotes or refers to the phrase in the `question` from which it is extracted.\n",
    "    *   Second, there should be EXACTLY ONE line of code which formalizes the computation in the solution line and assigns the resulting value to a new variable (this is the `output_variable`).\n",
    "\n",
    "5. **The Direct Substitution Rule:** This is the most important rule, which ensures that the `nl_template` is purely identical to the original solution line except that numerical values in computations have been replaced with variable placeholders: You MUST define variables in such a way that they can be DIRECTLY SUBSTITUTED into the solution text without changing any operators. For example:\n",
    "    *   If the solution line has a computation like `... / 5`, you MUST define a variable like `var = 5`.\n",
    "    *   If the solution line has a computation like `... * 1/5`, you MUST define a variable like `var = Fraction(1, 5)`. \n",
    "    *   If the solution line has a computation like `... * 0.2`, you MUST define a variable like `var = 0.2`.\n",
    "\n",
    "6. **Final Answer:** The line that assigns the final result to the `answer` variable must be immediately preceded by a line containing only the comment `# FA`. The last line of the function must always return the `answer` variable.\n",
    "\n",
    "## \"solution_line_template\"\n",
    "\n",
    "These artifacts will serve as precise links between the solution line and the code line. \n",
    "*   The template should be EXACTLY identical to the original solution line, with the ONLY CHANGES being that every numerical value used in a computation is replaced by its corresponding `{variable_name}` placeholder. This applies to the entire content of the solution line, including the inside and outside of the calculator annotations. \n",
    "*   In particular, EVERY SINGLE numerical value appearing inside the calculator annotation MUST be replaced with a `{variable_name}` placeholder.\n",
    "*   Note: The Direct Substitution Rule will ensure that for correctly defined variables, it will be possible to replace the numerical values with variable name placeholders while leaving all surrounding text, symbols, and operators unchanged. \n",
    "*   Thus, in a correct `nl_template`, the calculator annotation will not contain any numerical values, and moreover, replacing each `{variable_name}` by its value should exactly recover the original solution line, including the original calculator annotation. \n",
    "*   Note: If a number appears in different forms (e.g., as \"10%\" in the narrative and as \".1\" in the calculation), only the form that appears in the calculation should be replaced with a placeholder.\n",
    "\"\"\"\n",
    "\n",
    "def assemble_example(\n",
    "    index: int, \n",
    "    manifest_dir: Path, \n",
    "    dataset: 'datasets.Dataset'\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Assembles a single, complete few-shot example string (Input + Output). \n",
    "    Returns a formatted string for one few-shot example.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Construct the Input block\n",
    "        question = dataset[index]['question']\n",
    "        solution_map = build_solution_mapping(index, dataset, exclude_FA=True)\n",
    "        \n",
    "        input_data = {\n",
    "            \"index\": index,\n",
    "            \"question\": question,\n",
    "            \"solution_mapping\": solution_map\n",
    "        }\n",
    "        \n",
    "        input_json_str = json.dumps(input_data, indent=2)\n",
    "        input_block = f\"**Input:**\\n\\n{input_json_str}\\n\"\n",
    "\n",
    "        # 2. Construct the Output block\n",
    "        manifest_path = manifest_dir / f'_{index}_alt.json'\n",
    "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "            manifest_content = json.load(f)\n",
    "        \n",
    "        output_json_str = json.dumps(manifest_content, indent=2)\n",
    "        output_block = f\"**Output:**\\n```json\\n{output_json_str}\\n```\"\n",
    "\n",
    "        return f\"{input_block}\\n\\n{output_block}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to assemble example for index {index}. Error: {e}\")\n",
    "        return \"\" # Return empty string on failure\n",
    "\n",
    "def assemble_static_prefix(\n",
    "    format_guidelines: str = FORMAT_GUIDELINES,\n",
    "    example_indices: List[int] = EXAMPLE_INDICES,\n",
    "    manifest_dir: Path = SAMPLE_MANIFESTS_DIR,\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Constructs the static user prompt prefix, including guidelines and few-shot examples.\n",
    "\n",
    "    Args:\n",
    "        format_guidelines: The string containing the rules and schema.\n",
    "        example_indices: A list of integer indices for the few-shot examples.\n",
    "        manifest_dir: The path to the manifest directory.\n",
    "        dataset: The loaded Hugging Face dataset.\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the complete user prompt.\n",
    "    \"\"\"\n",
    "    # 1. Start with the guidelines\n",
    "    prompt_parts = [format_guidelines, \"\\n---\\n\", \"\\n### Examples\"]\n",
    "\n",
    "    # 2. Assemble and append each few-shot example\n",
    "    for index in example_indices:\n",
    "        example_str = assemble_example(index, manifest_dir, dataset)\n",
    "        if example_str:\n",
    "            prompt_parts.append(f\"\\n\\n---\\n\\n{example_str}\")\n",
    "            \n",
    "    return \"\".join(prompt_parts) # Use join for efficiency\n",
    "\n",
    "def assemble_user_prompt(\n",
    "    index_to_generate: int,\n",
    "    static_prefix: str,\n",
    "    dataset: 'datasets.Dataset' = GSM8K_TRAIN\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Appends the final task block to the static prefix to create a complete user prompt.\n",
    "\n",
    "    Args:\n",
    "        index_to_generate: The new problem index to generate a manifest for.\n",
    "        static_prefix: The pre-computed string containing guidelines and few-shot examples.\n",
    "        dataset: The loaded Hugging Face dataset.\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the complete user prompt, ready for an API call.\n",
    "    \"\"\"\n",
    "    # 1. Assemble the final input block for the new task\n",
    "    try:\n",
    "        question = dataset[index_to_generate]['question']\n",
    "        solution_map = build_solution_mapping(\n",
    "            index=index_to_generate, \n",
    "            dataset=dataset, \n",
    "            exclude_FA=True\n",
    "        )\n",
    "        \n",
    "        task_input_data = {\n",
    "            \"index\": index_to_generate,\n",
    "            \"question\": question,\n",
    "            \"solution_mapping\": solution_map\n",
    "        }\n",
    "        task_input_json_str = json.dumps(task_input_data, indent=2)\n",
    "        \n",
    "        # This is the final part of the prompt that asks the model for the new output\n",
    "        task_block = f\"\\n\\n--- TASK ---\\n\\n**Input:**\\n\\n{task_input_json_str}\\n\\n**Output:**\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not assemble task block for index {index_to_generate}. Error: {e}\")\n",
    "        return \"\" # Return empty string on failure\n",
    "\n",
    "    # 2. Combine the static prefix with the new task block\n",
    "    return static_prefix + task_block\n",
    "\n",
    "# Generate the static prefix ONCE.\n",
    "STATIC_USER_PROMPT_PREFIX = assemble_static_prefix()\n",
    "print(f\"Static user prompt prefix generated successfully. Length: {len(STATIC_USER_PROMPT_PREFIX)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed9a5d",
   "metadata": {},
   "source": [
    "### Visualizing a sample full user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a0a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX_TO_GENERATE = 49\n",
    "\n",
    "# full_user_prompt = assemble_user_prompt(\n",
    "#     index_to_generate=INDEX_TO_GENERATE,\n",
    "#     static_prefix=STATIC_USER_PROMPT_PREFIX\n",
    "# )\n",
    "\n",
    "# print(f\"Assembled user prompt for Index {INDEX_TO_GENERATE}. Length: {len(full_user_prompt)} characters.\")\n",
    "# print(\"\\n--- Full User Prompt Below ---\\n\")\n",
    "# print(full_user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719bf46",
   "metadata": {},
   "source": [
    "### Main code for making API calls to generate manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ffc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Helper Functions (Unchanged) ---\n",
    "# These two helpers are generic and can be used by any provider's function.\n",
    "\n",
    "async def _anthropic_throttle(tokens_needed: int):\n",
    "    # (Code for this function is unchanged)\n",
    "    global _anthropic_bucket\n",
    "    while True:\n",
    "        now = time.monotonic()\n",
    "        if now >= _anthropic_bucket[\"reset_at\"]:\n",
    "            _anthropic_bucket = {\"tokens\": 50_000, \"reset_at\": now + 60}\n",
    "        if tokens_needed <= _anthropic_bucket[\"tokens\"]:\n",
    "            _anthropic_bucket[\"tokens\"] -= tokens_needed\n",
    "            return\n",
    "        else:\n",
    "            to_sleep = _anthropic_bucket[\"reset_at\"] - now\n",
    "            await asyncio.sleep(max(to_sleep, 0.01))\n",
    "\n",
    "async def with_api_retries(send_coroutine_factory, *, max_attempts: int = 10, base_wait_seconds: int = 5):\n",
    "    # (Code for this function is unchanged)\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return await send_coroutine_factory()\n",
    "        except (openai.RateLimitError, anthropic.RateLimitError, Exception) as e:\n",
    "            if isinstance(e, (openai.RateLimitError, anthropic.RateLimitError)) or \"429\" in str(e):\n",
    "                if attempt == max_attempts - 1:\n",
    "                    raise\n",
    "                wait_time = base_wait_seconds * (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"🕒 Rate limit error encountered. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_attempts})\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "# --- 2. NEW: Provider-Specific API Calling Functions ---\n",
    "\n",
    "async def call_openai_async(model: str, system_prompt: str, user_prompt: str) -> (str, Dict[str, int]):\n",
    "    \"\"\"Handles an API call to OpenAI.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    response = await with_api_retries(lambda: openai_client_async.chat.completions.create(\n",
    "        model=model, messages=messages, temperature=0.1, max_tokens=4000, response_format={\"type\": \"json_object\"}\n",
    "    ))\n",
    "    \n",
    "    text_response = response.choices[0].message.content\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.prompt_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.completion_tokens\n",
    "        if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details:\n",
    "             usage[\"cached_tokens\"] = response.usage.prompt_tokens_details.get(\"cached_tokens\", 0)\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_google_async(model: str, system_prompt: str, user_prompt: str) -> (str, Dict[str, int]):\n",
    "    \"\"\"Handles an API call to Google.\"\"\"\n",
    "    gemini = genai.GenerativeModel(model_name=model, system_instruction=system_prompt)\n",
    "    cfg = genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4000)\n",
    "    response = await with_api_retries(lambda: gemini.generate_content_async(user_prompt, generation_config=cfg))\n",
    "\n",
    "    text_response = response.text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage_metadata:\n",
    "        usage[\"input_tokens\"] = response.usage_metadata.prompt_token_count\n",
    "        usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "    return text_response, usage\n",
    "\n",
    "async def call_anthropic_async(model: str, system_prompt: str, user_prompt: str) -> (str, Dict[str, int]):\n",
    "    \"\"\"Handles an API call to Anthropic, including prompt caching.\"\"\"\n",
    "    system_block = {\"type\": \"text\", \"text\": system_prompt, \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "    \n",
    "    est_tokens = math.ceil(1.2 * len(system_prompt.split()))\n",
    "    await _anthropic_throttle(est_tokens)\n",
    "\n",
    "    response = await with_api_retries(lambda: anthropic_client_async.messages.create(\n",
    "        model=model, max_tokens=4000, temperature=0.1,\n",
    "        system=[system_block], messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    ))\n",
    "\n",
    "    text_response = response.content[0].text\n",
    "    usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "    if response.usage:\n",
    "        usage[\"input_tokens\"] = response.usage.input_tokens\n",
    "        usage[\"output_tokens\"] = response.usage.output_tokens\n",
    "        usage[\"cached_tokens\"] = response.usage.cache_read_input_tokens if response.usage.cache_read_input_tokens else 0\n",
    "    return text_response, usage\n",
    "\n",
    "\n",
    "# --- 3. UPDATED: Per-Problem Orchestration (Dispatcher Logic) ---\n",
    "\n",
    "async def run_one_problem_async(\n",
    "    index: int, \n",
    "    static_prefix: str,\n",
    "    system_prompt: str,\n",
    "    model_dict: Dict[str, str],\n",
    "    provider_sems: Dict[str, asyncio.Semaphore], \n",
    "    output_dir: Path,\n",
    "    pbar: tqdm\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generates manifests for a single problem and returns a list of result dictionaries.\n",
    "    \"\"\"\n",
    "    user_prompt = assemble_user_prompt(index, static_prefix=static_prefix)\n",
    "    \n",
    "    # This list is now local to this function\n",
    "    problem_results = []\n",
    "    \n",
    "    tasks = []\n",
    "    for provider, model in model_dict.items():\n",
    "        async with provider_sems[provider]: # Acquire semaphore before creating the task\n",
    "            if provider == \"openai\":\n",
    "                coro = call_openai_async(model, system_prompt, user_prompt)\n",
    "            elif provider == \"google\":\n",
    "                coro = call_google_async(model, system_prompt, user_prompt)\n",
    "            elif provider == \"anthropic\":\n",
    "                coro = call_anthropic_async(model, system_prompt, user_prompt)\n",
    "            else:\n",
    "                # Create a coroutine that will immediately raise an error\n",
    "                async def unknown_provider(): raise ValueError(f\"Unknown provider: {provider}\")\n",
    "                coro = unknown_provider()\n",
    "        \n",
    "            task = asyncio.create_task(coro)\n",
    "            task.meta = {\"provider\": provider, \"model\": model, \"index\": index, \"start_time\": time.time()}\n",
    "            tasks.append(task)\n",
    "        \n",
    "    task_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    for task, result in zip(tasks, task_results):\n",
    "        meta = task.meta\n",
    "        elapsed = time.time() - meta[\"start_time\"]\n",
    "        status = \"Failed\"\n",
    "        usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "        \n",
    "        output_path = output_dir / str(meta['index']) / f\"{meta['provider']}_{meta['model']}.txt\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            error_message = f\"--- ERROR ---\\nIndex: {meta['index']}, Model: {meta['model']}\\n{type(result).__name__}: {result}\"\n",
    "            output_path.write_text(error_message, encoding='utf-8')\n",
    "            print(f\"❌ Error for Index {meta['index']}, Model {meta['model']}: {type(result).__name__}\")\n",
    "        else:\n",
    "            text_response, usage = result\n",
    "            output_path.write_text(text_response, encoding='utf-8')\n",
    "            status = \"Success\"\n",
    "        \n",
    "        # Append to the local list instead of a shared one\n",
    "        problem_results.append({\n",
    "            \"provider\": meta[\"provider\"], \"model\": meta[\"model\"], \"index\": meta[\"index\"],\n",
    "            \"status\": status, \"time_s\": round(elapsed, 2),\n",
    "            \"input_tokens\": usage[\"input_tokens\"], \"output_tokens\": usage[\"output_tokens\"],\n",
    "            \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "            \"utc_completed\": datetime.datetime.now(datetime.timezone.utc).isoformat(timespec=\"seconds\")\n",
    "        })\n",
    "    \n",
    "    pbar.update(1)\n",
    "    # Explicitly return the results for this specific problem\n",
    "    return problem_results\n",
    "\n",
    "\n",
    "# --- 4. UPDATED: Main Batch Generation Function ---\n",
    "\n",
    "async def generate_manifests_parallel(\n",
    "    indices_to_generate: List[int],\n",
    "    model_dict: Dict[str, str] = MODEL_DICT,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    output_dir: Path = MANIFEST_OUTPUT_DIR,\n",
    "    concurrency_limits: Dict[str, int] = API_CONCURRENCY_LIMITS\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs the manifest generation process and returns a DataFrame with performance stats.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Manifest Generation ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    static_prefix = assemble_static_prefix()\n",
    "    provider_semaphores = {prov: asyncio.Semaphore(limit) for prov, limit in concurrency_limits.items()}\n",
    "    \n",
    "    with tqdm(total=len(indices_to_generate), desc=\"Generating Manifests\") as pbar:\n",
    "        problem_tasks = [\n",
    "            run_one_problem_async(\n",
    "                index=idx, static_prefix=static_prefix, system_prompt=system_prompt,\n",
    "                model_dict=model_dict, provider_sems=provider_semaphores,\n",
    "                output_dir=output_dir, pbar=pbar\n",
    "            )\n",
    "            for idx in indices_to_generate\n",
    "        ]\n",
    "        # This will now be a list of lists, e.g., [[results_for_p0], [results_for_p1], ...]\n",
    "        all_results = await asyncio.gather(*problem_tasks)\n",
    "\n",
    "    # Flatten the list of lists into a single list of result dictionaries\n",
    "    flat_results = [item for sublist in all_results for item in sublist]\n",
    "\n",
    "    # Create and save the performance DataFrame\n",
    "    df = pd.DataFrame(flat_results)\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"generation_performance_{run_ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Manifest Generation Complete ---\")\n",
    "    print(f\"Processed {len(indices_to_generate)} indices in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Performance log saved to: {csv_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef86172",
   "metadata": {},
   "source": [
    "### Run the generation for a chosen set of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee586e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Manifest Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c988a6ff1e634091a4bc0637516f8223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Manifests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error for Index 0, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 2, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 7, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 3, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 6, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 4, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 1, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 8, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 5, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 9, Model claude-sonnet-4-20250514: NameError\n",
      "❌ Error for Index 9, Model gemini-2.5-pro: ValueError\n",
      "\n",
      "--- Manifest Generation Complete ---\n",
      "Processed 10 indices in 35.74 seconds.\n",
      "Performance log saved to: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_raw/generation_performance_20250706_195404.csv\n",
      "\n",
      "--- Performance Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>status</th>\n",
       "      <th>time_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>cached_tokens</th>\n",
       "      <th>utc_completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>0</td>\n",
       "      <td>Failed</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>16.82</td>\n",
       "      <td>5622</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>1</td>\n",
       "      <td>Failed</td>\n",
       "      <td>21.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>21.55</td>\n",
       "      <td>5623</td>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>2</td>\n",
       "      <td>Failed</td>\n",
       "      <td>17.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:46+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>17.84</td>\n",
       "      <td>5696</td>\n",
       "      <td>573</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:46+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>3</td>\n",
       "      <td>Failed</td>\n",
       "      <td>19.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>19.76</td>\n",
       "      <td>5722</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>4</td>\n",
       "      <td>Failed</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>20.42</td>\n",
       "      <td>5638</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>5</td>\n",
       "      <td>Failed</td>\n",
       "      <td>30.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>5</td>\n",
       "      <td>Success</td>\n",
       "      <td>30.56</td>\n",
       "      <td>5778</td>\n",
       "      <td>922</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>6</td>\n",
       "      <td>Failed</td>\n",
       "      <td>19.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>6</td>\n",
       "      <td>Success</td>\n",
       "      <td>19.96</td>\n",
       "      <td>5680</td>\n",
       "      <td>627</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>7</td>\n",
       "      <td>Failed</td>\n",
       "      <td>18.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>7</td>\n",
       "      <td>Success</td>\n",
       "      <td>18.01</td>\n",
       "      <td>5751</td>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>8</td>\n",
       "      <td>Failed</td>\n",
       "      <td>29.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>8</td>\n",
       "      <td>Success</td>\n",
       "      <td>29.56</td>\n",
       "      <td>5786</td>\n",
       "      <td>633</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:53:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-sonnet-4-20250514</td>\n",
       "      <td>9</td>\n",
       "      <td>Failed</td>\n",
       "      <td>35.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:54:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>9</td>\n",
       "      <td>Failed</td>\n",
       "      <td>35.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-06T19:54:04+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider                     model  index   status  time_s  input_tokens  \\\n",
       "0   anthropic  claude-sonnet-4-20250514      0   Failed   16.82             0   \n",
       "1      google            gemini-2.5-pro      0  Success   16.82          5622   \n",
       "2   anthropic  claude-sonnet-4-20250514      1   Failed   21.55             0   \n",
       "3      google            gemini-2.5-pro      1  Success   21.55          5623   \n",
       "4   anthropic  claude-sonnet-4-20250514      2   Failed   17.84             0   \n",
       "5      google            gemini-2.5-pro      2  Success   17.84          5696   \n",
       "6   anthropic  claude-sonnet-4-20250514      3   Failed   19.76             0   \n",
       "7      google            gemini-2.5-pro      3  Success   19.76          5722   \n",
       "8   anthropic  claude-sonnet-4-20250514      4   Failed   20.42             0   \n",
       "9      google            gemini-2.5-pro      4  Success   20.42          5638   \n",
       "10  anthropic  claude-sonnet-4-20250514      5   Failed   30.56             0   \n",
       "11     google            gemini-2.5-pro      5  Success   30.56          5778   \n",
       "12  anthropic  claude-sonnet-4-20250514      6   Failed   19.96             0   \n",
       "13     google            gemini-2.5-pro      6  Success   19.96          5680   \n",
       "14  anthropic  claude-sonnet-4-20250514      7   Failed   18.01             0   \n",
       "15     google            gemini-2.5-pro      7  Success   18.01          5751   \n",
       "16  anthropic  claude-sonnet-4-20250514      8   Failed   29.56             0   \n",
       "17     google            gemini-2.5-pro      8  Success   29.56          5786   \n",
       "18  anthropic  claude-sonnet-4-20250514      9   Failed   35.72             0   \n",
       "19     google            gemini-2.5-pro      9   Failed   35.72             0   \n",
       "\n",
       "    output_tokens  cached_tokens              utc_completed  \n",
       "0               0              0  2025-07-06T19:53:45+00:00  \n",
       "1             353              0  2025-07-06T19:53:45+00:00  \n",
       "2               0              0  2025-07-06T19:53:50+00:00  \n",
       "3             395              0  2025-07-06T19:53:50+00:00  \n",
       "4               0              0  2025-07-06T19:53:46+00:00  \n",
       "5             573              0  2025-07-06T19:53:46+00:00  \n",
       "6               0              0  2025-07-06T19:53:48+00:00  \n",
       "7             713              0  2025-07-06T19:53:48+00:00  \n",
       "8               0              0  2025-07-06T19:53:49+00:00  \n",
       "9             608              0  2025-07-06T19:53:49+00:00  \n",
       "10              0              0  2025-07-06T19:53:59+00:00  \n",
       "11            922              0  2025-07-06T19:53:59+00:00  \n",
       "12              0              0  2025-07-06T19:53:48+00:00  \n",
       "13            627              0  2025-07-06T19:53:48+00:00  \n",
       "14              0              0  2025-07-06T19:53:47+00:00  \n",
       "15            709              0  2025-07-06T19:53:47+00:00  \n",
       "16              0              0  2025-07-06T19:53:58+00:00  \n",
       "17            633              0  2025-07-06T19:53:58+00:00  \n",
       "18              0              0  2025-07-06T19:54:04+00:00  \n",
       "19              0              0  2025-07-06T19:54:04+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance_df = await generate_manifests_parallel(indices_to_generate=INDICES_TO_GENERATE)\n",
    "\n",
    "# Display the first few rows of the performance log\n",
    "print(\"\\n--- Performance Summary ---\")\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "413895e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic model IDs:\n",
      "- claude-opus-4-20250514\n",
      "- claude-sonnet-4-20250514\n",
      "- claude-3-7-sonnet-20250219\n",
      "- claude-3-5-sonnet-20241022\n",
      "- claude-3-5-haiku-20241022\n",
      "- claude-3-5-sonnet-20240620\n",
      "- claude-3-haiku-20240307\n",
      "- claude-3-opus-20240229\n"
     ]
    }
   ],
   "source": [
    "# Initialize the client (if not already done)\n",
    "anthropic_client_2 = anthropic.Client(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "model_list = anthropic_client_2.models.list()\n",
    "print(\"Available Anthropic model IDs:\")\n",
    "for model in model_list:\n",
    "    print(\"-\", model.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ba7fd",
   "metadata": {},
   "source": [
    "### Post-processing of generated manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94428d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No fence found, using full file: 0/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 0/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 0/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 0/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 0/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 0/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 0/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 1/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 1/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 1/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 1/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 1/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 1/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 1/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 2/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 2/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 2/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 2/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 2/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 2/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 2/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 3/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 3/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 3/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 3/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 3/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 3/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 3/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 4/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 4/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 4/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 4/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 4/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 4/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 4/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 5/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 5/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 5/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 5/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 5/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 5/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 5/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 6/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 6/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 6/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 6/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 6/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 6/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 6/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 7/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 7/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 7/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 7/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 7/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 7/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 7/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 8/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 8/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 8/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "✓ Extracted fenced JSON from: 8/google_gemini-2.5-flash.txt\n",
      "✓ Extracted fenced JSON from: 8/google_gemini-2.5-pro.txt\n",
      "⚠ No fence found, using full file: 8/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 8/openai_gpt-4.1.txt\n",
      "⚠ No fence found, using full file: 9/anthropic_claude-sonnet-4-20250514.txt\n",
      "❌ Invalid JSON content in: 9/anthropic_claude-sonnet-4-20250514.txt. File not written.\n",
      "✓ Extracted fenced JSON from: 9/google_gemini-2.5-flash-lite-preview-06-17.txt\n",
      "⚠ No fence found, using full file: 9/google_gemini-2.5-flash.txt\n",
      "❌ Invalid JSON content in: 9/google_gemini-2.5-flash.txt. File not written.\n",
      "⚠ No fence found, using full file: 9/google_gemini-2.5-pro.txt\n",
      "❌ Invalid JSON content in: 9/google_gemini-2.5-pro.txt. File not written.\n",
      "⚠ No fence found, using full file: 9/openai_gpt-4.1-mini.txt\n",
      "⚠ No fence found, using full file: 9/openai_gpt-4.1.txt\n",
      "\n",
      "--- Processing Complete ---\n",
      "Total files found: 60\n",
      "Successfully written: 48\n",
      "Failed to process: 12\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/0/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/1/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/2/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/3/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/4/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/5/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/6/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/7/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/8/anthropic_claude-sonnet-4-20250514.json\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/9/anthropic_claude-sonnet-4-20250514.json\n",
      "Concatenated output for model anthropic_claude-sonnet-4-20250514 with 0 characters.\n",
      "Warning: File not found: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/generated_manifests_json/9/google_gemini-2.5-pro.json\n",
      "Concatenated output for model google_gemini-2.5-pro with 16247 characters.\n",
      "Saved concatenated output for anthropic_claude-sonnet-4-20250514 to /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sample_manifests/anthropic_claude-sonnet-4-20250514_output.txt\n",
      "Saved concatenated output for google_gemini-2.5-pro to /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/sample_manifests/google_gemini-2.5-pro_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert to json and save to the destination directory\n",
    "process_and_clean_manifests()\n",
    "\n",
    "# Concatenate outputs for each model and save as text files\n",
    "save_concatenated_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
