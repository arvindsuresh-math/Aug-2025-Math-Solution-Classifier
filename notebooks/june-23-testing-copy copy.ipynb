{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c266cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/erdos-dl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root identified: /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math\n",
      "### Guidelines\n",
      "\n",
      "0. **Output wrapping**\n",
      "   Return the code inside a single ```python â€¦ ``` block, and nothing else.\n",
      "\n",
      "1.  **Function Naming & Docstring:** The function must be named `solve`. It must begin with a docstring that has exactly two lines:\n",
      "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
      "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
      "\n",
      "2.  **Function Arguments:** The function arguments must be derived from the 'Question' text. \n",
      "    *   Create a distinct argument for every numerical value that is directly stated in the text.\n",
      "    *   The arguments should be created **in the same order in which they appear in the question**.\n",
      "    *   **Note:** Some of these arguments may end up not being used in the function body. This is expected. Do not worry about this and leave the unused arguments in the function signature.\n",
      "\n",
      "3.  **Argument Formatting:** Each argument must include a type-hint (e.g., `int`, `float`) and a default value equal to its value in the 'Question'. You must also add a comment (`#`) next to each argument that quotes or refers to the phrase in the 'Question' it comes from. \n",
      "\n",
      "4.  **Function Body:** The body of the function should follow the logic of the provided 'Solution' dict, which contains the step-by-step solution to the problem. The keys of this dict are strings (e.g. `\"L1\"`, `\"L2\"`) which refer to the line number, and the values of the dict are the corresponding steps in the solution. \n",
      "    * For every relevant line in the 'Solution', you must include a comment in the Python code that indicates the line number (key) from the 'Solution' dict.\n",
      "    * These comments should be formatted as `#: L<n>`, where `<n>` is the line number from the 'Solution' dict.\n",
      "    * Immediately follow the comment with the Python statement that performs the calculation.\n",
      "    * Steps in the solution should result in the creation of new, intermediate variables, which should be named descriptively based on the context of the calculation.\n",
      "    * Wherever possible, in your code try to use only the variables from the function arguments and the intermediate variables you created before, and try to avoid using hard-coded numbers in the calculations.\n",
      "\n",
      "5.  **Calculator Annotations:** Pay close attention to the calculator annotations (e.g., `[[25*8=200]]`) in the 'Solution' as they reveal the precise mathematical operations to implement. **Note**: Some lines in the solution may not contain calculator annotations, but you should still pay attention to the logic and calculations described in those lines.\n",
      "\n",
      "6.  **Final Answer:** Store the final answer in a variable named 'answer', and on the same line, add the comment `# FINAL ANSWER`. In the next line, return the 'answer' variable.\n",
      "\n",
      "7. **No extra output:** Your output should end with the ``` closing the code block. Do not include any additional text, explanations, or comments outside of the code block.\n",
      "\n",
      "--- EXAMPLES ---\n",
      "\n",
      "*Index*: \n",
      "310\n",
      "\n",
      "*Question*: \n",
      "Janet hires six employees. Four of them are warehouse workers who make $15/hour, and the other two are managers who make $20/hour. Janet has to pay 10% of her workers' salaries in FICA taxes. If everyone works 25 days a month and 8 hours a day, how much does Janet owe total for their wages and taxes for one month?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"First figure out how many hours each worker works per month by multiplying the number of days they work by the number of hours a day they work: 25 days * 8 hours/day = [[25*8=200]]200 hours\", \"L2\": \"Then calculate how much one warehouse worker makes per month by multiplying their hourly rate by the number of hours they work: 200 hours * $15/hour = $[[200*15=3000]]3000\", \"L3\": \"Then multiply that number by 4 to find out how much all the warehouse workers make: $3000/worker * 4 workers = $[[3000*4=12000]]12,000\", \"L4\": \"Now multiply the hours each manager works (also 200) by their hourly wage to find out how much one manager makes per month: 200 hours * $20/hour = $[[200*20=4000]]4,000\", \"L5\": \"Now multiply one manager's wages by the number of managers (2) to find their total wage amount: $4,000/manager * 2 managers = $[[4000*2=8000]]8,000\", \"L6\": \"Now add the wages for the managers and the workers to find the total cost of the wages: $8,000 + $12,000 = $[[8000+12000=20000]]20,000\", \"L7\": \"Now multiply the total wage bill by 10% to find how much the FICA taxes are: $20,000 * .1 = $[[20000*.1=2000]]2,000\", \"L8\": \"Now add the total wage bill to the total tax amount to find the grand total: $2,000 + $20,000 = $[[2000+20000=22000]]22,000\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        num_employees: int = 6, # Janet hires six employees\n",
      "        num_warehouse_workers: int = 4, # Four of them are warehouse workers\n",
      "        num_managers: int = 2, # the other two are managers\n",
      "        hourly_wage_warehouse: int = 15, # warehouse workers make $15/hour\n",
      "        hourly_wage_manager: int = 20, # managers make $20/hour\n",
      "        fica_tax_rate: float = 0.1, # FICA tax rate is 10%\n",
      "        days_per_month: int = 25, # everyone works 25 days a month\n",
      "        hours_per_day: int = 8 # everyone works 8 hours a day\n",
      "    ):\n",
      "    \"\"\"Index: 310.\n",
      "    Returns: the monthly total wage bill, including FICA taxes.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    hours_per_month = days_per_month * hours_per_day\n",
      "\n",
      "    #: L2\n",
      "    monthly_wage_warehouse = hourly_wage_warehouse * hours_per_month\n",
      "\n",
      "    #: L3\n",
      "    total_wage_warehouse = monthly_wage_warehouse * num_warehouse_workers\n",
      "\n",
      "    #: L4\n",
      "    monthly_wage_manager = hourly_wage_manager * hours_per_month\n",
      "\n",
      "    #: L5\n",
      "    total_wage_manager = monthly_wage_manager * num_managers\n",
      "\n",
      "    #: L6\n",
      "    total_wages = total_wage_warehouse + total_wage_manager\n",
      "\n",
      "    #: L7\n",
      "    fica_taxes = total_wages * fica_tax_rate\n",
      "\n",
      "    #: L8\n",
      "    grand_total = total_wages + fica_taxes\n",
      "\n",
      "    answer = grand_total # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "*Index*: \n",
      "3822\n",
      "\n",
      "*Question*: \n",
      "Alec is running for Class President. He thinks that if he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him. Half of the class have already said they will vote for him but out of the remaining students, only 5 have said they are thinking about voting for him. He surveys the students who are thinking about voting for someone else, and changes his flyers to reflect the issues these students are concerned about. This results in a fifth of these students saying they'll vote for him. If Alec's class has 60 students and everyone who said they will vote for him does so, how many more votes does Alec need to reach his goal number of votes?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"To calculate Alec's goal number of votes, we need to know that 60 students / 4 = [[60/4=15]]15 students is equal to one-quarter of the class students.\", \"L2\": \"Alec's goal is therefore 15 students * 3 quarters = [[15*3=45]]45 votes.\", \"L3\": \"Half of the class said they will vote for him, so there are already 60 students / 2 = [[60/2=30]]30 votes.\", \"L4\": \"Another 5 students are thinking about voting for him which leaves a total so far of 30 + 5 = [[30+5=35]]35 votes.\", \"L5\": \"This means there are 60 students - 35 voting for Alec = [[60-35=25]]25 students not voting for Alec.\", \"L6\": \"A fifth of these decided to vote, so this is a further 25 students / 5 = [[25/5=5]]5 votes.\", \"L7\": \"Alec is therefore receiving a total of 35 + 5 = [[35+5=40]]40 votes.\", \"L8\": \"So he has missed his goal by 45 goal votes - 40 actual votes = [[45-40=5]]5 votes.\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "        fraction_needed_to_win: float = 3/4,  # f he can get three-quarters of the class to vote for him then there is no chance anyone else can beat him.\n",
      "        fraction_voting_for_him: float = 1/2,  # Half of the class have already said they will vote for him\n",
      "        students_thinking_about_it: int = 5,  # only 5 have said they are thinking about voting for him\n",
      "        total_students: int = 60  # Alec's class has 60 students\n",
      "):    \n",
      "    \"\"\"Index: 3822.\n",
      "    Returns: the number of votes by which Alec is short of his goal.\n",
      "    \"\"\"\n",
      "    #: L1\n",
      "    students_per_quarter = total_students / 4\n",
      "\n",
      "    #: L2\n",
      "    votes_needed = students_per_quarter * 3\n",
      "\n",
      "    #: L3\n",
      "    votes_for_him = total_students * fraction_voting_for_him\n",
      "\n",
      "    #: L4\n",
      "    votes_so_far = votes_for_him + students_thinking_about_it\n",
      "\n",
      "    #: L5\n",
      "    students_not_voting_for_him = total_students - votes_so_far\n",
      "\n",
      "    #: L6\n",
      "    new_votes = students_not_voting_for_him / 5\n",
      "\n",
      "    #: L7\n",
      "    total_votes_for_him = votes_so_far + new_votes\n",
      "\n",
      "    #: L8\n",
      "    votes_short_of_goal = votes_needed - total_votes_for_him\n",
      "\n",
      "    answer = votes_short_of_goal  # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "*Index*: \n",
      "7371\n",
      "\n",
      "*Question*: \n",
      "Karen's students are about to take a standardized test. Karen gets a $500 bonus if their average score is above 75, plus an extra $10 bonus for every additional point the average score increases above 75. So far, Karen has graded 8 tests, and the average is 70. Given that each student can have a maximum score of 150, what combined score do the last two tests need to have for Karen to earn a $600 bonus?\n",
      "\n",
      "*Solution*: \n",
      "{\"L1\": \"First subtract $500 from Karen's goal bonus amount to find how much she makes from the extra $10/point bonus: $600 - $500 = $[[600-500=100]]100\", \"L2\": \"Then divide the extra bonus by the extra rate: $100 / $10/point = [[100/10=10]]10 points\", \"L3\": \"Then add the 10 extra points to the baseline 75 point goal to find the students' average test score: 10 points + 75 points = [[10+75=85]]85 points\", \"L4\": \"Then added the 8 graded tests to the 2 ungraded tests to find the total number of tests: 2 tests + 8 tests = [[2+8=10]]10 tests\", \"L5\": \"Then multiply the 85 point average by the number of tests to find the total number of points the students need to earn: 85 points/test * 10 tests = 850 points\", \"L6\": \"Then multiply the current average by the current number of graded tests to find how many points have been earned so far: 70 points/test * 8 tests = [[70*8=560]]560 points\", \"L7\": \"Then subtract the number of points earned from the number of points needed to find the combine score the last two tests need: 850 points - 560 points = [[850-560=290]]290 points\"}\n",
      "\n",
      "*Code*:\n",
      "```python\n",
      "def solve(\n",
      "    baseline_bonus: int = 500, # Karen gets a $500 bonus\n",
      "    baseline_avg_score: int = 75, # if their average score is above 75\n",
      "    extra_bonus_per_point: int = 10, # plus an extra $10 bonus for every additional point the average score increases above 75\n",
      "    tests_graded_so_far: int = 8, # So far, Karen has graded 8 tests\n",
      "    avg_so_far: int = 70, # and the average is 70\n",
      "    max_score_per_student: int = 150, # each student can have a maximum score of 150\n",
      "    desired_bonus: int = 600 # Karen wants to earn a $600 bonus\n",
      "):\n",
      "    \"\"\"Index: 7371.\n",
      "    Returns: the combined score needed in the last two tests to ensure that Karen earns a $600 bonus.\"\"\"\n",
      "    #: L1\n",
      "    extra_bonus_needed = desired_bonus - baseline_bonus\n",
      "\n",
      "    #: L2\n",
      "    extra_points_needed = extra_bonus_needed / extra_bonus_per_point\n",
      "\n",
      "    #: L3\n",
      "    target_avg_score = baseline_avg_score + extra_points_needed\n",
      "\n",
      "    #: L4\n",
      "    total_tests = tests_graded_so_far + 2\n",
      "\n",
      "    #: L5\n",
      "    total_points_needed = target_avg_score * total_tests\n",
      "\n",
      "    #: L6\n",
      "    points_earned_so_far = avg_so_far * tests_graded_so_far\n",
      "\n",
      "    #: L7\n",
      "    points_needed_last_two_tests = total_points_needed - points_earned_so_far\n",
      "\n",
      "    answer = points_needed_last_two_tests  # FINAL ANSWER\n",
      "    return answer\n",
      "```\n",
      "Static prefix SHA-1: d9bc1bd1572e07a0fa59d113685bea13a4f536d9\n"
     ]
    }
   ],
   "source": [
    "# REFACTOR: Standard imports + asyncio and nest_asyncio for Jupyter compatibility.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI # REFACTOR: Import async client\n",
    "from anthropic import AsyncClient # REFACTOR: Import async client\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm # REFACTOR: Use notebook-friendly tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import textwrap, hashlib\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"Traverse upwards to find the project root, marked by the .git folder.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \".git\").is_dir():\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    raise FileNotFoundError(\"Could not find project root. Is this a git repository?\")\n",
    "\n",
    "# REFACTOR: Apply nest_asyncio to allow asyncio to run in a Jupyter notebook.\n",
    "# This must be done once per kernel.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- 1. Client and Model Configuration ---\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# REFACTOR: Initialize asynchronous clients.\n",
    "# The synchronous clients are no longer needed for the generation script.\n",
    "openai_client_async = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "anthropic_client_async = AsyncClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Define the project root as a global constant\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"Project root identified: {PROJECT_ROOT}\")\n",
    "BASE_OUTPUT_DIR = PROJECT_ROOT / 'data' / 'code_gen_outputs_raw'\n",
    "\n",
    "# A safe starting point is slightly below the documented RPM.\n",
    "API_CONCURRENCY_LIMITS = {\n",
    "    \"openai\": 5,    # limit to 5 concurrent requests for OpenAI\n",
    "    \"anthropic\": 5, # limit to 5 concurrent requests for Anthropic\n",
    "    \"google\": 5,    # limit to 5 concurrent requests for Google\n",
    "}\n",
    "\n",
    "# --- 2. System Prompt and Helper Functions (no changes needed) ---\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an expert Python programmer specializing in data formalization. Your role is to meticulously convert natural language math problems and their step-by-step solutions into a single, well-structured Python function. You will be presented with examples of the required format followed by a final task to complete.\"\n",
    "\n",
    "\n",
    "PROMPT_GUIDELINES = \"\"\"### Guidelines\n",
    "\n",
    "0. **Output wrapping**\n",
    "   Return the code inside a single ```python â€¦ ``` block, and nothing else.\n",
    "\n",
    "1.  **Function Naming & Docstring:** The function must be named `solve`. It must begin with a docstring that has exactly two lines:\n",
    "    *   The first line must be: \"Index: [Index].\" using the index from the task header.\n",
    "    *   The second line must be a succinct, one-sentence description of what the function returns (e.g., \"Returns: the total cost of wages and taxes.\").\n",
    "\n",
    "2.  **Function Arguments:** The function arguments must be derived from the 'Question' text. \n",
    "    *   Create a distinct argument for every numerical value that is directly stated in the text.\n",
    "    *   The arguments should be created **in the same order in which they appear in the question**.\n",
    "    *   **Note:** Some of these arguments may end up not being used in the function body. This is expected. Do not worry about this and leave the unused arguments in the function signature.\n",
    "\n",
    "3.  **Argument Formatting:** Each argument must include a type-hint (e.g., `int`, `float`) and a default value equal to its value in the 'Question'. You must also add a comment (`#`) next to each argument that quotes or refers to the phrase in the 'Question' it comes from. \n",
    "\n",
    "4.  **Function Body:** The body of the function should follow the logic of the provided 'Solution' dict, which contains the step-by-step solution to the problem. The keys of this dict are strings (e.g. `\"L1\"`, `\"L2\"`) which refer to the line number, and the values of the dict are the corresponding steps in the solution. \n",
    "    * For every relevant line in the 'Solution', you must include a comment in the Python code that indicates the line number (key) from the 'Solution' dict.\n",
    "    * These comments should be formatted as `#: L<n>`, where `<n>` is the line number from the 'Solution' dict.\n",
    "    * Immediately follow the comment with the Python statement that performs the calculation.\n",
    "    * Steps in the solution should result in the creation of new, intermediate variables, which should be named descriptively based on the context of the calculation.\n",
    "    * Wherever possible, in your code try to use only the variables from the function arguments and the intermediate variables you created before, and try to avoid using hard-coded numbers in the calculations.\n",
    "\n",
    "5.  **Calculator Annotations:** Pay close attention to the calculator annotations (e.g., `[[25*8=200]]`) in the 'Solution' as they reveal the precise mathematical operations to implement. **Note**: Some lines in the solution may not contain calculator annotations, but you should still pay attention to the logic and calculations described in those lines.\n",
    "\n",
    "6.  **Final Answer:** Store the final answer in a variable named 'answer', and on the same line, add the comment `# FINAL ANSWER`. In the next line, return the 'answer' variable.\n",
    "\n",
    "7. **No extra output:** Your output should end with the ``` closing the code block. Do not include any additional text, explanations, or comments outside of the code block.\"\"\"\n",
    "\n",
    "gsm8k_train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "def build_solution_mapping(\n",
    "    index: int,\n",
    "    dataset: Any,\n",
    "    convert_brackets: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : int\n",
    "        Position of the sample in the loaded dataset.\n",
    "    dataset : iterable / HuggingFace Dataset\n",
    "    convert_brackets : bool, default ``True``\n",
    "        If ``True`` replace every ``<< â€¦ >>`` calculator annotation with\n",
    "        the canonical ``[[ â€¦ ]]`` form so downstream code sees a single\n",
    "        bracket style.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        Mapping ``{\"L1\": <first non-empty line>, \"L2\": <second>, â€¦}``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Blank lines in ``sample[\"answer\"]`` are ignored.\n",
    "    * The line numbering reflects the *order* in the original solution\n",
    "      string; there is no semantic grouping beyond that.\n",
    "    \"\"\"\n",
    "    # extract & split solution text\n",
    "    solution_text = dataset[index][\"answer\"]\n",
    "    lines = [ln.strip() for ln in solution_text.splitlines() if ln.strip()]\n",
    "\n",
    "    # Remove the last line if it matches the '####' answer pattern\n",
    "    if lines and re.match(r\"^####\\s*\\d+(\\.\\d+)?$\", lines[-1]):\n",
    "        lines = lines[:-1]\n",
    "\n",
    "    # optional bracket normalisation\n",
    "    if convert_brackets:\n",
    "        angle = re.compile(r\"<<([^>]+)>>\")\n",
    "        lines = [angle.sub(r\"[[\\1]]\", ln) for ln in lines]\n",
    "    # build mapping\n",
    "    return {f\"L{i}\": line for i, line in enumerate(lines, 1)}\n",
    "\n",
    "\n",
    "def format_prompt_query(index: int, code_strings: dict, with_code: bool = False):\n",
    "    sample = gsm8k_train[index]\n",
    "    question = sample[\"question\"]\n",
    "    solution_mapping = build_solution_mapping(index, gsm8k_train)\n",
    "    solution = json.dumps(solution_mapping)\n",
    "    out = f\"\"\"*Index*: \n",
    "{index}\n",
    "\n",
    "*Question*: \n",
    "{question}\n",
    "\n",
    "*Solution*: \n",
    "{solution}\n",
    "\n",
    "*Code*:\"\"\"\n",
    "    if with_code:\n",
    "        out += f\"\"\"\\n```python\n",
    "{code_strings.get(index, \"# Code not found\")}\n",
    "```\"\"\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_code_strings(indices: List[int], savepath: Path = PROJECT_ROOT / 'data' / 'code_examples'):\n",
    "    \"\"\"\n",
    "    Reads code examples directly from .py files instead of importing them.\n",
    "    This is more robust and avoids Python's complex import path mechanics.\n",
    "    \"\"\"\n",
    "    code_strings = {}\n",
    "    for idx in indices:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(savepath, f\"_{idx}.py\")\n",
    "        try:\n",
    "            # Read the entire content of the file\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                code_strings[idx] = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find example file for index {idx} at: {filepath}\")\n",
    "            code_strings[idx] = f\"# Error: Code for example {idx} not found.\"\n",
    "    return code_strings\n",
    "\n",
    "\n",
    "EXAMPLE_INDICES = [310, 3822, 7371]          # keep fixed order\n",
    "_CODE_EXAMPLES  = get_code_strings(EXAMPLE_INDICES)\n",
    "\n",
    "\n",
    "def build_static_prefix() -> str:\n",
    "    \"\"\"Guidelines + few-shot examples rendered once for caching.\"\"\"\n",
    "    examples_block = \"\\n\".join(\n",
    "        format_prompt_query(idx, _CODE_EXAMPLES, with_code=True)\n",
    "        for idx in EXAMPLE_INDICES\n",
    "    )\n",
    "    prefix = \"\\n\".join([\n",
    "        PROMPT_GUIDELINES.strip(),\n",
    "        \"\\n--- EXAMPLES ---\\n\",\n",
    "        examples_block.strip()\n",
    "    ])\n",
    "    # Canonical whitespace â€“ important for cache hits\n",
    "    return textwrap.dedent(prefix).strip()\n",
    "\n",
    "\n",
    "STATIC_PREFIX = build_static_prefix()\n",
    "print(STATIC_PREFIX)\n",
    "print(\"Static prefix SHA-1:\", hashlib.sha1(STATIC_PREFIX.encode()).hexdigest())\n",
    "\n",
    "\n",
    "def build_task_prompt(index: int) -> str:\n",
    "    \"\"\"Problem-specific part that changes every call.\"\"\"\n",
    "    return \"\\n\".join([\n",
    "        \"--- TASK ---\\n\",\n",
    "        format_prompt_query(index=index,\n",
    "                            code_strings={},     # no code in the task part\n",
    "                            with_code=False)\n",
    "    ])\n",
    "\n",
    "\n",
    "# --- 3. Asynchronous API Calling Function ---\n",
    "\n",
    "async def call_model_api_async(\n",
    "        provider: str,\n",
    "        model: str,\n",
    "        static_prefix: str,\n",
    "        task_prompt: str,\n",
    "        semaphore: asyncio.Semaphore):\n",
    "\n",
    "    async with semaphore:\n",
    "        usage = {\"input_tokens\": 0,\n",
    "                 \"output_tokens\": 0,\n",
    "                 \"cached_tokens\": 0}\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Google (no prompt cache) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if provider == \"google\":\n",
    "            gemini = genai.GenerativeModel(\n",
    "                model_name=model,\n",
    "                system_instruction=static_prefix,\n",
    "            )\n",
    "            cfg = genai.types.GenerationConfig(temperature=0.1, max_output_tokens=4000)\n",
    "            response = await gemini.generate_content_async(task_prompt, generation_config=cfg)\n",
    "            text = response.text\n",
    "            if response.usage_metadata:\n",
    "                usage[\"input_tokens\"]  = response.usage_metadata.prompt_token_count\n",
    "                usage[\"output_tokens\"] = response.usage_metadata.candidates_token_count\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Anthropic (cache_control) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        elif provider == \"anthropic\":\n",
    "            system_block = {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": static_prefix,\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "            }\n",
    "            response = await anthropic_client_async.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=4000,\n",
    "                temperature=0.1,\n",
    "                system=[system_block],\n",
    "                messages=[{\"role\": \"user\", \"content\": task_prompt}],\n",
    "            )\n",
    "            text = response.content[0].text\n",
    "            usage[\"input_tokens\"]   = response.usage.input_tokens\n",
    "            usage[\"output_tokens\"]  = response.usage.output_tokens\n",
    "            usage[\"cached_tokens\"]  = response.usage.cache_read_input_tokens\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI (auto prompt cache) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        elif provider == \"openai\":\n",
    "            msgs = [\n",
    "                {\"role\": \"system\", \"content\": static_prefix},\n",
    "                {\"role\": \"user\",   \"content\": task_prompt},\n",
    "            ]\n",
    "            rsp = await openai_client_async.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=msgs,\n",
    "                temperature=0.1,\n",
    "                max_tokens=4000,\n",
    "            )\n",
    "            text = rsp.choices[0].message.content\n",
    "            usage[\"input_tokens\"]  = rsp.usage.prompt_tokens\n",
    "            usage[\"output_tokens\"] = rsp.usage.completion_tokens\n",
    "            try:\n",
    "                details = rsp.usage.prompt_tokens_details\n",
    "                usage[\"cached_tokens\"] = details.get(\"cached_tokens\", 0)\n",
    "            except AttributeError:\n",
    "                usage[\"cached_tokens\"] = 0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider {provider}\")\n",
    "\n",
    "        return text, usage\n",
    "\n",
    "\n",
    "# --- 4. Parallel Orchestration Function ---\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Parallel across PROBLEMS, but seed Anthropic cache with the first\n",
    "#  problem before launching the rest.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def generate_GSM8K_code_parallel(\n",
    "    model_dict: Dict[str, List[str]],\n",
    "    indices_to_generate: List[int],\n",
    "    example_indices: List[int],\n",
    "    output_dir: Path = BASE_OUTPUT_DIR,\n",
    "    max_parallel_problems: int | None = None,      # None = unlimited\n",
    ") -> pd.DataFrame:\n",
    "    # choose a unique run-id right at the start  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    run_ts = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    provider_sems = {\n",
    "        prov: asyncio.Semaphore(limit) for prov, limit in API_CONCURRENCY_LIMITS.items()\n",
    "    }\n",
    "    problem_sem = (\n",
    "        asyncio.Semaphore(max_parallel_problems)\n",
    "        if max_parallel_problems is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    static_prefix = STATIC_PREFIX\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    # â”€â”€ coroutine for a single GSM8K problem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    async def _run_one_problem(idx: int):\n",
    "        async def _guard():\n",
    "            nonlocal rows\n",
    "            task_prompt = build_task_prompt(idx)\n",
    "            p_dir = output_dir / str(idx)\n",
    "            p_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # launch model calls (your inner gather)\n",
    "            tasks = []\n",
    "            for prov, models in model_dict.items():\n",
    "                for m in models:\n",
    "                    t = asyncio.create_task(\n",
    "                        call_model_api_async(\n",
    "                            provider=prov,\n",
    "                            model=m,\n",
    "                            static_prefix=static_prefix,\n",
    "                            task_prompt=task_prompt,\n",
    "                            semaphore=provider_sems[prov],\n",
    "                        )\n",
    "                    )\n",
    "                    t.meta = {\"provider\": prov, \"model\": m, \"index\": idx, \"start\": time.time()}\n",
    "                    tasks.append(t)\n",
    "\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for t, res in zip(tasks, results):\n",
    "                meta = t.meta\n",
    "                elapsed = time.time() - meta[\"start\"]\n",
    "                status = \"Failed\"\n",
    "                text = \"\"\n",
    "                usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"cached_tokens\": 0}\n",
    "\n",
    "                if not isinstance(res, Exception):\n",
    "                    text, usage = res\n",
    "                    status = \"Success\"\n",
    "                    if text:\n",
    "                        (p_dir / f\"{meta['provider']}_{meta['model']}.txt\").write_text(\n",
    "                            text, encoding=\"utf-8\"\n",
    "                        )\n",
    "                else:\n",
    "                    print(f\"{meta['provider']}_{meta['model']} âŒ {res}\")\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"provider\": meta[\"provider\"],\n",
    "                        \"model\": meta[\"model\"],\n",
    "                        \"index\": meta[\"index\"],\n",
    "                        \"status\": status,\n",
    "                        \"time_s\": round(elapsed, 3),\n",
    "                        \"utc_completed\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "                        \"input_tokens\": usage[\"input_tokens\"],\n",
    "                        \"output_tokens\": usage[\"output_tokens\"],\n",
    "                        \"cached_tokens\": usage[\"cached_tokens\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return await _guard() if problem_sem is None else await problem_sem.__aenter__() or await _guard() or await problem_sem.__aexit__(None, None, None)\n",
    "\n",
    "    # â”€â”€ 1. Seed cache with the FIRST index (blocking) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    first_idx, *rest = indices_to_generate\n",
    "    await _run_one_problem(first_idx)\n",
    "\n",
    "    # â”€â”€ 2. Fan-out remaining problems concurrently â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    await asyncio.gather(*[_run_one_problem(i) for i in rest])\n",
    "\n",
    "    # â”€â”€ save CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = output_dir / \"generation_performance.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ“ All done â€” log at {csv_path}\")\n",
    "    return df\n",
    "\n",
    "# # Add any problem indices you have generated outputs for.\n",
    "# problem_indices_to_test = sorted([3331, 1647, 636, 399, 4670, 5918, 1531, 7364, 5464, 1205, 3518, 6732, 3779, 4483, 6237, 1202, 2345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4083e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# # â•‘  Vertex-AI Gemini - Static-Prefix Context Cache (75 % input discount) â•‘\n",
    "# # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# import os, datetime\n",
    "# import vertexai\n",
    "# from vertexai.generative_models import Content, GenerativeModel\n",
    "\n",
    "# # â”€â”€ 1. project / location  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PROJECT_ID = os.getenv(\"GOOGLE_PROJECT_ID\")          # you just added this\n",
    "# LOCATION   = \"us-central1\"                           # or \"us-east4\", etc.\n",
    "\n",
    "# # If ADC is in place (`gcloud auth application-default login`) you can omit creds.\n",
    "# vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# # â”€â”€ 2. Choose the model version youâ€™ll call later  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MODEL_FOR_CACHE = \"gemini-2.0-flash-thinking-exp\"    # must match future calls\n",
    "\n",
    "# # â”€â”€ 3.  Cache identifiers  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CACHE_DISPLAY  = \"gsm8k-static-1\"                    # human-readable name\n",
    "# GEMINI_CACHE_ID = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{LOCATION}/cachedContents/{CACHE_DISPLAY}\"\n",
    "# )\n",
    "\n",
    "# # â”€â”€ 4. Import the caching helper (path changed after 1.70)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# try:\n",
    "#     from vertexai.preview import caching            # â‰¥ 1.70\n",
    "# except ImportError:\n",
    "#     raise ImportError(\n",
    "#         \"Your vertexai SDK is too old â€” please `pip install -U google-cloud-aiplatform`\"\n",
    "#     )\n",
    "\n",
    "# # â”€â”€ 5. Create the cache if it doesnâ€™t exist  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TTL_SECONDS = 60 * 60                               # 1 hour\n",
    "# try:\n",
    "#     cached = caching.CachedContent(cached_content_name=GEMINI_CACHE_ID)\n",
    "#     cached.refresh()                                # raises if not found\n",
    "#     print(\"ğŸŸ¢ Context cache already exists:\", GEMINI_CACHE_ID)\n",
    "# except Exception:\n",
    "#     cached = caching.CachedContent.create(\n",
    "#         model_name       = MODEL_FOR_CACHE,\n",
    "#         system_instruction = STATIC_PREFIX,         # the long, static prefix\n",
    "#         contents         = [],                      # we cache only the system prompt\n",
    "#         ttl              = datetime.timedelta(seconds=TTL_SECONDS),\n",
    "#         display_name     = CACHE_DISPLAY,\n",
    "#     )\n",
    "#     GEMINI_CACHE_ID = cached.name                   # update in case the API rewrites it\n",
    "#     print(\"âœ… Created context cache:\", GEMINI_CACHE_ID)\n",
    "\n",
    "# # â”€â”€ 6. Helper to build the Content list in your async call  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# def gemini_prompt_with_cache(task_prompt: str):\n",
    "#     \"\"\"Return the list of Content objects that prepends cached tokens.\"\"\"\n",
    "#     return [\n",
    "#         Content(cached_content=GEMINI_CACHE_ID),    # prepended, billed at 25 %\n",
    "#         Content(role=\"user\", text=task_prompt),     # dynamic part\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab0fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750728541.409956 7935654 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "/var/folders/yk/x70ww_h95d3fgd5spjp1s0780000gn/T/ipykernel_3553/3957948032.py:364: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"utc_completed\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All done â€” log at /Users/arvindsuresh/Documents/Github/Erdos-DL-June25-Math/data/code_gen_outputs_raw/generation_performance.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Execution ---\n",
    "\n",
    "# Define your parameters here\n",
    "indices = [310, 3822, 7371] # Use the indices of your few-shot examples\n",
    "indices_to_generate = list(range(50))\n",
    "\n",
    "model_dict = {\n",
    "  \"anthropic\": [\"claude-3-5-haiku-20241022\"], \n",
    "  \"openai\": [\"gpt-4.1-mini\"],\n",
    "  \"google\": [\"gemini-2.0-flash-thinking-exp\", \n",
    "             \"gemini-2.5-flash-lite-preview-06-17\",\n",
    "             \"gemini-2.5-flash\"]\n",
    "}\n",
    "\n",
    "# REFACTOR: To run the async function, you must `await` it.\n",
    "# This will execute the entire parallel generation process.\n",
    "# The result (a pandas DataFrame with performance logs) will be stored in `perf_df`.\n",
    "\n",
    "perf_df = await generate_GSM8K_code_parallel(\n",
    "    model_dict=model_dict,\n",
    "    indices_to_generate=indices_to_generate,\n",
    "    example_indices=indices\n",
    ")\n",
    "\n",
    "# print(\"\\nFinal Performance Summary:\")\n",
    "# print(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9ac540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>model</th>\n",
       "      <th>index</th>\n",
       "      <th>status</th>\n",
       "      <th>time_s</th>\n",
       "      <th>utc_completed</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>cached_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.377</td>\n",
       "      <td>2025-06-24T01:29:04</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.378</td>\n",
       "      <td>2025-06-24T01:29:04</td>\n",
       "      <td>3162</td>\n",
       "      <td>120</td>\n",
       "      <td>3072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2025-06-24T01:29:04</td>\n",
       "      <td>3600</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.380</td>\n",
       "      <td>2025-06-24T01:29:04</td>\n",
       "      <td>3600</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>0</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.380</td>\n",
       "      <td>2025-06-24T01:29:04</td>\n",
       "      <td>3600</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2.432</td>\n",
       "      <td>2025-06-24T01:29:07</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2025-06-24T01:29:07</td>\n",
       "      <td>3163</td>\n",
       "      <td>121</td>\n",
       "      <td>3072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2025-06-24T01:29:07</td>\n",
       "      <td>3601</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2025-06-24T01:29:07</td>\n",
       "      <td>3601</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2025-06-24T01:29:07</td>\n",
       "      <td>3601</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.617</td>\n",
       "      <td>2025-06-24T01:29:08</td>\n",
       "      <td>201</td>\n",
       "      <td>225</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.618</td>\n",
       "      <td>2025-06-24T01:29:08</td>\n",
       "      <td>3223</td>\n",
       "      <td>136</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.618</td>\n",
       "      <td>2025-06-24T01:29:08</td>\n",
       "      <td>3672</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.618</td>\n",
       "      <td>2025-06-24T01:29:08</td>\n",
       "      <td>3672</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>2</td>\n",
       "      <td>Success</td>\n",
       "      <td>3.618</td>\n",
       "      <td>2025-06-24T01:29:08</td>\n",
       "      <td>3672</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>4.567</td>\n",
       "      <td>2025-06-24T01:29:09</td>\n",
       "      <td>223</td>\n",
       "      <td>222</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>4.568</td>\n",
       "      <td>2025-06-24T01:29:09</td>\n",
       "      <td>3243</td>\n",
       "      <td>154</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>4.568</td>\n",
       "      <td>2025-06-24T01:29:09</td>\n",
       "      <td>3696</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>4.569</td>\n",
       "      <td>2025-06-24T01:29:09</td>\n",
       "      <td>3696</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>3</td>\n",
       "      <td>Success</td>\n",
       "      <td>4.569</td>\n",
       "      <td>2025-06-24T01:29:09</td>\n",
       "      <td>3696</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anthropic</td>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>5.970</td>\n",
       "      <td>2025-06-24T01:29:10</td>\n",
       "      <td>146</td>\n",
       "      <td>221</td>\n",
       "      <td>3428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai</td>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>5.971</td>\n",
       "      <td>2025-06-24T01:29:10</td>\n",
       "      <td>3172</td>\n",
       "      <td>161</td>\n",
       "      <td>3072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>5.972</td>\n",
       "      <td>2025-06-24T01:29:10</td>\n",
       "      <td>3613</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash-lite-preview-06-17</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>5.972</td>\n",
       "      <td>2025-06-24T01:29:10</td>\n",
       "      <td>3613</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google</td>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>5.972</td>\n",
       "      <td>2025-06-24T01:29:10</td>\n",
       "      <td>3613</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider                                model  index   status  time_s  \\\n",
       "0   anthropic            claude-3-5-haiku-20241022      0  Success   3.377   \n",
       "1      openai                         gpt-4.1-mini      0  Success   3.378   \n",
       "2      google        gemini-2.0-flash-thinking-exp      0  Success   3.379   \n",
       "3      google  gemini-2.5-flash-lite-preview-06-17      0  Success   3.380   \n",
       "4      google                     gemini-2.5-flash      0  Success   3.380   \n",
       "5   anthropic            claude-3-5-haiku-20241022      1  Success   2.432   \n",
       "6      openai                         gpt-4.1-mini      1  Success   2.433   \n",
       "7      google        gemini-2.0-flash-thinking-exp      1  Success   2.433   \n",
       "8      google  gemini-2.5-flash-lite-preview-06-17      1  Success   2.433   \n",
       "9      google                     gemini-2.5-flash      1  Success   2.433   \n",
       "10  anthropic            claude-3-5-haiku-20241022      2  Success   3.617   \n",
       "11     openai                         gpt-4.1-mini      2  Success   3.618   \n",
       "12     google        gemini-2.0-flash-thinking-exp      2  Success   3.618   \n",
       "13     google  gemini-2.5-flash-lite-preview-06-17      2  Success   3.618   \n",
       "14     google                     gemini-2.5-flash      2  Success   3.618   \n",
       "15  anthropic            claude-3-5-haiku-20241022      3  Success   4.567   \n",
       "16     openai                         gpt-4.1-mini      3  Success   4.568   \n",
       "17     google        gemini-2.0-flash-thinking-exp      3  Success   4.568   \n",
       "18     google  gemini-2.5-flash-lite-preview-06-17      3  Success   4.569   \n",
       "19     google                     gemini-2.5-flash      3  Success   4.569   \n",
       "20  anthropic            claude-3-5-haiku-20241022      4  Success   5.970   \n",
       "21     openai                         gpt-4.1-mini      4  Success   5.971   \n",
       "22     google        gemini-2.0-flash-thinking-exp      4  Success   5.972   \n",
       "23     google  gemini-2.5-flash-lite-preview-06-17      4  Success   5.972   \n",
       "24     google                     gemini-2.5-flash      4  Success   5.972   \n",
       "\n",
       "          utc_completed  input_tokens  output_tokens  cached_tokens  \n",
       "0   2025-06-24T01:29:04           138            175           3428  \n",
       "1   2025-06-24T01:29:04          3162            120           3072  \n",
       "2   2025-06-24T01:29:04          3600            149              0  \n",
       "3   2025-06-24T01:29:04          3600            137              0  \n",
       "4   2025-06-24T01:29:04          3600            125              0  \n",
       "5   2025-06-24T01:29:07           142            142           3428  \n",
       "6   2025-06-24T01:29:07          3163            121           3072  \n",
       "7   2025-06-24T01:29:07          3601            132              0  \n",
       "8   2025-06-24T01:29:07          3601            128              0  \n",
       "9   2025-06-24T01:29:07          3601            133              0  \n",
       "10  2025-06-24T01:29:08           201            225           3428  \n",
       "11  2025-06-24T01:29:08          3223            136           3200  \n",
       "12  2025-06-24T01:29:08          3672            181              0  \n",
       "13  2025-06-24T01:29:08          3672            225              0  \n",
       "14  2025-06-24T01:29:08          3672            220              0  \n",
       "15  2025-06-24T01:29:09           223            222           3428  \n",
       "16  2025-06-24T01:29:09          3243            154           3200  \n",
       "17  2025-06-24T01:29:09          3696            193              0  \n",
       "18  2025-06-24T01:29:09          3696            262              0  \n",
       "19  2025-06-24T01:29:09          3696            199              0  \n",
       "20  2025-06-24T01:29:10           146            221           3428  \n",
       "21  2025-06-24T01:29:10          3172            161           3072  \n",
       "22  2025-06-24T01:29:10          3613            217              0  \n",
       "23  2025-06-24T01:29:10          3613            201              0  \n",
       "24  2025-06-24T01:29:10          3613            218              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da3cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81375b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
