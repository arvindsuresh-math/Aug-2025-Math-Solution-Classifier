{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Installations\n",
    "\n",
    "# 1.1 Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "# 1.2 Install required libraries\n",
    "# Note: TRL is included for consistency with your original script, but is not\n",
    "# strictly required for this sequence classification task.\n",
    "!pip install -Uq transformers\n",
    "!pip install -Uq peft\n",
    "!pip install -Uq trl\n",
    "!pip install -Uq accelerate\n",
    "!pip install -Uq datasets\n",
    "!pip install -Uq bitsandbytes\n",
    "\n",
    "# Install Flash Attention 2\n",
    "!pip install flash-attn==2.7.4.post1 \\\n",
    "  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "  --no-build-isolation\n",
    "\n",
    "# 1.3 Unzip the dataset\n",
    "# Assumes the dataset ZIP file is located in your Google Drive's root directory.\n",
    "# Adjust the path if it is stored elsewhere.\n",
    "!unzip -q -o /content/drive/My\\ Drive/level-1-binary.zip -d /content/\n",
    "print(\"Dataset unzipped to '/content/level-1-binary'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Project Configuration\n",
    "\n",
    "class Config:\n",
    "    # Model ID from Hugging Face Hub\n",
    "    MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "    # Local path to the unzipped dataset\n",
    "    DATASET_PATH = \"/content/level-1-binary\"\n",
    "\n",
    "    # Directory for saving the final model adapter\n",
    "    OUTPUT_DIR = \"/content/level1-classifier-output\"\n",
    "\n",
    "    # Number of labels for the classification task\n",
    "    NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5828e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Loading and Preprocessing\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 3.1 Load the tokenizer needed for preprocessing\n",
    "# This will be the same tokenizer used for the model later.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    Config.MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Set a padding token if one is not already defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3.2 Load the raw dataset from disk\n",
    "raw_dataset = load_from_disk(Config.DATASET_PATH)\n",
    "\n",
    "# 3.3 Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Formats the input text and tokenizes it for sequence classification.\n",
    "    The label is passed through untouched.\n",
    "    \"\"\"\n",
    "    # Create a single input string per example\n",
    "    # Note: We do not include the label (0 or 1) in the input text itself.\n",
    "    system_prompt = \"Analyze the following mathematical problem and solution to determine if the solution is correct or flawed.\"\n",
    "    input_texts = [\n",
    "        f\"{system_prompt}\\n\\n### Problem:\\n{q}\\n\\n### Solution:\\n{s}\"\n",
    "        for q, s in zip(examples[\"question\"], examples[\"solution\"])\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    # The tokenizer will return 'input_ids' and 'attention_mask'.\n",
    "    return tokenizer(\n",
    "        input_texts,\n",
    "        truncation=True,\n",
    "        max_length=512,  # A reasonable max length for these problems\n",
    "        padding=False    # Padding will be handled by the data collator\n",
    "    )\n",
    "\n",
    "# 3.4 Apply the preprocessing function to the dataset\n",
    "# We use batched=True for efficiency and remove original text columns.\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"question\", \"solution\"]\n",
    ")\n",
    "\n",
    "# 3.5 Verify the new dataset structure\n",
    "print(\"--- Tokenized dataset ---\")\n",
    "print(tokenized_dataset)\n",
    "print(\"\\nExample record:\")\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Merge Datasets for Training\n",
    "\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "# 3.5.1 Combine the 'train' and 'validation' splits\n",
    "# This creates a larger training set for the model.\n",
    "full_train_dataset = concatenate_datasets(\n",
    "    [tokenized_dataset[\"train\"], tokenized_dataset[\"validation\"]]\n",
    ")\n",
    "\n",
    "# 3.5.2 Create a new DatasetDict with the merged training set and the original test set\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": full_train_dataset,\n",
    "    \"test\": tokenized_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(\"--- Merged dataset for training ---\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model and Tokenizer Initialization\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "# 4.1 Define 4-bit Quantization Configuration\n",
    "# This enables memory-efficient training by quantizing the model weights.\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# 4.2 Load the Tokenizer\n",
    "# This is the same tokenizer instance from the previous cell, re-established for clarity.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    Config.MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4.3 Load the Model for Sequence Classification\n",
    "# This is the key change from the original script. We use AutoModelForSequenceClassification\n",
    "# to get a base model with a classification head suitable for our task.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    Config.MODEL_ID,\n",
    "    num_labels=Config.NUM_LABELS,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically maps model layers to available hardware (GPU/CPU)\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 4.4 Configure model's pad token ID\n",
    "# It's important that the model's configuration knows the pad token ID\n",
    "# to correctly handle padding during forward passes.\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 4.5 Verify model and configuration\n",
    "print(\"--- Model and Tokenizer Loaded ---\")\n",
    "print(f\"Model class: {type(model)}\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")\n",
    "print(f\"Model pad token ID set to: {model.config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08033443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LoRA and Model Preparation\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# 5.1 Prepare the quantized model for k-bit training.\n",
    "# This performs necessary operations to make the quantized model compatible with PEFT.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 5.2 Define the LoRA configuration.\n",
    "# PEFT (Parameter-Efficient Fine-Tuning) will insert adapter layers into the model.\n",
    "# Only these adapters and the classification head will be trained.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                     # The dimension of the LoRA update matrices.\n",
    "    lora_alpha=32,            # The scaling factor for the LoRA updates.\n",
    "    lora_dropout=0.05,        # Dropout probability for LoRA layers.\n",
    "    bias=\"none\",              # Do not train bias terms.\n",
    "    target_modules=\"all-linear\", # Apply LoRA to all linear layers.\n",
    "    task_type=\"SEQ_CLS\",      # Specify the task type for correct PEFT setup.\n",
    ")\n",
    "\n",
    "print(\"--- LoRA Configured ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c46257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Metrics Definition\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 6.1 Load the accuracy metric from the 'evaluate' library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 6.2 Define the function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Computes accuracy based on model predictions and true labels.\n",
    "    'p' is an EvalPrediction object containing predictions and label_ids.\n",
    "    \"\"\"\n",
    "    # The first element of p.predictions is the logits matrix\n",
    "    logits = p.predictions[0]\n",
    "    \n",
    "    # Get predictions by finding the index of the max logit (the predicted class)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # The 'label_ids' field contains the true labels\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # Compute accuracy using the loaded metric\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"--- Metrics function defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Trainer Setup\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "# 7.1 Define Training Arguments\n",
    "# These arguments control the training process.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=Config.OUTPUT_DIR,\n",
    "    \n",
    "    # --- Batching and Training ---\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, # Reduced for Colab stability\n",
    "    gradient_accumulation_steps=8, # Effective batch size = 4 * 8 = 32\n",
    "    \n",
    "    # --- Optimizer and Scheduling ---\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # --- Logging and Saving ---\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\", # Save a checkpoint at the end of each epoch\n",
    "    save_total_limit=1,    # Only keep the last checkpoint\n",
    "    \n",
    "    # --- Evaluation ---\n",
    "    # No evaluation is performed during training.\n",
    "    evaluation_strategy=\"no\",\n",
    "    \n",
    "    # --- Precision ---\n",
    "    bf16=True, # Use bfloat16 for performance if on Ampere GPU or newer\n",
    ")\n",
    "\n",
    "# 7.2 Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    # eval_dataset is omitted since evaluation_strategy is \"no\"\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer), # Handles padding batches\n",
    "    compute_metrics=compute_metrics, # Will be used for final evaluation\n",
    ")\n",
    "\n",
    "print(\"--- Trainer Initialized ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Execute Training\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f17c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Evaluation and Saving\n",
    "\n",
    "# 9.1 Evaluate the model on the test set\n",
    "print(\"\\n--- Evaluating on the test set ---\")\n",
    "test_results = trainer.evaluate(eval_dataset=final_dataset[\"test\"])\n",
    "\n",
    "# 9.2 Print the evaluation results\n",
    "print(\"Test set performance:\")\n",
    "print(test_results)\n",
    "\n",
    "# 9.3 Save the final trained LoRA adapter\n",
    "print(f\"\\nSaving final model adapter to {Config.OUTPUT_DIR}...\")\n",
    "trainer.save_model(Config.OUTPUT_DIR)\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
