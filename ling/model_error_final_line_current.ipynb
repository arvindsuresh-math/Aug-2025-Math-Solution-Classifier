{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4970e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.26.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34fc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Datasets and ML frameworks\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe07740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 7473\n",
      "First line: {\"id\": 0, \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", \"flawed_answer\": \"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 73\", \"label\": {\"verdict\": \"Flawed\", \"error_details\": {\"error_type\": \"computational_error\", \"erroneous_line_number\": \"L3\", \"explanation\": \"The final answer is too high by 1. It should be 72, not 73.\", \"error_in_text\": \"#### 72\", \"correction_in_text\": \"#### 72\"}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"gsm8k_train_flawed_plus1_final_answer.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"Total lines: {len(lines)}\")\n",
    "    print(\"First line:\", lines[0] if lines else \"No data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05357c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training set size: 14946\n",
      "Combined test set size: 2638\n"
     ]
    }
   ],
   "source": [
    "original_dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "flawed_train_final_answer = load_jsonl(\"gsm8k_train_flawed_plus1_final_answer.jsonl\")\n",
    "flawed_test = load_jsonl(\"gsm8k_test_flawed_plus1_final_answer.jsonl\")\n",
    "\n",
    "combined_train_final_answer = []\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"train\"]):\n",
    "    combined_train_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "for i, ex in enumerate(flawed_train_final_answer):\n",
    "    combined_train_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined training set size: {len(combined_train_final_answer)}\")\n",
    "\n",
    "combined_test_final_answer = []\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"test\"]):\n",
    "    combined_test_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "for i, ex in enumerate(flawed_test):\n",
    "    combined_test_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined test set size: {len(combined_test_final_answer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e1d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training set size: 14946\n",
      "Combined test set size: 2638\n"
     ]
    }
   ],
   "source": [
    "original_dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "flawed_train_2nd_last = load_jsonl(\"gsm8k_train_flawed_plus1_2nd_last.jsonl\")\n",
    "flawed_test = load_jsonl(\"gsm8k_test_flawed_plus1_2nd_last.jsonl\")\n",
    "\n",
    "combined_train_2nd_last = []\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"train\"]):\n",
    "    combined_train_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "for i, ex in enumerate(flawed_train_2nd_last):\n",
    "    combined_train_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined training set size: {len(combined_train_2nd_last)}\")\n",
    "\n",
    "combined_test_2nd_last = []\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"test\"]):\n",
    "    combined_test_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "for i, ex in enumerate(flawed_test):\n",
    "    combined_test_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined test set size: {len(combined_test_2nd_last)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac49204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW TEXT LENGTH ANALYSIS ===\n",
      "\n",
      "TRAINING Dataset Analysis:\n",
      "  CORRECT Examples (7473 total):\n",
      "    Question chars: min=42, max=985, avg=234.5\n",
      "    Solution chars: min=50, max=1228, avg=287.5\n",
      "    Combined chars: min=146, max=1711, avg=544.0\n",
      "    Question words: min=9, max=183, avg=45.1\n",
      "    Solution words: min=4, max=216, avg=51.7\n",
      "    Combined words: min=23, max=336, avg=98.8\n",
      "  FLAWED Examples (7473 total):\n",
      "    Question chars: min=42, max=985, avg=234.5\n",
      "    Solution chars: min=50, max=1228, avg=287.5\n",
      "    Combined chars: min=146, max=1711, avg=544.0\n",
      "    Question words: min=9, max=183, avg=45.1\n",
      "    Solution words: min=4, max=216, avg=51.7\n",
      "    Combined words: min=23, max=336, avg=98.8\n",
      "  COMPARISON:\n",
      "    Avg chars - Correct: 544.0, Flawed: 544.0\n",
      "    Avg words - Correct: 98.8, Flawed: 98.8\n",
      "\n",
      "TEST Dataset Analysis:\n",
      "  CORRECT Examples (1319 total):\n",
      "    Question chars: min=73, max=848, avg=239.9\n",
      "    Solution chars: min=48, max=1070, avg=292.9\n",
      "    Combined chars: min=182, max=1640, avg=554.8\n",
      "    Question words: min=15, max=164, avg=46.3\n",
      "    Solution words: min=5, max=173, avg=52.8\n",
      "    Combined words: min=29, max=314, avg=101.0\n",
      "  FLAWED Examples (1319 total):\n",
      "    Question chars: min=73, max=848, avg=239.9\n",
      "    Solution chars: min=48, max=1070, avg=292.9\n",
      "    Combined chars: min=182, max=1640, avg=554.8\n",
      "    Question words: min=15, max=164, avg=46.3\n",
      "    Solution words: min=5, max=173, avg=52.8\n",
      "    Combined words: min=29, max=314, avg=101.0\n",
      "  COMPARISON:\n",
      "    Avg chars - Correct: 554.8, Flawed: 554.8\n",
      "    Avg words - Correct: 101.0, Flawed: 101.0\n",
      "\n",
      "=== OVERALL STATISTICS ===\n",
      "Training set:\n",
      "  Character lengths: min=146, max=1711, avg=544.0\n",
      "  Word lengths: min=23, max=336, avg=98.8\n",
      "Test set:\n",
      "  Character lengths: min=182, max=1640, avg=554.8\n",
      "  Word lengths: min=29, max=314, avg=101.0\n",
      "\n",
      "=== ROUGH TOKEN ESTIMATION (words × 1.3) ===\n",
      "Training estimated tokens: min=30, max=437, avg=128\n",
      "Test estimated tokens: min=38, max=408, avg=131\n",
      "Estimated examples over 512 tokens: Train=0/14946 (0.0%), Test=0/2638 (0.0%)\n",
      "\n",
      "=== SAMPLE EXAMPLES ===\n",
      "Shortest example:\n",
      "  Length: 146 chars\n",
      "  Question: What is fifteen more than a quarter of 48?...\n",
      "  Solution: A quarter of 48 is 48/4=<<48/4=12>>12.\n",
      "The number is 12+15=<<12+15=27>>27.\n",
      "#### 27...\n",
      "\n",
      "Longest example:\n",
      "  Length: 1711 chars\n",
      "  Question: Peggy is moving and is looking to get rid of her record collection. Sammy says that he will buy all ...\n",
      "  Solution: Sammy is offering to take the whole collection of 200 records and pay Peggy 4 dollars each for them ...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 📏 ANALYZE RAW TEXT LENGTHS BEFORE TOKENIZATION\n",
    "print(\"=== RAW TEXT LENGTH ANALYSIS ===\")\n",
    "\n",
    "def analyze_text_lengths(dataset, name):\n",
    "    print(f\"\\n{name} Dataset Analysis:\")\n",
    "    \n",
    "    # Separate by label\n",
    "    correct_examples = [ex for ex in dataset if ex['label'] == 'Correct']\n",
    "    flawed_examples = [ex for ex in dataset if ex['label'] == 'Flawed']\n",
    "    \n",
    "    def get_text_stats(examples, label_name):\n",
    "        question_lengths = [len(ex['question']) for ex in examples]\n",
    "        solution_lengths = [len(ex['solution']) for ex in examples]\n",
    "        # 🔥 FIX: Create combined text outside f-string\n",
    "        combined_lengths = []\n",
    "        for ex in examples:\n",
    "            combined_text = f\"Question:\\n{ex['question']}\\n\\nSolution:\\n{ex['solution']}\"\n",
    "            combined_lengths.append(len(combined_text))\n",
    "        \n",
    "        question_words = [len(ex['question'].split()) for ex in examples]\n",
    "        solution_words = [len(ex['solution'].split()) for ex in examples]\n",
    "        # 🔥 FIX: Create combined text outside f-string\n",
    "        combined_words = []\n",
    "        for ex in examples:\n",
    "            combined_text = f\"Question:\\n{ex['question']}\\n\\nSolution:\\n{ex['solution']}\"\n",
    "            combined_words.append(len(combined_text.split()))\n",
    "        \n",
    "        print(f\"  {label_name} Examples ({len(examples)} total):\")\n",
    "        print(f\"    Question chars: min={min(question_lengths)}, max={max(question_lengths)}, avg={sum(question_lengths)/len(question_lengths):.1f}\")\n",
    "        print(f\"    Solution chars: min={min(solution_lengths)}, max={max(solution_lengths)}, avg={sum(solution_lengths)/len(solution_lengths):.1f}\")\n",
    "        print(f\"    Combined chars: min={min(combined_lengths)}, max={max(combined_lengths)}, avg={sum(combined_lengths)/len(combined_lengths):.1f}\")\n",
    "        print(f\"    Question words: min={min(question_words)}, max={max(question_words)}, avg={sum(question_words)/len(question_words):.1f}\")\n",
    "        print(f\"    Solution words: min={min(solution_words)}, max={max(solution_words)}, avg={sum(solution_words)/len(solution_words):.1f}\")\n",
    "        print(f\"    Combined words: min={min(combined_words)}, max={max(combined_words)}, avg={sum(combined_words)/len(combined_words):.1f}\")\n",
    "        \n",
    "        return combined_lengths, combined_words\n",
    "    \n",
    "    correct_char_lengths, correct_word_lengths = get_text_stats(correct_examples, \"CORRECT\")\n",
    "    flawed_char_lengths, flawed_word_lengths = get_text_stats(flawed_examples, \"FLAWED\")\n",
    "    \n",
    "    # Compare lengths between correct and flawed\n",
    "    print(f\"  COMPARISON:\")\n",
    "    print(f\"    Avg chars - Correct: {sum(correct_char_lengths)/len(correct_char_lengths):.1f}, Flawed: {sum(flawed_char_lengths)/len(flawed_char_lengths):.1f}\")\n",
    "    print(f\"    Avg words - Correct: {sum(correct_word_lengths)/len(correct_word_lengths):.1f}, Flawed: {sum(flawed_word_lengths)/len(flawed_word_lengths):.1f}\")\n",
    "    \n",
    "    return correct_char_lengths + flawed_char_lengths, correct_word_lengths + flawed_word_lengths\n",
    "\n",
    "# Analyze training data\n",
    "train_char_lengths, train_word_lengths = analyze_text_lengths(combined_train_final_answer, \"TRAINING\")\n",
    "\n",
    "# Analyze test data  \n",
    "test_char_lengths, test_word_lengths = analyze_text_lengths(combined_test_final_answer, \"TEST\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\n=== OVERALL STATISTICS ===\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Character lengths: min={min(train_char_lengths)}, max={max(train_char_lengths)}, avg={sum(train_char_lengths)/len(train_char_lengths):.1f}\")\n",
    "print(f\"  Word lengths: min={min(train_word_lengths)}, max={max(train_word_lengths)}, avg={sum(train_word_lengths)/len(train_word_lengths):.1f}\")\n",
    "\n",
    "print(f\"Test set:\")\n",
    "print(f\"  Character lengths: min={min(test_char_lengths)}, max={max(test_char_lengths)}, avg={sum(test_char_lengths)/len(test_char_lengths):.1f}\")\n",
    "print(f\"  Word lengths: min={min(test_word_lengths)}, max={max(test_word_lengths)}, avg={sum(test_word_lengths)/len(test_word_lengths):.1f}\")\n",
    "\n",
    "# Rough token estimation (1 word ≈ 1.3 tokens for English)\n",
    "print(f\"\\n=== ROUGH TOKEN ESTIMATION (words × 1.3) ===\")\n",
    "train_estimated_tokens = [w * 1.3 for w in train_word_lengths]\n",
    "test_estimated_tokens = [w * 1.3 for w in test_word_lengths]\n",
    "\n",
    "print(f\"Training estimated tokens: min={min(train_estimated_tokens):.0f}, max={max(train_estimated_tokens):.0f}, avg={sum(train_estimated_tokens)/len(train_estimated_tokens):.0f}\")\n",
    "print(f\"Test estimated tokens: min={min(test_estimated_tokens):.0f}, max={max(test_estimated_tokens):.0f}, avg={sum(test_estimated_tokens)/len(test_estimated_tokens):.0f}\")\n",
    "\n",
    "# Check how many would exceed 512 tokens\n",
    "train_over_512 = sum(1 for t in train_estimated_tokens if t > 512)\n",
    "test_over_512 = sum(1 for t in test_estimated_tokens if t > 512)\n",
    "print(f\"Estimated examples over 512 tokens: Train={train_over_512}/{len(train_estimated_tokens)} ({100*train_over_512/len(train_estimated_tokens):.1f}%), Test={test_over_512}/{len(test_estimated_tokens)} ({100*test_over_512/len(test_estimated_tokens):.1f}%)\")\n",
    "\n",
    "# Show a few examples\n",
    "print(f\"\\n=== SAMPLE EXAMPLES ===\")\n",
    "print(\"Shortest example:\")\n",
    "shortest_idx = train_char_lengths.index(min(train_char_lengths))\n",
    "shortest_example = combined_train_final_answer[shortest_idx]\n",
    "# 🔥 FIX: Create combined text outside f-string\n",
    "shortest_text = f\"Question:\\n{shortest_example['question']}\\n\\nSolution:\\n{shortest_example['solution']}\"\n",
    "print(f\"  Length: {len(shortest_text)} chars\")\n",
    "print(f\"  Question: {shortest_example['question'][:100]}...\")\n",
    "print(f\"  Solution: {shortest_example['solution'][:100]}...\")\n",
    "\n",
    "print(\"\\nLongest example:\")\n",
    "longest_idx = train_char_lengths.index(max(train_char_lengths))\n",
    "longest_example = combined_train_final_answer[longest_idx]\n",
    "# 🔥 FIX: Create combined text outside f-string\n",
    "longest_text = f\"Question:\\n{longest_example['question']}\\n\\nSolution:\\n{longest_example['solution']}\"\n",
    "print(f\"  Length: {len(longest_text)} chars\")\n",
    "print(f\"  Question: {longest_example['question'][:100]}...\")\n",
    "print(f\"  Solution: {longest_example['solution'][:100]}...\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2358a50",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b861ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(combined_train_final_answer)\n",
    "test_dataset = Dataset.from_list(combined_test_final_answer)\n",
    "\n",
    "# train_dataset = Dataset.from_list(combined_train_2nd_last)\n",
    "# test_dataset = Dataset.from_list(combined_test_2nd_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6aa79",
   "metadata": {},
   "source": [
    "# AutoTokenizer\n",
    "\n",
    "Converts text into numbers that neural networks can understand.\n",
    "\n",
    "### Special Tokens\n",
    "- `[CLS]` (101): Start of sequence\n",
    "- `[SEP]` (102): Separator between segments  \n",
    "- `[PAD]` (0): Padding token\n",
    "- `[UNK]`: Unknown/out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d3527",
   "metadata": {},
   "source": [
    "| Parameter | Purpose |\n",
    "|-----------|---------|\n",
    "| `truncation=True` | Cut text if > 512 tokens |\n",
    "| `padding=\"max_length\"` | Add padding to reach exactly 512 tokens |\n",
    "| `max_length=512` | Set maximum sequence length |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608af50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6674ac7d8bac4785b0778ddcffa07b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ae6ecb7798447c83695e2493cfea0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2638 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c694444ab5d471f805532769e63ae8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92705803187d417a8d6348fc41aa83da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2638 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing with tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 🔥 FIX: Add padding token for GPT-2 style models\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    # Combine the question and solution into a single input string\n",
    "    full_input = f\"Question:\\n{example['question']}\\n\\nSolution:\\n{example['solution']}\"\n",
    "    \n",
    "    # Tokenize the combined input with truncation, padding, and a max length of 512 tokens\n",
    "    return tokenizer(full_input, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "test_dataset = test_dataset.map(tokenize_fn)\n",
    "\n",
    "# Label encoding\n",
    "label_map = {\"Correct\": 0, \"Flawed\": 1}\n",
    "train_dataset = train_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})\n",
    "test_dataset = test_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3691e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING DATASET ===\n",
      "Train dataset columns: ['question', 'solution', 'label', 'input_ids', 'attention_mask', 'labels']\n",
      "Test dataset columns: ['question', 'solution', 'label', 'input_ids', 'attention_mask', 'labels']\n",
      "Sample keys: dict_keys(['question', 'solution', 'label', 'input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: 512\n",
      "Labels: 0\n",
      "Decoded input text:\n",
      "Question:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Solution:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 DEBUG: Check what's in your datasets\n",
    "print(\"=== DEBUGGING DATASET ===\")\n",
    "print(\"Train dataset columns:\", train_dataset.column_names)\n",
    "print(\"Test dataset columns:\", test_dataset.column_names)\n",
    "\n",
    "# Check a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Input IDs shape:\", sample['input_ids'].shape if hasattr(sample['input_ids'], 'shape') else len(sample['input_ids']))\n",
    "print(\"Labels:\", sample['labels'])\n",
    "\n",
    "# Decode the input to see what text is actually fed to model\n",
    "decoded_input = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(\"Decoded input text:\")\n",
    "print(decoded_input)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2698633",
   "metadata": {},
   "source": [
    "# Model Setup & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441695fb",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "An epoch refers to one complete pass through the entire training dataset. During an epoch, the model processes all the training samples once, updating its weights based on the computed loss. Training for multiple epochs allows the model to learn and refine its parameters iteratively.\n",
    "\n",
    "Increasing the number of epochs allows the model to learn more but risks overfitting if too high. A balance between batch size and epochs is crucial for optimal performance.\n",
    "\n",
    "#### Batch size\n",
    "\n",
    "Batch size determines the number of samples processed before the model updates its weights. For example, a batch size of 8 means 8 samples (e.g., 8 question-answer pairs) are processed together in one forward and backward pass during training.\n",
    "\n",
    "A smaller batch size uses less memory but may take longer to converge, while a larger batch size can speed up training but requires more memory. \n",
    "\n",
    "#### Optimizer - AdamW\n",
    "\n",
    "AdamW is an optimizer that implements the Adam algorithm with weight decay regularization. It helps prevent overfitting by penalizing large weights, which is particularly useful in deep learning models. AdamW is widely used because it combines the benefits of Adam (adaptive learning rates) with weight decay for better generalization.\n",
    "\n",
    "Other optimizer choices include:\n",
    "- **SGD (Stochastic Gradient Descent)**: A simple optimizer with momentum and learning rate decay options.\n",
    "- **RMSprop**: Designed for non-stationary objectives, often used in RNNs.\n",
    "- **Adagrad**: Adapts learning rates based on parameter updates, suitable for sparse data.\n",
    "- **Adadelta**: An extension of Adagrad that reduces aggressive learning rate decay.\n",
    "- **Adam**: Similar to AdamW but without weight decay.\n",
    "- **Nadam**: Adam with Nesterov momentum.\n",
    "\n",
    "#### tqdm\n",
    "\n",
    "`tqdm` is a Python library used to display progress bars for loops. It provides a visual representation of the progress of an iterable, such as a training loop or data processing, making it easier to monitor the execution time and completion percentage. It is especially useful in long-running tasks.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    # Simulate some work\n",
    "    pass\n",
    "```\n",
    "\n",
    "This will display a progress bar in the console, showing the percentage completed, elapsed time, and estimated time remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8396862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Learning Rate Finder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding LR:   0%|          | 0/935 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.10 GB, other allocations: 26.70 MB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Run learning rate finder\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Running Learning Rate Finder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m lrs, losses \u001b[38;5;241m=\u001b[39m \u001b[43mfind_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     75\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[28], line 37\u001b[0m, in \u001b[0;36mfind_learning_rate\u001b[0;34m(model, train_dataset, tokenizer, start_lr, end_lr, num_iter)\u001b[0m\n\u001b[1;32m     34\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m               \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Track loss and learning rate\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1375\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1375\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1389\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    402\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    403\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 404\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    413\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(\n\u001b[1;32m    332\u001b[0m         query_states, key_states, value_states, attention_mask, head_mask\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mattn_output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    348\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:41\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     40\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m---> 41\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 18.10 GB, other allocations: 26.70 MB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# 🔍 LEARNING RATE FINDER (FastAI Style) - FIXED FOR MPS\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_learning_rate(model, train_dataset, tokenizer, start_lr=1e-7, end_lr=1e-1, num_iter=100):\n",
    "    \"\"\"\n",
    "    FastAI-style learning rate finder - Fixed for MPS device\n",
    "    \"\"\"\n",
    "    # 🔥 FIX: Ensure model is on the correct device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=start_lr)\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_mult = (end_lr / start_lr) ** (1.0 / num_iter)\n",
    "    \n",
    "    losses = []\n",
    "    lrs = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=\"Finding LR\")):\n",
    "        if i >= num_iter:\n",
    "            break\n",
    "        \n",
    "        # 🔥 FIX: Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'], \n",
    "                       attention_mask=batch['attention_mask'],\n",
    "                       labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Track loss and learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(current_lr)\n",
    "        \n",
    "        # Stop if loss explodes\n",
    "        if loss.item() > best_loss * 4:\n",
    "            break\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_mult\n",
    "    \n",
    "    return lrs, losses\n",
    "\n",
    "# 🔥 INITIALIZE MODEL FIRST\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 🔥 FIX: Set pad_token_id in model config to match tokenizer\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Run learning rate finder\n",
    "print(\"🔍 Running Learning Rate Finder...\")\n",
    "lrs, losses = find_learning_rate(model, train_dataset, tokenizer)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lrs, losses)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate Finder')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal learning rate (usually where loss decreases fastest)\n",
    "min_loss_idx = losses.index(min(losses))\n",
    "optimal_lr = lrs[min_loss_idx]\n",
    "print(f\"📊 Suggested Learning Rate: {optimal_lr:.2e}\")\n",
    "print(f\"📊 Consider using: {optimal_lr/10:.2e} to {optimal_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8a5a3",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Learning rate scheduler\n",
    "\n",
    "torch.optim.lr_scheduler\n",
    "\n",
    "https://www.datacamp.com/tutorial/fine-tuning-large-language-models\n",
    "\n",
    "Increase batch_size \n",
    "\n",
    "total_norm_util = clip_grad_norm_(model.parameters(), max_norm=float('inf')) \n",
    "\n",
    "weight-decay (decided based on if it is overfitting)\n",
    "\n",
    "cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00c9b732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b32c18ef3e44efa87cc457bef026f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/3_/q57jjf4d3fv22x72mdkrm9fw0000gn/T/ipykernel_89281/1852775810.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Trainer...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot handle batch sizes > 1 if no padding token is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training with Trainer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1397\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     batch_size, sequence_length \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle batch sizes > 1 if no padding token is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1399\u001b[0m     last_non_pad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot handle batch sizes > 1 if no padding token is defined."
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # 🔥 LEARNING RATE - Most Important Change\n",
    "    learning_rate=5e-4,  # Increased from 5e-5 - try 1e-4 to 3e-4 range\n",
    "    \n",
    "    # 🔥 MORE TRAINING\n",
    "    num_train_epochs=5,  # Increased from 3 - more learning time\n",
    "    \n",
    "    # 🔥 BATCH SIZE - Better gradient estimates\n",
    "    per_device_train_batch_size=16,   # Good starting point\n",
    "    per_device_eval_batch_size=64,\n",
    "    \n",
    "    # 🔥 LEARNING RATE SCHEDULING\n",
    "    lr_scheduler_type=\"cosine\",  # Add learning rate decay\n",
    "    warmup_steps=500,           # Gradual warmup\n",
    "    \n",
    "    # 🔥 REGULARIZATION ADJUSTMENTS\n",
    "    weight_decay=0.1,           # Increased from 0.01\n",
    "    \n",
    "    # 🔥 EVALUATION & EARLY STOPPING\n",
    "    eval_steps=250,             # Evaluate more frequently\n",
    "    load_best_model_at_end=True,  # Load best checkpoint\n",
    "    metric_for_best_model=\"eval_f1\",  # Use F1 score for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # 🔥 IMPROVED LOGGING\n",
    "    logging_steps=50,           # More frequent logging\n",
    "    save_total_limit=3,         # Keep more checkpoints\n",
    "    \n",
    "    # Keep these settings\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "# Complete the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Ensure datasets are properly formatted\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # Use tokenizer instead of processing_class\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training with Trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22efb91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/3_/q57jjf4d3fv22x72mdkrm9fw0000gn/T/ipykernel_70331/3791621515.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 08:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.693432</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.612620</td>\n",
       "      <td>0.682094</td>\n",
       "      <td>0.705965</td>\n",
       "      <td>0.656658</td>\n",
       "      <td>0.763278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.512835</td>\n",
       "      <td>0.688923</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.400607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.513430</td>\n",
       "      <td>0.739757</td>\n",
       "      <td>0.755175</td>\n",
       "      <td>0.712938</td>\n",
       "      <td>0.802731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.438555</td>\n",
       "      <td>0.745827</td>\n",
       "      <td>0.686036</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.555387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.410214</td>\n",
       "      <td>0.748103</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.692580</td>\n",
       "      <td>0.892261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.395925</td>\n",
       "      <td>0.774659</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.920930</td>\n",
       "      <td>0.600910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.418233</td>\n",
       "      <td>0.763278</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.722151</td>\n",
       "      <td>0.855842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.406329</td>\n",
       "      <td>0.773141</td>\n",
       "      <td>0.746825</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.669196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.761760</td>\n",
       "      <td>0.764264</td>\n",
       "      <td>0.756315</td>\n",
       "      <td>0.772382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # 🔥 LEARNING RATE - Most Important Change\n",
    "    learning_rate=5e-4,  # Increased from 5e-5 - try 1e-4 to 3e-4 range\n",
    "    \n",
    "    # 🔥 MORE TRAINING\n",
    "    num_train_epochs=10,  # Increased from 3 - more learning time\n",
    "    \n",
    "    # 🔥 BATCH SIZE - Better gradient estimates\n",
    "    per_device_train_batch_size=32,  # Increased from 8\n",
    "    per_device_eval_batch_size=32,   # Increased from 8\n",
    "    \n",
    "    # 🔥 LEARNING RATE SCHEDULING\n",
    "    lr_scheduler_type=\"cosine\",  # Add learning rate decay\n",
    "    warmup_steps=500,           # Gradual warmup\n",
    "    \n",
    "    # 🔥 REGULARIZATION ADJUSTMENTS\n",
    "    weight_decay=0.1,           # Increased from 0.01\n",
    "    \n",
    "    # 🔥 EVALUATION & EARLY STOPPING\n",
    "    eval_steps=250,             # Evaluate more frequently\n",
    "    load_best_model_at_end=True,  # Load best checkpoint\n",
    "    metric_for_best_model=\"eval_f1\",  # Use F1 score for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # 🔥 IMPROVED LOGGING\n",
    "    logging_steps=50,           # More frequent logging\n",
    "    save_total_limit=3,         # Keep more checkpoints\n",
    "    \n",
    "    # Keep these settings\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "# Complete the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Ensure datasets are properly formatted\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # Use tokenizer instead of processing_class\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training with Trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Training completed and model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsm8k-verifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
