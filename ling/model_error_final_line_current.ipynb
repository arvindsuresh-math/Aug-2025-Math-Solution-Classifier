{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4970e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.26.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed74589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Datasets and ML frameworks\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe07740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 7473\n",
      "First line: {\"id\": 0, \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", \"flawed_answer\": \"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 73\", \"label\": {\"verdict\": \"Flawed\", \"error_details\": {\"error_type\": \"computational_error\", \"erroneous_line_number\": \"L3\", \"explanation\": \"The final answer is too high by 1. It should be 72, not 73.\", \"error_in_text\": \"#### 72\", \"correction_in_text\": \"#### 72\"}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"gsm8k_train_flawed_plus1_final_answer.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"Total lines: {len(lines)}\")\n",
    "    print(\"First line:\", lines[0] if lines else \"No data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c05357c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first 3736 out of 7473 training examples\n",
      "Using first 3736 out of 7473 flawed examples\n",
      "Combined training set size: 7472\n",
      "Using first 659 out of 1319 test examples\n",
      "Using first 659 out of 1319 flawed test examples\n",
      "Combined test set size: 1318\n"
     ]
    }
   ],
   "source": [
    "original_dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "flawed_train_final_answer = load_jsonl(\"gsm8k_train_flawed_plus1_final_answer.jsonl\")\n",
    "flawed_test = load_jsonl(\"gsm8k_test_flawed_plus1_final_answer.jsonl\")\n",
    "\n",
    "combined_train_final_answer = []\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of correct examples from original dataset\n",
    "train_size = len(original_dataset[\"train\"])\n",
    "half_train_size = train_size // 2\n",
    "print(f\"Using first {half_train_size} out of {train_size} training examples\")\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"train\"]):\n",
    "    if i >= half_train_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_train_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of flawed examples\n",
    "flawed_size = len(flawed_train_final_answer)\n",
    "half_flawed_size = flawed_size // 2\n",
    "print(f\"Using first {half_flawed_size} out of {flawed_size} flawed examples\")\n",
    "\n",
    "for i, ex in enumerate(flawed_train_final_answer):\n",
    "    if i >= half_flawed_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_train_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined training set size: {len(combined_train_final_answer)}\")\n",
    "\n",
    "combined_test_final_answer = []\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of correct test examples\n",
    "test_size = len(original_dataset[\"test\"])\n",
    "half_test_size = test_size // 2\n",
    "print(f\"Using first {half_test_size} out of {test_size} test examples\")\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"test\"]):\n",
    "    if i >= half_test_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_test_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of flawed test examples\n",
    "flawed_test_size = len(flawed_test)\n",
    "half_flawed_test_size = flawed_test_size // 2\n",
    "print(f\"Using first {half_flawed_test_size} out of {flawed_test_size} flawed test examples\")\n",
    "\n",
    "for i, ex in enumerate(flawed_test):\n",
    "    if i >= half_flawed_test_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_test_final_answer.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined test set size: {len(combined_test_final_answer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e1d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first 3736 out of 7473 correct training examples\n",
      "Using first 3736 out of 7473 flawed training examples\n",
      "Combined training set size: 7472\n",
      "Using first 659 out of 1319 correct test examples\n",
      "Using first 659 out of 1319 flawed test examples\n",
      "Combined test set size: 1318\n"
     ]
    }
   ],
   "source": [
    "original_dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "flawed_train_2nd_last = load_jsonl(\"gsm8k_train_flawed_plus1_2nd_last.jsonl\")\n",
    "flawed_test = load_jsonl(\"gsm8k_test_flawed_plus1_2nd_last.jsonl\")\n",
    "\n",
    "combined_train_2nd_last = []\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of correct examples from original dataset\n",
    "train_size = len(original_dataset[\"train\"])\n",
    "half_train_size = train_size // 2\n",
    "print(f\"Using first {half_train_size} out of {train_size} correct training examples\")\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"train\"]):\n",
    "    if i >= half_train_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_train_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of flawed examples\n",
    "flawed_size = len(flawed_train_2nd_last)\n",
    "half_flawed_size = flawed_size // 2\n",
    "print(f\"Using first {half_flawed_size} out of {flawed_size} flawed training examples\")\n",
    "\n",
    "for i, ex in enumerate(flawed_train_2nd_last):\n",
    "    if i >= half_flawed_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_train_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined training set size: {len(combined_train_2nd_last)}\")\n",
    "\n",
    "combined_test_2nd_last = []\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of correct test examples\n",
    "test_size = len(original_dataset[\"test\"])\n",
    "half_test_size = test_size // 2\n",
    "print(f\"Using first {half_test_size} out of {test_size} correct test examples\")\n",
    "\n",
    "for i, ex in enumerate(original_dataset[\"test\"]):\n",
    "    if i >= half_test_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_test_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"answer\"],\n",
    "        \"label\": \"Correct\"\n",
    "    })\n",
    "\n",
    "# ðŸ”¥ Add FIRST HALF of flawed test examples\n",
    "flawed_test_size = len(flawed_test)\n",
    "half_flawed_test_size = flawed_test_size // 2\n",
    "print(f\"Using first {half_flawed_test_size} out of {flawed_test_size} flawed test examples\")\n",
    "\n",
    "for i, ex in enumerate(flawed_test):\n",
    "    if i >= half_flawed_test_size:  # Stop at halfway point\n",
    "        break\n",
    "    combined_test_2nd_last.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"solution\": ex[\"flawed_answer\"],\n",
    "        \"label\": \"Flawed\"\n",
    "    })\n",
    "\n",
    "print(f\"Combined test set size: {len(combined_test_2nd_last)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34fc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2358a50",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b861ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(combined_train_final_answer)\n",
    "test_dataset = Dataset.from_list(combined_test_final_answer)\n",
    "\n",
    "# train_dataset = Dataset.from_list(combined_train_2nd_last)\n",
    "# test_dataset = Dataset.from_list(combined_test_2nd_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6aa79",
   "metadata": {},
   "source": [
    "# AutoTokenizer\n",
    "\n",
    "Converts text into numbers that neural networks can understand.\n",
    "\n",
    "### Special Tokens\n",
    "- `[CLS]` (101): Start of sequence\n",
    "- `[SEP]` (102): Separator between segments  \n",
    "- `[PAD]` (0): Padding token\n",
    "- `[UNK]`: Unknown/out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608af50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', 'world']\n",
      "IDs: [7592, 2088]\n",
      "Tokenization result: [101, 7592, 2088, 102, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# Use the same model for both tokenization and training\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "# \"gpt2-tiny\"\n",
    "# \"distilbert-base-uncased\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Text to token\n",
    "text = \"Hello world\"\n",
    "tokens = tokenizer.tokenize(text)  # ['hello', 'world']\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)  # [7592, 2088]\n",
    "print(\"IDs:\", ids)\n",
    "\n",
    "# All-in-one tokenization\n",
    "result = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# input_ids: numerical representation of the text; special tokens like [CLS] and [SEP] are included\n",
    "# attention_mask: indicates which tokens are real (1) vs padding (0)\n",
    "print(\"Tokenization result:\", result['input_ids'][:10], result['attention_mask'][:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d3527",
   "metadata": {},
   "source": [
    "| Parameter | Purpose |\n",
    "|-----------|---------|\n",
    "| `truncation=True` | Cut text if > 512 tokens |\n",
    "| `padding=\"max_length\"` | Add padding to reach exactly 512 tokens |\n",
    "| `max_length=512` | Set maximum sequence length |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50ba8256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3318fa0e36b64793ac2e061ad0bdd0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a55e7357774868b2781c5483cdf0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    # Combine the question and solution into a single input string\n",
    "    full_input = f\"Question:\\n{example['question']}\\n\\nSolution:\\n{example['solution']}\"\n",
    "\n",
    "    # Tokenize the combined input with truncation, padding, and a max length of 512 tokens\n",
    "    return tokenizer(full_input, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Apply the tokenization function to the training dataset\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "\n",
    "# Apply the tokenization function to the test dataset\n",
    "test_dataset = test_dataset.map(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3691e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'solution', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8e492a6ddf49f3bcbc07706ba66607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a76a9c7563c45978f012de463aaac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label encoding\n",
    "label_map = {\"Correct\": 0, \"Flawed\": 1}\n",
    "train_dataset = train_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})\n",
    "test_dataset = test_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2698633",
   "metadata": {},
   "source": [
    "# Model Setup & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441695fb",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "An epoch refers to one complete pass through the entire training dataset. During an epoch, the model processes all the training samples once, updating its weights based on the computed loss. Training for multiple epochs allows the model to learn and refine its parameters iteratively.\n",
    "\n",
    "Increasing the number of epochs allows the model to learn more but risks overfitting if too high. A balance between batch size and epochs is crucial for optimal performance.\n",
    "\n",
    "#### Batch size\n",
    "\n",
    "Batch size determines the number of samples processed before the model updates its weights. For example, a batch size of 8 means 8 samples (e.g., 8 question-answer pairs) are processed together in one forward and backward pass during training.\n",
    "\n",
    "A smaller batch size uses less memory but may take longer to converge, while a larger batch size can speed up training but requires more memory. \n",
    "\n",
    "#### Optimizer - AdamW\n",
    "\n",
    "AdamW is an optimizer that implements the Adam algorithm with weight decay regularization. It helps prevent overfitting by penalizing large weights, which is particularly useful in deep learning models. AdamW is widely used because it combines the benefits of Adam (adaptive learning rates) with weight decay for better generalization.\n",
    "\n",
    "Other optimizer choices include:\n",
    "- **SGD (Stochastic Gradient Descent)**: A simple optimizer with momentum and learning rate decay options.\n",
    "- **RMSprop**: Designed for non-stationary objectives, often used in RNNs.\n",
    "- **Adagrad**: Adapts learning rates based on parameter updates, suitable for sparse data.\n",
    "- **Adadelta**: An extension of Adagrad that reduces aggressive learning rate decay.\n",
    "- **Adam**: Similar to AdamW but without weight decay.\n",
    "- **Nadam**: Adam with Nesterov momentum.\n",
    "\n",
    "#### tqdm\n",
    "\n",
    "`tqdm` is a Python library used to display progress bars for loops. It provides a visual representation of the progress of an iterable, such as a training loop or data processing, making it easier to monitor the execution time and completion percentage. It is especially useful in long-running tasks.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    # Simulate some work\n",
    "    pass\n",
    "```\n",
    "\n",
    "This will display a progress bar in the console, showing the percentage completed, elapsed time, and estimated time remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8a5a3",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Check if labels were fed to the model\n",
    "\n",
    "Model: gpt2\n",
    "\n",
    "Learning rate scheduler\n",
    "\n",
    "torch.optim.lr_scheduler\n",
    "\n",
    "https://www.datacamp.com/tutorial/fine-tuning-large-language-models\n",
    "\n",
    "Increase batch_size \n",
    "\n",
    "total_norm_util = clip_grad_norm_(model.parameters(), max_norm=float('inf')) \n",
    "\n",
    "weight-decay (decided based on if it is overfitting)\n",
    "\n",
    "cross validation\n",
    "\n",
    "1500 - 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/3_/q57jjf4d3fv22x72mdkrm9fw0000gn/T/ipykernel_70331/3014540342.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1170' max='1170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1170/1170 03:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.693155</td>\n",
       "      <td>0.499241</td>\n",
       "      <td>0.359223</td>\n",
       "      <td>0.498652</td>\n",
       "      <td>0.280728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.591831</td>\n",
       "      <td>0.682853</td>\n",
       "      <td>0.605660</td>\n",
       "      <td>0.800499</td>\n",
       "      <td>0.487102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.482739</td>\n",
       "      <td>0.742033</td>\n",
       "      <td>0.668616</td>\n",
       "      <td>0.934605</td>\n",
       "      <td>0.520486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.403382</td>\n",
       "      <td>0.788316</td>\n",
       "      <td>0.783888</td>\n",
       "      <td>0.800633</td>\n",
       "      <td>0.767830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.389379</td>\n",
       "      <td>0.801214</td>\n",
       "      <td>0.793049</td>\n",
       "      <td>0.827018</td>\n",
       "      <td>0.761760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # ðŸ”¥ LEARNING RATE - Most Important Change\n",
    "    learning_rate=5e-4,  # Increased from 5e-5 - try 1e-4 to 3e-4 range\n",
    "    \n",
    "    # ðŸ”¥ MORE TRAINING\n",
    "    num_train_epochs=5,  # Increased from 3 - more learning time\n",
    "    \n",
    "    # ðŸ”¥ BATCH SIZE - Better gradient estimates\n",
    "    per_device_train_batch_size=32,  # Increased from 8\n",
    "    per_device_eval_batch_size=32,   # Increased from 8\n",
    "    \n",
    "    # ðŸ”¥ LEARNING RATE SCHEDULING\n",
    "    lr_scheduler_type=\"cosine\",  # Add learning rate decay\n",
    "    warmup_steps=500,           # Gradual warmup\n",
    "    \n",
    "    # ðŸ”¥ REGULARIZATION ADJUSTMENTS\n",
    "    weight_decay=0.1,           # Increased from 0.01\n",
    "    \n",
    "    # ðŸ”¥ EVALUATION & EARLY STOPPING\n",
    "    eval_steps=250,             # Evaluate more frequently\n",
    "    load_best_model_at_end=True,  # Load best checkpoint\n",
    "    metric_for_best_model=\"eval_f1\",  # Use F1 score for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ðŸ”¥ IMPROVED LOGGING\n",
    "    logging_steps=50,           # More frequent logging\n",
    "    save_total_limit=3,         # Keep more checkpoints\n",
    "    \n",
    "    # Keep these settings\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "# Complete the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Ensure datasets are properly formatted\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # Use tokenizer instead of processing_class\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training with Trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22efb91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/3_/q57jjf4d3fv22x72mdkrm9fw0000gn/T/ipykernel_70331/3791621515.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 08:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.693432</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.612620</td>\n",
       "      <td>0.682094</td>\n",
       "      <td>0.705965</td>\n",
       "      <td>0.656658</td>\n",
       "      <td>0.763278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.512835</td>\n",
       "      <td>0.688923</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.400607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.513430</td>\n",
       "      <td>0.739757</td>\n",
       "      <td>0.755175</td>\n",
       "      <td>0.712938</td>\n",
       "      <td>0.802731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.438555</td>\n",
       "      <td>0.745827</td>\n",
       "      <td>0.686036</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.555387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.410214</td>\n",
       "      <td>0.748103</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.692580</td>\n",
       "      <td>0.892261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.395925</td>\n",
       "      <td>0.774659</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.920930</td>\n",
       "      <td>0.600910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.418233</td>\n",
       "      <td>0.763278</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.722151</td>\n",
       "      <td>0.855842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.406329</td>\n",
       "      <td>0.773141</td>\n",
       "      <td>0.746825</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.669196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.761760</td>\n",
       "      <td>0.764264</td>\n",
       "      <td>0.756315</td>\n",
       "      <td>0.772382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # ðŸ”¥ LEARNING RATE - Most Important Change\n",
    "    learning_rate=5e-4,  # Increased from 5e-5 - try 1e-4 to 3e-4 range\n",
    "    \n",
    "    # ðŸ”¥ MORE TRAINING\n",
    "    num_train_epochs=10,  # Increased from 3 - more learning time\n",
    "    \n",
    "    # ðŸ”¥ BATCH SIZE - Better gradient estimates\n",
    "    per_device_train_batch_size=32,  # Increased from 8\n",
    "    per_device_eval_batch_size=32,   # Increased from 8\n",
    "    \n",
    "    # ðŸ”¥ LEARNING RATE SCHEDULING\n",
    "    lr_scheduler_type=\"cosine\",  # Add learning rate decay\n",
    "    warmup_steps=500,           # Gradual warmup\n",
    "    \n",
    "    # ðŸ”¥ REGULARIZATION ADJUSTMENTS\n",
    "    weight_decay=0.1,           # Increased from 0.01\n",
    "    \n",
    "    # ðŸ”¥ EVALUATION & EARLY STOPPING\n",
    "    eval_steps=250,             # Evaluate more frequently\n",
    "    load_best_model_at_end=True,  # Load best checkpoint\n",
    "    metric_for_best_model=\"eval_f1\",  # Use F1 score for model selection\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # ðŸ”¥ IMPROVED LOGGING\n",
    "    logging_steps=50,           # More frequent logging\n",
    "    save_total_limit=3,         # Keep more checkpoints\n",
    "    \n",
    "    # Keep these settings\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    skip_memory_metrics=True,\n",
    ")\n",
    "\n",
    "# Complete the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Ensure datasets are properly formatted\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # Use tokenizer instead of processing_class\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training with Trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75d7d9",
   "metadata": {},
   "source": [
    "## Manual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c8d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: prajjwal1/bert-tiny\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec9d2bd5de548b580a1d99f90372089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1d8fa65e4b4080b8bdc909d4f220d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f29bb168cda4daabffe79c26e5904a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70479d635b3f4a8b97a51fda9a95b68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Choose ONE model and stick with it\n",
    "model_name = \"prajjwal1/bert-tiny\"  # Faster option\n",
    "# \"distilbert-base-uncased\"\n",
    "\n",
    "# STEP 2: Initialize tokenizer ONCE\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# STEP 3: Tokenize your datasets with the SAME tokenizer\n",
    "def tokenize_fn(example):\n",
    "    full_input = f\"Question:\\n{example['question']}\\n\\nSolution:\\n{example['solution']}\"\n",
    "    return tokenizer(full_input, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "train_dataset = Dataset.from_list(combined_train_final_answer)\n",
    "test_dataset = Dataset.from_list(combined_test_final_answer)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "test_dataset = test_dataset.map(tokenize_fn)\n",
    "\n",
    "label_map = {\"Correct\": 0, \"Flawed\": 1}\n",
    "train_dataset = train_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})\n",
    "test_dataset = test_dataset.map(lambda e: {\"labels\": label_map[e[\"label\"]]})\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# STEP 6: Continue with training using the SAME model and tokenizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3d0dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first 2000 training samples\n",
      "Using first 500 test samples\n",
      "Testing LR 1.00e-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eval Loss: 0.2322\n",
      "Testing LR 2.00e-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eval Loss: 0.1340\n",
      "Testing LR 3.00e-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eval Loss: 0.0862\n",
      "Testing LR 5.00e-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eval Loss: 0.0427\n",
      "ðŸ† Best LR: 5.00e-04\n",
      "\n",
      "ðŸš€ Training full dataset with LR 5.00e-04...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 467/467 [00:49<00:00,  9.34it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:02<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.6977, Accuracy: 0.5000\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 467/467 [00:43<00:00, 10.83it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:01<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.6969, Accuracy: 0.5000\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 467/467 [00:42<00:00, 10.89it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:02<00:00, 41.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.6941, Accuracy: 0.5539\n",
      "âœ… Model saved to './verifier_model_lr5e-04'\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¥ SIMPLE: Just take first 2000 training samples and first 500 test samples\n",
    "train_subset = train_dataset.select(range(2000))  # First 2000 samples\n",
    "test_subset = test_dataset.select(range(500))     # First 500 samples\n",
    "\n",
    "print(f\"Using first {len(train_subset)} training samples\")\n",
    "print(f\"Using first {len(test_subset)} test samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ðŸŽ¯ QUICK LR FINDER (much shorter)\n",
    "def find_best_lr_quick(model, train_dataloader, eval_dataloader, device):\n",
    "    learning_rates = [1e-4, 2e-4, 3e-4, 5e-4]\n",
    "    best_lr = 2e-4\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"Testing LR {lr:.2e}...\")\n",
    "        \n",
    "        # Fresh model copy\n",
    "        test_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        test_model.to(device)\n",
    "        optimizer = AdamW(test_model.parameters(), lr=lr)\n",
    "        \n",
    "        # Quick training - just 20 batches\n",
    "        test_model.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            if i >= 20: break\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = test_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Quick eval - just 10 batches\n",
    "        test_model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(eval_dataloader):\n",
    "                if i >= 10: break\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = test_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                eval_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_eval_loss = eval_loss / 10\n",
    "        print(f\"  Eval Loss: {avg_eval_loss:.4f}\")\n",
    "        \n",
    "        if avg_eval_loss < best_loss:\n",
    "            best_loss = avg_eval_loss\n",
    "            best_lr = lr\n",
    "        \n",
    "        del test_model, optimizer\n",
    "        torch.mps.empty_cache() if device.type == 'mps' else None\n",
    "    \n",
    "    print(f\"ðŸ† Best LR: {best_lr:.2e}\")\n",
    "    return best_lr\n",
    "\n",
    "# Find best LR\n",
    "best_lr = find_best_lr_quick(model, train_dataloader, eval_dataloader, device)\n",
    "\n",
    "# ðŸš€ TRAIN ON FULL DATASET WITH BEST LR\n",
    "print(f\"\\nðŸš€ Training full dataset with LR {best_lr:.2e}...\")\n",
    "\n",
    "# Create full dataset loaders\n",
    "full_train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "full_eval_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Reset model for full training\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Simple training loop\n",
    "for epoch in range(3):\n",
    "    print(f\"\\nEpoch {epoch + 1}/3\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(full_train_dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += outputs.loss.item()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(full_eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(full_train_dataloader)\n",
    "    \n",
    "    print(f\"  Train Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f\"./model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(f\"./verifier_model_lr{best_lr:.0e}\")\n",
    "tokenizer.save_pretrained(f\"./verifier_model_lr{best_lr:.0e}\")\n",
    "print(f\"âœ… Model saved to './verifier_model_lr{best_lr:.0e}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d3381",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42fb5642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–Š        | 339/1869 [13:21<1:00:17,  2.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 49\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 49\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/torch/optim/adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the pre-trained DistilBERT model for sequence classification with 2 labels (e.g., Correct and Flawed)\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Determine the device to use for training (MPS for Apple Silicon, if available, otherwise CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# Set the dataset format to PyTorch tensors for compatibility with the DataLoader\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Prepare DataLoader objects for training and evaluation datasets\n",
    "# Batch size is set to 8, and training data is shuffled\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize the AdamW optimizer with a learning rate of 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the training function for one epoch\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # Initialize total loss for the epoch\n",
    "    \n",
    "    # Iterate through batches in the DataLoader\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        # Move input data and labels to the selected device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Perform a forward pass and compute the loss\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Perform a backward pass and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Return the average loss for the epoch\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []  # List to store predictions\n",
    "    true_labels = []  # List to store true labels\n",
    "    total_loss = 0  # Initialize total loss for evaluation\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move input data and labels to the selected device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Perform a forward pass and compute the loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions by taking the argmax of the logits\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute accuracy using sklearn's accuracy_score\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Training loop for multiple epochs\n",
    "num_epochs = 3  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train the model for one epoch and compute the training loss\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the model and compute the evaluation loss and accuracy\n",
    "    eval_loss, eval_accuracy = evaluate(model, eval_dataloader, device)\n",
    "    print(f\"Evaluation loss: {eval_loss:.4f}\")\n",
    "    print(f\"Evaluation accuracy: {eval_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model checkpoint for the current epoch\n",
    "    torch.save(model.state_dict(), f\"./verifier_model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97838d",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c51983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Check current versions first\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4cd635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a lightweight and stable pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Load model for binary classification\n",
    "\n",
    "# Define training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./verifier_model\",  # Directory to save the model and checkpoints\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_dir=\"./logs\",  # Directory to save logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    report_to=\"none\",  # Disable reporting to external tools like WandB\n",
    "    # Fix for compatibility issue\n",
    "    dataloader_drop_last=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,  # Disable multiprocessing\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Add to your trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=test_dataset,  # Evaluation dataset\n",
    "    processing_class=tokenizer,  # Use this instead of tokenizer\n",
    "    compute_metrics=compute_metrics  # Function to compute metrics during evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b39d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'in_order'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model using the Trainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:2270\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2268\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2270\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2272\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/transformers/trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n\u001b[1;32m   1027\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n\u001b[0;32m-> 1029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/accelerator.py:1432\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1431\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/accelerator.py:1433\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1431\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1433\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1434\u001b[0m     )\n\u001b[1;32m   1435\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/accelerator.py:1279\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass:\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader):\n\u001b[0;32m-> 1279\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1281\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_model(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/accelerator.py:2416\u001b[0m, in \u001b[0;36mAccelerator.prepare_data_loader\u001b[0;34m(self, data_loader, device_placement, slice_fn_for_dispatch)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_placement \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2414\u001b[0m device_mesh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_device_mesh()\n\u001b[0;32m-> 2416\u001b[0m prepared_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mput_on_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdispatch_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43meven_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meven_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_fn_for_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_fn_for_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_seedable_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_seedable_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2430\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_device_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloaders\u001b[38;5;241m.\u001b[39mappend(prepared_data_loader)\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_data_loader\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/data_loader.py:1287\u001b[0m, in \u001b[0;36mprepare_data_loader\u001b[0;34m(dataloader, device, num_processes, process_index, split_batches, put_on_device, rng_types, dispatch_batches, even_batches, slice_fn_for_dispatch, use_seedable_sampler, data_seed, non_blocking, use_stateful_dataloader, torch_device_mesh)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoaderShard(\n\u001b[1;32m   1275\u001b[0m         new_dataset,\n\u001b[1;32m   1276\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m put_on_device \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoaderShard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mput_on_device\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDistributedType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXLA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_batch_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrng_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronized_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronized_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_drop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampler, SeedableRandomSampler) \u001b[38;5;129;01mand\u001b[39;00m use_seedable_sampler:\n\u001b[1;32m   1300\u001b[0m     dataloader\u001b[38;5;241m.\u001b[39mset_sampler(sampler)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/data_loader.py:548\u001b[0m, in \u001b[0;36mDataLoaderShard.__init__\u001b[0;34m(self, dataset, device, rng_types, synchronized_generator, skip_batches, use_stateful_dataloader, _drop_last, _non_blocking, torch_device_mesh, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    547\u001b[0m ):\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_stateful_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng_types \u001b[38;5;241m=\u001b[39m rng_types\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gsm8k-verifier/lib/python3.10/site-packages/accelerate/data_loader.py:433\u001b[0m, in \u001b[0;36mDataLoaderAdapter.__init__\u001b[0;34m(self, dataset, use_stateful_dataloader, batch_sampler, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataloader \u001b[38;5;241m=\u001b[39m StatefulDataLoader(dataset, batch_sampler\u001b[38;5;241m=\u001b[39mbatch_sampler, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataloader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataloader\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "\u001b[0;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'in_order'"
     ]
    }
   ],
   "source": [
    "# Train the model using the Trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4760c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4907c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsm8k-verifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
